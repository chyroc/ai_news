<?xml version="1.0" encoding="UTF-8"?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"


>
    <channel>
        <title><![CDATA[Huggingface Daily Papers]]></title>
        <link>https://huggingface.co/papers</link>
        <atom:link href="https://rsshub.app/huggingface/daily-papers" rel="self" type="application/rss+xml" />
        <description><![CDATA[Huggingface Daily Papers - Made with love by RSSHub(https://github.com/DIYgod/RSSHub)]]></description>
        <generator>RSSHub</generator>
        <webMaster>i@diygod.me (DIYgod)</webMaster>
        
        
        
        <language>zh-cn</language>
        
        <lastBuildDate>Mon, 18 Dec 2023 03:32:30 GMT</lastBuildDate>
        <ttl>120</ttl>
        
        <item>
            <title><![CDATA[Challenges with unsupervised LLM knowledge discovery]]></title>
            <description><![CDATA[We show that existing unsupervised methods on large language model (LLM) activations do not discover knowledge -- instead they seem to discover whatever feature of the activations is most prominent. The idea behind unsupervised knowledge elicitation is that knowledge satisfies a consistency structure, which can be used to discover knowledge. We first prove theoretically that arbitrary features (not just knowledge) satisfy the consistency structure of a particular leading unsupervised knowledge-elicitation method, contrast-consistent search (Burns et al. - arXiv:2212.03827). We then present a series of experiments showing settings in which unsupervised methods result in classifiers that do not predict knowledge, but instead predict a different prominent feature. We conclude that existing unsupervised methods for discovering latent knowledge are insufficient, and we contribute sanity checks to apply to evaluating future knowledge elicitation methods. Conceptually, we hypothesise that the identification issues explored here, e.g. distinguishing a model's knowledge from that of a simulated character's, will persist for future unsupervised methods.]]></description>
            <pubDate>Mon, 18 Dec 2023 03:00:55 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2312.10029</guid>
            <link>https://arxiv.org/abs/2312.10029</link>
            
            
            
            <author><![CDATA[Sebastian Farquhar, Vikrant Varma, Zachary Kenton, Johannes Gasteiger, Vladimir Mikulik, Rohin Shah]]></author>
            
                
            
            
        </item>
        
        <item>
            <title><![CDATA[Faithful Persona-based Conversational Dataset Generation with Large Language Models]]></title>
            <description><![CDATA[High-quality conversational datasets are essential for developing AI models that can communicate with users. One way to foster deeper interactions between a chatbot and its user is through personas, aspects of the user's character that provide insights into their personality, motivations, and behaviors. Training Natural Language Processing (NLP) models on a diverse and comprehensive persona-based dataset can lead to conversational models that create a deeper connection with the user, and maintain their engagement. In this paper, we leverage the power of Large Language Models (LLMs) to create a large, high-quality conversational dataset from a seed dataset. We propose a Generator-Critic architecture framework to expand the initial dataset, while improving the quality of its conversations. The Generator is an LLM prompted to output conversations. The Critic consists of a mixture of expert LLMs that control the quality of the generated conversations. These experts select the best generated conversations, which we then use to improve the Generator. We release Synthetic-Persona-Chat, consisting of 20k conversations seeded from Persona-Chat. We evaluate the quality of Synthetic-Persona-Chat and our generation framework on different dimensions through extensive experiments, and observe that the losing rate of Synthetic-Persona-Chat against Persona-Chat during Turing test decreases from 17.2% to 8.8% over three iterations.]]></description>
            <pubDate>Mon, 18 Dec 2023 02:59:30 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2312.10007</guid>
            <link>https://arxiv.org/abs/2312.10007</link>
            
            
            
            <author><![CDATA[Pegah Jandaghi, XiangHai Sheng, Xinyi Bai, Jay Pujara, Hakim Sidahmed]]></author>
            
                
            
            
        </item>
        
        <item>
            <title><![CDATA[ReST meets ReAct: Self-Improvement for Multi-Step Reasoning LLM Agent]]></title>
            <description><![CDATA[Answering complex natural language questions often necessitates multi-step reasoning and integrating external information. Several systems have combined knowledge retrieval with a large language model (LLM) to answer such questions. These systems, however, suffer from various failure cases, and we cannot directly train them end-to-end to fix such failures, as interaction with external knowledge is non-differentiable. To address these deficiencies, we define a ReAct-style LLM agent with the ability to reason and act upon external knowledge. We further refine the agent through a ReST-like method that iteratively trains on previous trajectories, employing growing-batch reinforcement learning with AI feedback for continuous self-improvement and self-distillation. Starting from a prompted large model and after just two iterations of the algorithm, we can produce a fine-tuned small model that achieves comparable performance on challenging compositional question-answering benchmarks with two orders of magnitude fewer parameters.]]></description>
            <pubDate>Mon, 18 Dec 2023 02:56:42 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2312.10003</guid>
            <link>https://arxiv.org/abs/2312.10003</link>
            
            
            
            <author><![CDATA[Renat Aksitov, Sobhan Miryoosefi, Zonglin Li, Daliang Li, Sheila Babayan, Kavya Kopparapu, Zachary Fisher, Ruiqi Guo, Sushant Prakash, Pranesh Srinivasan, Manzil Zaheer, Felix Yu, Sanjiv Kumar]]></author>
            
                
            
            
        </item>
        
        <item>
            <title><![CDATA[Self-Evaluation Improves Selective Generation in Large Language Models]]></title>
            <description><![CDATA[Safe deployment of large language models (LLMs) may benefit from a reliable method for assessing their generated content to determine when to abstain or to selectively generate. While likelihood-based metrics such as perplexity are widely employed, recent research has demonstrated the limitations of using sequence-level probability estimates given by LLMs as reliable indicators of generation quality. Conversely, LLMs have demonstrated strong calibration at the token level, particularly when it comes to choosing correct answers in multiple-choice questions or evaluating true/false statements. In this work, we reformulate open-ended generation tasks into token-level prediction tasks, and leverage LLMs' superior calibration at the token level. We instruct an LLM to self-evaluate its answers, employing either a multi-way comparison or a point-wise evaluation approach, with the option to include a ``None of the above'' option to express the model's uncertainty explicitly. We benchmark a range of scoring methods based on self-evaluation and evaluate their performance in selective generation using TruthfulQA and TL;DR. Through experiments with PaLM-2 and GPT-3, we demonstrate that self-evaluation based scores not only improve accuracy, but also correlate better with the overall quality of generated content.]]></description>
            <pubDate>Mon, 18 Dec 2023 02:03:12 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2312.09300</guid>
            <link>https://arxiv.org/abs/2312.09300</link>
            
            
            
            <author><![CDATA[Jie Ren, Yao Zhao, Tu Vu, Peter J. Liu, Balaji Lakshminarayanan]]></author>
            
                
            
            
        </item>
        
        <item>
            <title><![CDATA[Weight subcloning: direct initialization of transformers using larger pretrained ones]]></title>
            <description><![CDATA[Training large transformer models from scratch for a target task requires lots of data and is computationally demanding. The usual practice of transfer learning overcomes this challenge by initializing the model with weights of a pretrained model of the same size and specification to increase the convergence and training speed. However, what if no pretrained model of the required size is available? In this paper, we introduce a simple yet effective technique to transfer the knowledge of a pretrained model to smaller variants. Our approach called weight subcloning expedites the training of scaled-down transformers by initializing their weights from larger pretrained models.   Weight subcloning involves an operation on the pretrained model to obtain the equivalent initialized scaled-down model. It consists of two key steps: first, we introduce neuron importance ranking to decrease the embedding dimension per layer in the pretrained model. Then, we remove blocks from the transformer model to match the number of layers in the scaled-down network. The result is a network ready to undergo training, which gains significant improvements in training speed compared to random initialization. For instance, we achieve 4x faster training for vision transformers in image classification and language models designed for next token prediction.]]></description>
            <pubDate>Mon, 18 Dec 2023 01:56:31 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2312.09299</guid>
            <link>https://arxiv.org/abs/2312.09299</link>
            
            
            
            <author><![CDATA[Mohammad Samragh, Mehrdad Farajtabar, Sachin Mehta, Raviteja Vemulapalli, Fartash Faghri, Devang Naik, Oncel Tuzel, Mohammad Rastegari]]></author>
            
                
            
            
        </item>
        
        <item>
            <title><![CDATA[Weak-to-Strong Generalization: Eliciting Strong Capabilities With Weak Supervision]]></title>
            <description><![CDATA[Widely used alignment techniques, such as reinforcement learning from human feedback (RLHF), rely on the ability of humans to supervise model behavior - for example, to evaluate whether a model faithfully followed instructions or generated safe outputs. However, future superhuman models will behave in complex ways too difficult for humans to reliably evaluate; humans will only be able to weakly supervise superhuman models. We study an analogy to this problem: can weak model supervision elicit the full capabilities of a much stronger model? We test this using a range of pretrained language models in the GPT-4 family on natural language processing (NLP), chess, and reward modeling tasks. We find that when we naively finetune strong pretrained models on labels generated by a weak model, they consistently perform better than their weak supervisors, a phenomenon we call weak-to-strong generalization. However, we are still far from recovering the full capabilities of strong models with naive finetuning alone, suggesting that techniques like RLHF may scale poorly to superhuman models without further work. We find that simple methods can often significantly improve weak-to-strong generalization: for example, when finetuning GPT-4 with a GPT-2-level supervisor and an auxiliary confidence loss, we can recover close to GPT-3.5-level performance on NLP tasks. Our results suggest that it is feasible to make empirical progress today on a fundamental challenge of aligning superhuman models.]]></description>
            <pubDate>Mon, 18 Dec 2023 01:52:55 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2312.09390</guid>
            <link>https://arxiv.org/abs/2312.09390</link>
            
            
            
            <author><![CDATA[Collin Burns, Pavel Izmailov, Jan Hendrik Kirchner, Bowen Baker, Leo Gao, Leopold Aschenbrenner, Yining Chen, Adrien Ecoffet, Manas Joglekar, Jan Leike, Ilya Sutskever, Jeff Wu]]></author>
            
                
            
            
        </item>
        
    </channel>
</rss>
