<?xml version="1.0" encoding="UTF-8"?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"


>
    <channel>
        <title><![CDATA[Huggingface Daily Papers]]></title>
        <link>https://huggingface.co/papers</link>
        <atom:link href="https://rsshub.app/huggingface/daily-papers" rel="self" type="application/rss+xml" />
        <description><![CDATA[Huggingface Daily Papers - Made with love by RSSHub(https://github.com/DIYgod/RSSHub)]]></description>
        <generator>RSSHub</generator>
        <webMaster>i@diygod.me (DIYgod)</webMaster>
        
        
        
        <language>zh-cn</language>
        
        <lastBuildDate>Wed, 03 Jan 2024 02:10:28 GMT</lastBuildDate>
        <ttl>120</ttl>
        
        <item>
            <title><![CDATA[VideoDrafter: Content-Consistent Multi-Scene Video Generation with LLM]]></title>
            <description><![CDATA[The recent innovations and breakthroughs in diffusion models have significantly expanded the possibilities of generating high-quality videos for the given prompts. Most existing works tackle the single-scene scenario with only one video event occurring in a single background. Extending to generate multi-scene videos nevertheless is not trivial and necessitates to nicely manage the logic in between while preserving the consistent visual appearance of key content across video scenes. In this paper, we propose a novel framework, namely VideoDrafter, for content-consistent multi-scene video generation. Technically, VideoDrafter leverages Large Language Models (LLM) to convert the input prompt into comprehensive multi-scene script that benefits from the logical knowledge learnt by LLM. The script for each scene includes a prompt describing the event, the foreground/background entities, as well as camera movement. VideoDrafter identifies the common entities throughout the script and asks LLM to detail each entity. The resultant entity description is then fed into a text-to-image model to generate a reference image for each entity. Finally, VideoDrafter outputs a multi-scene video by generating each scene video via a diffusion process that takes the reference images, the descriptive prompt of the event and camera movement into account. The diffusion model incorporates the reference images as the condition and alignment to strengthen the content consistency of multi-scene videos. Extensive experiments demonstrate that VideoDrafter outperforms the SOTA video generation models in terms of visual quality, content consistency, and user preference.]]></description>
            <pubDate>Wed, 03 Jan 2024 01:59:08 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2401.01256</guid>
            <link>https://arxiv.org/abs/2401.01256</link>
            
            
            
            <author><![CDATA[Fuchen Long, Zhaofan Qiu, Ting Yao, Tao Mei]]></author>
            
                
            
            
        </item>
        
        <item>
            <title><![CDATA[TrailBlazer: Trajectory Control for Diffusion-Based Video Generation]]></title>
            <description><![CDATA[Within recent approaches to text-to-video (T2V) generation, achieving controllability in the synthesized video is often a challenge. Typically, this issue is addressed by providing low-level per-frame guidance in the form of edge maps, depth maps, or an existing video to be altered. However, the process of obtaining such guidance can be labor-intensive. This paper focuses on enhancing controllability in video synthesis by employing straightforward bounding boxes to guide the subject in various ways, all without the need for neural network training, finetuning, optimization at inference time, or the use of pre-existing videos. Our algorithm, TrailBlazer, is constructed upon a pre-trained (T2V) model, and easy to implement. The subject is directed by a bounding box through the proposed spatial and temporal attention map editing. Moreover, we introduce the concept of keyframing, allowing the subject trajectory and overall appearance to be guided by both a moving bounding box and corresponding prompts, without the need to provide a detailed mask. The method is efficient, with negligible additional computation relative to the underlying pre-trained model. Despite the simplicity of the bounding box guidance, the resulting motion is surprisingly natural, with emergent effects including perspective and movement toward the virtual camera as the box size increases.]]></description>
            <pubDate>Wed, 03 Jan 2024 01:43:06 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2401.00896</guid>
            <link>https://arxiv.org/abs/2401.00896</link>
            
            
            
            <author><![CDATA[Wan-Duo Kurt Ma, J. P. Lewis, W. Bastiaan Kleijn]]></author>
            
                
            
            
        </item>
        
    </channel>
</rss>
