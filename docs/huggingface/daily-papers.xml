<?xml version="1.0" encoding="UTF-8"?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"


>
    <channel>
        <title><![CDATA[Huggingface Daily Papers]]></title>
        <link>https://huggingface.co/papers</link>
        <atom:link href="https://rsshub.app/huggingface/daily-papers" rel="self" type="application/rss+xml" />
        <description><![CDATA[Huggingface Daily Papers - Made with love by RSSHub(https://github.com/DIYgod/RSSHub)]]></description>
        <generator>RSSHub</generator>
        <webMaster>i@diygod.me (DIYgod)</webMaster>
        
        
        
        <language>zh-cn</language>
        
        <lastBuildDate>Thu, 07 Dec 2023 05:26:12 GMT</lastBuildDate>
        <ttl>120</ttl>
        
        <item>
            <title><![CDATA[DreamComposer: Controllable 3D Object Generation via Multi-View Conditions]]></title>
            <description><![CDATA[Utilizing pre-trained 2D large-scale generative models, recent works are capable of generating high-quality novel views from a single in-the-wild image. However, due to the lack of information from multiple views, these works encounter difficulties in generating controllable novel views. In this paper, we present DreamComposer, a flexible and scalable framework that can enhance existing view-aware diffusion models by injecting multi-view conditions. Specifically, DreamComposer first uses a view-aware 3D lifting module to obtain 3D representations of an object from multiple views. Then, it renders the latent features of the target view from 3D representations with the multi-view feature fusion module. Finally the target view features extracted from multi-view inputs are injected into a pre-trained diffusion model. Experiments show that DreamComposer is compatible with state-of-the-art diffusion models for zero-shot novel view synthesis, further enhancing them to generate high-fidelity novel view images with multi-view conditions, ready for controllable 3D object reconstruction and various other applications.]]></description>
            <pubDate>Thu, 07 Dec 2023 03:49:34 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2312.03611</guid>
            <link>https://arxiv.org/abs/2312.03611</link>
            
            
            
            <author><![CDATA[Yunhan Yang, Yukun Huang, Xiaoyang Wu, Yuan-Chen Guo, Song-Hai Zhang, Hengshuang Zhao, Tong He, Xihui Liu]]></author>
            
                
            
            
        </item>
        
        <item>
            <title><![CDATA[Schrodinger Bridges Beat Diffusion Models on Text-to-Speech Synthesis]]></title>
            <description><![CDATA[In text-to-speech (TTS) synthesis, diffusion models have achieved promising generation quality. However, because of the pre-defined data-to-noise diffusion process, their prior distribution is restricted to a noisy representation, which provides little information of the generation target. In this work, we present a novel TTS system, Bridge-TTS, making the first attempt to substitute the noisy Gaussian prior in established diffusion-based TTS methods with a clean and deterministic one, which provides strong structural information of the target. Specifically, we leverage the latent representation obtained from text input as our prior, and build a fully tractable Schrodinger bridge between it and the ground-truth mel-spectrogram, leading to a data-to-data process. Moreover, the tractability and flexibility of our formulation allow us to empirically study the design spaces such as noise schedules, as well as to develop stochastic and deterministic samplers. Experimental results on the LJ-Speech dataset illustrate the effectiveness of our method in terms of both synthesis quality and sampling efficiency, significantly outperforming our diffusion counterpart Grad-TTS in 50-step/1000-step synthesis and strong fast TTS models in few-step scenarios. Project page: https://bridge-tts.github.io/]]></description>
            <pubDate>Thu, 07 Dec 2023 03:43:55 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2312.03491</guid>
            <link>https://arxiv.org/abs/2312.03491</link>
            
            
            
            <author><![CDATA[Zehua Chen, Guande He, Kaiwen Zheng, Xu Tan, Jun Zhu]]></author>
            
                
            
            
        </item>
        
        <item>
            <title><![CDATA[Gaussian Head Avatar: Ultra High-fidelity Head Avatar via Dynamic Gaussians]]></title>
            <description><![CDATA[Creating high-fidelity 3D head avatars has always been a research hotspot, but there remains a great challenge under lightweight sparse view setups. In this paper, we propose Gaussian Head Avatar represented by controllable 3D Gaussians for high-fidelity head avatar modeling. We optimize the neutral 3D Gaussians and a fully learned MLP-based deformation field to capture complex expressions. The two parts benefit each other, thereby our method can model fine-grained dynamic details while ensuring expression accuracy. Furthermore, we devise a well-designed geometry-guided initialization strategy based on implicit SDF and Deep Marching Tetrahedra for the stability and convergence of the training procedure. Experiments show our approach outperforms other state-of-the-art sparse-view methods, achieving ultra high-fidelity rendering quality at 2K resolution even under exaggerated expressions.]]></description>
            <pubDate>Thu, 07 Dec 2023 03:35:44 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2312.03029</guid>
            <link>https://arxiv.org/abs/2312.03029</link>
            
            
            
            <author><![CDATA[Yuelang Xu, Benwang Chen, Zhe Li, Hongwen Zhang, Lizhen Wang, Zerong Zheng, Yebin Liu]]></author>
            
                
            
            
        </item>
        
        <item>
            <title><![CDATA[HiFi4G: High-Fidelity Human Performance Rendering via Compact Gaussian Splatting]]></title>
            <description><![CDATA[We have recently seen tremendous progress in photo-real human modeling and rendering. Yet, efficiently rendering realistic human performance and integrating it into the rasterization pipeline remains challenging. In this paper, we present HiFi4G, an explicit and compact Gaussian-based approach for high-fidelity human performance rendering from dense footage. Our core intuition is to marry the 3D Gaussian representation with non-rigid tracking, achieving a compact and compression-friendly representation. We first propose a dual-graph mechanism to obtain motion priors, with a coarse deformation graph for effective initialization and a fine-grained Gaussian graph to enforce subsequent constraints. Then, we utilize a 4D Gaussian optimization scheme with adaptive spatial-temporal regularizers to effectively balance the non-rigid prior and Gaussian updating. We also present a companion compression scheme with residual compensation for immersive experiences on various platforms. It achieves a substantial compression rate of approximately 25 times, with less than 2MB of storage per frame. Extensive experiments demonstrate the effectiveness of our approach, which significantly outperforms existing approaches in terms of optimization speed, rendering quality, and storage overhead.]]></description>
            <pubDate>Thu, 07 Dec 2023 03:28:35 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2312.03461</guid>
            <link>https://arxiv.org/abs/2312.03461</link>
            
            
            
            <author><![CDATA[Yuheng Jiang, Zhehao Shen, Penghao Wang, Zhuo Su, Yu Hong, Yingliang Zhang, Jingyi Yu, Lan Xu]]></author>
            
                
            
            
        </item>
        
        <item>
            <title><![CDATA[HybridNeRF: Efficient Neural Rendering via Adaptive Volumetric Surfaces]]></title>
            <description><![CDATA[Neural radiance fields provide state-of-the-art view synthesis quality but tend to be slow to render. One reason is that they make use of volume rendering, thus requiring many samples (and model queries) per ray at render time. Although this representation is flexible and easy to optimize, most real-world objects can be modeled more efficiently with surfaces instead of volumes, requiring far fewer samples per ray. This observation has spurred considerable progress in surface representations such as signed distance functions, but these may struggle to model semi-opaque and thin structures. We propose a method, HybridNeRF, that leverages the strengths of both representations by rendering most objects as surfaces while modeling the (typically) small fraction of challenging regions volumetrically. We evaluate HybridNeRF against the challenging Eyeful Tower dataset along with other commonly used view synthesis datasets. When comparing to state-of-the-art baselines, including recent rasterization-based approaches, we improve error rates by 15-30% while achieving real-time framerates (at least 36 FPS) for virtual-reality resolutions (2Kx2K).]]></description>
            <pubDate>Thu, 07 Dec 2023 03:15:14 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2312.03160</guid>
            <link>https://arxiv.org/abs/2312.03160</link>
            
            
            
            <author><![CDATA[Haithem Turki, Vasu Agrawal, Samuel Rota Bulò, Lorenzo Porzi, Peter Kontschieder, Deva Ramanan, Michael Zollhöfer, Christian Richardt]]></author>
            
                
            
            
        </item>
        
        <item>
            <title><![CDATA[MotionCtrl: A Unified and Flexible Motion Controller for Video Generation]]></title>
            <description><![CDATA[Motions in a video primarily consist of camera motion, induced by camera movement, and object motion, resulting from object movement. Accurate control of both camera and object motion is essential for video generation. However, existing works either mainly focus on one type of motion or do not clearly distinguish between the two, limiting their control capabilities and diversity. Therefore, this paper presents MotionCtrl, a unified and flexible motion controller for video generation designed to effectively and independently control camera and object motion. The architecture and training strategy of MotionCtrl are carefully devised, taking into account the inherent properties of camera motion, object motion, and imperfect training data. Compared to previous methods, MotionCtrl offers three main advantages: 1) It effectively and independently controls camera motion and object motion, enabling more fine-grained motion control and facilitating flexible and diverse combinations of both types of motion. 2) Its motion conditions are determined by camera poses and trajectories, which are appearance-free and minimally impact the appearance or shape of objects in generated videos. 3) It is a relatively generalizable model that can adapt to a wide array of camera poses and trajectories once trained. Extensive qualitative and quantitative experiments have been conducted to demonstrate the superiority of MotionCtrl over existing methods.]]></description>
            <pubDate>Thu, 07 Dec 2023 03:06:23 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2312.03641</guid>
            <link>https://arxiv.org/abs/2312.03641</link>
            
            
            
            <author><![CDATA[Zhouxia Wang, Ziyang Yuan, Xintao Wang, Tianshui Chen, Menghan Xia, Ping Luo, Ying Shan]]></author>
            
                
            
            
        </item>
        
        <item>
            <title><![CDATA[MagicStick: Controllable Video Editing via Control Handle Transformations]]></title>
            <description><![CDATA[Text-based video editing has recently attracted considerable interest in changing the style or replacing the objects with a similar structure. Beyond this, we demonstrate that properties such as shape, size, location, motion, etc., can also be edited in videos. Our key insight is that the keyframe transformations of the specific internal feature (e.g., edge maps of objects or human pose), can easily propagate to other frames to provide generation guidance. We thus propose MagicStick, a controllable video editing method that edits the video properties by utilizing the transformation on the extracted internal control signals. In detail, to keep the appearance, we inflate both the pretrained image diffusion model and ControlNet to the temporal dimension and train low-rank adaptions (LORA) layers to fit the specific scenes. Then, in editing, we perform an inversion and editing framework. Differently, finetuned ControlNet is introduced in both inversion and generation for attention guidance with the proposed attention remix between the spatial attention maps of inversion and editing. Yet succinct, our method is the first method to show the ability of video property editing from the pre-trained text-to-image model. We present experiments on numerous examples within our unified framework. We also compare with shape-aware text-based editing and handcrafted motion video generation, demonstrating our superior temporal consistency and editing capability than previous works. The code and models will be made publicly available.]]></description>
            <pubDate>Thu, 07 Dec 2023 02:57:15 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2312.03047</guid>
            <link>https://arxiv.org/abs/2312.03047</link>
            
            
            
            <author><![CDATA[Yue Ma, Xiaodong Cun, Yingqing He, Chenyang Qi, Xintao Wang, Ying Shan, Xiu Li, Qifeng Chen]]></author>
            
                
            
            
        </item>
        
        <item>
            <title><![CDATA[Self-conditioned Image Generation via Generating Representations]]></title>
            <description><![CDATA[This paper presents Representation-Conditioned image Generation (RCG), a simple yet effective image generation framework which sets a new benchmark in class-unconditional image generation. RCG does not condition on any human annotations. Instead, it conditions on a self-supervised representation distribution which is mapped from the image distribution using a pre-trained encoder. During generation, RCG samples from such representation distribution using a representation diffusion model (RDM), and employs a pixel generator to craft image pixels conditioned on the sampled representation. Such a design provides substantial guidance during the generative process, resulting in high-quality image generation. Tested on ImageNet 256times256, RCG achieves a Frechet Inception Distance (FID) of 3.31 and an Inception Score (IS) of 253.4. These results not only significantly improve the state-of-the-art of class-unconditional image generation but also rival the current leading methods in class-conditional image generation, bridging the long-standing performance gap between these two tasks. Code is available at https://github.com/LTH14/rcg.]]></description>
            <pubDate>Thu, 07 Dec 2023 02:48:06 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2312.03701</guid>
            <link>https://arxiv.org/abs/2312.03701</link>
            
            
            
            <author><![CDATA[Tianhong Li, Dina Katabi, Kaiming He]]></author>
            
                
            
            
        </item>
        
        <item>
            <title><![CDATA[Context Diffusion: In-Context Aware Image Generation]]></title>
            <description><![CDATA[We propose Context Diffusion, a diffusion-based framework that enables image generation models to learn from visual examples presented in context. Recent work tackles such in-context learning for image generation, where a query image is provided alongside context examples and text prompts. However, the quality and fidelity of the generated images deteriorate when the prompt is not present, demonstrating that these models are unable to truly learn from the visual context. To address this, we propose a novel framework that separates the encoding of the visual context and preserving the structure of the query images. This results in the ability to learn from the visual context and text prompts, but also from either one of them. Furthermore, we enable our model to handle few-shot settings, to effectively address diverse in-context learning scenarios. Our experiments and user study demonstrate that Context Diffusion excels in both in-domain and out-of-domain tasks, resulting in an overall enhancement in image quality and fidelity compared to counterpart models.]]></description>
            <pubDate>Thu, 07 Dec 2023 02:45:23 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2312.03584</guid>
            <link>https://arxiv.org/abs/2312.03584</link>
            
            
            
            <author><![CDATA[Ivona Najdenkoska, Animesh Sinha, Abhimanyu Dubey, Dhruv Mahajan, Vignesh Ramanathan, Filip Radenovic]]></author>
            
                
            
            
        </item>
        
        <item>
            <title><![CDATA[Kandinsky 3.0 Technical Report]]></title>
            <description><![CDATA[We present Kandinsky 3.0, a large-scale text-to-image generation model based on latent diffusion, continuing the series of text-to-image Kandinsky models and reflecting our progress to achieve higher quality and realism of image generation. Compared to previous versions of Kandinsky 2.x, Kandinsky 3.0 leverages a two times larger U-Net backbone, a ten times larger text encoder and removes diffusion mapping. We describe the architecture of the model, the data collection procedure, the training technique, and the production system of user interaction. We focus on the key components that, as we have identified as a result of a large number of experiments, had the most significant impact on improving the quality of our model compared to the others. By our side-by-side comparisons, Kandinsky becomes better in text understanding and works better on specific domains. Project page: https://ai-forever.github.io/Kandinsky-3]]></description>
            <pubDate>Thu, 07 Dec 2023 02:39:01 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2312.03511</guid>
            <link>https://arxiv.org/abs/2312.03511</link>
            
            
            
            <author><![CDATA[Vladimir Arkhipkin, Andrei Filatov, Viacheslav Vasilev, Anastasia Maltseva, Said Azizov, Igor Pavlov, Julia Agafonova, Andrey Kuznetsov, Denis Dimitrov]]></author>
            
                
            
            
        </item>
        
        <item>
            <title><![CDATA[Cache Me if You Can: Accelerating Diffusion Models through Block Caching]]></title>
            <description><![CDATA[Diffusion models have recently revolutionized the field of image synthesis due to their ability to generate photorealistic images. However, one of the major drawbacks of diffusion models is that the image generation process is costly. A large image-to-image network has to be applied many times to iteratively refine an image from random noise. While many recent works propose techniques to reduce the number of required steps, they generally treat the underlying denoising network as a black box. In this work, we investigate the behavior of the layers within the network and find that 1) the layers' output changes smoothly over time, 2) the layers show distinct patterns of change, and 3) the change from step to step is often very small. We hypothesize that many layer computations in the denoising network are redundant. Leveraging this, we introduce block caching, in which we reuse outputs from layer blocks of previous steps to speed up inference. Furthermore, we propose a technique to automatically determine caching schedules based on each block's changes over timesteps. In our experiments, we show through FID, human evaluation and qualitative analysis that Block Caching allows to generate images with higher visual quality at the same computational cost. We demonstrate this for different state-of-the-art models (LDM and EMU) and solvers (DDIM and DPM).]]></description>
            <pubDate>Thu, 07 Dec 2023 02:27:20 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2312.03209</guid>
            <link>https://arxiv.org/abs/2312.03209</link>
            
            
            
            <author><![CDATA[Felix Wimbauer, Bichen Wu, Edgar Schoenfeld, Xiaoliang Dai, Ji Hou, Zijian He, Artsiom Sanakoyeu, Peizhao Zhang, Sam Tsai, Jonas Kohler, Christian Rupprecht, Daniel Cremers, Peter Vajda, Jialiang Wang]]></author>
            
                
            
            
        </item>
        
        <item>
            <title><![CDATA[LooseControl: Lifting ControlNet for Generalized Depth Conditioning]]></title>
            <description><![CDATA[We present LooseControl to allow generalized depth conditioning for diffusion-based image generation. ControlNet, the SOTA for depth-conditioned image generation, produces remarkable results but relies on having access to detailed depth maps for guidance. Creating such exact depth maps, in many scenarios, is challenging. This paper introduces a generalized version of depth conditioning that enables many new content-creation workflows. Specifically, we allow (C1) scene boundary control for loosely specifying scenes with only boundary conditions, and (C2) 3D box control for specifying layout locations of the target objects rather than the exact shape and appearance of the objects. Using LooseControl, along with text guidance, users can create complex environments (e.g., rooms, street views, etc.) by specifying only scene boundaries and locations of primary objects. Further, we provide two editing mechanisms to refine the results: (E1) 3D box editing enables the user to refine images by changing, adding, or removing boxes while freezing the style of the image. This yields minimal changes apart from changes induced by the edited boxes. (E2) Attribute editing proposes possible editing directions to change one particular aspect of the scene, such as the overall object density or a particular object. Extensive tests and comparisons with baselines demonstrate the generality of our method. We believe that LooseControl can become an important design tool for easily creating complex environments and be extended to other forms of guidance channels. Code and more information are available at https://shariqfarooq123.github.io/loose-control/ .]]></description>
            <pubDate>Thu, 07 Dec 2023 02:16:28 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2312.03079</guid>
            <link>https://arxiv.org/abs/2312.03079</link>
            
            
            
            <author><![CDATA[Shariq Farooq Bhat, Niloy J. Mitra, Peter Wonka]]></author>
            
                
            
            
        </item>
        
        <item>
            <title><![CDATA[Relightable Gaussian Codec Avatars]]></title>
            <description><![CDATA[The fidelity of relighting is bounded by both geometry and appearance representations. For geometry, both mesh and volumetric approaches have difficulty modeling intricate structures like 3D hair geometry. For appearance, existing relighting models are limited in fidelity and often too slow to render in real-time with high-resolution continuous environments. In this work, we present Relightable Gaussian Codec Avatars, a method to build high-fidelity relightable head avatars that can be animated to generate novel expressions. Our geometry model based on 3D Gaussians can capture 3D-consistent sub-millimeter details such as hair strands and pores on dynamic face sequences. To support diverse materials of human heads such as the eyes, skin, and hair in a unified manner, we present a novel relightable appearance model based on learnable radiance transfer. Together with global illumination-aware spherical harmonics for the diffuse components, we achieve real-time relighting with spatially all-frequency reflections using spherical Gaussians. This appearance model can be efficiently relit under both point light and continuous illumination. We further improve the fidelity of eye reflections and enable explicit gaze control by introducing relightable explicit eye models. Our method outperforms existing approaches without compromising real-time performance. We also demonstrate real-time relighting of avatars on a tethered consumer VR headset, showcasing the efficiency and fidelity of our avatars.]]></description>
            <pubDate>Thu, 07 Dec 2023 02:09:29 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2312.03704</guid>
            <link>https://arxiv.org/abs/2312.03704</link>
            
            
            
            <author><![CDATA[Shunsuke Saito, Gabriel Schwartz, Tomas Simon, Junxuan Li, Giljoo Nam]]></author>
            
                
            
            
        </item>
        
        <item>
            <title><![CDATA[OneLLM: One Framework to Align All Modalities with Language]]></title>
            <description><![CDATA[Multimodal large language models (MLLMs) have gained significant attention due to their strong multimodal understanding capability. However, existing works rely heavily on modality-specific encoders, which usually differ in architecture and are limited to common modalities. In this paper, we present OneLLM, an MLLM that aligns eight modalities to language using a unified framework. We achieve this through a unified multimodal encoder and a progressive multimodal alignment pipeline. In detail, we first train an image projection module to connect a vision encoder with LLM. Then, we build a universal projection module (UPM) by mixing multiple image projection modules and dynamic routing. Finally, we progressively align more modalities to LLM with the UPM. To fully leverage the potential of OneLLM in following instructions, we also curated a comprehensive multimodal instruction dataset, including 2M items from image, audio, video, point cloud, depth/normal map, IMU and fMRI brain activity. OneLLM is evaluated on 25 diverse benchmarks, encompassing tasks such as multimodal captioning, question answering and reasoning, where it delivers excellent performance. Code, data, model and online demo are available at https://github.com/csuhan/OneLLM]]></description>
            <pubDate>Thu, 07 Dec 2023 02:05:37 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2312.03700</guid>
            <link>https://arxiv.org/abs/2312.03700</link>
            
            
            
            <author><![CDATA[Jiaming Han, Kaixiong Gong, Yiyuan Zhang, Jiaqi Wang, Kaipeng Zhang, Dahua Lin, Yu Qiao, Peng Gao, Xiangyu Yue]]></author>
            
                
            
            
        </item>
        
        <item>
            <title><![CDATA[Generative agent-based modeling with actions grounded in physical, social, or digital space using Concordia]]></title>
            <description><![CDATA[Agent-based modeling has been around for decades, and applied widely across the social and natural sciences. The scope of this research method is now poised to grow dramatically as it absorbs the new affordances provided by Large Language Models (LLM)s. Generative Agent-Based Models (GABM) are not just classic Agent-Based Models (ABM)s where the agents talk to one another. Rather, GABMs are constructed using an LLM to apply common sense to situations, act "reasonably", recall common semantic knowledge, produce API calls to control digital technologies like apps, and communicate both within the simulation and to researchers viewing it from the outside. Here we present Concordia, a library to facilitate constructing and working with GABMs. Concordia makes it easy to construct language-mediated simulations of physically- or digitally-grounded environments. Concordia agents produce their behavior using a flexible component system which mediates between two fundamental operations: LLM calls and associative memory retrieval. A special agent called the Game Master (GM), which was inspired by tabletop role-playing games, is responsible for simulating the environment where the agents interact. Agents take actions by describing what they want to do in natural language. The GM then translates their actions into appropriate implementations. In a simulated physical world, the GM checks the physical plausibility of agent actions and describes their effects. In digital environments simulating technologies such as apps and services, the GM may handle API calls to integrate with external tools such as general AI assistants (e.g., Bard, ChatGPT), and digital apps (e.g., Calendar, Email, Search, etc.). Concordia was designed to support a wide array of applications both in scientific research and for evaluating performance of real digital services by simulating users and/or generating synthetic data.]]></description>
            <pubDate>Thu, 07 Dec 2023 02:02:35 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2312.03664</guid>
            <link>https://arxiv.org/abs/2312.03664</link>
            
            
            
            <author><![CDATA[Alexander Sasha Vezhnevets, John P. Agapiou, Avia Aharon, Ron Ziv, Jayd Matyas, Edgar A. Duéñez-Guzmán, William A. Cunningham, Simon Osindero, Danny Karmon, Joel Z. Leibo]]></author>
            
                
            
            
        </item>
        
        <item>
            <title><![CDATA[Multimodal Data and Resource Efficient Device-Directed Speech Detection with Large Foundation Models]]></title>
            <description><![CDATA[Interactions with virtual assistants typically start with a trigger phrase followed by a command. In this work, we explore the possibility of making these interactions more natural by eliminating the need for a trigger phrase. Our goal is to determine whether a user addressed the virtual assistant based on signals obtained from the streaming audio recorded by the device microphone. We address this task by combining 1-best hypotheses and decoder signals from an automatic speech recognition system with acoustic representations from an audio encoder as input features to a large language model (LLM). In particular, we are interested in data and resource efficient systems that require only a small amount of training data and can operate in scenarios with only a single frozen LLM available on a device. For this reason, our model is trained on 80k or less examples of multimodal data using a combination of low-rank adaptation and prefix tuning. We compare the proposed system to unimodal baselines and show that the multimodal approach achieves lower equal-error-rates (EERs), while using only a fraction of the training data. We also show that low-dimensional specialized audio representations lead to lower EERs than high-dimensional general audio representations.]]></description>
            <pubDate>Thu, 07 Dec 2023 02:01:04 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2312.03632</guid>
            <link>https://arxiv.org/abs/2312.03632</link>
            
            
            
            <author><![CDATA[Dominik Wagner, Alexander Churchill, Siddharth Sigtia, Panayiotis Georgiou, Matt Mirsamadi, Aarshee Mishra, Erik Marchi]]></author>
            
                
            
            
        </item>
        
        <item>
            <title><![CDATA[Language-Informed Visual Concept Learning]]></title>
            <description><![CDATA[Our understanding of the visual world is centered around various concept axes, characterizing different aspects of visual entities. While different concept axes can be easily specified by language, e.g. color, the exact visual nuances along each axis often exceed the limitations of linguistic articulations, e.g. a particular style of painting. In this work, our goal is to learn a language-informed visual concept representation, by simply distilling large pre-trained vision-language models. Specifically, we train a set of concept encoders to encode the information pertinent to a set of language-informed concept axes, with an objective of reproducing the input image through a pre-trained Text-to-Image (T2I) model. To encourage better disentanglement of different concept encoders, we anchor the concept embeddings to a set of text embeddings obtained from a pre-trained Visual Question Answering (VQA) model. At inference time, the model extracts concept embeddings along various axes from new test images, which can be remixed to generate images with novel compositions of visual concepts. With a lightweight test-time finetuning procedure, it can also generalize to novel concepts unseen at training.]]></description>
            <pubDate>Thu, 07 Dec 2023 01:58:47 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2312.03587</guid>
            <link>https://arxiv.org/abs/2312.03587</link>
            
            
            
            <author><![CDATA[Sharon Lee, Yunzhi Zhang, Shangzhe Wu, Jiajun Wu]]></author>
            
                
            
            
        </item>
        
    </channel>
</rss>
