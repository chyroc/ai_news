<?xml version="1.0" encoding="UTF-8"?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"


>
    <channel>
        <title><![CDATA[Huggingface Daily Papers]]></title>
        <link>https://huggingface.co/papers</link>
        <atom:link href="https://rsshub.app/huggingface/daily-papers" rel="self" type="application/rss+xml" />
        <description><![CDATA[Huggingface Daily Papers - Made with love by RSSHub(https://github.com/DIYgod/RSSHub)]]></description>
        <generator>RSSHub</generator>
        <webMaster>i@diygod.me (DIYgod)</webMaster>
        
        
        
        <language>zh-cn</language>
        
        <lastBuildDate>Fri, 22 Dec 2023 16:18:56 GMT</lastBuildDate>
        <ttl>120</ttl>
        
        <item>
            <title><![CDATA[DyBluRF: Dynamic Deblurring Neural Radiance Fields for Blurry Monocular Video]]></title>
            <description><![CDATA[Video view synthesis, allowing for the creation of visually appealing frames from arbitrary viewpoints and times, offers immersive viewing experiences. Neural radiance fields, particularly NeRF, initially developed for static scenes, have spurred the creation of various methods for video view synthesis. However, the challenge for video view synthesis arises from motion blur, a consequence of object or camera movement during exposure, which hinders the precise synthesis of sharp spatio-temporal views. In response, we propose a novel dynamic deblurring NeRF framework for blurry monocular video, called DyBluRF, consisting of an Interleave Ray Refinement (IRR) stage and a Motion Decomposition-based Deblurring (MDD) stage. Our DyBluRF is the first that addresses and handles the novel view synthesis for blurry monocular video. The IRR stage jointly reconstructs dynamic 3D scenes and refines the inaccurate camera pose information to combat imprecise pose information extracted from the given blurry frames. The MDD stage is a novel incremental latent sharp-rays prediction (ILSP) approach for the blurry monocular video frames by decomposing the latent sharp rays into global camera motion and local object motion components. Extensive experimental results demonstrate that our DyBluRF outperforms qualitatively and quantitatively the very recent state-of-the-art methods. Our project page including source codes and pretrained model are publicly available at https://kaist-viclab.github.io/dyblurf-site/.]]></description>
            <pubDate>Fri, 22 Dec 2023 04:55:58 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2312.13528</guid>
            <link>https://arxiv.org/abs/2312.13528</link>
            
            
            
            <author><![CDATA[Minh-Quan Viet Bui, Jongmin Park, Jihyong Oh, Munchurl Kim]]></author>
            
                
            
            
        </item>
        
        <item>
            <title><![CDATA[ShowRoom3D: Text to High-Quality 3D Room Generation Using 3D Priors]]></title>
            <description><![CDATA[We introduce ShowRoom3D, a three-stage approach for generating high-quality 3D room-scale scenes from texts. Previous methods using 2D diffusion priors to optimize neural radiance fields for generating room-scale scenes have shown unsatisfactory quality. This is primarily attributed to the limitations of 2D priors lacking 3D awareness and constraints in the training methodology. In this paper, we utilize a 3D diffusion prior, MVDiffusion, to optimize the 3D room-scale scene. Our contributions are in two aspects. Firstly, we propose a progressive view selection process to optimize NeRF. This involves dividing the training process into three stages, gradually expanding the camera sampling scope. Secondly, we propose the pose transformation method in the second stage. It will ensure MVDiffusion provide the accurate view guidance. As a result, ShowRoom3D enables the generation of rooms with improved structural integrity, enhanced clarity from any view, reduced content repetition, and higher consistency across different perspectives. Extensive experiments demonstrate that our method, significantly outperforms state-of-the-art approaches by a large margin in terms of user study.]]></description>
            <pubDate>Fri, 22 Dec 2023 04:43:20 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2312.13324</guid>
            <link>https://arxiv.org/abs/2312.13324</link>
            
            
            
            <author><![CDATA[Weijia Mao, Yan-Pei Cao, Jia-Wei Liu, Zhongcong Xu, Mike Zheng Shou]]></author>
            
                
            
            
        </item>
        
        <item>
            <title><![CDATA[Unlocking Pre-trained Image Backbones for Semantic Image Synthesis]]></title>
            <description><![CDATA[Semantic image synthesis, i.e., generating images from user-provided semantic label maps, is an important conditional image generation task as it allows to control both the content as well as the spatial layout of generated images. Although diffusion models have pushed the state of the art in generative image modeling, the iterative nature of their inference process makes them computationally demanding. Other approaches such as GANs are more efficient as they only need a single feed-forward pass for generation, but the image quality tends to suffer on large and diverse datasets. In this work, we propose a new class of GAN discriminators for semantic image synthesis that generates highly realistic images by exploiting feature backbone networks pre-trained for tasks such as image classification. We also introduce a new generator architecture with better context modeling and using cross-attention to inject noise into latent variables, leading to more diverse generated images. Our model, which we dub DP-SIMS, achieves state-of-the-art results in terms of image quality and consistency with the input label maps on ADE-20K, COCO-Stuff, and Cityscapes, surpassing recent diffusion models while requiring two orders of magnitude less compute for inference.]]></description>
            <pubDate>Fri, 22 Dec 2023 04:30:18 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2312.13314</guid>
            <link>https://arxiv.org/abs/2312.13314</link>
            
            
            
            <author><![CDATA[Tariq Berrada, Jakob Verbeek, Camille Couprie, Karteek Alahari]]></author>
            
                
            
            
        </item>
        
        <item>
            <title><![CDATA[AppAgent: Multimodal Agents as Smartphone Users]]></title>
            <description><![CDATA[Recent advancements in large language models (LLMs) have led to the creation of intelligent agents capable of performing complex tasks. This paper introduces a novel LLM-based multimodal agent framework designed to operate smartphone applications. Our framework enables the agent to operate smartphone applications through a simplified action space, mimicking human-like interactions such as tapping and swiping. This novel approach bypasses the need for system back-end access, thereby broadening its applicability across diverse apps. Central to our agent's functionality is its innovative learning method. The agent learns to navigate and use new apps either through autonomous exploration or by observing human demonstrations. This process generates a knowledge base that the agent refers to for executing complex tasks across different applications. To demonstrate the practicality of our agent, we conducted extensive testing over 50 tasks in 10 different applications, including social media, email, maps, shopping, and sophisticated image editing tools. The results affirm our agent's proficiency in handling a diverse array of high-level tasks.]]></description>
            <pubDate>Fri, 22 Dec 2023 04:13:54 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2312.13771</guid>
            <link>https://arxiv.org/abs/2312.13771</link>
            
            
            
            <author><![CDATA[Zhao Yang, Jiaxuan Liu, Yucheng Han, Xin Chen, Zebiao Huang, Bin Fu, Gang Yu]]></author>
            
                
            
            
        </item>
        
        <item>
            <title><![CDATA[Time is Encoded in the Weights of Finetuned Language Models]]></title>
            <description><![CDATA[We present time vectors, a simple tool to customize language models to new time periods. Time vectors are created by finetuning a language model on data from a single time (e.g., a year or month), and then subtracting the weights of the original pretrained model. This vector specifies a direction in weight space that, as our experiments show, improves performance on text from that time period. Time vectors specialized to adjacent time periods appear to be positioned closer together in a manifold. Using this structure, we interpolate between time vectors to induce new models that perform better on intervening and future time periods, without any additional training. We demonstrate the consistency of our findings across different tasks, domains, model sizes, and time scales. Our results suggest that time is encoded in the weight space of finetuned models.]]></description>
            <pubDate>Fri, 22 Dec 2023 04:08:55 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2312.13401</guid>
            <link>https://arxiv.org/abs/2312.13401</link>
            
            
            
            <author><![CDATA[Kai Nylund, Suchin Gururangan, Noah A. Smith]]></author>
            
                
            
            
        </item>
        
        <item>
            <title><![CDATA[HeadCraft: Modeling High-Detail Shape Variations for Animated 3DMMs]]></title>
            <description><![CDATA[Current advances in human head modeling allow to generate plausible-looking 3D head models via neural representations. Nevertheless, constructing complete high-fidelity head models with explicitly controlled animation remains an issue. Furthermore, completing the head geometry based on a partial observation, e.g. coming from a depth sensor, while preserving details is often problematic for the existing methods. We introduce a generative model for detailed 3D head meshes on top of an articulated 3DMM which allows explicit animation and high-detail preservation at the same time. Our method is trained in two stages. First, we register a parametric head model with vertex displacements to each mesh of the recently introduced NPHM dataset of accurate 3D head scans. The estimated displacements are baked into a hand-crafted UV layout. Second, we train a StyleGAN model in order to generalize over the UV maps of displacements. The decomposition of the parametric model and high-quality vertex displacements allows us to animate the model and modify it semantically. We demonstrate the results of unconditional generation and fitting to the full or partial observation. The project page is available at https://seva100.github.io/headcraft.]]></description>
            <pubDate>Fri, 22 Dec 2023 03:47:19 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2312.14140</guid>
            <link>https://arxiv.org/abs/2312.14140</link>
            
            
            
            <author><![CDATA[Artem Sevastopolsky, Philip-William Grassal, Simon Giebenhain, ShahRukh Athar, Luisa Verdoliva, Matthias Niessner]]></author>
            
                
            
            
        </item>
        
        <item>
            <title><![CDATA[HD-Painter: High-Resolution and Prompt-Faithful Text-Guided Image Inpainting with Diffusion Models]]></title>
            <description><![CDATA[Recent progress in text-guided image inpainting, based on the unprecedented success of text-to-image diffusion models, has led to exceptionally realistic and visually plausible results. However, there is still significant potential for improvement in current text-to-image inpainting models, particularly in better aligning the inpainted area with user prompts and performing high-resolution inpainting. Therefore, in this paper we introduce HD-Painter, a completely training-free approach that accurately follows to prompts and coherently scales to high-resolution image inpainting. To this end, we design the Prompt-Aware Introverted Attention (PAIntA) layer enhancing self-attention scores by prompt information and resulting in better text alignment generations. To further improve the prompt coherence we introduce the Reweighting Attention Score Guidance (RASG) mechanism seamlessly integrating a post-hoc sampling strategy into general form of DDIM to prevent out-of-distribution latent shifts. Moreover, HD-Painter allows extension to larger scales by introducing a specialized super-resolution technique customized for inpainting, enabling the completion of missing regions in images of up to 2K resolution. Our experiments demonstrate that HD-Painter surpasses existing state-of-the-art approaches qualitatively and quantitatively, achieving an impressive generation accuracy improvement of 61.4% vs 51.9%. We will make the codes publicly available at: https://github.com/Picsart-AI-Research/HD-Painter]]></description>
            <pubDate>Fri, 22 Dec 2023 03:29:29 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2312.14091</guid>
            <link>https://arxiv.org/abs/2312.14091</link>
            
            
            
            <author><![CDATA[Hayk Manukyan, Andranik Sargsyan, Barsegh Atanyan, Zhangyang Wang, Shant Navasardyan, Humphrey Shi]]></author>
            
                
            
            
        </item>
        
        <item>
            <title><![CDATA[Carve3D: Improving Multi-view Reconstruction Consistency for Diffusion Models with RL Finetuning]]></title>
            <description><![CDATA[Recent advancements in the text-to-3D task leverage finetuned text-to-image diffusion models to generate multi-view images, followed by NeRF reconstruction. Yet, existing supervised finetuned (SFT) diffusion models still suffer from multi-view inconsistency and the resulting NeRF artifacts. Although training longer with SFT improves consistency, it also causes distribution shift, which reduces diversity and realistic details. We argue that the SFT of multi-view diffusion models resembles the instruction finetuning stage of the LLM alignment pipeline and can benefit from RL finetuning (RLFT) methods. Essentially, RLFT methods optimize models beyond their SFT data distribution by using their own outputs, effectively mitigating distribution shift. To this end, we introduce Carve3D, a RLFT method coupled with the Multi-view Reconstruction Consistency (MRC) metric, to improve the consistency of multi-view diffusion models. To compute MRC on a set of multi-view images, we compare them with their corresponding renderings of the reconstructed NeRF at the same viewpoints. We validate the robustness of MRC with extensive experiments conducted under controlled inconsistency levels. We enhance the base RLFT algorithm to stabilize the training process, reduce distribution shift, and identify scaling laws. Through qualitative and quantitative experiments, along with a user study, we demonstrate Carve3D's improved multi-view consistency, the resulting superior NeRF reconstruction quality, and minimal distribution shift compared to longer SFT. Project webpage: https://desaixie.github.io/carve-3d.]]></description>
            <pubDate>Fri, 22 Dec 2023 03:22:07 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2312.13980</guid>
            <link>https://arxiv.org/abs/2312.13980</link>
            
            
            
            <author><![CDATA[Desai Xie, Jiahao Li, Hao Tan, Xin Sun, Zhixin Shu, Yi Zhou, Sai Bi, Sören Pirk, Arie E. Kaufman]]></author>
            
                
            
            
        </item>
        
        <item>
            <title><![CDATA[PIA: Your Personalized Image Animator via Plug-and-Play Modules in Text-to-Image Models]]></title>
            <description><![CDATA[Recent advancements in personalized text-to-image (T2I) models have revolutionized content creation, empowering non-experts to generate stunning images with unique styles. While promising, adding realistic motions into these personalized images by text poses significant challenges in preserving distinct styles, high-fidelity details, and achieving motion controllability by text. In this paper, we present PIA, a Personalized Image Animator that excels in aligning with condition images, achieving motion controllability by text, and the compatibility with various personalized T2I models without specific tuning. To achieve these goals, PIA builds upon a base T2I model with well-trained temporal alignment layers, allowing for the seamless transformation of any personalized T2I model into an image animation model. A key component of PIA is the introduction of the condition module, which utilizes the condition frame and inter-frame affinity as input to transfer appearance information guided by the affinity hint for individual frame synthesis in the latent space. This design mitigates the challenges of appearance-related image alignment within and allows for a stronger focus on aligning with motion-related guidance.]]></description>
            <pubDate>Fri, 22 Dec 2023 02:48:53 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2312.13964</guid>
            <link>https://arxiv.org/abs/2312.13964</link>
            
            
            
            <author><![CDATA[Yiming Zhang, Zhening Xing, Yanhong Zeng, Youqing Fang, Kai Chen]]></author>
            
                
            
            
        </item>
        
        <item>
            <title><![CDATA[Paint3D: Paint Anything 3D with Lighting-Less Texture Diffusion Models]]></title>
            <description><![CDATA[This paper presents Paint3D, a novel coarse-to-fine generative framework that is capable of producing high-resolution, lighting-less, and diverse 2K UV texture maps for untextured 3D meshes conditioned on text or image inputs. The key challenge addressed is generating high-quality textures without embedded illumination information, which allows the textures to be re-lighted or re-edited within modern graphics pipelines. To achieve this, our method first leverages a pre-trained depth-aware 2D diffusion model to generate view-conditional images and perform multi-view texture fusion, producing an initial coarse texture map. However, as 2D models cannot fully represent 3D shapes and disable lighting effects, the coarse texture map exhibits incomplete areas and illumination artifacts. To resolve this, we train separate UV Inpainting and UVHD diffusion models specialized for the shape-aware refinement of incomplete areas and the removal of illumination artifacts. Through this coarse-to-fine process, Paint3D can produce high-quality 2K UV textures that maintain semantic consistency while being lighting-less, significantly advancing the state-of-the-art in texturing 3D objects.]]></description>
            <pubDate>Fri, 22 Dec 2023 02:30:44 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2312.13913</guid>
            <link>https://arxiv.org/abs/2312.13913</link>
            
            
            
            <author><![CDATA[Xianfang Zeng]]></author>
            
                
            
            
        </item>
        
        <item>
            <title><![CDATA[Fairy: Fast Parallelized Instruction-Guided Video-to-Video Synthesis]]></title>
            <description><![CDATA[In this paper, we introduce Fairy, a minimalist yet robust adaptation of image-editing diffusion models, enhancing them for video editing applications. Our approach centers on the concept of anchor-based cross-frame attention, a mechanism that implicitly propagates diffusion features across frames, ensuring superior temporal coherence and high-fidelity synthesis. Fairy not only addresses limitations of previous models, including memory and processing speed. It also improves temporal consistency through a unique data augmentation strategy. This strategy renders the model equivariant to affine transformations in both source and target images. Remarkably efficient, Fairy generates 120-frame 512x384 videos (4-second duration at 30 FPS) in just 14 seconds, outpacing prior works by at least 44x. A comprehensive user study, involving 1000 generated samples, confirms that our approach delivers superior quality, decisively outperforming established methods.]]></description>
            <pubDate>Fri, 22 Dec 2023 02:17:41 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2312.13834</guid>
            <link>https://arxiv.org/abs/2312.13834</link>
            
            
            
            <author><![CDATA[Bichen Wu, Ching-Yao Chuang, Xiaoyan Wang, Yichen Jia, Kapil Krishnakumar, Tong Xiao, Feng Liang, Licheng Yu, Peter Vajda]]></author>
            
                
            
            
        </item>
        
        <item>
            <title><![CDATA[TinySAM: Pushing the Envelope for Efficient Segment Anything Model]]></title>
            <description><![CDATA[Recently segment anything model (SAM) has shown powerful segmentation capability and has drawn great attention in computer vision fields. Massive following works have developed various applications based on the pretrained SAM and achieved impressive performance on downstream vision tasks. However, SAM consists of heavy architectures and requires massive computational capacity, which hinders the further application of SAM on computation constrained edge devices. To this end, in this paper we propose a framework to obtain a tiny segment anything model (TinySAM) while maintaining the strong zero-shot performance. We first propose a full-stage knowledge distillation method with online hard prompt sampling strategy to distill a lightweight student model. We also adapt the post-training quantization to the promptable segmentation task and further reduce the computational cost. Moreover, a hierarchical segmenting everything strategy is proposed to accelerate the everything inference by 2times with almost no performance degradation. With all these proposed methods, our TinySAM leads to orders of magnitude computational reduction and pushes the envelope for efficient segment anything task. Extensive experiments on various zero-shot transfer tasks demonstrate the significantly advantageous performance of our TinySAM against counterpart methods. Pre-trained models and codes will be available at https://github.com/xinghaochen/TinySAM and https://gitee.com/mindspore/models/tree/master/research/cv/TinySAM.]]></description>
            <pubDate>Fri, 22 Dec 2023 02:12:07 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2312.13789</guid>
            <link>https://arxiv.org/abs/2312.13789</link>
            
            
            
            <author><![CDATA[Han Shu, Wenshuo Li, Yehui Tang, Yiman Zhang, Yihao Chen, Houqiang Li, Yunhe Wang, Xinghao Chen]]></author>
            
                
            
            
        </item>
        
        <item>
            <title><![CDATA[DreamTuner: Single Image is Enough for Subject-Driven Generation]]></title>
            <description><![CDATA[Diffusion-based models have demonstrated impressive capabilities for text-to-image generation and are expected for personalized applications of subject-driven generation, which require the generation of customized concepts with one or a few reference images. However, existing methods based on fine-tuning fail to balance the trade-off between subject learning and the maintenance of the generation capabilities of pretrained models. Moreover, other methods that utilize additional image encoders tend to lose important details of the subject due to encoding compression. To address these challenges, we propose DreamTurner, a novel method that injects reference information from coarse to fine to achieve subject-driven image generation more effectively. DreamTurner introduces a subject-encoder for coarse subject identity preservation, where the compressed general subject features are introduced through an attention layer before visual-text cross-attention. We then modify the self-attention layers within pretrained text-to-image models to self-subject-attention layers to refine the details of the target subject. The generated image queries detailed features from both the reference image and itself in self-subject-attention. It is worth emphasizing that self-subject-attention is an effective, elegant, and training-free method for maintaining the detailed features of customized subjects and can serve as a plug-and-play solution during inference. Finally, with additional subject-driven fine-tuning, DreamTurner achieves remarkable performance in subject-driven image generation, which can be controlled by a text or other conditions such as pose. For further details, please visit the project page at https://dreamtuner-diffusion.github.io/.]]></description>
            <pubDate>Fri, 22 Dec 2023 02:06:25 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2312.13691</guid>
            <link>https://arxiv.org/abs/2312.13691</link>
            
            
            
            <author><![CDATA[Miao Hua, Jiawei Liu, Fei Ding, Wei Liu, Jie Wu, Qian He]]></author>
            
                
            
            
        </item>
        
        <item>
            <title><![CDATA[DREAM-Talk: Diffusion-based Realistic Emotional Audio-driven Method for Single Image Talking Face Generation]]></title>
            <description><![CDATA[The generation of emotional talking faces from a single portrait image remains a significant challenge. The simultaneous achievement of expressive emotional talking and accurate lip-sync is particularly difficult, as expressiveness is often compromised for the accuracy of lip-sync. As widely adopted by many prior works, the LSTM network often fails to capture the subtleties and variations of emotional expressions. To address these challenges, we introduce DREAM-Talk, a two-stage diffusion-based audio-driven framework, tailored for generating diverse expressions and accurate lip-sync concurrently. In the first stage, we propose EmoDiff, a novel diffusion module that generates diverse highly dynamic emotional expressions and head poses in accordance with the audio and the referenced emotion style. Given the strong correlation between lip motion and audio, we then refine the dynamics with enhanced lip-sync accuracy using audio features and emotion style. To this end, we deploy a video-to-video rendering module to transfer the expressions and lip motions from our proxy 3D avatar to an arbitrary portrait. Both quantitatively and qualitatively, DREAM-Talk outperforms state-of-the-art methods in terms of expressiveness, lip-sync accuracy and perceptual quality.]]></description>
            <pubDate>Fri, 22 Dec 2023 02:01:31 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2312.13578</guid>
            <link>https://arxiv.org/abs/2312.13578</link>
            
            
            
            <author><![CDATA[Chenxu Zhang, Chao Wang, Jianfeng Zhang, Hongyi Xu, Guoxian Song, You Xie, Linjie Luo, Yapeng Tian, Xiaohu Guo, Jiashi Feng]]></author>
            
                
            
            
        </item>
        
        <item>
            <title><![CDATA[Neural feels with neural fields: Visuo-tactile perception for in-hand manipulation]]></title>
            <description><![CDATA[To achieve human-level dexterity, robots must infer spatial awareness from multimodal sensing to reason over contact interactions. During in-hand manipulation of novel objects, such spatial awareness involves estimating the object's pose and shape. The status quo for in-hand perception primarily employs vision, and restricts to tracking a priori known objects. Moreover, visual occlusion of objects in-hand is imminent during manipulation, preventing current systems to push beyond tasks without occlusion. We combine vision and touch sensing on a multi-fingered hand to estimate an object's pose and shape during in-hand manipulation. Our method, NeuralFeels, encodes object geometry by learning a neural field online and jointly tracks it by optimizing a pose graph problem. We study multimodal in-hand perception in simulation and the real-world, interacting with different objects via a proprioception-driven policy. Our experiments show final reconstruction F-scores of 81% and average pose drifts of 4.7,mm, further reduced to 2.3,mm with known CAD models. Additionally, we observe that under heavy visual occlusion we can achieve up to 94% improvements in tracking compared to vision-only methods. Our results demonstrate that touch, at the very least, refines and, at the very best, disambiguates visual estimates during in-hand manipulation. We release our evaluation dataset of 70 experiments, FeelSight, as a step towards benchmarking in this domain. Our neural representation driven by multimodal sensing can serve as a perception backbone towards advancing robot dexterity. Videos can be found on our project website https://suddhu.github.io/neural-feels/]]></description>
            <pubDate>Fri, 22 Dec 2023 01:56:35 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2312.13469</guid>
            <link>https://arxiv.org/abs/2312.13469</link>
            
            
            
            <author><![CDATA[Sudharshan Suresh, Haozhi Qi, Tingfan Wu, Taosha Fan, Luis Pineda, Mike Lambeta, Jitendra Malik, Mrinal Kalakrishnan, Roberto Calandra, Michael Kaess, Joseph Ortiz, Mustafa Mukadam]]></author>
            
                
            
            
        </item>
        
        <item>
            <title><![CDATA[VideoPoet: A Large Language Model for Zero-Shot Video Generation]]></title>
            <description><![CDATA[We present VideoPoet, a language model capable of synthesizing high-quality video, with matching audio, from a large variety of conditioning signals. VideoPoet employs a decoder-only transformer architecture that processes multimodal inputs -- including images, videos, text, and audio. The training protocol follows that of Large Language Models (LLMs), consisting of two stages: pretraining and task-specific adaptation. During pretraining, VideoPoet incorporates a mixture of multimodal generative objectives within an autoregressive Transformer framework. The pretrained LLM serves as a foundation that can be adapted for a range of video generation tasks. We present empirical results demonstrating the model's state-of-the-art capabilities in zero-shot video generation, specifically highlighting VideoPoet's ability to generate high-fidelity motions. Project page: http://sites.research.google/videopoet/]]></description>
            <pubDate>Fri, 22 Dec 2023 01:41:09 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2312.14125</guid>
            <link>https://arxiv.org/abs/2312.14125</link>
            
            
            
            <author><![CDATA[Dan Kondratyuk, Lijun Yu, Xiuye Gu, José Lezama, Jonathan Huang, Rachel Hornung, Hartwig Adam, Hassan Akbari, Yair Alon, Vighnesh Birodkar, Yong Cheng, Ming-Chang Chiu, Josh Dillon, Irfan Essa, Agrim Gupta, Meera Hahn, Anja Hauth, David Hendon, Alonso Martinez, David Minnen, David Ross, Grant Schindler, Mikhail Sirotenko, Kihyuk Sohn, Krishna Somandepalli, Huisheng Wang, Jimmy Yan, Ming-Hsuan Yang, Xuan Yang, Bryan Seybold, Lu Jiang]]></author>
            
                
            
            
        </item>
        
        <item>
            <title><![CDATA[Align Your Gaussians: Text-to-4D with Dynamic 3D Gaussians and Composed Diffusion Models]]></title>
            <description><![CDATA[Text-guided diffusion models have revolutionized image and video generation and have also been successfully used for optimization-based 3D object synthesis. Here, we instead focus on the underexplored text-to-4D setting and synthesize dynamic, animated 3D objects using score distillation methods with an additional temporal dimension. Compared to previous work, we pursue a novel compositional generation-based approach, and combine text-to-image, text-to-video, and 3D-aware multiview diffusion models to provide feedback during 4D object optimization, thereby simultaneously enforcing temporal consistency, high-quality visual appearance and realistic geometry. Our method, called Align Your Gaussians (AYG), leverages dynamic 3D Gaussian Splatting with deformation fields as 4D representation. Crucial to AYG is a novel method to regularize the distribution of the moving 3D Gaussians and thereby stabilize the optimization and induce motion. We also propose a motion amplification mechanism as well as a new autoregressive synthesis scheme to generate and combine multiple 4D sequences for longer generation. These techniques allow us to synthesize vivid dynamic scenes, outperform previous work qualitatively and quantitatively and achieve state-of-the-art text-to-4D performance. Due to the Gaussian 4D representation, different 4D animations can be seamlessly combined, as we demonstrate. AYG opens up promising avenues for animation, simulation and digital content creation as well as synthetic data generation.]]></description>
            <pubDate>Fri, 22 Dec 2023 01:39:36 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2312.13763</guid>
            <link>https://arxiv.org/abs/2312.13763</link>
            
            
            
            <author><![CDATA[Huan Ling, Seung Wook Kim, Antonio Torralba, Sanja Fidler, Karsten Kreis]]></author>
            
                
            
            
        </item>
        
    </channel>
</rss>
