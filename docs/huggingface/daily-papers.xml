<?xml version="1.0" encoding="UTF-8"?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"


>
    <channel>
        <title><![CDATA[Huggingface Daily Papers]]></title>
        <link>https://huggingface.co/papers</link>
        <atom:link href="https://rsshub.app/huggingface/daily-papers" rel="self" type="application/rss+xml" />
        <description><![CDATA[Huggingface Daily Papers - Made with love by RSSHub(https://github.com/DIYgod/RSSHub)]]></description>
        <generator>RSSHub</generator>
        <webMaster>i@diygod.me (DIYgod)</webMaster>
        
        
        
        <language>zh-cn</language>
        
        <lastBuildDate>Fri, 02 Feb 2024 03:50:51 GMT</lastBuildDate>
        <ttl>120</ttl>
        
        <item>
            <title><![CDATA[EE-Tuning: An Economical yet Scalable Solution for Tuning Early-Exit Large Language Models]]></title>
            <description><![CDATA[This work introduces EE-Tuning, a lightweight and economical solution to training/tuning early-exit large language models (LLMs). In contrast to the common approach of full-parameter pre-training, EE-Tuning augments any pre-trained (and possibly fine-tuned) standard LLM with additional early-exit layers that are tuned in a parameter-efficient manner, which requires significantly less computational resources and training data. Our implementation of EE-Tuning achieves outstanding training efficiency via extensive performance optimizations, as well as scalability due to its full compatibility with 3D parallelism. Results of systematic experiments validate the efficacy of EE-Tuning, confirming that effective early-exit LLM inference can be achieved with a limited training budget. In hope of making early-exit LLMs accessible to the community, we release the source code of our implementation of EE-Tuning at https://github.com/pan-x-c/EE-LLM.]]></description>
            <pubDate>Fri, 02 Feb 2024 03:50:29 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2402.00518</guid>
            <link>https://arxiv.org/abs/2402.00518</link>
            
            
            
            <author><![CDATA[Xuchen Pan, Yanxi Chen, Yaliang Li, Bolin Ding, Jingren Zhou]]></author>
            
                
            
            
        </item>
        
        <item>
            <title><![CDATA[Efficient Exploration for LLMs]]></title>
            <description><![CDATA[We present evidence of substantial benefit from efficient exploration in gathering human feedback to improve large language models. In our experiments, an agent sequentially generates queries while fitting a reward model to the feedback received. Our best-performing agent generates queries using double Thompson sampling, with uncertainty represented by an epistemic neural network. Our results demonstrate that efficient exploration enables high levels of performance with far fewer queries. Further, both uncertainty estimation and the choice of exploration scheme play critical roles.]]></description>
            <pubDate>Fri, 02 Feb 2024 03:47:27 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2402.00396</guid>
            <link>https://arxiv.org/abs/2402.00396</link>
            
            
            
            <author><![CDATA[Vikranth Dwaracherla, Seyed Mohammad Asghari, Botao Hao, Benjamin Van Roy]]></author>
            
                
            
            
        </item>
        
        <item>
            <title><![CDATA[Dolma: an Open Corpus of Three Trillion Tokens for Language Model Pretraining Research]]></title>
            <description><![CDATA[Language models have become a critical technology to tackling a wide range of natural language processing tasks, yet many details about how the best-performing language models were developed are not reported. In particular, information about their pretraining corpora is seldom discussed: commercial language models rarely provide any information about their data; even open models rarely release datasets they are trained on, or an exact recipe to reproduce them. As a result, it is challenging to conduct certain threads of language modeling research, such as understanding how training data impacts model capabilities and shapes their limitations. To facilitate open research on language model pretraining, we release Dolma, a three trillion tokens English corpus, built from a diverse mixture of web content, scientific papers, code, public-domain books, social media, and encyclopedic materials. In addition, we open source our data curation toolkit to enable further experimentation and reproduction of our work. In this report, we document Dolma, including its design principles, details about its construction, and a summary of its contents. We interleave this report with analyses and experimental results from training language models on intermediate states of Dolma to share what we have learned about important data curation practices, including the role of content or quality filters, deduplication, and multi-source mixing. Dolma has been used to train OLMo, a state-of-the-art, open language model and framework designed to build and study the science of language modeling.]]></description>
            <pubDate>Fri, 02 Feb 2024 03:12:55 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2402.00159</guid>
            <link>https://arxiv.org/abs/2402.00159</link>
            
            
            
            <author><![CDATA[Luca Soldaini, Rodney Kinney, Akshita Bhagia, Dustin Schwenk, David Atkinson, Russell Authur, Ben Bogin, Khyathi Chandu, Jennifer Dumas, Yanai Elazar, Valentin Hofmann, Ananya Harsh Jha, Sachin Kumar, Li Lucy, Xinxi Lyu, Nathan Lambert, Ian Magnusson, Jacob Morrison, Niklas Muennighoff, Aakanksha Naik, Crystal Nam, Matthew E. Peters, Abhilasha Ravichander, Kyle Richardson, Zejiang Shen, Emma Strubell, Nishant Subramani, Oyvind Tafjord, Pete Walsh, Luke Zettlemoyer, Noah A. Smith, Hannaneh Hajishirzi, Iz Beltagy, Dirk Groeneveld, Jesse Dodge, Kyle Lo]]></author>
            
                
            
            
        </item>
        
        <item>
            <title><![CDATA[AnimateLCM: Accelerating the Animation of Personalized Diffusion Models and Adapters with Decoupled Consistency Learning]]></title>
            <description><![CDATA[Video diffusion models has been gaining increasing attention for its ability to produce videos that are both coherent and of high fidelity. However, the iterative denoising process makes it computationally intensive and time-consuming, thus limiting its applications. Inspired by the Consistency Model (CM) that distills pretrained image diffusion models to accelerate the sampling with minimal steps and its successful extension Latent Consistency Model (LCM) on conditional image generation, we propose AnimateLCM, allowing for high-fidelity video generation within minimal steps. Instead of directly conducting consistency learning on the raw video dataset, we propose a decoupled consistency learning strategy that decouples the distillation of image generation priors and motion generation priors, which improves the training efficiency and enhance the generation visual quality. Additionally, to enable the combination of plug-and-play adapters in stable diffusion community to achieve various functions (e.g., ControlNet for controllable generation). we propose an efficient strategy to adapt existing adapters to our distilled text-conditioned video consistency model or train adapters from scratch without harming the sampling speed. We validate the proposed strategy in image-conditioned video generation and layout-conditioned video generation, all achieving top-performing results. Experimental results validate the effectiveness of our proposed method. Code and weights will be made public. More details are available at https://github.com/G-U-N/AnimateLCM.]]></description>
            <pubDate>Fri, 02 Feb 2024 02:14:21 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2402.00769</guid>
            <link>https://arxiv.org/abs/2402.00769</link>
            
            
            
            <author><![CDATA[Fu-Yun Wang, Zhaoyang Huang, Xiaoyu Shi, Weikang Bian, Guanglu Song, Yu Liu, Hongsheng Li]]></author>
            
                
            
            
        </item>
        
    </channel>
</rss>
