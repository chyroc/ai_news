<?xml version="1.0" encoding="UTF-8"?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"


>
    <channel>
        <title><![CDATA[Huggingface Daily Papers]]></title>
        <link>https://huggingface.co/papers</link>
        <atom:link href="https://rsshub.app/huggingface/daily-papers" rel="self" type="application/rss+xml" />
        <description><![CDATA[Huggingface Daily Papers - Made with love by RSSHub(https://github.com/DIYgod/RSSHub)]]></description>
        <generator>RSSHub</generator>
        <webMaster>i@diygod.me (DIYgod)</webMaster>
        
        
        
        <language>zh-cn</language>
        
        <lastBuildDate>Mon, 05 Feb 2024 03:05:53 GMT</lastBuildDate>
        <ttl>120</ttl>
        
        <item>
            <title><![CDATA[StepCoder: Improve Code Generation with Reinforcement Learning from Compiler Feedback]]></title>
            <description><![CDATA[The advancement of large language models (LLMs) has significantly propelled the field of code generation. Previous work integrated reinforcement learning (RL) with compiler feedback for exploring the output space of LLMs to enhance code generation quality. However, the lengthy code generated by LLMs in response to complex human requirements makes RL exploration a challenge. Also, since the unit tests may not cover the complicated code, optimizing LLMs by using these unexecuted code snippets is ineffective. To tackle these challenges, we introduce StepCoder, a novel RL framework for code generation, consisting of two main components: CCCS addresses the exploration challenge by breaking the long sequences code generation task into a Curriculum of Code Completion Subtasks, while FGO only optimizes the model by masking the unexecuted code segments to provide Fine-Grained Optimization. In addition, we furthermore construct the APPS+ dataset for RL training, which is manually verified to ensure the correctness of unit tests. Experimental results show that our method improves the ability to explore the output space and outperforms state-of-the-art approaches in corresponding benchmarks.]]></description>
            <pubDate>Mon, 05 Feb 2024 03:03:18 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2402.01391</guid>
            <link>https://arxiv.org/abs/2402.01391</link>
            
            
            
            <author><![CDATA[Shihan Dou, Yan Liu, Haoxiang Jia, Limao Xiong, Enyu Zhou, Junjie Shan, Caishuang Huang, Wei Shen, Xiaoran Fan, Zhiheng Xi, Yuhao Zhou, Tao Ji, Rui Zheng, Qi Zhang, Xuanjing Huang, Tao Gui]]></author>
            
                
            
            
        </item>
        
        <item>
            <title><![CDATA[PokéLLMon: A Human-Parity Agent for Pokémon Battles with Large Language Models]]></title>
            <description><![CDATA[We introduce Pok\'eLLMon, the first LLM-embodied agent that achieves human-parity performance in tactical battle games, as demonstrated in Pok\'emon battles. The design of Pok\'eLLMon incorporates three key strategies: (i) In-context reinforcement learning that instantly consumes text-based feedback derived from battles to iteratively refine the policy; (ii) Knowledge-augmented generation that retrieves external knowledge to counteract hallucination and enables the agent to act timely and properly; (iii) Consistent action generation to mitigate the panic switching phenomenon when the agent faces a powerful opponent and wants to elude the battle. We show that online battles against human demonstrates Pok\'eLLMon's human-like battle strategies and just-in-time decision making, achieving 49\% of win rate in the Ladder competitions and 56\% of win rate in the invited battles. Our implementation and playable battle logs are available at: https://github.com/git-disl/PokeLLMon.]]></description>
            <pubDate>Mon, 05 Feb 2024 02:49:37 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2402.01118</guid>
            <link>https://arxiv.org/abs/2402.01118</link>
            
            
            
            <author><![CDATA[Sihao Hu, Tiansheng Huang, Ling Liu]]></author>
            
                
            
            
        </item>
        
        <item>
            <title><![CDATA[Specialized Language Models with Cheap Inference from Limited Domain Data]]></title>
            <description><![CDATA[Large language models have emerged as a versatile tool but are challenging to apply to tasks lacking large inference budgets and large in-domain training sets. This work formalizes these constraints and distinguishes four important variables: the pretraining budget (for training before the target domain is known), the specialization budget (for training after the target domain is known), the inference budget, and the in-domain training set size. Across these settings, we compare different approaches from the machine learning literature. Limited by inference cost, we find better alternatives to the standard practice of training very large vanilla transformer models. In particular, we show that hyper-networks and mixture of experts have better perplexity for large pretraining budgets, while small models trained on importance sampled datasets are attractive for large specialization budgets.]]></description>
            <pubDate>Mon, 05 Feb 2024 02:43:43 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2402.01093</guid>
            <link>https://arxiv.org/abs/2402.01093</link>
            
            
            
            <author><![CDATA[David Grangier, Angelos Katharopoulos, Pierre Ablin, Awni Hannun]]></author>
            
                
            
            
        </item>
        
        <item>
            <title><![CDATA[Repeat After Me: Transformers are Better than State Space Models at Copying]]></title>
            <description><![CDATA[Transformers are the dominant architecture for sequence modeling, but there is growing interest in models that use a fixed-size latent state that does not depend on the sequence length, which we refer to as "generalized state space models" (GSSMs). In this paper we show that while GSSMs are promising in terms of inference-time efficiency, they are limited compared to transformer models on tasks that require copying from the input context. We start with a theoretical analysis of the simple task of string copying and prove that a two layer transformer can copy strings of exponential length while GSSMs are fundamentally limited by their fixed-size latent state. Empirically, we find that transformers outperform GSSMs in terms of efficiency and generalization on synthetic tasks that require copying the context. Finally, we evaluate pretrained large language models and find that transformer models dramatically outperform state space models at copying and retrieving information from context. Taken together, these results suggest a fundamental gap between transformers and GSSMs on tasks of practical interest.]]></description>
            <pubDate>Mon, 05 Feb 2024 02:42:02 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2402.01032</guid>
            <link>https://arxiv.org/abs/2402.01032</link>
            
            
            
            <author><![CDATA[Samy Jelassi, David Brandfonbrener, Sham M. Kakade, Eran Malach]]></author>
            
                
            
            
        </item>
        
        <item>
            <title><![CDATA[EVA-GAN: Enhanced Various Audio Generation via Scalable Generative Adversarial Networks]]></title>
            <description><![CDATA[The advent of Large Models marks a new era in machine learning, significantly outperforming smaller models by leveraging vast datasets to capture and synthesize complex patterns. Despite these advancements, the exploration into scaling, especially in the audio generation domain, remains limited, with previous efforts didn't extend into the high-fidelity (HiFi) 44.1kHz domain and suffering from both spectral discontinuities and blurriness in the high-frequency domain, alongside a lack of robustness against out-of-domain data. These limitations restrict the applicability of models to diverse use cases, including music and singing generation. Our work introduces Enhanced Various Audio Generation via Scalable Generative Adversarial Networks (EVA-GAN), yields significant improvements over previous state-of-the-art in spectral and high-frequency reconstruction and robustness in out-of-domain data performance, enabling the generation of HiFi audios by employing an extensive dataset of 36,000 hours of 44.1kHz audio, a context-aware module, a Human-In-The-Loop artifact measurement toolkit, and expands the model to approximately 200 million parameters. Demonstrations of our work are available at https://double-blind-eva-gan.cc.]]></description>
            <pubDate>Mon, 05 Feb 2024 02:36:42 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2402.00892</guid>
            <link>https://arxiv.org/abs/2402.00892</link>
            
            
            
            <author><![CDATA[Shijia Liao, Shiyi Lan, Arun George Zachariah]]></author>
            
                
            
            
        </item>
        
    </channel>
</rss>
