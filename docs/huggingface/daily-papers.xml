<?xml version="1.0" encoding="UTF-8"?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"


>
    <channel>
        <title><![CDATA[Huggingface Daily Papers]]></title>
        <link>https://huggingface.co/papers</link>
        <atom:link href="https://rsshub.app/huggingface/daily-papers" rel="self" type="application/rss+xml" />
        <description><![CDATA[Huggingface Daily Papers - Made with love by RSSHub(https://github.com/DIYgod/RSSHub)]]></description>
        <generator>RSSHub</generator>
        <webMaster>i@diygod.me (DIYgod)</webMaster>
        
        
        
        <language>zh-cn</language>
        
        <lastBuildDate>Thu, 14 Dec 2023 15:15:47 GMT</lastBuildDate>
        <ttl>120</ttl>
        
        <item>
            <title><![CDATA[Invariant Graph Transformer]]></title>
            <description><![CDATA[Rationale discovery is defined as finding a subset of the input data that maximally supports the prediction of downstream tasks. In graph machine learning context, graph rationale is defined to locate the critical subgraph in the given graph topology, which fundamentally determines the prediction results. In contrast to the rationale subgraph, the remaining subgraph is named the environment subgraph. Graph rationalization can enhance the model performance as the mapping between the graph rationale and prediction label is viewed as invariant, by assumption. To ensure the discriminative power of the extracted rationale subgraphs, a key technique named "intervention" is applied. The core idea of intervention is that given any changing environment subgraphs, the semantics from the rationale subgraph is invariant, which guarantees the correct prediction result. However, most, if not all, of the existing rationalization works on graph data develop their intervention strategies on the graph level, which is coarse-grained. In this paper, we propose well-tailored intervention strategies on graph data. Our idea is driven by the development of Transformer models, whose self-attention module provides rich interactions between input nodes. Based on the self-attention module, our proposed invariant graph Transformer (IGT) can achieve fine-grained, more specifically, node-level and virtual node-level intervention. Our comprehensive experiments involve 7 real-world datasets, and the proposed IGT shows significant performance advantages compared to 13 baseline methods.]]></description>
            <pubDate>Thu, 14 Dec 2023 04:27:26 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2312.07859</guid>
            <link>https://arxiv.org/abs/2312.07859</link>
            
            
            
            <author><![CDATA[Zhe Xu, Menghai Pan, Yuzhong Chen, Huiyuan Chen, Yuchen Yan, Mahashweta Das, Hanghang Tong]]></author>
            
                
            
            
        </item>
        
        <item>
            <title><![CDATA[ProNeRF: Learning Efficient Projection-Aware Ray Sampling for Fine-Grained Implicit Neural Radiance Fields]]></title>
            <description><![CDATA[Recent advances in neural rendering have shown that, albeit slow, implicit compact models can learn a scene's geometries and view-dependent appearances from multiple views. To maintain such a small memory footprint but achieve faster inference times, recent works have adopted `sampler' networks that adaptively sample a small subset of points along each ray in the implicit neural radiance fields. Although these methods achieve up to a 10times reduction in rendering time, they still suffer from considerable quality degradation compared to the vanilla NeRF. In contrast, we propose ProNeRF, which provides an optimal trade-off between memory footprint (similar to NeRF), speed (faster than HyperReel), and quality (better than K-Planes). ProNeRF is equipped with a novel projection-aware sampling (PAS) network together with a new training strategy for ray exploration and exploitation, allowing for efficient fine-grained particle sampling. Our ProNeRF yields state-of-the-art metrics, being 15-23x faster with 0.65dB higher PSNR than NeRF and yielding 0.95dB higher PSNR than the best published sampler-based method, HyperReel. Our exploration and exploitation training strategy allows ProNeRF to learn the full scenes' color and density distributions while also learning efficient ray sampling focused on the highest-density regions. We provide extensive experimental results that support the effectiveness of our method on the widely adopted forward-facing and 360 datasets, LLFF and Blender, respectively.]]></description>
            <pubDate>Thu, 14 Dec 2023 03:25:55 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2312.08136</guid>
            <link>https://arxiv.org/abs/2312.08136</link>
            
            
            
            <author><![CDATA[Juan Luis Gonzalez Bello, Minh-Quan Viet Bui, Munchurl Kim]]></author>
            
                
            
            
        </item>
        
        <item>
            <title><![CDATA[CLIP as RNN: Segment Countless Visual Concepts without Training Endeavor]]></title>
            <description><![CDATA[Existing open-vocabulary image segmentation methods require a fine-tuning step on mask annotations and/or image-text datasets. Mask labels are labor-intensive, which limits the number of categories in segmentation datasets. As a result, the open-vocabulary capacity of pre-trained VLMs is severely reduced after fine-tuning. However, without fine-tuning, VLMs trained under weak image-text supervision tend to make suboptimal mask predictions when there are text queries referring to non-existing concepts in the image. To alleviate these issues, we introduce a novel recurrent framework that progressively filters out irrelevant texts and enhances mask quality without training efforts. The recurrent unit is a two-stage segmenter built upon a VLM with frozen weights. Thus, our model retains the VLM's broad vocabulary space and strengthens its segmentation capability. Experimental results show that our method outperforms not only the training-free counterparts, but also those fine-tuned with millions of additional data samples, and sets new state-of-the-art records for both zero-shot semantic and referring image segmentation tasks. Specifically, we improve the current record by 28.8, 16.0, and 6.9 mIoU on Pascal VOC, COCO Object, and Pascal Context.]]></description>
            <pubDate>Thu, 14 Dec 2023 03:10:24 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2312.07661</guid>
            <link>https://arxiv.org/abs/2312.07661</link>
            
            
            
            <author><![CDATA[Shuyang Sun, Runjia Li, Philip Torr, Xiuye Gu, Siyang Li]]></author>
            
                
            
            
        </item>
        
        <item>
            <title><![CDATA[Clockwork Diffusion: Efficient Generation With Model-Step Distillation]]></title>
            <description><![CDATA[This work aims to improve the efficiency of text-to-image diffusion models. While diffusion models use computationally expensive UNet-based denoising operations in every generation step, we identify that not all operations are equally relevant for the final output quality. In particular, we observe that UNet layers operating on high-res feature maps are relatively sensitive to small perturbations. In contrast, low-res feature maps influence the semantic layout of the final image and can often be perturbed with no noticeable change in the output. Based on this observation, we propose Clockwork Diffusion, a method that periodically reuses computation from preceding denoising steps to approximate low-res feature maps at one or more subsequent steps. For multiple baselines, and for both text-to-image generation and image editing, we demonstrate that Clockwork leads to comparable or improved perceptual scores with drastically reduced computational complexity. As an example, for Stable Diffusion v1.5 with 8 DPM++ steps we save 32% of FLOPs with negligible FID and CLIP change.]]></description>
            <pubDate>Thu, 14 Dec 2023 03:03:02 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2312.08128</guid>
            <link>https://arxiv.org/abs/2312.08128</link>
            
            
            
            <author><![CDATA[Amirhossein Habibian, Amir Ghodrati, Noor Fathima, Guillaume Sautiere, Risheek Garrepalli, Fatih Porikli, Jens Petersen]]></author>
            
                
            
            
        </item>
        
        <item>
            <title><![CDATA[Distributed Inference and Fine-tuning of Large Language Models Over The Internet]]></title>
            <description><![CDATA[Large language models (LLMs) are useful in many NLP tasks and become more capable with size, with the best open-source models having over 50 billion parameters. However, using these 50B+ models requires high-end hardware, making them inaccessible to most researchers. In this work, we investigate methods for cost-efficient inference and fine-tuning of LLMs, comparing local and distributed strategies. We observe that a large enough model (50B+) can run efficiently even on geodistributed devices in a consumer-grade network. This could allow running LLM efficiently by pooling together idle compute resources of multiple research groups and volunteers. We address two open problems: (1) how to perform inference and fine-tuning reliably if any device can disconnect abruptly and (2) how to partition LLMs between devices with uneven hardware, joining and leaving at will. In order to do that, we develop special fault-tolerant inference algorithms and load-balancing protocols that automatically assign devices to maximize the total system throughput. We showcase these algorithms in Petals - a decentralized system that runs Llama 2 (70B) and BLOOM (176B) over the Internet up to 10x faster than offloading for interactive generation. We evaluate the performance of our system in simulated conditions and a real-world setup spanning two continents.]]></description>
            <pubDate>Thu, 14 Dec 2023 02:44:48 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2312.08361</guid>
            <link>https://arxiv.org/abs/2312.08361</link>
            
            
            
            <author><![CDATA[Alexander Borzunov, Max Ryabinin, Artem Chumachenko, Dmitry Baranchuk, Tim Dettmers, Younes Belkada, Pavel Samygin, Colin Raffel]]></author>
            
                
            
            
        </item>
        
        <item>
            <title><![CDATA[FoundationPose: Unified 6D Pose Estimation and Tracking of Novel Objects]]></title>
            <description><![CDATA[We present FoundationPose, a unified foundation model for 6D object pose estimation and tracking, supporting both model-based and model-free setups. Our approach can be instantly applied at test-time to a novel object without fine-tuning, as long as its CAD model is given, or a small number of reference images are captured. We bridge the gap between these two setups with a neural implicit representation that allows for effective novel view synthesis, keeping the downstream pose estimation modules invariant under the same unified framework. Strong generalizability is achieved via large-scale synthetic training, aided by a large language model (LLM), a novel transformer-based architecture, and contrastive learning formulation. Extensive evaluation on multiple public datasets involving challenging scenarios and objects indicate our unified approach outperforms existing methods specialized for each task by a large margin. In addition, it even achieves comparable results to instance-level methods despite the reduced assumptions. Project page: https://nvlabs.github.io/FoundationPose/]]></description>
            <pubDate>Thu, 14 Dec 2023 02:37:03 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2312.08344</guid>
            <link>https://arxiv.org/abs/2312.08344</link>
            
            
            
            <author><![CDATA[Bowen Wen, Wei Yang, Jan Kautz, Stan Birchfield]]></author>
            
                
            
            
        </item>
        
        <item>
            <title><![CDATA[SwitchHead: Accelerating Transformers with Mixture-of-Experts Attention]]></title>
            <description><![CDATA[The costly self-attention layers in modern Transformers require memory and compute quadratic in sequence length. Existing approximation methods usually underperform and fail to obtain significant speedups in practice. Here we present SwitchHead - a novel method that reduces both compute and memory requirements and achieves wall-clock speedup, while matching the language modeling performance of baseline Transformers with the same parameter budget. SwitchHead uses Mixture-of-Experts (MoE) layers for the value and output projections and requires 4 to 8 times fewer attention matrices than standard Transformers. Our novel attention can also be combined with MoE MLP layers, resulting in an efficient fully-MoE "SwitchHead" Transformer model. Our code is public.]]></description>
            <pubDate>Thu, 14 Dec 2023 02:29:22 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2312.07987</guid>
            <link>https://arxiv.org/abs/2312.07987</link>
            
            
            
            <author><![CDATA[Róbert Csordás, Piotr Piękos, Kazuki Irie]]></author>
            
                
            
            
        </item>
        
        <item>
            <title><![CDATA[PromptBench: A Unified Library for Evaluation of Large Language Models]]></title>
            <description><![CDATA[The evaluation of large language models (LLMs) is crucial to assess their performance and mitigate potential security risks. In this paper, we introduce PromptBench, a unified library to evaluate LLMs. It consists of several key components that are easily used and extended by researchers: prompt construction, prompt engineering, dataset and model loading, adversarial prompt attack, dynamic evaluation protocols, and analysis tools. PromptBench is designed to be an open, general, and flexible codebase for research purposes that can facilitate original study in creating new benchmarks, deploying downstream applications, and designing new evaluation protocols. The code is available at: https://github.com/microsoft/promptbench and will be continuously supported.]]></description>
            <pubDate>Thu, 14 Dec 2023 02:22:31 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2312.07910</guid>
            <link>https://arxiv.org/abs/2312.07910</link>
            
            
            
            <author><![CDATA[Kaijie Zhu, Qinlin Zhao, Hao Chen, Jindong Wang, Xing Xie]]></author>
            
                
            
            
        </item>
        
        <item>
            <title><![CDATA[Foundation Models in Robotics: Applications, Challenges, and the Future]]></title>
            <description><![CDATA[We survey applications of pretrained foundation models in robotics. Traditional deep learning models in robotics are trained on small datasets tailored for specific tasks, which limits their adaptability across diverse applications. In contrast, foundation models pretrained on internet-scale data appear to have superior generalization capabilities, and in some instances display an emergent ability to find zero-shot solutions to problems that are not present in the training data. Foundation models may hold the potential to enhance various components of the robot autonomy stack, from perception to decision-making and control. For example, large language models can generate code or provide common sense reasoning, while vision-language models enable open-vocabulary visual recognition. However, significant open research challenges remain, particularly around the scarcity of robot-relevant training data, safety guarantees and uncertainty quantification, and real-time execution. In this survey, we study recent papers that have used or built foundation models to solve robotics problems. We explore how foundation models contribute to improving robot capabilities in the domains of perception, decision-making, and control. We discuss the challenges hindering the adoption of foundation models in robot autonomy and provide opportunities and potential pathways for future advancements. The GitHub project corresponding to this paper (Preliminary release. We are committed to further enhancing and updating this work to ensure its quality and relevance) can be found here: https://github.com/robotics-survey/Awesome-Robotics-Foundation-Models]]></description>
            <pubDate>Thu, 14 Dec 2023 02:14:55 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2312.07843</guid>
            <link>https://arxiv.org/abs/2312.07843</link>
            
            
            
            <author><![CDATA[Roya Firoozi, Johnathan Tucker, Stephen Tian, Anirudha Majumdar, Jiankai Sun, Weiyu Liu, Yuke Zhu, Shuran Song, Ashish Kapoor, Karol Hausman, Brian Ichter, Danny Driess, Jiajun Wu, Cewu Lu, Mac Schwager]]></author>
            
                
            
            
        </item>
        
    </channel>
</rss>
