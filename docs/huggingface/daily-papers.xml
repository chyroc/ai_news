<?xml version="1.0" encoding="UTF-8"?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"


>
    <channel>
        <title><![CDATA[Huggingface Daily Papers]]></title>
        <link>https://huggingface.co/papers</link>
        <atom:link href="https://rsshub.app/huggingface/daily-papers" rel="self" type="application/rss+xml" />
        <description><![CDATA[Huggingface Daily Papers - Made with love by RSSHub(https://github.com/DIYgod/RSSHub)]]></description>
        <generator>RSSHub</generator>
        <webMaster>i@diygod.me (DIYgod)</webMaster>
        
        
        
        <language>zh-cn</language>
        
        <lastBuildDate>Fri, 29 Dec 2023 03:24:43 GMT</lastBuildDate>
        <ttl>120</ttl>
        
        <item>
            <title><![CDATA[I2V-Adapter: A General Image-to-Video Adapter for Video Diffusion Models]]></title>
            <description><![CDATA[In the rapidly evolving domain of digital content generation, the focus has shifted from text-to-image (T2I) models to more advanced video diffusion models, notably text-to-video (T2V) and image-to-video (I2V). This paper addresses the intricate challenge posed by I2V: converting static images into dynamic, lifelike video sequences while preserving the original image fidelity. Traditional methods typically involve integrating entire images into diffusion processes or using pretrained encoders for cross attention. However, these approaches often necessitate altering the fundamental weights of T2I models, thereby restricting their reusability. We introduce a novel solution, namely I2V-Adapter, designed to overcome such limitations. Our approach preserves the structural integrity of T2I models and their inherent motion modules. The I2V-Adapter operates by processing noised video frames in parallel with the input image, utilizing a lightweight adapter module. This module acts as a bridge, efficiently linking the input to the model's self-attention mechanism, thus maintaining spatial details without requiring structural changes to the T2I model. Moreover, I2V-Adapter requires only a fraction of the parameters of conventional models and ensures compatibility with existing community-driven T2I models and controlling tools. Our experimental results demonstrate I2V-Adapter's capability to produce high-quality video outputs. This performance, coupled with its versatility and reduced need for trainable parameters, represents a substantial advancement in the field of AI-driven video generation, particularly for creative applications.]]></description>
            <pubDate>Fri, 29 Dec 2023 03:20:55 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2312.16693</guid>
            <link>https://arxiv.org/abs/2312.16693</link>
            
            
            
            <author><![CDATA[Xun Guo, Mingwu Zheng, Liang Hou, Yuan Gao, Yufan Deng, Chongyang Ma, Weiming Hu, Zhengjun Zha, Haibin Huang, Pengfei Wan, Di Zhang]]></author>
            
                
            
            
        </item>
        
        <item>
            <title><![CDATA[The LLM Surgeon]]></title>
            <description><![CDATA[State-of-the-art language models are becoming increasingly large in an effort to achieve the highest performance on large corpora of available textual data. However, the sheer size of the Transformer architectures makes it difficult to deploy models within computational, environmental or device-specific constraints. We explore data-driven compression of existing pretrained models as an alternative to training smaller models from scratch. To do so, we scale Kronecker-factored curvature approximations of the target loss landscape to large language models. In doing so, we can compute both the dynamic allocation of structures that can be removed as well as updates of remaining weights that account for the removal. We provide a general framework for unstructured, semi-structured and structured pruning and improve upon weight updates to capture more correlations between weights, while remaining computationally efficient. Experimentally, our method can prune rows and columns from a range of OPT models and Llamav2-7B by 20%-30%, with a negligible loss in performance, and achieve state-of-the-art results in unstructured and semi-structured pruning of large language models.]]></description>
            <pubDate>Fri, 29 Dec 2023 03:14:15 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2312.17244</guid>
            <link>https://arxiv.org/abs/2312.17244</link>
            
            
            
            <author><![CDATA[Tycho F. A. van der Ouderaa, Markus Nagel, Mart van Baalen, Yuki M. Asano, Tijmen Blankevoort]]></author>
            
                
            
            
        </item>
        
        <item>
            <title><![CDATA[Generative AI for Math: Part I -- MathPile: A Billion-Token-Scale Pretraining Corpus for Math]]></title>
            <description><![CDATA[High-quality, large-scale corpora are the cornerstone of building foundation models. In this work, we introduce MathPile, a diverse and high-quality math-centric corpus comprising about 9.5 billion tokens. Throughout its creation, we adhered to the principle of ``less is more'', firmly believing in the supremacy of data quality over quantity, even in the pre-training phase. Our meticulous data collection and processing efforts included a complex suite of preprocessing, prefiltering, language identification, cleaning, filtering, and deduplication, ensuring the high quality of our corpus. Furthermore, we performed data contamination detection on downstream benchmark test sets to eliminate duplicates. We hope our MathPile can help to enhance the mathematical reasoning abilities of language models. We plan to open-source different versions of \mathpile with the scripts used for processing, to facilitate future developments in this field.]]></description>
            <pubDate>Fri, 29 Dec 2023 03:11:06 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2312.17120</guid>
            <link>https://arxiv.org/abs/2312.17120</link>
            
            
            
            <author><![CDATA[Zengzhi Wang, Rui Xia, Pengfei Liu]]></author>
            
                
            
            
        </item>
        
        <item>
            <title><![CDATA[MobileVLM : A Fast, Reproducible and Strong Vision Language Assistant for Mobile Devices]]></title>
            <description><![CDATA[We present MobileVLM, a competent multimodal vision language model (MMVLM) targeted to run on mobile devices. It is an amalgamation of a myriad of architectural designs and techniques that are mobile-oriented, which comprises a set of language models at the scale of 1.4B and 2.7B parameters, trained from scratch, a multimodal vision model that is pre-trained in the CLIP fashion, cross-modality interaction via an efficient projector. We evaluate MobileVLM on several typical VLM benchmarks. Our models demonstrate on par performance compared with a few much larger models. More importantly, we measure the inference speed on both a Qualcomm Snapdragon 888 CPU and an NVIDIA Jeston Orin GPU, and we obtain state-of-the-art performance of 21.5 tokens and 65.3 tokens per second, respectively. Our code will be made available at: https://github.com/Meituan-AutoML/MobileVLM.]]></description>
            <pubDate>Fri, 29 Dec 2023 03:02:22 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2312.16886</guid>
            <link>https://arxiv.org/abs/2312.16886</link>
            
            
            
            <author><![CDATA[Xiangxiang Chu, Limeng Qiao, Xinyang Lin, Shuang Xu, Yang Yang, Yiming Hu, Fei Wei, Xinyu Zhang, Bo Zhang, Xiaolin Wei, Chunhua Shen]]></author>
            
                
            
            
        </item>
        
        <item>
            <title><![CDATA[TinyGPT-V: Efficient Multimodal Large Language Model via Small Backbones]]></title>
            <description><![CDATA[In the era of advanced multimodel learning, multimodal large language models (MLLMs) such as GPT-4V have made remarkable strides towards bridging language and visual elements. However, the closed-source nature and considerable computational demand present notable challenges for universal usage and modifications. This is where open-source MLLMs like LLaVA and MiniGPT-4 come in, presenting groundbreaking achievements across tasks. Despite these accomplishments, computational efficiency remains an unresolved issue, as these models, like LLaVA-v1.5-13B, require substantial resources. Addressing these issues, we introduce TinyGPT-V, a new-wave model marrying impressive performance with commonplace computational capacity. It stands out by requiring merely a 24G GPU for training and an 8G GPU or CPU for inference. Built upon Phi-2, TinyGPT-V couples an effective language backbone with pre-trained vision modules from BLIP-2 or CLIP. TinyGPT-V's 2.8B parameters can undergo a unique quantisation process, suitable for local deployment and inference tasks on 8G various devices. Our work fosters further developments for designing cost-effective, efficient, and high-performing MLLMs, expanding their applicability in a broad array of real-world scenarios. Furthermore this paper proposed a new paradigm of Multimodal Large Language Model via small backbones. Our code and training weights are placed at: https://github.com/DLYuanGod/TinyGPT-V and https://huggingface.co/Tyrannosaurus/TinyGPT-V respectively.]]></description>
            <pubDate>Fri, 29 Dec 2023 02:32:13 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2312.16862</guid>
            <link>https://arxiv.org/abs/2312.16862</link>
            
            
            
            <author><![CDATA[Zhengqing Yuan, Zhaoxu Li, Lichao Sun]]></author>
            
                
            
            
        </item>
        
    </channel>
</rss>
