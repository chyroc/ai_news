<?xml version="1.0" encoding="UTF-8"?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"


>
    <channel>
        <title><![CDATA[Huggingface Daily Papers]]></title>
        <link>https://huggingface.co/papers</link>
        <atom:link href="https://rsshub.app/huggingface/daily-papers" rel="self" type="application/rss+xml" />
        <description><![CDATA[Huggingface Daily Papers - Made with love by RSSHub(https://github.com/DIYgod/RSSHub)]]></description>
        <generator>RSSHub</generator>
        <webMaster>i@diygod.me (DIYgod)</webMaster>
        
        
        
        <language>zh-cn</language>
        
        <lastBuildDate>Fri, 26 Jan 2024 04:22:59 GMT</lastBuildDate>
        <ttl>120</ttl>
        
        <item>
            <title><![CDATA[Rethinking Patch Dependence for Masked Autoencoders]]></title>
            <description><![CDATA[In this work, we re-examine inter-patch dependencies in the decoding mechanism of masked autoencoders (MAE). We decompose this decoding mechanism for masked patch reconstruction in MAE into self-attention and cross-attention. Our investigations suggest that self-attention between mask patches is not essential for learning good representations. To this end, we propose a novel pretraining framework: Cross-Attention Masked Autoencoders (CrossMAE). CrossMAE's decoder leverages only cross-attention between masked and visible tokens, with no degradation in downstream performance. This design also enables decoding only a small subset of mask tokens, boosting efficiency. Furthermore, each decoder block can now leverage different encoder features, resulting in improved representation learning. CrossMAE matches MAE in performance with 2.5 to 3.7times less decoding compute. It also surpasses MAE on ImageNet classification and COCO instance segmentation under the same compute. Code and models: https://crossmae.github.io]]></description>
            <pubDate>Fri, 26 Jan 2024 04:09:12 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2401.14391</guid>
            <link>https://arxiv.org/abs/2401.14391</link>
            
            
            
            <author><![CDATA[Letian Fu, Long Lian, Renhao Wang, Baifeng Shi, Xudong Wang, Adam Yala, Trevor Darrell, Alexei A. Efros, Ken Goldberg]]></author>
            
                
            
            
        </item>
        
        <item>
            <title><![CDATA[Genie: Achieving Human Parity in Content-Grounded Datasets Generation]]></title>
            <description><![CDATA[The lack of high-quality data for content-grounded generation tasks has been identified as a major obstacle to advancing these tasks. To address this gap, we propose Genie, a novel method for automatically generating high-quality content-grounded data. It consists of three stages: (a) Content Preparation, (b) Generation: creating task-specific examples from the content (e.g., question-answer pairs or summaries). (c) Filtering mechanism aiming to ensure the quality and faithfulness of the generated data. We showcase this methodology by generating three large-scale synthetic data, making wishes, for Long-Form Question-Answering (LFQA), summarization, and information extraction. In a human evaluation, our generated data was found to be natural and of high quality. Furthermore, we compare models trained on our data with models trained on human-written data -- ELI5 and ASQA for LFQA and CNN-DailyMail for Summarization. We show that our models are on par with or outperforming models trained on human-generated data and consistently outperforming them in faithfulness. Finally, we applied our method to create LFQA data within the medical domain and compared a model trained on it with models trained on other domains.]]></description>
            <pubDate>Fri, 26 Jan 2024 04:06:13 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2401.14367</guid>
            <link>https://arxiv.org/abs/2401.14367</link>
            
            
            
            <author><![CDATA[Asaf Yehudai, Boaz Carmeli, Yosi Mass, Ofir Arviv, Nathaniel Mills, Assaf Toledo, Eyal Shnarch, Leshem Choshen]]></author>
            
                
            
            
        </item>
        
        <item>
            <title><![CDATA[Deconstructing Denoising Diffusion Models for Self-Supervised Learning]]></title>
            <description><![CDATA[In this study, we examine the representation learning abilities of Denoising Diffusion Models (DDM) that were originally purposed for image generation. Our philosophy is to deconstruct a DDM, gradually transforming it into a classical Denoising Autoencoder (DAE). This deconstructive procedure allows us to explore how various components of modern DDMs influence self-supervised representation learning. We observe that only a very few modern components are critical for learning good representations, while many others are nonessential. Our study ultimately arrives at an approach that is highly simplified and to a large extent resembles a classical DAE. We hope our study will rekindle interest in a family of classical methods within the realm of modern self-supervised learning.]]></description>
            <pubDate>Fri, 26 Jan 2024 03:09:23 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2401.14404</guid>
            <link>https://arxiv.org/abs/2401.14404</link>
            
            
            
            <author><![CDATA[Xinlei Chen, Zhuang Liu, Saining Xie, Kaiming He]]></author>
            
                
            
            
        </item>
        
        <item>
            <title><![CDATA[DeepSeek-Coder: When the Large Language Model Meets Programming -- The Rise of Code Intelligence]]></title>
            <description><![CDATA[The rapid development of large language models has revolutionized code intelligence in software development. However, the predominance of closed-source models has restricted extensive research and development. To address this, we introduce the DeepSeek-Coder series, a range of open-source code models with sizes from 1.3B to 33B, trained from scratch on 2 trillion tokens. These models are pre-trained on a high-quality project-level code corpus and employ a fill-in-the-blank task with a 16K window to enhance code generation and infilling. Our extensive evaluations demonstrate that DeepSeek-Coder not only achieves state-of-the-art performance among open-source code models across multiple benchmarks but also surpasses existing closed-source models like Codex and GPT-3.5. Furthermore, DeepSeek-Coder models are under a permissive license that allows for both research and unrestricted commercial use.]]></description>
            <pubDate>Fri, 26 Jan 2024 03:05:28 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2401.14196</guid>
            <link>https://arxiv.org/abs/2401.14196</link>
            
            
            
            <author><![CDATA[Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang, Guanting Chen, Xiao Bi, Y. Wu, Y. K. Li, Fuli Luo, Yingfei Xiong, Wenfeng Liang]]></author>
            
                
            
            
        </item>
        
        <item>
            <title><![CDATA[FP6-LLM: Efficiently Serving Large Language Models Through FP6-Centric Algorithm-System Co-Design]]></title>
            <description><![CDATA[Six-bit quantization (FP6) can effectively reduce the size of large language models (LLMs) and preserve the model quality consistently across varied applications. However, existing systems do not provide Tensor Core support for FP6 quantization and struggle to achieve practical performance improvements during LLM inference. It is challenging to support FP6 quantization on GPUs due to (1) unfriendly memory access of model weights with irregular bit-width and (2) high runtime overhead of weight de-quantization. To address these problems, we propose TC-FPx, the first full-stack GPU kernel design scheme with unified Tensor Core support of float-point weights for various quantization bit-width. We integrate TC-FPx kernel into an existing inference system, providing new end-to-end support (called FP6-LLM) for quantized LLM inference, where better trade-offs between inference cost and model quality are achieved. Experiments show that FP6-LLM enables the inference of LLaMA-70b using only a single GPU, achieving 1.69x-2.65x higher normalized inference throughput than the FP16 baseline. The source code will be publicly available soon.]]></description>
            <pubDate>Fri, 26 Jan 2024 02:57:20 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2401.14112</guid>
            <link>https://arxiv.org/abs/2401.14112</link>
            
            
            
            <author><![CDATA[Haojun Xia, Zhen Zheng, Xiaoxia Wu, Shiyang Chen, Zhewei Yao, Stephen Youn, Arash Bakhtiari, Michael Wyatt, Donglin Zhuang, Zhongzhu Zhou, Olatunji Ruwase, Yuxiong He, Shuaiwen Leon Song]]></author>
            
                
            
            
        </item>
        
        <item>
            <title><![CDATA[Unitxt: Flexible, Shareable and Reusable Data Preparation and Evaluation for Generative AI]]></title>
            <description><![CDATA[In the dynamic landscape of generative NLP, traditional text processing pipelines limit research flexibility and reproducibility, as they are tailored to specific dataset, task, and model combinations. The escalating complexity, involving system prompts, model-specific formats, instructions, and more, calls for a shift to a structured, modular, and customizable solution. Addressing this need, we present Unitxt, an innovative library for customizable textual data preparation and evaluation tailored to generative language models. Unitxt natively integrates with common libraries like HuggingFace and LM-eval-harness and deconstructs processing flows into modular components, enabling easy customization and sharing between practitioners. These components encompass model-specific formats, task prompts, and many other comprehensive dataset processing definitions. The Unitxt-Catalog centralizes these components, fostering collaboration and exploration in modern textual data workflows. Beyond being a tool, Unitxt is a community-driven platform, empowering users to build, share, and advance their pipelines collaboratively. Join the Unitxt community at https://github.com/IBM/unitxt!]]></description>
            <pubDate>Fri, 26 Jan 2024 02:36:18 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2401.14019</guid>
            <link>https://arxiv.org/abs/2401.14019</link>
            
            
            
            <author><![CDATA[Elron Bandel, Yotam Perlitz, Elad Venezian, Roni Friedman-Melamed, Ofir Arviv, Matan Orbach, Shachar Don-Yehyia, Dafna Sheinwald, Ariel Gera, Leshem Choshen, Michal Shmueli-Scheuer, Yoav Katz]]></author>
            
                
            
            
        </item>
        
        <item>
            <title><![CDATA[WebVoyager: Building an End-to-End Web Agent with Large Multimodal Models]]></title>
            <description><![CDATA[The advancement of large language models (LLMs) leads to a new era marked by the development of autonomous applications in the real world, which drives innovation in the creation of advanced web-based agents. Existing web agents typically only handle one input modality and are evaluated only in simplified web simulators or static web snapshots, greatly limiting their applicability in real-world scenarios. To bridge this gap, we introduce WebVoyager, an innovative Large Multimodal Model (LMM) powered web agent that can complete user instructions end-to-end by interacting with real-world websites. Moreover, we propose a new evaluation protocol for web agents to address the challenges of automatic evaluation of open-ended web agent tasks, leveraging the robust multimodal comprehension capabilities of GPT-4V. We create a new benchmark by gathering real-world tasks from 15 widely used websites to evaluate our agents. We show that WebVoyager achieves a 55.7% task success rate, significantly surpassing the performance of both GPT-4 (All Tools) and the WebVoyager (text-only) setups, underscoring the exceptional capability of WebVoyager in practical applications. We found that our proposed automatic evaluation achieves 85.3% agreement with human judgment, paving the way for further development of web agents in a real-world setting.]]></description>
            <pubDate>Fri, 26 Jan 2024 02:28:38 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2401.13919</guid>
            <link>https://arxiv.org/abs/2401.13919</link>
            
            
            
            <author><![CDATA[Hongliang He, Wenlin Yao, Kaixin Ma, Wenhao Yu, Yong Dai, Hongming Zhang, Zhenzhong Lan, Dong Yu]]></author>
            
                
            
            
        </item>
        
        <item>
            <title><![CDATA[Sketch2NeRF: Multi-view Sketch-guided Text-to-3D Generation]]></title>
            <description><![CDATA[Recently, text-to-3D approaches have achieved high-fidelity 3D content generation using text description. However, the generated objects are stochastic and lack fine-grained control. Sketches provide a cheap approach to introduce such fine-grained control. Nevertheless, it is challenging to achieve flexible control from these sketches due to their abstraction and ambiguity. In this paper, we present a multi-view sketch-guided text-to-3D generation framework (namely, Sketch2NeRF) to add sketch control to 3D generation. Specifically, our method leverages pretrained 2D diffusion models (e.g., Stable Diffusion and ControlNet) to supervise the optimization of a 3D scene represented by a neural radiance field (NeRF). We propose a novel synchronized generation and reconstruction method to effectively optimize the NeRF. In the experiments, we collected two kinds of multi-view sketch datasets to evaluate the proposed method. We demonstrate that our method can synthesize 3D consistent contents with fine-grained sketch control while being high-fidelity to text prompts. Extensive results show that our method achieves state-of-the-art performance in terms of sketch similarity and text alignment.]]></description>
            <pubDate>Fri, 26 Jan 2024 02:21:11 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2401.14257</guid>
            <link>https://arxiv.org/abs/2401.14257</link>
            
            
            
            <author><![CDATA[Minglin Chen, Longguang Wang, Weihao Yuan, Yukun Wang, Zhe Sheng, Yisheng He, Zilong Dong, Liefeng Bo, Yulan Guo]]></author>
            
                
            
            
        </item>
        
        <item>
            <title><![CDATA[CreativeSynth: Creative Blending and Synthesis of Visual Arts based on Multimodal Diffusion]]></title>
            <description><![CDATA[Large-scale text-to-image generative models have made impressive strides, showcasing their ability to synthesize a vast array of high-quality images. However, adapting these models for artistic image editing presents two significant challenges. Firstly, users struggle to craft textual prompts that meticulously detail visual elements of the input image. Secondly, prevalent models, when effecting modifications in specific zones, frequently disrupt the overall artistic style, complicating the attainment of cohesive and aesthetically unified artworks. To surmount these obstacles, we build the innovative unified framework CreativeSynth, which is based on a diffusion model with the ability to coordinate multimodal inputs and multitask in the field of artistic image generation. By integrating multimodal features with customized attention mechanisms, CreativeSynth facilitates the importation of real-world semantic content into the domain of art through inversion and real-time style transfer. This allows for the precise manipulation of image style and content while maintaining the integrity of the original model parameters. Rigorous qualitative and quantitative evaluations underscore that CreativeSynth excels in enhancing artistic images' fidelity and preserves their innate aesthetic essence. By bridging the gap between generative models and artistic finesse, CreativeSynth becomes a custom digital palette.]]></description>
            <pubDate>Fri, 26 Jan 2024 02:19:04 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2401.14066</guid>
            <link>https://arxiv.org/abs/2401.14066</link>
            
            
            
            <author><![CDATA[Nisha Huang, Weiming Dong, Yuxin Zhang, Fan Tang, Ronghui Li, Chongyang Ma, Xiu Li, Changsheng Xu]]></author>
            
                
            
            
        </item>
        
        <item>
            <title><![CDATA[BootPIG: Bootstrapping Zero-shot Personalized Image Generation Capabilities in Pretrained Diffusion Models]]></title>
            <description><![CDATA[Recent text-to-image generation models have demonstrated incredible success in generating images that faithfully follow input prompts. However, the requirement of using words to describe a desired concept provides limited control over the appearance of the generated concepts. In this work, we address this shortcoming by proposing an approach to enable personalization capabilities in existing text-to-image diffusion models. We propose a novel architecture (BootPIG) that allows a user to provide reference images of an object in order to guide the appearance of a concept in the generated images.   The proposed BootPIG architecture makes minimal modifications to a pretrained text-to-image diffusion model and utilizes a separate UNet model to steer the generations toward the desired appearance. We introduce a training procedure that allows us to bootstrap personalization capabilities in the BootPIG architecture using data generated from pretrained text-to-image models, LLM chat agents, and image segmentation models. In contrast to existing methods that require several days of pretraining, the BootPIG architecture can be trained in approximately 1 hour. Experiments on the DreamBooth dataset demonstrate that BootPIG outperforms existing zero-shot methods while being comparable with test-time finetuning approaches. Through a user study, we validate the preference for BootPIG generations over existing methods both in maintaining fidelity to the reference object's appearance and aligning with textual prompts.]]></description>
            <pubDate>Fri, 26 Jan 2024 02:10:34 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2401.13974</guid>
            <link>https://arxiv.org/abs/2401.13974</link>
            
            
            
            <author><![CDATA[Senthil Purushwalkam, Akash Gokul, Shafiq Joty, Nikhil Naik]]></author>
            
                
            
            
        </item>
        
    </channel>
</rss>
