<?xml version="1.0" encoding="UTF-8"?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"


>
    <channel>
        <title><![CDATA[Huggingface Daily Papers]]></title>
        <link>https://huggingface.co/papers</link>
        <atom:link href="https://rsshub.app/huggingface/daily-papers" rel="self" type="application/rss+xml" />
        <description><![CDATA[Huggingface Daily Papers - Made with love by RSSHub(https://github.com/DIYgod/RSSHub)]]></description>
        <generator>RSSHub</generator>
        <webMaster>i@diygod.me (DIYgod)</webMaster>
        
        
        
        <language>zh-cn</language>
        
        <lastBuildDate>Tue, 05 Dec 2023 03:05:20 GMT</lastBuildDate>
        <ttl>120</ttl>
        
        <item>
            <title><![CDATA[VMC: Video Motion Customization using Temporal Attention Adaption for Text-to-Video Diffusion Models]]></title>
            <description><![CDATA[Text-to-video diffusion models have advanced video generation significantly. However, customizing these models to generate videos with tailored motions presents a substantial challenge. In specific, they encounter hurdles in (a) accurately reproducing motion from a target video, and (b) creating diverse visual variations. For example, straightforward extensions of static image customization methods to video often lead to intricate entanglements of appearance and motion data. To tackle this, here we present the Video Motion Customization (VMC) framework, a novel one-shot tuning approach crafted to adapt temporal attention layers within video diffusion models. Our approach introduces a novel motion distillation objective using residual vectors between consecutive frames as a motion reference. The diffusion process then preserves low-frequency motion trajectories while mitigating high-frequency motion-unrelated noise in image space. We validate our method against state-of-the-art video generative models across diverse real-world motions and contexts. Our codes, data and the project demo can be found at https://video-motion-customization.github.io]]></description>
            <pubDate>Tue, 05 Dec 2023 02:54:14 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2312.00845</guid>
            <link>https://arxiv.org/abs/2312.00845</link>
            
            
            
            <author><![CDATA[Hyeonho Jeong, Geon Yeong Park, Jong Chul Ye]]></author>
            
                
            
            
        </item>
        
        <item>
            <title><![CDATA[DiffiT: Diffusion Vision Transformers for Image Generation]]></title>
            <description><![CDATA[Diffusion models with their powerful expressivity and high sample quality have enabled many new applications and use-cases in various domains. For sample generation, these models rely on a denoising neural network that generates images by iterative denoising. Yet, the role of denoising network architecture is not well-studied with most efforts relying on convolutional residual U-Nets. In this paper, we study the effectiveness of vision transformers in diffusion-based generative learning. Specifically, we propose a new model, denoted as Diffusion Vision Transformers (DiffiT), which consists of a hybrid hierarchical architecture with a U-shaped encoder and decoder. We introduce a novel time-dependent self-attention module that allows attention layers to adapt their behavior at different stages of the denoising process in an efficient manner. We also introduce latent DiffiT which consists of transformer model with the proposed self-attention layers, for high-resolution image generation. Our results show that DiffiT is surprisingly effective in generating high-fidelity images, and it achieves state-of-the-art (SOTA) benchmarks on a variety of class-conditional and unconditional synthesis tasks. In the latent space, DiffiT achieves a new SOTA FID score of 1.73 on ImageNet-256 dataset. Repository: https://github.com/NVlabs/DiffiT]]></description>
            <pubDate>Tue, 05 Dec 2023 02:47:33 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2312.02139</guid>
            <link>https://arxiv.org/abs/2312.02139</link>
            
            
            
            <author><![CDATA[Ali Hatamizadeh, Jiaming Song, Guilin Liu, Jan Kautz, Arash Vahdat]]></author>
            
                
            
            
        </item>
        
        <item>
            <title><![CDATA[Style Aligned Image Generation via Shared Attention]]></title>
            <description><![CDATA[Large-scale Text-to-Image (T2I) models have rapidly gained prominence across creative fields, generating visually compelling outputs from textual prompts. However, controlling these models to ensure consistent style remains challenging, with existing methods necessitating fine-tuning and manual intervention to disentangle content and style. In this paper, we introduce StyleAligned, a novel technique designed to establish style alignment among a series of generated images. By employing minimal `attention sharing' during the diffusion process, our method maintains style consistency across images within T2I models. This approach allows for the creation of style-consistent images using a reference style through a straightforward inversion operation. Our method's evaluation across diverse styles and text prompts demonstrates high-quality synthesis and fidelity, underscoring its efficacy in achieving consistent style across various inputs.]]></description>
            <pubDate>Tue, 05 Dec 2023 02:45:08 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2312.02133</guid>
            <link>https://arxiv.org/abs/2312.02133</link>
            
            
            
            <author><![CDATA[Amir Hertz, Andrey Voynov, Shlomi Fruchter, Daniel Cohen-Or]]></author>
            
                
            
            
        </item>
        
        <item>
            <title><![CDATA[GIVT: Generative Infinite-Vocabulary Transformers]]></title>
            <description><![CDATA[We introduce generative infinite-vocabulary transformers (GIVT) which generate vector sequences with real-valued entries, instead of discrete tokens from a finite vocabulary. To this end, we propose two surprisingly simple modifications to decoder-only transformers: 1) at the input, we replace the finite-vocabulary lookup table with a linear projection of the input vectors; and 2) at the output, we replace the logits prediction (usually mapped to a categorical distribution) with the parameters of a multivariate Gaussian mixture model. Inspired by the image-generation paradigm of VQ-GAN and MaskGIT, where transformers are used to model the discrete latent sequences of a VQ-VAE, we use GIVT to model the unquantized real-valued latent sequences of a VAE. When applying GIVT to class-conditional image generation with iterative masked modeling, we show competitive results with MaskGIT, while our approach outperforms both VQ-GAN and MaskGIT when using it for causal modeling. Finally, we obtain competitive results outside of image generation when applying our approach to panoptic segmentation and depth estimation with a VAE-based variant of the UViM framework.]]></description>
            <pubDate>Tue, 05 Dec 2023 02:42:50 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2312.02116</guid>
            <link>https://arxiv.org/abs/2312.02116</link>
            
            
            
            <author><![CDATA[Michael Tschannen, Cian Eastwood, Fabian Mentzer]]></author>
            
                
            
            
        </item>
        
        <item>
            <title><![CDATA[Generative Rendering: Controllable 4D-Guided Video Generation with 2D Diffusion Models]]></title>
            <description><![CDATA[Traditional 3D content creation tools empower users to bring their imagination to life by giving them direct control over a scene's geometry, appearance, motion, and camera path. Creating computer-generated videos, however, is a tedious manual process, which can be automated by emerging text-to-video diffusion models. Despite great promise, video diffusion models are difficult to control, hindering a user to apply their own creativity rather than amplifying it. To address this challenge, we present a novel approach that combines the controllability of dynamic 3D meshes with the expressivity and editability of emerging diffusion models. For this purpose, our approach takes an animated, low-fidelity rendered mesh as input and injects the ground truth correspondence information obtained from the dynamic mesh into various stages of a pre-trained text-to-image generation model to output high-quality and temporally consistent frames. We demonstrate our approach on various examples where motion can be obtained by animating rigged assets or changing the camera path.]]></description>
            <pubDate>Tue, 05 Dec 2023 02:40:03 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2312.01409</guid>
            <link>https://arxiv.org/abs/2312.01409</link>
            
            
            
            <author><![CDATA[Shengqu Cai, Duygu Ceylan, Matheus Gadelha, Chun-Hao Paul Huang, Tuanfeng Yang Wang, Gordon Wetzstein]]></author>
            
                
            
            
        </item>
        
        <item>
            <title><![CDATA[Magicoder: Source Code Is All You Need]]></title>
            <description><![CDATA[We introduce Magicoder, a series of fully open-source (code, weights, and data) Large Language Models (LLMs) for code that significantly closes the gap with top code models while having no more than 7B parameters. Magicoder models are trained on 75K synthetic instruction data using OSS-Instruct, a novel approach to enlightening LLMs with open-source code snippets to generate high-quality instruction data for code. Our main motivation is to mitigate the inherent bias of the synthetic data generated by LLMs by empowering them with a wealth of open-source references for the production of more diverse, realistic, and controllable data. The orthogonality of OSS-Instruct and other data generation methods like Evol-Instruct further enables us to build an enhanced MagicoderS. Both Magicoder and MagicoderS substantially outperform state-of-the-art code models with similar or even larger sizes on a wide range of coding benchmarks, including Python text-to-code generation, multilingual coding, and data-science program completion. Notably, MagicoderS-CL-7B based on CodeLlama even surpasses the prominent ChatGPT on HumanEval+ (66.5 vs. 65.9 in pass@1). Overall, OSS-Instruct opens a new direction for low-bias and high-quality instruction tuning using abundant open-source references.]]></description>
            <pubDate>Tue, 05 Dec 2023 02:37:08 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2312.02120</guid>
            <link>https://arxiv.org/abs/2312.02120</link>
            
            
            
            <author><![CDATA[Yuxiang Wei, Zhe Wang, Jiawei Liu, Yifeng Ding, Lingming Zhang]]></author>
            
                
            
            
        </item>
        
        <item>
            <title><![CDATA[RLHF-V: Towards Trustworthy MLLMs via Behavior Alignment from Fine-grained Correctional Human Feedback]]></title>
            <description><![CDATA[Multimodal Large Language Models (MLLMs) have recently demonstrated impressive capabilities in multimodal understanding, reasoning, and interaction. However, existing MLLMs prevalently suffer from serious hallucination problems, generating text that is not factually grounded in associated images. The problem makes existing MLLMs untrustworthy and thus impractical in real-world (especially high-stakes) applications. To address the challenge, we present RLHF-V, which enhances MLLM trustworthiness via behavior alignment from fine-grained correctional human feedback. Specifically, RLHF-V collects human preference in the form of segment-level corrections on hallucinations, and performs dense direct preference optimization over the human feedback. Comprehensive experiments on five benchmarks in both automatic and human evaluation show that, RLHF-V can enable substantially more trustworthy MLLM behaviors with promising data and computation efficiency. Remarkably, using 1.4k annotated data samples, RLHF-V significantly reduces the hallucination rate of the base MLLM by 34.8%, outperforming the concurrent LLaVA-RLHF trained on 10k annotated data. The final model achieves state-of-the-art performance in trustworthiness among open-source MLLMs, and shows better robustness than GPT-4V in preventing hallucinations aroused from over-generalization. We open-source our code, model, and data at https://github.com/RLHF-V/RLHF-V.]]></description>
            <pubDate>Tue, 05 Dec 2023 02:27:25 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2312.00849</guid>
            <link>https://arxiv.org/abs/2312.00849</link>
            
            
            
            <author><![CDATA[Tianyu Yu, Yuan Yao, Haoye Zhang, Taiwen He, Yifeng Han, Ganqu Cui, Jinyi Hu, Zhiyuan Liu, Hai-Tao Zheng, Maosong Sun, Tat-Seng Chua]]></author>
            
                
            
            
        </item>
        
    </channel>
</rss>
