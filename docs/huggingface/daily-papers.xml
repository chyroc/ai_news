<?xml version="1.0" encoding="UTF-8"?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"


>
    <channel>
        <title><![CDATA[Huggingface Daily Papers]]></title>
        <link>https://huggingface.co/papers</link>
        <atom:link href="https://rsshub.app/huggingface/daily-papers" rel="self" type="application/rss+xml" />
        <description><![CDATA[Huggingface Daily Papers - Made with love by RSSHub(https://github.com/DIYgod/RSSHub)]]></description>
        <generator>RSSHub</generator>
        <webMaster>i@diygod.me (DIYgod)</webMaster>
        
        
        
        <language>zh-cn</language>
        
        <lastBuildDate>Thu, 14 Dec 2023 03:05:29 GMT</lastBuildDate>
        <ttl>120</ttl>
        
        <item>
            <title><![CDATA[Clockwork Diffusion: Efficient Generation With Model-Step Distillation]]></title>
            <description><![CDATA[This work aims to improve the efficiency of text-to-image diffusion models. While diffusion models use computationally expensive UNet-based denoising operations in every generation step, we identify that not all operations are equally relevant for the final output quality. In particular, we observe that UNet layers operating on high-res feature maps are relatively sensitive to small perturbations. In contrast, low-res feature maps influence the semantic layout of the final image and can often be perturbed with no noticeable change in the output. Based on this observation, we propose Clockwork Diffusion, a method that periodically reuses computation from preceding denoising steps to approximate low-res feature maps at one or more subsequent steps. For multiple baselines, and for both text-to-image generation and image editing, we demonstrate that Clockwork leads to comparable or improved perceptual scores with drastically reduced computational complexity. As an example, for Stable Diffusion v1.5 with 8 DPM++ steps we save 32% of FLOPs with negligible FID and CLIP change.]]></description>
            <pubDate>Thu, 14 Dec 2023 03:03:02 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2312.08128</guid>
            <link>https://arxiv.org/abs/2312.08128</link>
            
            
            
            <author><![CDATA[Amirhossein Habibian, Amir Ghodrati, Noor Fathima, Guillaume Sautiere, Risheek Garrepalli, Fatih Porikli, Jens Petersen]]></author>
            
                
            
            
        </item>
        
        <item>
            <title><![CDATA[Distributed Inference and Fine-tuning of Large Language Models Over The Internet]]></title>
            <description><![CDATA[Large language models (LLMs) are useful in many NLP tasks and become more capable with size, with the best open-source models having over 50 billion parameters. However, using these 50B+ models requires high-end hardware, making them inaccessible to most researchers. In this work, we investigate methods for cost-efficient inference and fine-tuning of LLMs, comparing local and distributed strategies. We observe that a large enough model (50B+) can run efficiently even on geodistributed devices in a consumer-grade network. This could allow running LLM efficiently by pooling together idle compute resources of multiple research groups and volunteers. We address two open problems: (1) how to perform inference and fine-tuning reliably if any device can disconnect abruptly and (2) how to partition LLMs between devices with uneven hardware, joining and leaving at will. In order to do that, we develop special fault-tolerant inference algorithms and load-balancing protocols that automatically assign devices to maximize the total system throughput. We showcase these algorithms in Petals - a decentralized system that runs Llama 2 (70B) and BLOOM (176B) over the Internet up to 10x faster than offloading for interactive generation. We evaluate the performance of our system in simulated conditions and a real-world setup spanning two continents.]]></description>
            <pubDate>Thu, 14 Dec 2023 02:44:48 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2312.08361</guid>
            <link>https://arxiv.org/abs/2312.08361</link>
            
            
            
            <author><![CDATA[Alexander Borzunov, Max Ryabinin, Artem Chumachenko, Dmitry Baranchuk, Tim Dettmers, Younes Belkada, Pavel Samygin, Colin Raffel]]></author>
            
                
            
            
        </item>
        
        <item>
            <title><![CDATA[FoundationPose: Unified 6D Pose Estimation and Tracking of Novel Objects]]></title>
            <description><![CDATA[We present FoundationPose, a unified foundation model for 6D object pose estimation and tracking, supporting both model-based and model-free setups. Our approach can be instantly applied at test-time to a novel object without fine-tuning, as long as its CAD model is given, or a small number of reference images are captured. We bridge the gap between these two setups with a neural implicit representation that allows for effective novel view synthesis, keeping the downstream pose estimation modules invariant under the same unified framework. Strong generalizability is achieved via large-scale synthetic training, aided by a large language model (LLM), a novel transformer-based architecture, and contrastive learning formulation. Extensive evaluation on multiple public datasets involving challenging scenarios and objects indicate our unified approach outperforms existing methods specialized for each task by a large margin. In addition, it even achieves comparable results to instance-level methods despite the reduced assumptions. Project page: https://nvlabs.github.io/FoundationPose/]]></description>
            <pubDate>Thu, 14 Dec 2023 02:37:03 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2312.08344</guid>
            <link>https://arxiv.org/abs/2312.08344</link>
            
            
            
            <author><![CDATA[Bowen Wen, Wei Yang, Jan Kautz, Stan Birchfield]]></author>
            
                
            
            
        </item>
        
        <item>
            <title><![CDATA[SwitchHead: Accelerating Transformers with Mixture-of-Experts Attention]]></title>
            <description><![CDATA[The costly self-attention layers in modern Transformers require memory and compute quadratic in sequence length. Existing approximation methods usually underperform and fail to obtain significant speedups in practice. Here we present SwitchHead - a novel method that reduces both compute and memory requirements and achieves wall-clock speedup, while matching the language modeling performance of baseline Transformers with the same parameter budget. SwitchHead uses Mixture-of-Experts (MoE) layers for the value and output projections and requires 4 to 8 times fewer attention matrices than standard Transformers. Our novel attention can also be combined with MoE MLP layers, resulting in an efficient fully-MoE "SwitchHead" Transformer model. Our code is public.]]></description>
            <pubDate>Thu, 14 Dec 2023 02:29:22 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2312.07987</guid>
            <link>https://arxiv.org/abs/2312.07987</link>
            
            
            
            <author><![CDATA[Róbert Csordás, Piotr Piękos, Kazuki Irie]]></author>
            
                
            
            
        </item>
        
        <item>
            <title><![CDATA[PromptBench: A Unified Library for Evaluation of Large Language Models]]></title>
            <description><![CDATA[The evaluation of large language models (LLMs) is crucial to assess their performance and mitigate potential security risks. In this paper, we introduce PromptBench, a unified library to evaluate LLMs. It consists of several key components that are easily used and extended by researchers: prompt construction, prompt engineering, dataset and model loading, adversarial prompt attack, dynamic evaluation protocols, and analysis tools. PromptBench is designed to be an open, general, and flexible codebase for research purposes that can facilitate original study in creating new benchmarks, deploying downstream applications, and designing new evaluation protocols. The code is available at: https://github.com/microsoft/promptbench and will be continuously supported.]]></description>
            <pubDate>Thu, 14 Dec 2023 02:22:31 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2312.07910</guid>
            <link>https://arxiv.org/abs/2312.07910</link>
            
            
            
            <author><![CDATA[Kaijie Zhu, Qinlin Zhao, Hao Chen, Jindong Wang, Xing Xie]]></author>
            
                
            
            
        </item>
        
        <item>
            <title><![CDATA[Foundation Models in Robotics: Applications, Challenges, and the Future]]></title>
            <description><![CDATA[We survey applications of pretrained foundation models in robotics. Traditional deep learning models in robotics are trained on small datasets tailored for specific tasks, which limits their adaptability across diverse applications. In contrast, foundation models pretrained on internet-scale data appear to have superior generalization capabilities, and in some instances display an emergent ability to find zero-shot solutions to problems that are not present in the training data. Foundation models may hold the potential to enhance various components of the robot autonomy stack, from perception to decision-making and control. For example, large language models can generate code or provide common sense reasoning, while vision-language models enable open-vocabulary visual recognition. However, significant open research challenges remain, particularly around the scarcity of robot-relevant training data, safety guarantees and uncertainty quantification, and real-time execution. In this survey, we study recent papers that have used or built foundation models to solve robotics problems. We explore how foundation models contribute to improving robot capabilities in the domains of perception, decision-making, and control. We discuss the challenges hindering the adoption of foundation models in robot autonomy and provide opportunities and potential pathways for future advancements. The GitHub project corresponding to this paper (Preliminary release. We are committed to further enhancing and updating this work to ensure its quality and relevance) can be found here: https://github.com/robotics-survey/Awesome-Robotics-Foundation-Models]]></description>
            <pubDate>Thu, 14 Dec 2023 02:14:55 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2312.07843</guid>
            <link>https://arxiv.org/abs/2312.07843</link>
            
            
            
            <author><![CDATA[Roya Firoozi, Johnathan Tucker, Stephen Tian, Anirudha Majumdar, Jiankai Sun, Weiyu Liu, Yuke Zhu, Shuran Song, Ashish Kapoor, Karol Hausman, Brian Ichter, Danny Driess, Jiajun Wu, Cewu Lu, Mac Schwager]]></author>
            
                
            
            
        </item>
        
    </channel>
</rss>
