<?xml version="1.0" encoding="UTF-8"?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"


>
    <channel>
        <title><![CDATA[Huggingface Daily Papers]]></title>
        <link>https://huggingface.co/papers</link>
        <atom:link href="https://rsshub.app/huggingface/daily-papers" rel="self" type="application/rss+xml" />
        <description><![CDATA[Huggingface Daily Papers - Made with love by RSSHub(https://github.com/DIYgod/RSSHub)]]></description>
        <generator>RSSHub</generator>
        <webMaster>i@diygod.me (DIYgod)</webMaster>
        
        
        
        <language>zh-cn</language>
        
        <lastBuildDate>Mon, 22 Jan 2024 02:37:17 GMT</lastBuildDate>
        <ttl>120</ttl>
        
        <item>
            <title><![CDATA[Medusa: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads]]></title>
            <description><![CDATA[The inference process in Large Language Models (LLMs) is often limited due to the absence of parallelism in the auto-regressive decoding process, resulting in most operations being restricted by the memory bandwidth of accelerators. While methods such as speculative decoding have been suggested to address this issue, their implementation is impeded by the challenges associated with acquiring and maintaining a separate draft model. In this paper, we present Medusa, an efficient method that augments LLM inference by adding extra decoding heads to predict multiple subsequent tokens in parallel. Using a tree-based attention mechanism, Medusa constructs multiple candidate continuations and verifies them simultaneously in each decoding step. By leveraging parallel processing, Medusa introduces only minimal overhead in terms of single-step latency while substantially reducing the number of decoding steps required.   We present two levels of fine-tuning procedures for Medusa to meet the needs of different use cases: Medusa-1: Medusa is directly fine-tuned on top of a frozen backbone LLM, enabling lossless inference acceleration. Medusa-2: Medusa is fine-tuned together with the backbone LLM, enabling better prediction accuracy of Medusa heads and higher speedup but needing a special training recipe that preserves the backbone model's capabilities.   Moreover, we propose several extensions that improve or expand the utility of Medusa, including a self-distillation to handle situations where no training data is available and a typical acceptance scheme to boost the acceptance rate while maintaining generation quality. We evaluate Medusa on models of various sizes and training procedures. Our experiments demonstrate that Medusa-1 can achieve over 2.2x speedup without compromising generation quality, while Medusa-2 further improves the speedup to 2.3-3.6x.]]></description>
            <pubDate>Mon, 22 Jan 2024 02:27:22 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2401.10774</guid>
            <link>https://arxiv.org/abs/2401.10774</link>
            
            
            
            <author><![CDATA[Tianle Cai, Yuhong Li, Zhengyang Geng, Hongwu Peng, Jason D. Lee, Deming Chen, Tri Dao]]></author>
            
                
            
            
        </item>
        
        <item>
            <title><![CDATA[Inflation with Diffusion: Efficient Temporal Adaptation for Text-to-Video Super-Resolution]]></title>
            <description><![CDATA[We propose an efficient diffusion-based text-to-video super-resolution (SR) tuning approach that leverages the readily learned capacity of pixel level image diffusion model to capture spatial information for video generation. To accomplish this goal, we design an efficient architecture by inflating the weightings of the text-to-image SR model into our video generation framework. Additionally, we incorporate a temporal adapter to ensure temporal coherence across video frames. We investigate different tuning approaches based on our inflated architecture and report trade-offs between computational costs and super-resolution quality. Empirical evaluation, both quantitative and qualitative, on the Shutterstock video dataset, demonstrates that our approach is able to perform text-to-video SR generation with good visual quality and temporal consistency. To evaluate temporal coherence, we also present visualizations in video format in https://drive.google.com/drive/folders/1YVc-KMSJqOrEUdQWVaI-Yfu8Vsfu_1aO?usp=sharing .]]></description>
            <pubDate>Mon, 22 Jan 2024 02:14:35 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2401.10404</guid>
            <link>https://arxiv.org/abs/2401.10404</link>
            
            
            
            <author><![CDATA[Xin Yuan, Jinoo Baek, Keyang Xu, Omer Tov, Hongliang Fei]]></author>
            
                
            
            
        </item>
        
    </channel>
</rss>
