<?xml version="1.0" encoding="UTF-8"?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"


>
    <channel>
        <title><![CDATA[Huggingface Daily Papers]]></title>
        <link>https://huggingface.co/papers</link>
        <atom:link href="https://rsshub.app/huggingface/daily-papers" rel="self" type="application/rss+xml" />
        <description><![CDATA[Huggingface Daily Papers - Made with love by RSSHub(https://github.com/DIYgod/RSSHub)]]></description>
        <generator>RSSHub</generator>
        <webMaster>i@diygod.me (DIYgod)</webMaster>
        
        
        
        <language>zh-cn</language>
        
        <lastBuildDate>Sun, 07 Jan 2024 12:28:06 GMT</lastBuildDate>
        <ttl>120</ttl>
        
        <item>
            <title><![CDATA[Learning the 3D Fauna of the Web]]></title>
            <description><![CDATA[Learning 3D models of all animals on the Earth requires massively scaling up existing solutions. With this ultimate goal in mind, we develop 3D-Fauna, an approach that learns a pan-category deformable 3D animal model for more than 100 animal species jointly. One crucial bottleneck of modeling animals is the limited availability of training data, which we overcome by simply learning from 2D Internet images. We show that prior category-specific attempts fail to generalize to rare species with limited training images. We address this challenge by introducing the Semantic Bank of Skinned Models (SBSM), which automatically discovers a small set of base animal shapes by combining geometric inductive priors with semantic knowledge implicitly captured by an off-the-shelf self-supervised feature extractor. To train such a model, we also contribute a new large-scale dataset of diverse animal species. At inference time, given a single image of any quadruped animal, our model reconstructs an articulated 3D mesh in a feed-forward fashion within seconds.]]></description>
            <pubDate>Fri, 05 Jan 2024 06:03:30 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2401.02400</guid>
            <link>https://arxiv.org/abs/2401.02400</link>
            
            
            
            <author><![CDATA[Zizhang Li, Dor Litvak, Ruining Li, Yunzhi Zhang, Tomas Jakab, Christian Rupprecht, Shangzhe Wu, Andrea Vedaldi, Jiajun Wu]]></author>
            
                
            
            
        </item>
        
        <item>
            <title><![CDATA[Mobile ALOHA: Learning Bimanual Mobile Manipulation with Low-Cost Whole-Body Teleoperation]]></title>
            <description><![CDATA[Imitation learning from human demonstrations has shown impressive performance in robotics. However, most results focus on table-top manipulation, lacking the mobility and dexterity necessary for generally useful tasks. In this work, we develop a system for imitating mobile manipulation tasks that are bimanual and require whole-body control. We first present Mobile ALOHA, a low-cost and whole-body teleoperation system for data collection. It augments the ALOHA system with a mobile base, and a whole-body teleoperation interface. Using data collected with Mobile ALOHA, we then perform supervised behavior cloning and find that co-training with existing static ALOHA datasets boosts performance on mobile manipulation tasks. With 50 demonstrations for each task, co-training can increase success rates by up to 90%, allowing Mobile ALOHA to autonomously complete complex mobile manipulation tasks such as sauteing and serving a piece of shrimp, opening a two-door wall cabinet to store heavy cooking pots, calling and entering an elevator, and lightly rinsing a used pan using a kitchen faucet. Project website: https://mobile-aloha.github.io]]></description>
            <pubDate>Fri, 05 Jan 2024 05:59:50 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2401.02117</guid>
            <link>https://arxiv.org/abs/2401.02117</link>
            
            
            
            <author><![CDATA[Zipeng Fu, Tony Z. Zhao, Chelsea Finn]]></author>
            
                
            
            
        </item>
        
        <item>
            <title><![CDATA[Improving Diffusion-Based Image Synthesis with Context Prediction]]></title>
            <description><![CDATA[Diffusion models are a new class of generative models, and have dramatically promoted image generation with unprecedented quality and diversity. Existing diffusion models mainly try to reconstruct input image from a corrupted one with a pixel-wise or feature-wise constraint along spatial axes. However, such point-based reconstruction may fail to make each predicted pixel/feature fully preserve its neighborhood context, impairing diffusion-based image synthesis. As a powerful source of automatic supervisory signal, context has been well studied for learning representations. Inspired by this, we for the first time propose ConPreDiff to improve diffusion-based image synthesis with context prediction. We explicitly reinforce each point to predict its neighborhood context (i.e., multi-stride features/tokens/pixels) with a context decoder at the end of diffusion denoising blocks in training stage, and remove the decoder for inference. In this way, each point can better reconstruct itself by preserving its semantic connections with neighborhood context. This new paradigm of ConPreDiff can generalize to arbitrary discrete and continuous diffusion backbones without introducing extra parameters in sampling procedure. Extensive experiments are conducted on unconditional image generation, text-to-image generation and image inpainting tasks. Our ConPreDiff consistently outperforms previous methods and achieves a new SOTA text-to-image generation results on MS-COCO, with a zero-shot FID score of 6.21.]]></description>
            <pubDate>Fri, 05 Jan 2024 05:50:57 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2401.02015</guid>
            <link>https://arxiv.org/abs/2401.02015</link>
            
            
            
            <author><![CDATA[Ling Yang, Jingwei Liu, Shenda Hong, Zhilong Zhang, Zhilin Huang, Zheming Cai, Wentao Zhang, Bin Cui]]></author>
            
                
            
            
        </item>
        
        <item>
            <title><![CDATA[LLaMA Pro: Progressive LLaMA with Block Expansion]]></title>
            <description><![CDATA[Humans generally acquire new skills without compromising the old; however, the opposite holds for Large Language Models (LLMs), e.g., from LLaMA to CodeLLaMA. To this end, we propose a new post-pretraining method for LLMs with an expansion of Transformer blocks. We tune the expanded blocks using only new corpus, efficiently and effectively improving the model's knowledge without catastrophic forgetting. In this paper, we experiment on the corpus of code and math, yielding LLaMA Pro-8.3B, a versatile foundation model initialized from LLaMA2-7B, excelling in general tasks, programming, and mathematics. LLaMA Pro and its instruction-following counterpart (LLaMA Pro-Instruct) achieve advanced performance among various benchmarks, demonstrating superiority over existing open models in the LLaMA family and the immense potential of reasoning and addressing diverse tasks as an intelligent agent. Our findings provide valuable insights into integrating natural and programming languages, laying a solid foundation for developing advanced language agents that operate effectively in various environments.]]></description>
            <pubDate>Fri, 05 Jan 2024 05:01:27 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2401.02415</guid>
            <link>https://arxiv.org/abs/2401.02415</link>
            
            
            
            <author><![CDATA[Chengyue Wu, Yukang Gan, Yixiao Ge, Zeyu Lu, Jiahao Wang, Ye Feng, Ping Luo, Ying Shan]]></author>
            
                
            
            
        </item>
        
        <item>
            <title><![CDATA[LLaVA-$Ï†$: Efficient Multi-Modal Assistant with Small Language Model]]></title>
            <description><![CDATA[In this paper, we introduce LLaVA-phi (LLaVA-Phi), an efficient multi-modal assistant that harnesses the power of the recently advanced small language model, Phi-2, to facilitate multi-modal dialogues. LLaVA-Phi marks a notable advancement in the realm of compact multi-modal models. It demonstrates that even smaller language models, with as few as 2.7B parameters, can effectively engage in intricate dialogues that integrate both textual and visual elements, provided they are trained with high-quality corpora. Our model delivers commendable performance on publicly available benchmarks that encompass visual comprehension, reasoning, and knowledge-based perception. Beyond its remarkable performance in multi-modal dialogue tasks, our model opens new avenues for applications in time-sensitive environments and systems that require real-time interaction, such as embodied agents. It highlights the potential of smaller language models to achieve sophisticated levels of understanding and interaction, while maintaining greater resource efficiency.The project is available at {https://github.com/zhuyiche/llava-phi}.]]></description>
            <pubDate>Fri, 05 Jan 2024 04:57:04 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2401.02330</guid>
            <link>https://arxiv.org/abs/2401.02330</link>
            
            
            
            <author><![CDATA[Yichen Zhu, Minjie Zhu, Ning Liu, Zhicai Ou, Xiaofeng Mou, Jian Tang]]></author>
            
                
            
            
        </item>
        
        <item>
            <title><![CDATA[Towards Truly Zero-shot Compositional Visual Reasoning with LLMs as Programmers]]></title>
            <description><![CDATA[Visual reasoning is dominated by end-to-end neural networks scaled to billions of model parameters and training examples. However, even the largest models struggle with compositional reasoning, generalization, fine-grained spatial and temporal reasoning, and counting. Visual reasoning with large language models (LLMs) as controllers can, in principle, address these limitations by decomposing the task and solving subtasks by orchestrating a set of (visual) tools. Recently, these models achieved great performance on tasks such as compositional visual question answering, visual grounding, and video temporal reasoning. Nevertheless, in their current form, these models heavily rely on human engineering of in-context examples in the prompt, which are often dataset- and task-specific and require significant labor by highly skilled programmers. In this work, we present a framework that mitigates these issues by introducing spatially and temporally abstract routines and by leveraging a small number of labeled examples to automatically generate in-context examples, thereby avoiding human-created in-context examples. On a number of visual reasoning tasks, we show that our framework leads to consistent gains in performance, makes LLMs as controllers setup more robust, and removes the need for human engineering of in-context examples.]]></description>
            <pubDate>Fri, 05 Jan 2024 04:08:40 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2401.01974</guid>
            <link>https://arxiv.org/abs/2401.01974</link>
            
            
            
            <author><![CDATA[Aleksandar StaniÄ‡, Sergi Caelles, Michael Tschannen]]></author>
            
                
            
            
        </item>
        
        <item>
            <title><![CDATA[FMGS: Foundation Model Embedded 3D Gaussian Splatting for Holistic 3D Scene Understanding]]></title>
            <description><![CDATA[Precisely perceiving the geometric and semantic properties of real-world 3D objects is crucial for the continued evolution of augmented reality and robotic applications. To this end, we present  (), which incorporates vision-language embeddings of foundation models into 3D Gaussian Splatting (GS). The key contribution of this work is an efficient method to reconstruct and represent 3D vision-language models. This is achieved by distilling feature maps generated from image-based foundation models into those rendered from our 3D model. To ensure high-quality rendering and fast training, we introduce a novel scene representation by integrating strengths from both GS and multi-resolution hash encodings (MHE). Our effective training procedure also introduces a pixel alignment loss that makes the rendered feature distance of same semantic entities close, following the pixel-level semantic boundaries. Our results demonstrate remarkable multi-view semantic consistency, facilitating diverse downstream tasks, beating state-of-the-art methods by 10.2 percent on open-vocabulary language-based object detection, despite that we are 851times faster for inference. This research explores the intersection of vision, language, and 3D scene representation, paving the way for enhanced scene understanding in uncontrolled real-world environments. We plan to release the code upon paper acceptance.]]></description>
            <pubDate>Fri, 05 Jan 2024 04:05:55 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2401.01970</guid>
            <link>https://arxiv.org/abs/2401.01970</link>
            
            
            
            <author><![CDATA[Xingxing Zuo, Pouya Samangouei, Yunwen Zhou, Yan Di, Mingyang Li]]></author>
            
                
            
            
        </item>
        
        <item>
            <title><![CDATA[TinyLlama: An Open-Source Small Language Model]]></title>
            <description><![CDATA[We present TinyLlama, a compact 1.1B language model pretrained on around 1 trillion tokens for approximately 3 epochs. Building on the architecture and tokenizer of Llama 2, TinyLlama leverages various advances contributed by the open-source community (e.g., FlashAttention), achieving better computational efficiency. Despite its relatively small size, TinyLlama demonstrates remarkable performance in a series of downstream tasks. It significantly outperforms existing open-source language models with comparable sizes. Our model checkpoints and code are publicly available on GitHub at https://github.com/jzhang38/TinyLlama.]]></description>
            <pubDate>Fri, 05 Jan 2024 04:02:04 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2401.02385</guid>
            <link>https://arxiv.org/abs/2401.02385</link>
            
            
            
            <author><![CDATA[Peiyuan Zhang, Guangtao Zeng, Tianduo Wang, Wei Lu]]></author>
            
                
            
            
        </item>
        
        <item>
            <title><![CDATA[What You See is What You GAN: Rendering Every Pixel for High-Fidelity Geometry in 3D GANs]]></title>
            <description><![CDATA[3D-aware Generative Adversarial Networks (GANs) have shown remarkable progress in learning to generate multi-view-consistent images and 3D geometries of scenes from collections of 2D images via neural volume rendering. Yet, the significant memory and computational costs of dense sampling in volume rendering have forced 3D GANs to adopt patch-based training or employ low-resolution rendering with post-processing 2D super resolution, which sacrifices multiview consistency and the quality of resolved geometry. Consequently, 3D GANs have not yet been able to fully resolve the rich 3D geometry present in 2D images. In this work, we propose techniques to scale neural volume rendering to the much higher resolution of native 2D images, thereby resolving fine-grained 3D geometry with unprecedented detail. Our approach employs learning-based samplers for accelerating neural rendering for 3D GAN training using up to 5 times fewer depth samples. This enables us to explicitly "render every pixel" of the full-resolution image during training and inference without post-processing superresolution in 2D. Together with our strategy to learn high-quality surface geometry, our method synthesizes high-resolution 3D geometry and strictly view-consistent images while maintaining image quality on par with baselines relying on post-processing super resolution. We demonstrate state-of-the-art 3D gemetric quality on FFHQ and AFHQ, setting a new standard for unsupervised learning of 3D shapes in 3D GANs.]]></description>
            <pubDate>Fri, 05 Jan 2024 03:32:35 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2401.02411</guid>
            <link>https://arxiv.org/abs/2401.02411</link>
            
            
            
            <author><![CDATA[Alex Trevithick, Matthew Chan, Towaki Takikawa, Umar Iqbal, Shalini De Mello, Manmohan Chandraker, Ravi Ramamoorthi, Koki Nagano]]></author>
            
                
            
            
        </item>
        
        <item>
            <title><![CDATA[LLM Augmented LLMs: Expanding Capabilities through Composition]]></title>
            <description><![CDATA[Foundational models with billions of parameters which have been trained on large corpora of data have demonstrated non-trivial skills in a variety of domains. However, due to their monolithic structure, it is challenging and expensive to augment them or impart new skills. On the other hand, due to their adaptation abilities, several new instances of these models are being trained towards new domains and tasks. In this work, we study the problem of efficient and practical composition of existing foundation models with more specific models to enable newer capabilities. To this end, we propose CALM -- Composition to Augment Language Models -- which introduces cross-attention between models to compose their representations and enable new capabilities. Salient features of CALM are: (i) Scales up LLMs on new tasks by 're-using' existing LLMs along with a few additional parameters and data, (ii) Existing model weights are kept intact, and hence preserves existing capabilities, and (iii) Applies to diverse domains and settings. We illustrate that augmenting PaLM2-S with a smaller model trained on low-resource languages results in an absolute improvement of up to 13\% on tasks like translation into English and arithmetic reasoning for low-resource languages. Similarly, when PaLM2-S is augmented with a code-specific model, we see a relative improvement of 40\% over the base model for code generation and explanation tasks -- on-par with fully fine-tuned counterparts.]]></description>
            <pubDate>Fri, 05 Jan 2024 03:21:16 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2401.02412</guid>
            <link>https://arxiv.org/abs/2401.02412</link>
            
            
            
            <author><![CDATA[Rachit Bansal, Bidisha Samanta, Siddharth Dalmia, Nitish Gupta, Shikhar Vashishth, Sriram Ganapathy, Abhishek Bapna, Prateek Jain, Partha Talukdar]]></author>
            
                
            
            
        </item>
        
        <item>
            <title><![CDATA[ODIN: A Single Model for 2D and 3D Perception]]></title>
            <description><![CDATA[State-of-the-art models on contemporary 3D perception benchmarks like ScanNet consume and label dataset-provided 3D point clouds, obtained through post processing of sensed multiview RGB-D images. They are typically trained in-domain, forego large-scale 2D pre-training and outperform alternatives that featurize the posed RGB-D multiview images instead. The gap in performance between methods that consume posed images versus post-processed 3D point clouds has fueled the belief that 2D and 3D perception require distinct model architectures. In this paper, we challenge this view and propose ODIN (Omni-Dimensional INstance segmentation), a model that can segment and label both 2D RGB images and 3D point clouds, using a transformer architecture that alternates between 2D within-view and 3D cross-view information fusion. Our model differentiates 2D and 3D feature operations through the positional encodings of the tokens involved, which capture pixel coordinates for 2D patch tokens and 3D coordinates for 3D feature tokens. ODIN achieves state-of-the-art performance on ScanNet200, Matterport3D and AI2THOR 3D instance segmentation benchmarks, and competitive performance on ScanNet, S3DIS and COCO. It outperforms all previous works by a wide margin when the sensed 3D point cloud is used in place of the point cloud sampled from 3D mesh. When used as the 3D perception engine in an instructable embodied agent architecture, it sets a new state-of-the-art on the TEACh action-from-dialogue benchmark. Our code and checkpoints can be found at the project website: https://odin-seg.github.io.]]></description>
            <pubDate>Fri, 05 Jan 2024 03:17:24 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2401.02416</guid>
            <link>https://arxiv.org/abs/2401.02416</link>
            
            
            
            <author><![CDATA[Ayush Jain, Pushkal Katara, Nikolaos Gkanatsios, Adam W. Harley, Gabriel Sarch, Kriti Aggarwal, Vishrav Chaudhary, Katerina Fragkiadaki]]></author>
            
                
            
            
        </item>
        
        <item>
            <title><![CDATA[Instruct-Imagen: Image Generation with Multi-modal Instruction]]></title>
            <description><![CDATA[This paper presents instruct-imagen, a model that tackles heterogeneous image generation tasks and generalizes across unseen tasks. We introduce *multi-modal instruction* for image generation, a task representation articulating a range of generation intents with precision. It uses natural language to amalgamate disparate modalities (e.g., text, edge, style, subject, etc.), such that abundant generation intents can be standardized in a uniform format.   We then build instruct-imagen by fine-tuning a pre-trained text-to-image diffusion model with a two-stage framework. First, we adapt the model using the retrieval-augmented training, to enhance model's capabilities to ground its generation on external multimodal context. Subsequently, we fine-tune the adapted model on diverse image generation tasks that requires vision-language understanding (e.g., subject-driven generation, etc.), each paired with a multi-modal instruction encapsulating the task's essence. Human evaluation on various image generation datasets reveals that instruct-imagen matches or surpasses prior task-specific models in-domain and demonstrates promising generalization to unseen and more complex tasks.]]></description>
            <pubDate>Fri, 05 Jan 2024 03:09:43 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2401.01952</guid>
            <link>https://arxiv.org/abs/2401.01952</link>
            
            
            
            <author><![CDATA[Hexiang Hu, Kelvin C. K. Chan, Yu-Chuan Su, Wenhu Chen, Yandong Li, Kihyuk Sohn, Yang Zhao, Xue Ben, Boqing Gong, William Cohen, Ming-Wei Chang, Xuhui Jia]]></author>
            
                
            
            
        </item>
        
        <item>
            <title><![CDATA[ICE-GRT: Instruction Context Enhancement by Generative Reinforcement based Transformers]]></title>
            <description><![CDATA[The emergence of Large Language Models (LLMs) such as ChatGPT and LLaMA encounter limitations in domain-specific tasks, with these models often lacking depth and accuracy in specialized areas, and exhibiting a decrease in general capabilities when fine-tuned, particularly analysis ability in small sized models. To address these gaps, we introduce ICE-GRT, utilizing Reinforcement Learning from Human Feedback (RLHF) grounded in Proximal Policy Optimization (PPO), demonstrating remarkable ability in in-domain scenarios without compromising general task performance. Our exploration of ICE-GRT highlights its understanding and reasoning ability to not only generate robust answers but also to provide detailed analyses of the reasons behind the answer. This capability marks a significant progression beyond the scope of Supervised Fine-Tuning models. The success of ICE-GRT is dependent on several crucial factors, including Appropriate Data, Reward Size Scaling, KL-Control, Advantage Normalization, etc. The ICE-GRT model exhibits state-of-the-art performance in domain-specific tasks and across 12 general Language tasks against equivalent size and even larger size LLMs, highlighting the effectiveness of our approach. We provide a comprehensive analysis of the ICE-GRT, underscoring the significant advancements it brings to the field of LLM.]]></description>
            <pubDate>Fri, 05 Jan 2024 03:06:23 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2401.02072</guid>
            <link>https://arxiv.org/abs/2401.02072</link>
            
            
            
            <author><![CDATA[Chen Zheng, Ke Sun, Da Tang, Yukun Ma, Yuyu Zhang, Chenguang Xi, Xun Zhou]]></author>
            
                
            
            
        </item>
        
        <item>
            <title><![CDATA[Understanding LLMs: A Comprehensive Overview from Training to Inference]]></title>
            <description><![CDATA[The introduction of ChatGPT has led to a significant increase in the utilization of Large Language Models (LLMs) for addressing downstream tasks. There's an increasing focus on cost-efficient training and deployment within this context. Low-cost training and deployment of LLMs represent the future development trend. This paper reviews the evolution of large language model training techniques and inference deployment technologies aligned with this emerging trend. The discussion on training includes various aspects, including data preprocessing, training architecture, pre-training tasks, parallel training, and relevant content related to model fine-tuning. On the inference side, the paper covers topics such as model compression, parallel computation, memory scheduling, and structural optimization. It also explores LLMs' utilization and provides insights into their future development.]]></description>
            <pubDate>Fri, 05 Jan 2024 03:00:58 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2401.02038</guid>
            <link>https://arxiv.org/abs/2401.02038</link>
            
            
            
            <author><![CDATA[Yiheng Liu, Hao He, Tianle Han, Xu Zhang, Mengyuan Liu, Jiaming Tian, Yutong Zhang, Jiaqi Wang, Xiaohui Gao, Tianyang Zhong, Yi Pan, Shaochen Xu, Zihao Wu, Zhengliang Liu, Xin Zhang, Shu Zhang, Xintao Hu, Tuo Zhang, Ning Qiang, Tianming Liu, Bao Ge]]></author>
            
                
            
            
        </item>
        
    </channel>
</rss>
