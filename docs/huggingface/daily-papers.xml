<?xml version="1.0" encoding="UTF-8"?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"


>
    <channel>
        <title><![CDATA[Huggingface Daily Papers]]></title>
        <link>https://huggingface.co/papers</link>
        <atom:link href="http://rsshub.app/huggingface/daily-papers" rel="self" type="application/rss+xml" />
        <description><![CDATA[Huggingface Daily Papers - Made with love by RSSHub(https://github.com/DIYgod/RSSHub)]]></description>
        <generator>RSSHub</generator>
        <webMaster>i@diygod.me (DIYgod)</webMaster>
        
        
        
        <language>zh-cn</language>
        
        <lastBuildDate>Wed, 20 Mar 2024 02:55:19 GMT</lastBuildDate>
        <ttl>120</ttl>
        
        <item>
            <title><![CDATA[ComboVerse: Compositional 3D Assets Creation Using Spatially-Aware Diffusion Guidance]]></title>
            <description><![CDATA[Generating high-quality 3D assets from a given image is highly desirable in various applications such as AR/VR. Recent advances in single-image 3D generation explore feed-forward models that learn to infer the 3D model of an object without optimization. Though promising results have been achieved in single object generation, these methods often struggle to model complex 3D assets that inherently contain multiple objects. In this work, we present ComboVerse, a 3D generation framework that produces high-quality 3D assets with complex compositions by learning to combine multiple models. 1) We first perform an in-depth analysis of this ``multi-object gap'' from both model and data perspectives. 2) Next, with reconstructed 3D models of different objects, we seek to adjust their sizes, rotation angles, and locations to create a 3D asset that matches the given image. 3) To automate this process, we apply spatially-aware score distillation sampling (SSDS) from pretrained diffusion models to guide the positioning of objects. Our proposed framework emphasizes spatial alignment of objects, compared with standard score distillation sampling, and thus achieves more accurate results. Extensive experiments validate ComboVerse achieves clear improvements over existing methods in generating compositional 3D assets.]]></description>
            <pubDate>Wed, 20 Mar 2024 02:09:57 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2403.12409</guid>
            <link>https://arxiv.org/abs/2403.12409</link>
            
            
            
            <author><![CDATA[Yongwei Chen, Tengfei Wang, Tong Wu, Xingang Pan, Kui Jia, Ziwei Liu]]></author>
            
                
            
            
        </item>
        
        <item>
            <title><![CDATA[FouriScale: A Frequency Perspective on Training-Free High-Resolution Image Synthesis]]></title>
            <description><![CDATA[In this study, we delve into the generation of high-resolution images from pre-trained diffusion models, addressing persistent challenges, such as repetitive patterns and structural distortions, that emerge when models are applied beyond their trained resolutions. To address this issue, we introduce an innovative, training-free approach FouriScale from the perspective of frequency domain analysis. We replace the original convolutional layers in pre-trained diffusion models by incorporating a dilation technique along with a low-pass operation, intending to achieve structural consistency and scale consistency across resolutions, respectively. Further enhanced by a padding-then-crop strategy, our method can flexibly handle text-to-image generation of various aspect ratios. By using the FouriScale as guidance, our method successfully balances the structural integrity and fidelity of generated images, achieving an astonishing capacity of arbitrary-size, high-resolution, and high-quality generation. With its simplicity and compatibility, our method can provide valuable insights for future explorations into the synthesis of ultra-high-resolution images. The code will be released at https://github.com/LeonHLJ/FouriScale.]]></description>
            <pubDate>Wed, 20 Mar 2024 02:04:23 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2403.12963</guid>
            <link>https://arxiv.org/abs/2403.12963</link>
            
            
            
            <author><![CDATA[Linjiang Huang, Rongyao Fang, Aiping Zhang, Guanglu Song, Si Liu, Yu Liu, Hongsheng Li]]></author>
            
                
            
            
        </item>
        
        <item>
            <title><![CDATA[AnimateDiff-Lightning: Cross-Model Diffusion Distillation]]></title>
            <description><![CDATA[We present AnimateDiff-Lightning for lightning-fast video generation. Our model uses progressive adversarial diffusion distillation to achieve new state-of-the-art in few-step video generation. We discuss our modifications to adapt it for the video modality. Furthermore, we propose to simultaneously distill the probability flow of multiple base diffusion models, resulting in a single distilled motion module with broader style compatibility. We are pleased to release our distilled AnimateDiff-Lightning model for the community's use.]]></description>
            <pubDate>Wed, 20 Mar 2024 01:52:01 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2403.12706</guid>
            <link>https://arxiv.org/abs/2403.12706</link>
            
            
            
            <author><![CDATA[Shanchuan Lin, Xiao Yang]]></author>
            
                
            
            
        </item>
        
    </channel>
</rss>
