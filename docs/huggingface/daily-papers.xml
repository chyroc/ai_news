<?xml version="1.0" encoding="UTF-8"?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"


>
    <channel>
        <title><![CDATA[Huggingface Daily Papers]]></title>
        <link>https://huggingface.co/papers</link>
        <atom:link href="https://rsshub.app/huggingface/daily-papers" rel="self" type="application/rss+xml" />
        <description><![CDATA[Huggingface Daily Papers - Made with love by RSSHub(https://github.com/DIYgod/RSSHub)]]></description>
        <generator>RSSHub</generator>
        <webMaster>i@diygod.me (DIYgod)</webMaster>
        
        
        
        <language>zh-cn</language>
        
        <lastBuildDate>Mon, 05 Feb 2024 05:06:18 GMT</lastBuildDate>
        <ttl>120</ttl>
        
        <item>
            <title><![CDATA[TravelPlanner: A Benchmark for Real-World Planning with Language Agents]]></title>
            <description><![CDATA[Planning has been part of the core pursuit for artificial intelligence since its conception, but earlier AI agents mostly focused on constrained settings because many of the cognitive substrates necessary for human-level planning have been lacking. Recently, language agents powered by large language models (LLMs) have shown interesting capabilities such as tool use and reasoning. Are these language agents capable of planning in more complex settings that are out of the reach of prior AI agents? To advance this investigation, we propose TravelPlanner, a new planning benchmark that focuses on travel planning, a common real-world planning scenario. It provides a rich sandbox environment, various tools for accessing nearly four million data records, and 1,225 meticulously curated planning intents and reference plans. Comprehensive evaluations show that the current language agents are not yet capable of handling such complex planning tasks-even GPT-4 only achieves a success rate of 0.6%. Language agents struggle to stay on task, use the right tools to collect information, or keep track of multiple constraints. However, we note that the mere possibility for language agents to tackle such a complex problem is in itself non-trivial progress. TravelPlanner provides a challenging yet meaningful testbed for future language agents.]]></description>
            <pubDate>Mon, 05 Feb 2024 03:57:28 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2402.01622</guid>
            <link>https://arxiv.org/abs/2402.01622</link>
            
            
            
            <author><![CDATA[Jian Xie, Kai Zhang, Jiangjie Chen, Tinghui Zhu, Renze Lou, Yuandong Tian, Yanghua Xiao, Yu Su]]></author>
            
                
            
            
        </item>
        
        <item>
            <title><![CDATA[K-Level Reasoning with Large Language Models]]></title>
            <description><![CDATA[While Large Language Models (LLMs) have demonstrated their proficiency in complex reasoning tasks, their performance in dynamic, interactive, and competitive scenarios - such as business strategy and stock market analysis - remains underexplored. To bridge this gap, we formally explore the dynamic reasoning capabilities of LLMs for decision-making in rapidly evolving environments. We introduce two game theory-based pilot challenges that mirror the complexities of real-world dynamic decision-making. These challenges are well-defined, enabling clear, controllable, and precise evaluation of LLMs' dynamic reasoning abilities. Through extensive experiments, we find that existing reasoning methods tend to falter in dynamic settings that require k-level thinking - a key concept not tackled by previous works. To address this, we propose a novel reasoning approach for LLMs, named "K-Level Reasoning". This approach adopts the perspective of rivals to recursively employ k-level thinking based on available historical information, which significantly improves the prediction accuracy of rivals' subsequent moves and informs more strategic decision-making. This research not only sets a robust quantitative benchmark for the assessment of dynamic reasoning but also markedly enhances the proficiency of LLMs in dynamic contexts.]]></description>
            <pubDate>Mon, 05 Feb 2024 03:27:48 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2402.01521</guid>
            <link>https://arxiv.org/abs/2402.01521</link>
            
            
            
            <author><![CDATA[Yadong Zhang, Shaoguang Mao, Tao Ge, Xun Wang, Yan Xia, Man Lan, Furu Wei]]></author>
            
                
            
            
        </item>
        
        <item>
            <title><![CDATA[StepCoder: Improve Code Generation with Reinforcement Learning from Compiler Feedback]]></title>
            <description><![CDATA[The advancement of large language models (LLMs) has significantly propelled the field of code generation. Previous work integrated reinforcement learning (RL) with compiler feedback for exploring the output space of LLMs to enhance code generation quality. However, the lengthy code generated by LLMs in response to complex human requirements makes RL exploration a challenge. Also, since the unit tests may not cover the complicated code, optimizing LLMs by using these unexecuted code snippets is ineffective. To tackle these challenges, we introduce StepCoder, a novel RL framework for code generation, consisting of two main components: CCCS addresses the exploration challenge by breaking the long sequences code generation task into a Curriculum of Code Completion Subtasks, while FGO only optimizes the model by masking the unexecuted code segments to provide Fine-Grained Optimization. In addition, we furthermore construct the APPS+ dataset for RL training, which is manually verified to ensure the correctness of unit tests. Experimental results show that our method improves the ability to explore the output space and outperforms state-of-the-art approaches in corresponding benchmarks.]]></description>
            <pubDate>Mon, 05 Feb 2024 03:03:18 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2402.01391</guid>
            <link>https://arxiv.org/abs/2402.01391</link>
            
            
            
            <author><![CDATA[Shihan Dou, Yan Liu, Haoxiang Jia, Limao Xiong, Enyu Zhou, Junjie Shan, Caishuang Huang, Wei Shen, Xiaoran Fan, Zhiheng Xi, Yuhao Zhou, Tao Ji, Rui Zheng, Qi Zhang, Xuanjing Huang, Tao Gui]]></author>
            
                
            
            
        </item>
        
        <item>
            <title><![CDATA[PokéLLMon: A Human-Parity Agent for Pokémon Battles with Large Language Models]]></title>
            <description><![CDATA[We introduce Pok\'eLLMon, the first LLM-embodied agent that achieves human-parity performance in tactical battle games, as demonstrated in Pok\'emon battles. The design of Pok\'eLLMon incorporates three key strategies: (i) In-context reinforcement learning that instantly consumes text-based feedback derived from battles to iteratively refine the policy; (ii) Knowledge-augmented generation that retrieves external knowledge to counteract hallucination and enables the agent to act timely and properly; (iii) Consistent action generation to mitigate the panic switching phenomenon when the agent faces a powerful opponent and wants to elude the battle. We show that online battles against human demonstrates Pok\'eLLMon's human-like battle strategies and just-in-time decision making, achieving 49\% of win rate in the Ladder competitions and 56\% of win rate in the invited battles. Our implementation and playable battle logs are available at: https://github.com/git-disl/PokeLLMon.]]></description>
            <pubDate>Mon, 05 Feb 2024 02:49:37 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2402.01118</guid>
            <link>https://arxiv.org/abs/2402.01118</link>
            
            
            
            <author><![CDATA[Sihao Hu, Tiansheng Huang, Ling Liu]]></author>
            
                
            
            
        </item>
        
        <item>
            <title><![CDATA[Specialized Language Models with Cheap Inference from Limited Domain Data]]></title>
            <description><![CDATA[Large language models have emerged as a versatile tool but are challenging to apply to tasks lacking large inference budgets and large in-domain training sets. This work formalizes these constraints and distinguishes four important variables: the pretraining budget (for training before the target domain is known), the specialization budget (for training after the target domain is known), the inference budget, and the in-domain training set size. Across these settings, we compare different approaches from the machine learning literature. Limited by inference cost, we find better alternatives to the standard practice of training very large vanilla transformer models. In particular, we show that hyper-networks and mixture of experts have better perplexity for large pretraining budgets, while small models trained on importance sampled datasets are attractive for large specialization budgets.]]></description>
            <pubDate>Mon, 05 Feb 2024 02:43:43 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2402.01093</guid>
            <link>https://arxiv.org/abs/2402.01093</link>
            
            
            
            <author><![CDATA[David Grangier, Angelos Katharopoulos, Pierre Ablin, Awni Hannun]]></author>
            
                
            
            
        </item>
        
        <item>
            <title><![CDATA[Repeat After Me: Transformers are Better than State Space Models at Copying]]></title>
            <description><![CDATA[Transformers are the dominant architecture for sequence modeling, but there is growing interest in models that use a fixed-size latent state that does not depend on the sequence length, which we refer to as "generalized state space models" (GSSMs). In this paper we show that while GSSMs are promising in terms of inference-time efficiency, they are limited compared to transformer models on tasks that require copying from the input context. We start with a theoretical analysis of the simple task of string copying and prove that a two layer transformer can copy strings of exponential length while GSSMs are fundamentally limited by their fixed-size latent state. Empirically, we find that transformers outperform GSSMs in terms of efficiency and generalization on synthetic tasks that require copying the context. Finally, we evaluate pretrained large language models and find that transformer models dramatically outperform state space models at copying and retrieving information from context. Taken together, these results suggest a fundamental gap between transformers and GSSMs on tasks of practical interest.]]></description>
            <pubDate>Mon, 05 Feb 2024 02:42:02 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2402.01032</guid>
            <link>https://arxiv.org/abs/2402.01032</link>
            
            
            
            <author><![CDATA[Samy Jelassi, David Brandfonbrener, Sham M. Kakade, Eran Malach]]></author>
            
                
            
            
        </item>
        
        <item>
            <title><![CDATA[EVA-GAN: Enhanced Various Audio Generation via Scalable Generative Adversarial Networks]]></title>
            <description><![CDATA[The advent of Large Models marks a new era in machine learning, significantly outperforming smaller models by leveraging vast datasets to capture and synthesize complex patterns. Despite these advancements, the exploration into scaling, especially in the audio generation domain, remains limited, with previous efforts didn't extend into the high-fidelity (HiFi) 44.1kHz domain and suffering from both spectral discontinuities and blurriness in the high-frequency domain, alongside a lack of robustness against out-of-domain data. These limitations restrict the applicability of models to diverse use cases, including music and singing generation. Our work introduces Enhanced Various Audio Generation via Scalable Generative Adversarial Networks (EVA-GAN), yields significant improvements over previous state-of-the-art in spectral and high-frequency reconstruction and robustness in out-of-domain data performance, enabling the generation of HiFi audios by employing an extensive dataset of 36,000 hours of 44.1kHz audio, a context-aware module, a Human-In-The-Loop artifact measurement toolkit, and expands the model to approximately 200 million parameters. Demonstrations of our work are available at https://double-blind-eva-gan.cc.]]></description>
            <pubDate>Mon, 05 Feb 2024 02:36:42 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2402.00892</guid>
            <link>https://arxiv.org/abs/2402.00892</link>
            
            
            
            <author><![CDATA[Shijia Liao, Shiyi Lan, Arun George Zachariah]]></author>
            
                
            
            
        </item>
        
    </channel>
</rss>
