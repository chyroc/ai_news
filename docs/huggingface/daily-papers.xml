<?xml version="1.0" encoding="UTF-8"?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"


>
    <channel>
        <title><![CDATA[Huggingface Daily Papers]]></title>
        <link>https://huggingface.co/papers</link>
        <atom:link href="https://rsshub.app/huggingface/daily-papers" rel="self" type="application/rss+xml" />
        <description><![CDATA[Huggingface Daily Papers - Made with love by RSSHub(https://github.com/DIYgod/RSSHub)]]></description>
        <generator>RSSHub</generator>
        <webMaster>i@diygod.me (DIYgod)</webMaster>
        
        
        
        <language>zh-cn</language>
        
        <lastBuildDate>Mon, 11 Dec 2023 17:24:13 GMT</lastBuildDate>
        <ttl>120</ttl>
        
        <item>
            <title><![CDATA[Text-to-3D Generation with Bidirectional Diffusion using both 2D and 3D priors]]></title>
            <description><![CDATA[Most 3D generation research focuses on up-projecting 2D foundation models into the 3D space, either by minimizing 2D Score Distillation Sampling (SDS) loss or fine-tuning on multi-view datasets. Without explicit 3D priors, these methods often lead to geometric anomalies and multi-view inconsistency. Recently, researchers have attempted to improve the genuineness of 3D objects by directly training on 3D datasets, albeit at the cost of low-quality texture generation due to the limited texture diversity in 3D datasets. To harness the advantages of both approaches, we propose Bidirectional Diffusion(BiDiff), a unified framework that incorporates both a 3D and a 2D diffusion process, to preserve both 3D fidelity and 2D texture richness, respectively. Moreover, as a simple combination may yield inconsistent generation results, we further bridge them with novel bidirectional guidance. In addition, our method can be used as an initialization of optimization-based models to further improve the quality of 3D model and efficiency of optimization, reducing the generation process from 3.4 hours to 20 minutes. Experimental results have shown that our model achieves high-quality, diverse, and scalable 3D generation. Project website: https://bidiff.github.io/.]]></description>
            <pubDate>Mon, 11 Dec 2023 03:28:31 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2312.04963</guid>
            <link>https://arxiv.org/abs/2312.04963</link>
            
            
            
            <author><![CDATA[Lihe Ding, Shaocong Dong, Zhanpeng Huang, Zibin Wang, Yiyuan Zhang, Kaixiong Gong, Dan Xu, Tianfan Xue]]></author>
            
                
            
            
        </item>
        
        <item>
            <title><![CDATA[MVDD: Multi-View Depth Diffusion Models]]></title>
            <description><![CDATA[Denoising diffusion models have demonstrated outstanding results in 2D image generation, yet it remains a challenge to replicate its success in 3D shape generation. In this paper, we propose leveraging multi-view depth, which represents complex 3D shapes in a 2D data format that is easy to denoise. We pair this representation with a diffusion model, MVDD, that is capable of generating high-quality dense point clouds with 20K+ points with fine-grained details. To enforce 3D consistency in multi-view depth, we introduce an epipolar line segment attention that conditions the denoising step for a view on its neighboring views. Additionally, a depth fusion module is incorporated into diffusion steps to further ensure the alignment of depth maps. When augmented with surface reconstruction, MVDD can also produce high-quality 3D meshes. Furthermore, MVDD stands out in other tasks such as depth completion, and can serve as a 3D prior, significantly boosting many downstream tasks, such as GAN inversion. State-of-the-art results from extensive experiments demonstrate MVDD's excellent ability in 3D shape generation, depth completion, and its potential as a 3D prior for downstream tasks.]]></description>
            <pubDate>Mon, 11 Dec 2023 03:21:59 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2312.04875</guid>
            <link>https://arxiv.org/abs/2312.04875</link>
            
            
            
            <author><![CDATA[Zhen Wang, Qiangeng Xu, Feitong Tan, Menglei Chai, Shichen Liu, Rohit Pandey, Sean Fanello, Achuta Kadambi, Yinda Zhang]]></author>
            
                
            
            
        </item>
        
        <item>
            <title><![CDATA[DreaMoving: A Human Dance Video Generation Framework based on Diffusion Models]]></title>
            <description><![CDATA[In this paper, we present DreaMoving, a diffusion-based controllable video generation framework to produce high-quality customized human dance videos. Specifically, given target identity and posture sequences, DreaMoving can generate a video of the target identity dancing anywhere driven by the posture sequences. To this end, we propose a Video ControlNet for motion-controlling and a Content Guider for identity preserving. The proposed model is easy to use and can be adapted to most stylized diffusion models to generate diverse results. The project page is available at https://dreamoving.github.io/dreamoving.]]></description>
            <pubDate>Mon, 11 Dec 2023 03:17:36 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2312.05107</guid>
            <link>https://arxiv.org/abs/2312.05107</link>
            
            
            
            <author><![CDATA[Mengyang Feng, Jinlin Liu, Kai Yu, Yuan Yao, Zheng Hui, Xiefan Guo, Xianhui Lin, Haolan Xue, Chen Shi, Xiaowen Li, Aojie Li, Miaomiao Cui, Peiran Ren, Xuansong Xie]]></author>
            
                
            
            
        </item>
        
        <item>
            <title><![CDATA[Customizing Motion in Text-to-Video Diffusion Models]]></title>
            <description><![CDATA[We introduce an approach for augmenting text-to-video generation models with customized motions, extending their capabilities beyond the motions depicted in the original training data. By leveraging a few video samples demonstrating specific movements as input, our method learns and generalizes the input motion patterns for diverse, text-specified scenarios. Our contributions are threefold. First, to achieve our results, we finetune an existing text-to-video model to learn a novel mapping between the depicted motion in the input examples to a new unique token. To avoid overfitting to the new custom motion, we introduce an approach for regularization over videos. Second, by leveraging the motion priors in a pretrained model, our method can produce novel videos featuring multiple people doing the custom motion, and can invoke the motion in combination with other motions. Furthermore, our approach extends to the multimodal customization of motion and appearance of individualized subjects, enabling the generation of videos featuring unique characters and distinct motions. Third, to validate our method, we introduce an approach for quantitatively evaluating the learned custom motion and perform a systematic ablation study. We show that our method significantly outperforms prior appearance-based customization approaches when extended to the motion customization task.]]></description>
            <pubDate>Mon, 11 Dec 2023 03:14:11 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2312.04966</guid>
            <link>https://arxiv.org/abs/2312.04966</link>
            
            
            
            <author><![CDATA[Joanna Materzynska, Josef Sivic, Eli Shechtman, Antonio Torralba, Richard Zhang, Bryan Russell]]></author>
            
                
            
            
        </item>
        
        <item>
            <title><![CDATA[PathFinder: Guided Search over Multi-Step Reasoning Paths]]></title>
            <description><![CDATA[With recent advancements in large language models, methods like chain-of-thought prompting to elicit reasoning chains have been shown to improve results on reasoning tasks. However, tasks that require multiple steps of reasoning still pose significant challenges to state-of-the-art models. Drawing inspiration from the beam search algorithm, we propose PathFinder, a tree-search-based reasoning path generation approach. It enhances diverse branching and multi-hop reasoning through the integration of dynamic decoding, enabled by varying sampling methods and parameters. Using constrained reasoning, PathFinder integrates novel quality constraints, pruning, and exploration methods to enhance the efficiency and the quality of generation. Moreover, it includes scoring and ranking features to improve candidate selection. Our approach outperforms competitive baselines on three complex arithmetic and commonsense reasoning tasks by 6% on average. Our model generalizes well to longer, unseen reasoning chains, reflecting similar complexities to beam search with large branching factors.]]></description>
            <pubDate>Mon, 11 Dec 2023 03:08:05 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2312.05180</guid>
            <link>https://arxiv.org/abs/2312.05180</link>
            
            
            
            <author><![CDATA[Olga Golovneva, Sean O&#39;Brien, Ramakanth Pasunuru, Tianlu Wang, Luke Zettlemoyer, Maryam Fazel-Zarandi, Asli Celikyilmaz]]></author>
            
                
            
            
        </item>
        
        <item>
            <title><![CDATA[SparQ Attention: Bandwidth-Efficient LLM Inference]]></title>
            <description><![CDATA[Generative large language models (LLMs) have opened up numerous novel possibilities, but due to their significant computational requirements their ubiquitous use remains challenging. Some of the most useful applications require processing large numbers of samples at a time and using long contexts, both significantly increasing the memory communication load of the models. We introduce SparQ Attention, a technique for increasing the inference throughput of LLMs by reducing the memory bandwidth requirements within the attention blocks through selective fetching of the cached history. Our proposed technique can be applied directly to off-the-shelf LLMs during inference, without requiring any modification to the pre-training setup or additional fine-tuning. We show how SparQ Attention can decrease the attention memory bandwidth requirements up to eight times without any loss in accuracy by evaluating Llama 2 and Pythia models on a wide range of downstream tasks.]]></description>
            <pubDate>Mon, 11 Dec 2023 03:04:31 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2312.04985</guid>
            <link>https://arxiv.org/abs/2312.04985</link>
            
            
            
            <author><![CDATA[Luka Ribar, Ivan Chelombiev, Luke Hudlass-Galley, Charlie Blake, Carlo Luschi, Douglas Orr]]></author>
            
                
            
            
        </item>
        
        <item>
            <title><![CDATA[EE-LLM: Large-Scale Training and Inference of Early-Exit Large Language Models with 3D Parallelism]]></title>
            <description><![CDATA[We present EE-LLM, a framework for large-scale training and inference of early-exit large language models (LLMs). While recent works have shown preliminary evidence for the efficacy of early exiting in accelerating LLM inference, EE-LLM makes a foundational step towards scaling up early-exit LLMs by supporting their training and inference with massive 3D parallelism. Built upon Megatron-LM, EE-LLM implements a variety of algorithmic innovations and performance optimizations tailored to early exiting, including a lightweight method that facilitates backpropagation for the early-exit training objective with pipeline parallelism, techniques of leveraging idle resources in the original pipeline schedule for computation related to early-exit layers, and two approaches of early-exit inference that are compatible with KV caching for autoregressive generation. Our analytical and empirical study shows that EE-LLM achieves great training efficiency with negligible computational overhead compared to standard LLM training, as well as outstanding inference speedup without compromising output quality. To facilitate further research and adoption, we release EE-LLM at https://github.com/pan-x-c/EE-LLM.]]></description>
            <pubDate>Mon, 11 Dec 2023 02:38:54 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2312.04916</guid>
            <link>https://arxiv.org/abs/2312.04916</link>
            
            
            
            <author><![CDATA[Yanxi Chen, Xuchen Pan, Yaliang Li, Bolin Ding, Jingren Zhou]]></author>
            
                
            
            
        </item>
        
        <item>
            <title><![CDATA[Localized Symbolic Knowledge Distillation for Visual Commonsense Models]]></title>
            <description><![CDATA[Instruction following vision-language (VL) models offer a flexible interface that supports a broad range of multimodal tasks in a zero-shot fashion. However, interfaces that operate on full images do not directly enable the user to "point to" and access specific regions within images. This capability is important not only to support reference-grounded VL benchmarks, but also, for practical applications that require precise within-image reasoning. We build Localized Visual Commonsense models, which allow users to specify (multiple) regions as input. We train our model by sampling localized commonsense knowledge from a large language model (LLM): specifically, we prompt an LLM to collect commonsense knowledge given a global literal image description and a local literal region description automatically generated by a set of VL models. With a separately trained critic model that selects high-quality examples, we find that training on the localized commonsense corpus can successfully distill existing VL models to support a reference-as-input interface. Empirical results and human evaluations in a zero-shot setup demonstrate that our distillation method results in more precise VL models of reasoning compared to a baseline of passing a generated referring expression to an LLM.]]></description>
            <pubDate>Mon, 11 Dec 2023 02:33:48 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2312.04837</guid>
            <link>https://arxiv.org/abs/2312.04837</link>
            
            
            
            <author><![CDATA[Jae Sung Park, Jack Hessel, Khyathi Raghavi Chandu, Paul Pu Liang, Ximing Lu, Peter West, Youngjae Yu, Qiuyuan Huang, Jianfeng Gao, Ali Farhadi, Yejin Choi]]></author>
            
                
            
            
        </item>
        
        <item>
            <title><![CDATA[Purple Llama CyberSecEval: A Secure Coding Benchmark for Language Models]]></title>
            <description><![CDATA[This paper presents CyberSecEval, a comprehensive benchmark developed to help bolster the cybersecurity of Large Language Models (LLMs) employed as coding assistants. As what we believe to be the most extensive unified cybersecurity safety benchmark to date, CyberSecEval provides a thorough evaluation of LLMs in two crucial security domains: their propensity to generate insecure code and their level of compliance when asked to assist in cyberattacks. Through a case study involving seven models from the Llama 2, Code Llama, and OpenAI GPT large language model families, CyberSecEval effectively pinpointed key cybersecurity risks. More importantly, it offered practical insights for refining these models. A significant observation from the study was the tendency of more advanced models to suggest insecure code, highlighting the critical need for integrating security considerations in the development of sophisticated LLMs. CyberSecEval, with its automated test case generation and evaluation pipeline covers a broad scope and equips LLM designers and researchers with a tool to broadly measure and enhance the cybersecurity safety properties of LLMs, contributing to the development of more secure AI systems.]]></description>
            <pubDate>Mon, 11 Dec 2023 02:27:42 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2312.04724</guid>
            <link>https://arxiv.org/abs/2312.04724</link>
            
            
            
            <author><![CDATA[Manish Bhatt, Sahana Chennabasappa, Cyrus Nikolaidis, Shengye Wan, Ivan Evtimov, Dominik Gabi, Daniel Song, Faizan Ahmad, Cornelius Aschermann, Lorenzo Fontana, Sasha Frolov, Ravi Prakash Giri, Dhaval Kapil, Yiannis Kozyrakis, David LeBlanc, James Milazzo, Aleksandar Straumann, Gabriel Synnaeve, Varun Vontimitta, Spencer Whitman, Joshua Saxe]]></author>
            
                
            
            
        </item>
        
        <item>
            <title><![CDATA[ECLIPSE: A Resource-Efficient Text-to-Image Prior for Image Generations]]></title>
            <description><![CDATA[Text-to-image (T2I) diffusion models, notably the unCLIP models (e.g., DALL-E-2), achieve state-of-the-art (SOTA) performance on various compositional T2I benchmarks, at the cost of significant computational resources. The unCLIP stack comprises T2I prior and diffusion image decoder. The T2I prior model alone adds a billion parameters compared to the Latent Diffusion Models, which increases the computational and high-quality data requirements. We introduce ECLIPSE, a novel contrastive learning method that is both parameter and data-efficient. ECLIPSE leverages pre-trained vision-language models (e.g., CLIP) to distill the knowledge into the prior model. We demonstrate that the ECLIPSE trained prior, with only 3.3% of the parameters and trained on a mere 2.8% of the data, surpasses the baseline T2I priors with an average of 71.6% preference score under resource-limited setting. It also attains performance on par with SOTA big models, achieving an average of 63.36% preference score in terms of the ability to follow the text compositions. Extensive experiments on two unCLIP diffusion image decoders, Karlo and Kandinsky, affirm that ECLIPSE priors consistently deliver high performance while significantly reducing resource dependency.]]></description>
            <pubDate>Mon, 11 Dec 2023 02:17:26 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2312.04655</guid>
            <link>https://arxiv.org/abs/2312.04655</link>
            
            
            
            <author><![CDATA[Maitreya Patel, Changhoon Kim, Sheng Cheng, Chitta Baral, Yezhou Yang]]></author>
            
                
            
            
        </item>
        
    </channel>
</rss>
