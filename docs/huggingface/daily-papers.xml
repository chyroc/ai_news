<?xml version="1.0" encoding="UTF-8"?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"


>
    <channel>
        <title><![CDATA[Huggingface Daily Papers]]></title>
        <link>https://huggingface.co/papers</link>
        <atom:link href="https://rsshub.app/huggingface/daily-papers" rel="self" type="application/rss+xml" />
        <description><![CDATA[Huggingface Daily Papers - Made with love by RSSHub(https://github.com/DIYgod/RSSHub)]]></description>
        <generator>RSSHub</generator>
        <webMaster>i@diygod.me (DIYgod)</webMaster>
        
        
        
        <language>zh-cn</language>
        
        <lastBuildDate>Thu, 08 Feb 2024 08:26:13 GMT</lastBuildDate>
        <ttl>120</ttl>
        
        <item>
            <title><![CDATA[EfficientViT-SAM: Accelerated Segment Anything Model Without Performance Loss]]></title>
            <description><![CDATA[We present EfficientViT-SAM, a new family of accelerated segment anything models. We retain SAM's lightweight prompt encoder and mask decoder while replacing the heavy image encoder with EfficientViT. For the training, we begin with the knowledge distillation from the SAM-ViT-H image encoder to EfficientViT. Subsequently, we conduct end-to-end training on the SA-1B dataset. Benefiting from EfficientViT's efficiency and capacity, EfficientViT-SAM delivers 48.9x measured TensorRT speedup on A100 GPU over SAM-ViT-H without sacrificing performance. Our code and pre-trained models are released at https://github.com/mit-han-lab/efficientvit.]]></description>
            <pubDate>Thu, 08 Feb 2024 05:54:04 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2402.05008</guid>
            <link>https://arxiv.org/abs/2402.05008</link>
            
            
            
            <author><![CDATA[Zhuoyang Zhang, Han Cai, Song Han]]></author>
            
                
            
            
        </item>
        
        <item>
            <title><![CDATA[Grandmaster-Level Chess Without Search]]></title>
            <description><![CDATA[The recent breakthrough successes in machine learning are mainly attributed to scale: namely large-scale attention-based architectures and datasets of unprecedented scale. This paper investigates the impact of training at scale for chess. Unlike traditional chess engines that rely on complex heuristics, explicit search, or a combination of both, we train a 270M parameter transformer model with supervised learning on a dataset of 10 million chess games. We annotate each board in the dataset with action-values provided by the powerful Stockfish 16 engine, leading to roughly 15 billion data points. Our largest model reaches a Lichess blitz Elo of 2895 against humans, and successfully solves a series of challenging chess puzzles, without any domain-specific tweaks or explicit search algorithms. We also show that our model outperforms AlphaZero's policy and value networks (without MCTS) and GPT-3.5-turbo-instruct. A systematic investigation of model and dataset size shows that strong chess performance only arises at sufficient scale. To validate our results, we perform an extensive series of ablations of design choices and hyperparameters.]]></description>
            <pubDate>Thu, 08 Feb 2024 05:37:00 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2402.04494</guid>
            <link>https://arxiv.org/abs/2402.04494</link>
            
            
            
            <author><![CDATA[Anian Ruoss, Grégoire Delétang, Sourabh Medapati, Jordi Grau-Moya, Li Kevin Wenliang, Elliot Catt, John Reid, Tim Genewein]]></author>
            
                
            
            
        </item>
        
        <item>
            <title><![CDATA[The Hedgehog & the Porcupine: Expressive Linear Attentions with Softmax Mimicry]]></title>
            <description><![CDATA[Linear attentions have shown potential for improving Transformer efficiency, reducing attention's quadratic complexity to linear in sequence length. This holds exciting promise for (1) training linear Transformers from scratch, (2) "finetuned-conversion" of task-specific Transformers into linear versions that recover task performance, and (3) "pretrained-conversion" of Transformers such as large language models into linear versions finetunable on downstream tasks. However, linear attentions often underperform standard softmax attention in quality. To close this performance gap, we find prior linear attentions lack key properties of softmax attention tied to good performance: low-entropy (or "spiky") weights and dot-product monotonicity. We further observe surprisingly simple feature maps that retain these properties and match softmax performance, but are inefficient to compute in linear attention. We thus propose Hedgehog, a learnable linear attention that retains the spiky and monotonic properties of softmax attention while maintaining linear complexity. Hedgehog uses simple trainable MLPs to produce attention weights mimicking softmax attention. Experiments show Hedgehog recovers over 99% of standard Transformer quality in train-from-scratch and finetuned-conversion settings, outperforming prior linear attentions up to 6 perplexity points on WikiText-103 with causal GPTs, and up to 8.7 GLUE score points on finetuned bidirectional BERTs. Hedgehog also enables pretrained-conversion. Converting a pretrained GPT-2 into a linear attention variant achieves state-of-the-art 16.7 perplexity on WikiText-103 for 125M subquadratic decoder models. We finally turn a pretrained Llama-2 7B into a viable linear attention Llama. With low-rank adaptation, Hedgehog-Llama2 7B achieves 28.1 higher ROUGE-1 points over the base standard attention model, where prior linear attentions lead to 16.5 point drops.]]></description>
            <pubDate>Thu, 08 Feb 2024 05:34:34 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2402.04347</guid>
            <link>https://arxiv.org/abs/2402.04347</link>
            
            
            
            <author><![CDATA[Michael Zhang, Kush Bhatia, Hermann Kumbong, Christopher Ré]]></author>
            
                
            
            
        </item>
        
        <item>
            <title><![CDATA[LGM: Large Multi-View Gaussian Model for High-Resolution 3D Content Creation]]></title>
            <description><![CDATA[3D content creation has achieved significant progress in terms of both quality and speed. Although current feed-forward models can produce 3D objects in seconds, their resolution is constrained by the intensive computation required during training. In this paper, we introduce Large Multi-View Gaussian Model (LGM), a novel framework designed to generate high-resolution 3D models from text prompts or single-view images. Our key insights are two-fold: 1) 3D Representation: We propose multi-view Gaussian features as an efficient yet powerful representation, which can then be fused together for differentiable rendering. 2) 3D Backbone: We present an asymmetric U-Net as a high-throughput backbone operating on multi-view images, which can be produced from text or single-view image input by leveraging multi-view diffusion models. Extensive experiments demonstrate the high fidelity and efficiency of our approach. Notably, we maintain the fast speed to generate 3D objects within 5 seconds while boosting the training resolution to 512, thereby achieving high-resolution 3D content generation.]]></description>
            <pubDate>Thu, 08 Feb 2024 05:24:23 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2402.05054</guid>
            <link>https://arxiv.org/abs/2402.05054</link>
            
            
            
            <author><![CDATA[Jiaxiang Tang, Zhaoxi Chen, Xiaokang Chen, Tengfei Wang, Gang Zeng, Ziwei Liu]]></author>
            
                
            
            
        </item>
        
        <item>
            <title><![CDATA[Fast Timing-Conditioned Latent Audio Diffusion]]></title>
            <description><![CDATA[Generating long-form 44.1kHz stereo audio from text prompts can be computationally demanding. Further, most previous works do not tackle that music and sound effects naturally vary in their duration. Our research focuses on the efficient generation of long-form, variable-length stereo music and sounds at 44.1kHz using text prompts with a generative model. Stable Audio is based on latent diffusion, with its latent defined by a fully-convolutional variational autoencoder. It is conditioned on text prompts as well as timing embeddings, allowing for fine control over both the content and length of the generated music and sounds. Stable Audio is capable of rendering stereo signals of up to 95 sec at 44.1kHz in 8 sec on an A100 GPU. Despite its compute efficiency and fast inference, it is one of the best in two public text-to-music and -audio benchmarks and, differently from state-of-the-art models, can generate music with structure and stereo sounds.]]></description>
            <pubDate>Thu, 08 Feb 2024 05:16:59 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2402.04825</guid>
            <link>https://arxiv.org/abs/2402.04825</link>
            
            
            
            <author><![CDATA[Zach Evans, CJ Carr, Josiah Taylor, Scott H. Hawley, Jordi Pons]]></author>
            
                
            
            
        </item>
        
        <item>
            <title><![CDATA[Hydragen: High-Throughput LLM Inference with Shared Prefixes]]></title>
            <description><![CDATA[Transformer-based large language models (LLMs) are now deployed to hundreds of millions of users. LLM inference is commonly performed on batches of sequences that share a prefix, such as few-shot examples or a chatbot system prompt. Decoding in this large-batch setting can be bottlenecked by the attention operation, which reads large key-value (KV) caches from memory and computes inefficient matrix-vector products for every sequence in the batch. In this work, we introduce Hydragen, a hardware-aware exact implementation of attention with shared prefixes. Hydragen computes attention over the shared prefix and unique suffixes separately. This decomposition enables efficient prefix attention by batching queries together across sequences, reducing redundant memory reads and enabling the use of hardware-friendly matrix multiplications. Our method can improve end-to-end LLM throughput by up to 32x against competitive baselines, with speedup growing with the batch size and shared prefix length. Hydragen also enables the use of very long shared contexts: with a high batch size, increasing the prefix length from 1K to 16K tokens decreases Hydragen throughput by less than 15%, while the throughput of baselines drops by over 90%. Hydragen generalizes beyond simple prefix-suffix decomposition and can be applied to tree-based prompt sharing patterns, allowing us to further reduce inference time on competitive programming problems by 55%.]]></description>
            <pubDate>Thu, 08 Feb 2024 04:32:21 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2402.05099</guid>
            <link>https://arxiv.org/abs/2402.05099</link>
            
            
            
            <author><![CDATA[Jordan Juravsky, Bradley Brown, Ryan Ehrlich, Daniel Y. Fu, Christopher Ré, Azalia Mirhoseini]]></author>
            
                
            
            
        </item>
        
        <item>
            <title><![CDATA[TP-Aware Dequantization]]></title>
            <description><![CDATA[In this paper, we present a novel method that reduces model inference latency during distributed deployment of Large Language Models (LLMs). Our contribution is an optimized inference deployment scheme that address the current limitations of state-of-the-art quantization kernels when used in conjunction with Tensor Parallel (TP). Our method preserves data locality in GPU memory access patterns and exploits a priori knowledge of TP to reduce global communication. We demonstrate an up to 1.81x speedup over existing methods for Llama-70B and up to 1.78x speedup for IBM WatsonX's Granite-20B MLP layer problem sizes on A100 and H100 NVIDIA DGX Systems for a variety of TP settings.]]></description>
            <pubDate>Thu, 08 Feb 2024 04:09:01 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2402.04925</guid>
            <link>https://arxiv.org/abs/2402.04925</link>
            
            
            
            <author><![CDATA[Adnan Hoque, Mudhakar Srivatsa, Chih-Chieh Yang, Raghu Ganti]]></author>
            
                
            
            
        </item>
        
        <item>
            <title><![CDATA[CodeIt: Self-Improving Language Models with Prioritized Hindsight Replay]]></title>
            <description><![CDATA[Large language models are increasingly solving tasks that are commonly believed to require human-level reasoning ability. However, these models still perform very poorly on benchmarks of general intelligence such as the Abstraction and Reasoning Corpus (ARC). In this paper, we approach ARC as a programming-by-examples problem, and introduce a novel and scalable method for language model self-improvement called Code Iteration (CodeIt). Our method iterates between 1) program sampling and hindsight relabeling, and 2) learning from prioritized experience replay. By relabeling the goal of an episode (i.e., the target program output given input) to the realized output produced by the sampled program, our method effectively deals with the extreme sparsity of rewards in program synthesis. Applying CodeIt to the ARC dataset, we demonstrate that prioritized hindsight replay, along with pre-training and data-augmentation, leads to successful inter-task generalization. CodeIt is the first neuro-symbolic approach that scales to the full ARC evaluation dataset. Our method solves 15% of ARC evaluation tasks, achieving state-of-the-art performance and outperforming existing neural and symbolic baselines.]]></description>
            <pubDate>Thu, 08 Feb 2024 03:39:25 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2402.04858</guid>
            <link>https://arxiv.org/abs/2402.04858</link>
            
            
            
            <author><![CDATA[Natasha Butt, Blazej Manczak, Auke Wiggers, Corrado Rainone, David Zhang, Michaël Defferrard, Taco Cohen]]></author>
            
                
            
            
        </item>
        
        <item>
            <title><![CDATA[Direct Language Model Alignment from Online AI Feedback]]></title>
            <description><![CDATA[Direct alignment from preferences (DAP) methods, such as DPO, have recently emerged as efficient alternatives to reinforcement learning from human feedback (RLHF), that do not require a separate reward model. However, the preference datasets used in DAP methods are usually collected ahead of training and never updated, thus the feedback is purely offline. Moreover, responses in these datasets are often sampled from a language model distinct from the one being aligned, and since the model evolves over training, the alignment phase is inevitably off-policy. In this study, we posit that online feedback is key and improves DAP methods. Our method, online AI feedback (OAIF), uses an LLM as annotator: on each training iteration, we sample two responses from the current model and prompt the LLM annotator to choose which one is preferred, thus providing online feedback. Despite its simplicity, we demonstrate via human evaluation in several tasks that OAIF outperforms both offline DAP and RLHF methods. We further show that the feedback leveraged in OAIF is easily controllable, via instruction prompts to the LLM annotator.]]></description>
            <pubDate>Thu, 08 Feb 2024 02:43:23 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2402.04792</guid>
            <link>https://arxiv.org/abs/2402.04792</link>
            
            
            
            <author><![CDATA[Shangmin Guo, Biao Zhang, Tianlin Liu, Tianqi Liu, Misha Khalman, Felipe Llinares, Alexandre Rame, Thomas Mesnard, Yao Zhao, Bilal Piot, Johan Ferret, Mathieu Blondel]]></author>
            
                
            
            
        </item>
        
        <item>
            <title><![CDATA[Progressive Gradient Flow for Robust N:M Sparsity Training in Transformers]]></title>
            <description><![CDATA[N:M Structured sparsity has garnered significant interest as a result of relatively modest overhead and improved efficiency. Additionally, this form of sparsity holds considerable appeal for reducing the memory footprint owing to their modest representation overhead. There have been efforts to develop training recipes for N:M structured sparsity, they primarily focus on low-sparsity regions (sim50\%). Nonetheless, performance of models trained using these approaches tends to decline when confronted with high-sparsity regions (&gt;80\%). In this work, we study the effectiveness of existing sparse training recipes at high-sparsity regions and argue that these methods fail to sustain the model quality on par with low-sparsity regions. We demonstrate that the significant factor contributing to this disparity is the presence of elevated levels of induced noise in the gradient magnitudes. To mitigate this undesirable effect, we employ decay mechanisms to progressively restrict the flow of gradients towards pruned elements. Our approach improves the model quality by up to 2% and 5% in vision and language models at high sparsity regime, respectively. We also evaluate the trade-off between model accuracy and training compute cost in terms of FLOPs. At iso-training FLOPs, our method yields better performance compared to conventional sparse training recipes, exhibiting an accuracy improvement of up to 2%. The source code is available at https://github.com/abhibambhaniya/progressive_gradient_flow_nm_sparsity.]]></description>
            <pubDate>Thu, 08 Feb 2024 02:40:24 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2402.04744</guid>
            <link>https://arxiv.org/abs/2402.04744</link>
            
            
            
            <author><![CDATA[Abhimanyu Rajeshkumar Bambhaniya, Amir Yazdanbakhsh, Suvinay Subramanian, Sheng-Chun Kao, Shivani Agrawal, Utku Evci, Tushar Krishna]]></author>
            
                
            
            
        </item>
        
        <item>
            <title><![CDATA[ScreenAI: A Vision-Language Model for UI and Infographics Understanding]]></title>
            <description><![CDATA[Screen user interfaces (UIs) and infographics, sharing similar visual language and design principles, play important roles in human communication and human-machine interaction. We introduce ScreenAI, a vision-language model that specializes in UI and infographics understanding. Our model improves upon the PaLI architecture with the flexible patching strategy of pix2struct and is trained on a unique mixture of datasets. At the heart of this mixture is a novel screen annotation task in which the model has to identify the type and location of UI elements. We use these text annotations to describe screens to Large Language Models and automatically generate question-answering (QA), UI navigation, and summarization training datasets at scale. We run ablation studies to demonstrate the impact of these design choices. At only 5B parameters, ScreenAI achieves new state-of-the-artresults on UI- and infographics-based tasks (Multi-page DocVQA, WebSRC, MoTIF and Widget Captioning), and new best-in-class performance on others (Chart QA, DocVQA, and InfographicVQA) compared to models of similar size. Finally, we release three new datasets: one focused on the screen annotation task and two others focused on question answering.]]></description>
            <pubDate>Thu, 08 Feb 2024 02:31:37 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2402.04615</guid>
            <link>https://arxiv.org/abs/2402.04615</link>
            
            
            
            <author><![CDATA[Gilles Baechler, Srinivas Sunkara, Maria Wang, Fedir Zubach, Hassan Mansoor, Vincent Etter, Victor Cărbune, Jason Lin, Jindong Chen, Abhanshu Sharma]]></author>
            
                
            
            
        </item>
        
        <item>
            <title><![CDATA[Fine-Tuned Language Models Generate Stable Inorganic Materials as Text]]></title>
            <description><![CDATA[We propose fine-tuning large language models for generation of stable materials. While unorthodox, fine-tuning large language models on text-encoded atomistic data is simple to implement yet reliable, with around 90% of sampled structures obeying physical constraints on atom positions and charges. Using energy above hull calculations from both learned ML potentials and gold-standard DFT calculations, we show that our strongest model (fine-tuned LLaMA-2 70B) can generate materials predicted to be metastable at about twice the rate (49% vs 28%) of CDVAE, a competing diffusion model. Because of text prompting's inherent flexibility, our models can simultaneously be used for unconditional generation of stable material, infilling of partial structures and text-conditional generation. Finally, we show that language models' ability to capture key symmetries of crystal structures improves with model scale, suggesting that the biases of pretrained LLMs are surprisingly well-suited for atomistic data.]]></description>
            <pubDate>Thu, 08 Feb 2024 02:24:14 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2402.04379</guid>
            <link>https://arxiv.org/abs/2402.04379</link>
            
            
            
            <author><![CDATA[Nate Gruver, Anuroop Sriram, Andrea Madotto, Andrew Gordon Wilson, C. Lawrence Zitnick, Zachary Ulissi]]></author>
            
                
            
            
        </item>
        
        <item>
            <title><![CDATA[BiLLM: Pushing the Limit of Post-Training Quantization for LLMs]]></title>
            <description><![CDATA[Pretrained large language models (LLMs) exhibit exceptional general language processing capabilities but come with significant demands on memory and computational resources. As a powerful compression technology, binarization can extremely reduce model weights to a mere 1 bit, lowering the expensive computation and memory requirements. However, existing quantization techniques fall short of maintaining LLM performance under ultra-low bit-widths. In response to this challenge, we present BiLLM, a groundbreaking 1-bit post-training quantization scheme tailored for pretrained LLMs. Based on the weight distribution of LLMs, BiLLM first identifies and structurally selects salient weights, and minimizes the compression loss through an effective binary residual approximation strategy. Moreover, considering the bell-shaped distribution of the non-salient weights, we propose an optimal splitting search to group and binarize them accurately. BiLLM achieving for the first time high-accuracy inference (e.g. 8.41 perplexity on LLaMA2-70B) with only 1.08-bit weights across various LLMs families and evaluation metrics, outperforms SOTA quantization methods of LLM by significant margins. Moreover, BiLLM enables the binarization process of the LLM with 7 billion weights within 0.5 hours on a single GPU, demonstrating satisfactory time efficiency.]]></description>
            <pubDate>Thu, 08 Feb 2024 02:20:01 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2402.04291</guid>
            <link>https://arxiv.org/abs/2402.04291</link>
            
            
            
            <author><![CDATA[Wei Huang, Yangdong Liu, Haotong Qin, Ying Li, Shiming Zhang, Xianglong Liu, Michele Magno, Xiaojuan Qi]]></author>
            
                
            
            
        </item>
        
        <item>
            <title><![CDATA[ConsistI2V: Enhancing Visual Consistency for Image-to-Video Generation]]></title>
            <description><![CDATA[Image-to-video (I2V) generation aims to use the initial frame (alongside a text prompt) to create a video sequence. A grand challenge in I2V generation is to maintain visual consistency throughout the video: existing methods often struggle to preserve the integrity of the subject, background, and style from the first frame, as well as ensure a fluid and logical progression within the video narrative. To mitigate these issues, we propose ConsistI2V, a diffusion-based method to enhance visual consistency for I2V generation. Specifically, we introduce (1) spatiotemporal attention over the first frame to maintain spatial and motion consistency, (2) noise initialization from the low-frequency band of the first frame to enhance layout consistency. These two approaches enable ConsistI2V to generate highly consistent videos. We also extend the proposed approaches to show their potential to improve consistency in auto-regressive long video generation and camera motion control. To verify the effectiveness of our method, we propose I2V-Bench, a comprehensive evaluation benchmark for I2V generation. Our automatic and human evaluation results demonstrate the superiority of ConsistI2V over existing methods.]]></description>
            <pubDate>Thu, 08 Feb 2024 02:16:36 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2402.04324</guid>
            <link>https://arxiv.org/abs/2402.04324</link>
            
            
            
            <author><![CDATA[Weiming Ren, Harry Yang, Ge Zhang, Cong Wei, Xinrun Du, Stephen Huang, Wenhu Chen]]></author>
            
                
            
            
        </item>
        
    </channel>
</rss>
