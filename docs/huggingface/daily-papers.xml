<?xml version="1.0" encoding="UTF-8"?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"


>
    <channel>
        <title><![CDATA[Huggingface Daily Papers]]></title>
        <link>https://huggingface.co/papers</link>
        <atom:link href="https://rsshub.app/huggingface/daily-papers" rel="self" type="application/rss+xml" />
        <description><![CDATA[Huggingface Daily Papers - Made with love by RSSHub(https://github.com/DIYgod/RSSHub)]]></description>
        <generator>RSSHub</generator>
        <webMaster>i@diygod.me (DIYgod)</webMaster>
        
        
        
        <language>zh-cn</language>
        
        <lastBuildDate>Tue, 19 Dec 2023 18:27:42 GMT</lastBuildDate>
        <ttl>120</ttl>
        
        <item>
            <title><![CDATA[MAG-Edit: Localized Image Editing in Complex Scenarios via Mask-Based Attention-Adjusted Guidance]]></title>
            <description><![CDATA[Recent diffusion-based image editing approaches have exhibited impressive editing capabilities in images with simple compositions. However, localized editing in complex scenarios has not been well-studied in the literature, despite its growing real-world demands. Existing mask-based inpainting methods fall short of retaining the underlying structure within the edit region. Meanwhile, mask-free attention-based methods often exhibit editing leakage and misalignment in more complex compositions. In this work, we develop MAG-Edit, a training-free, inference-stage optimization method, which enables localized image editing in complex scenarios. In particular, MAG-Edit optimizes the noise latent feature in diffusion models by maximizing two mask-based cross-attention constraints of the edit token, which in turn gradually enhances the local alignment with the desired prompt. Extensive quantitative and qualitative experiments demonstrate the effectiveness of our method in achieving both text alignment and structure preservation for localized editing within complex scenarios.]]></description>
            <pubDate>Tue, 19 Dec 2023 06:42:09 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2312.11396</guid>
            <link>https://arxiv.org/abs/2312.11396</link>
            
            
            
            <author><![CDATA[Qi Mao, Lan Chen, Yuchao Gu, Zhen Fang, Mike Zheng Shou]]></author>
            
                
            
            
        </item>
        
        <item>
            <title><![CDATA[Your Student is Better Than Expected: Adaptive Teacher-Student Collaboration for Text-Conditional Diffusion Models]]></title>
            <description><![CDATA[Knowledge distillation methods have recently shown to be a promising direction to speedup the synthesis of large-scale diffusion models by requiring only a few inference steps. While several powerful distillation methods were recently proposed, the overall quality of student samples is typically lower compared to the teacher ones, which hinders their practical usage. In this work, we investigate the relative quality of samples produced by the teacher text-to-image diffusion model and its distilled student version. As our main empirical finding, we discover that a noticeable portion of student samples exhibit superior fidelity compared to the teacher ones, despite the ``approximate'' nature of the student. Based on this finding, we propose an adaptive collaboration between student and teacher diffusion models for effective text-to-image synthesis. Specifically, the distilled model produces the initial sample, and then an oracle decides whether it needs further improvements with a slow teacher model. Extensive experiments demonstrate that the designed pipeline surpasses state-of-the-art text-to-image alternatives for various inference budgets in terms of human preference. Furthermore, the proposed approach can be naturally used in popular applications such as text-guided image editing and controllable generation.]]></description>
            <pubDate>Tue, 19 Dec 2023 06:35:49 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2312.10835</guid>
            <link>https://arxiv.org/abs/2312.10835</link>
            
            
            
            <author><![CDATA[Nikita Starodubcev, Artem Fedorov, Artem Babenko, Dmitry Baranchuk]]></author>
            
                
            
            
        </item>
        
        <item>
            <title><![CDATA[VecFusion: Vector Font Generation with Diffusion]]></title>
            <description><![CDATA[We present VecFusion, a new neural architecture that can generate vector fonts with varying topological structures and precise control point positions. Our approach is a cascaded diffusion model which consists of a raster diffusion model followed by a vector diffusion model. The raster model generates low-resolution, rasterized fonts with auxiliary control point information, capturing the global style and shape of the font, while the vector model synthesizes vector fonts conditioned on the low-resolution raster fonts from the first stage. To synthesize long and complex curves, our vector diffusion model uses a transformer architecture and a novel vector representation that enables the modeling of diverse vector geometry and the precise prediction of control points. Our experiments show that, in contrast to previous generative models for vector graphics, our new cascaded vector diffusion model generates higher quality vector fonts, with complex structures and diverse styles.]]></description>
            <pubDate>Tue, 19 Dec 2023 06:32:41 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2312.10540</guid>
            <link>https://arxiv.org/abs/2312.10540</link>
            
            
            
            <author><![CDATA[Vikas Thamizharasan, Difan Liu, Shantanu Agarwal, Matthew Fisher, Michael Gharbi, Oliver Wang, Alec Jacobson, Evangelos Kalogerakis]]></author>
            
                
            
            
        </item>
        
        <item>
            <title><![CDATA[GauFRe: Gaussian Deformation Fields for Real-time Dynamic Novel View Synthesis]]></title>
            <description><![CDATA[We propose a method for dynamic scene reconstruction using deformable 3D Gaussians that is tailored for monocular video. Building upon the efficiency of Gaussian splatting, our approach extends the representation to accommodate dynamic elements via a deformable set of Gaussians residing in a canonical space, and a time-dependent deformation field defined by a multi-layer perceptron (MLP). Moreover, under the assumption that most natural scenes have large regions that remain static, we allow the MLP to focus its representational power by additionally including a static Gaussian point cloud. The concatenated dynamic and static point clouds form the input for the Gaussian Splatting rasterizer, enabling real-time rendering. The differentiable pipeline is optimized end-to-end with a self-supervised rendering loss. Our method achieves results that are comparable to state-of-the-art dynamic neural radiance field methods while allowing much faster optimization and rendering. Project website: https://lynl7130.github.io/gaufre/index.html]]></description>
            <pubDate>Tue, 19 Dec 2023 06:23:32 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2312.11458</guid>
            <link>https://arxiv.org/abs/2312.11458</link>
            
            
            
            <author><![CDATA[Yiqing Liang, Numair Khan, Zhengqin Li, Thu Nguyen-Phuoc, Douglas Lanman, James Tompkin, Lei Xiao]]></author>
            
                
            
            
        </item>
        
        <item>
            <title><![CDATA[GAvatar: Animatable 3D Gaussian Avatars with Implicit Mesh Learning]]></title>
            <description><![CDATA[Gaussian splatting has emerged as a powerful 3D representation that harnesses the advantages of both explicit (mesh) and implicit (NeRF) 3D representations. In this paper, we seek to leverage Gaussian splatting to generate realistic animatable avatars from textual descriptions, addressing the limitations (e.g., flexibility and efficiency) imposed by mesh or NeRF-based representations. However, a naive application of Gaussian splatting cannot generate high-quality animatable avatars and suffers from learning instability; it also cannot capture fine avatar geometries and often leads to degenerate body parts. To tackle these problems, we first propose a primitive-based 3D Gaussian representation where Gaussians are defined inside pose-driven primitives to facilitate animation. Second, to stabilize and amortize the learning of millions of Gaussians, we propose to use neural implicit fields to predict the Gaussian attributes (e.g., colors). Finally, to capture fine avatar geometries and extract detailed meshes, we propose a novel SDF-based implicit mesh learning approach for 3D Gaussians that regularizes the underlying geometries and extracts highly detailed textured meshes. Our proposed method, GAvatar, enables the large-scale generation of diverse animatable avatars using only text prompts. GAvatar significantly surpasses existing methods in terms of both appearance and geometry quality, and achieves extremely fast rendering (100 fps) at 1K resolution.]]></description>
            <pubDate>Tue, 19 Dec 2023 06:18:07 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2312.11461</guid>
            <link>https://arxiv.org/abs/2312.11461</link>
            
            
            
            <author><![CDATA[Ye Yuan, Xueting Li, Yangyi Huang, Shalini De Mello, Koki Nagano, Jan Kautz, Umar Iqbal]]></author>
            
                
            
            
        </item>
        
        <item>
            <title><![CDATA[VolumeDiffusion: Flexible Text-to-3D Generation with Efficient Volumetric Encoder]]></title>
            <description><![CDATA[This paper introduces a pioneering 3D volumetric encoder designed for text-to-3D generation. To scale up the training data for the diffusion model, a lightweight network is developed to efficiently acquire feature volumes from multi-view images. The 3D volumes are then trained on a diffusion model for text-to-3D generation using a 3D U-Net. This research further addresses the challenges of inaccurate object captions and high-dimensional feature volumes. The proposed model, trained on the public Objaverse dataset, demonstrates promising outcomes in producing diverse and recognizable samples from text prompts. Notably, it empowers finer control over object part characteristics through textual cues, fostering model creativity by seamlessly combining multiple concepts within a single object. This research significantly contributes to the progress of 3D generation by introducing an efficient, flexible, and scalable representation methodology. Code is available at https://github.com/tzco/VolumeDiffusion.]]></description>
            <pubDate>Tue, 19 Dec 2023 06:13:26 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2312.11459</guid>
            <link>https://arxiv.org/abs/2312.11459</link>
            
            
            
            <author><![CDATA[Zhicong Tang, Shuyang Gu, Chunyu Wang, Ting Zhang, Jianmin Bao, Dong Chen, Baining Guo]]></author>
            
                
            
            
        </item>
        
        <item>
            <title><![CDATA[SCEdit: Efficient and Controllable Image Diffusion Generation via Skip Connection Editing]]></title>
            <description><![CDATA[Image diffusion models have been utilized in various tasks, such as text-to-image generation and controllable image synthesis. Recent research has introduced tuning methods that make subtle adjustments to the original models, yielding promising results in specific adaptations of foundational generative diffusion models. Rather than modifying the main backbone of the diffusion model, we delve into the role of skip connection in U-Net and reveal that hierarchical features aggregating long-distance information across encoder and decoder make a significant impact on the content and quality of image generation. Based on the observation, we propose an efficient generative tuning framework, dubbed SCEdit, which integrates and edits Skip Connection using a lightweight tuning module named SC-Tuner. Furthermore, the proposed framework allows for straightforward extension to controllable image synthesis by injecting different conditions with Controllable SC-Tuner, simplifying and unifying the network design for multi-condition inputs. Our SCEdit substantially reduces training parameters, memory usage, and computational expense due to its lightweight tuners, with backward propagation only passing to the decoder blocks. Extensive experiments conducted on text-to-image generation and controllable image synthesis tasks demonstrate the superiority of our method in terms of efficiency and performance. Project page: https://scedit.github.io/]]></description>
            <pubDate>Tue, 19 Dec 2023 06:07:33 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2312.11392</guid>
            <link>https://arxiv.org/abs/2312.11392</link>
            
            
            
            <author><![CDATA[Zeyinzi Jiang, Chaojie Mao, Yulin Pan, Zhen Han, Jingfeng Zhang]]></author>
            
                
            
            
        </item>
        
        <item>
            <title><![CDATA[MagicScroll: Nontypical Aspect-Ratio Image Generation for Visual Storytelling via Multi-Layered Semantic-Aware Denoising]]></title>
            <description><![CDATA[Visual storytelling often uses nontypical aspect-ratio images like scroll paintings, comic strips, and panoramas to create an expressive and compelling narrative. While generative AI has achieved great success and shown the potential to reshape the creative industry, it remains a challenge to generate coherent and engaging content with arbitrary size and controllable style, concept, and layout, all of which are essential for visual storytelling. To overcome the shortcomings of previous methods including repetitive content, style inconsistency, and lack of controllability, we propose MagicScroll, a multi-layered, progressive diffusion-based image generation framework with a novel semantic-aware denoising process. The model enables fine-grained control over the generated image on object, scene, and background levels with text, image, and layout conditions. We also establish the first benchmark for nontypical aspect-ratio image generation for visual storytelling including mediums like paintings, comics, and cinematic panoramas, with customized metrics for systematic evaluation. Through comparative and ablation studies, MagicScroll showcases promising results in aligning with the narrative text, improving visual coherence, and engaging the audience. We plan to release the code and benchmark in the hope of a better collaboration between AI researchers and creative practitioners involving visual storytelling.]]></description>
            <pubDate>Tue, 19 Dec 2023 05:54:10 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2312.10899</guid>
            <link>https://arxiv.org/abs/2312.10899</link>
            
            
            
            <author><![CDATA[Bingyuan Wang, Hengyu Meng, Zeyu Cai, Lanjiong Li, Yue Ma, Qifeng Chen, Zeyu Wang]]></author>
            
                
            
            
        </item>
        
        <item>
            <title><![CDATA[Cascade Speculative Drafting for Even Faster LLM Inference]]></title>
            <description><![CDATA[Speculative decoding enhances the efficiency of large language models (LLMs) by leveraging a draft model to draft for a larger target model to review. However, drafting in speculative decoding involves slow autoregressive generation and generating tokens of different importance with the same time allocation. These two inefficiencies lead to its suboptimal performance. To address this issue, we introduce Cascade Speculative Drafting (CS. Drafting), a novel approach that employs two types of cascades. The Vertical Cascade eliminates autoregressive generation from neural models. The Horizontal Cascade constitutes efficient time allocation in drafting with its optimality supported by our theoretical analysis. Combining both cascades, our CS. Drafting algorithm has achieved up to 72 percent additional speedup over speculative decoding in our experiments while keeping the same output distribution.]]></description>
            <pubDate>Tue, 19 Dec 2023 05:50:46 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2312.11462</guid>
            <link>https://arxiv.org/abs/2312.11462</link>
            
            
            
            <author><![CDATA[Ziyi Chen, Xiaocong Yang, Jiacheng Lin, Chenkai Sun, Jie Huang, Kevin Chen-Chuan Chang]]></author>
            
                
            
            
        </item>
        
        <item>
            <title><![CDATA[M3DBench: Let's Instruct Large Models with Multi-modal 3D Prompts]]></title>
            <description><![CDATA[Recently, 3D understanding has become popular to facilitate autonomous agents to perform further decisionmaking. However, existing 3D datasets and methods are often limited to specific tasks. On the other hand, recent progress in Large Language Models (LLMs) and Multimodal Language Models (MLMs) have demonstrated exceptional general language and imagery tasking performance. Therefore, it is interesting to unlock MLM's potential to be 3D generalist for wider tasks. However, current MLMs' research has been less focused on 3D tasks due to a lack of large-scale 3D instruction-following datasets. In this work, we introduce a comprehensive 3D instructionfollowing dataset called M3DBench, which possesses the following characteristics: 1) It supports general multimodal instructions interleaved with text, images, 3D objects, and other visual prompts. 2) It unifies diverse 3D tasks at both region and scene levels, covering a variety of fundamental abilities in real-world 3D environments. 3) It is a large-scale 3D instruction-following dataset with over 320k instruction-response pairs. Furthermore, we establish a new benchmark for assessing the performance of large models in understanding multi-modal 3D prompts. Extensive experiments demonstrate the effectiveness of our dataset and baseline, supporting general 3D-centric tasks, which can inspire future research.]]></description>
            <pubDate>Tue, 19 Dec 2023 05:46:47 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2312.10763</guid>
            <link>https://arxiv.org/abs/2312.10763</link>
            
            
            
            <author><![CDATA[Mingsheng Li, Xin Chen, Chi Zhang, Sijin Chen, Hongyuan Zhu, Fukun Yin, Gang Yu, Tao Chen]]></author>
            
                
            
            
        </item>
        
        <item>
            <title><![CDATA[Paloma: A Benchmark for Evaluating Language Model Fit]]></title>
            <description><![CDATA[Language models (LMs) commonly report perplexity on monolithic data held out from training. Implicitly or explicitly, this data is composed of domainsx2013varying distributions of language. Rather than assuming perplexity on one distribution extrapolates to others, Perplexity Analysis for Language Model Assessment (Paloma), measures LM fit to 585 text domains, ranging from nytimes.com to r/depression on Reddit. We invite submissions to our benchmark and organize results by comparability based on compliance with guidelines such as removal of benchmark contamination from pretraining. Submissions can also record parameter and training token count to make comparisons of Pareto efficiency for performance as a function of these measures of cost. We populate our benchmark with results from 6 baselines pretrained on popular corpora. In case studies, we demonstrate analyses that are possible with Paloma, such as finding that pretraining without data beyond Common Crawl leads to inconsistent fit to many domains.]]></description>
            <pubDate>Tue, 19 Dec 2023 05:44:16 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2312.10523</guid>
            <link>https://arxiv.org/abs/2312.10523</link>
            
            
            
            <author><![CDATA[Ian Magnusson, Akshita Bhagia, Valentin Hofmann, Luca Soldaini, Ananya Harsh Jha, Oyvind Tafjord, Dustin Schwenk, Evan Pete Walsh, Yanai Elazar, Kyle Lo, Dirk Groeneveld, Iz Beltagy, Hannaneh Hajishirzi, Noah A. Smith, Kyle Richardson, Jesse Dodge]]></author>
            
                
            
            
        </item>
        
        <item>
            <title><![CDATA[Catwalk: A Unified Language Model Evaluation Framework for Many Datasets]]></title>
            <description><![CDATA[The success of large language models has shifted the evaluation paradigms in natural language processing (NLP). The community's interest has drifted towards comparing NLP models across many tasks, domains, and datasets, often at an extreme scale. This imposes new engineering challenges: efforts in constructing datasets and models have been fragmented, and their formats and interfaces are incompatible. As a result, it often takes extensive (re)implementation efforts to make fair and controlled comparisons at scale.   Catwalk aims to address these issues. Catwalk provides a unified interface to a broad range of existing NLP datasets and models, ranging from both canonical supervised training and fine-tuning, to more modern paradigms like in-context learning. Its carefully-designed abstractions allow for easy extensions to many others. Catwalk substantially lowers the barriers to conducting controlled experiments at scale. For example, we finetuned and evaluated over 64 models on over 86 datasets with a single command, without writing any code. Maintained by the AllenNLP team at the Allen Institute for Artificial Intelligence (AI2), Catwalk is an ongoing open-source effort: https://github.com/allenai/catwalk.]]></description>
            <pubDate>Tue, 19 Dec 2023 05:40:26 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2312.10253</guid>
            <link>https://arxiv.org/abs/2312.10253</link>
            
            
            
            <author><![CDATA[Dirk Groeneveld, Anas Awadalla, Iz Beltagy, Akshita Bhagia, Ian Magnusson, Hao Peng, Oyvind Tafjord, Pete Walsh, Kyle Richardson, Jesse Dodge]]></author>
            
                
            
            
        </item>
        
        <item>
            <title><![CDATA[ProTIP: Progressive Tool Retrieval Improves Planning]]></title>
            <description><![CDATA[Large language models (LLMs) are increasingly employed for complex multi-step planning tasks, where the tool retrieval (TR) step is crucial for achieving successful outcomes. Two prevalent approaches for TR are single-step retrieval, which utilizes the complete query, and sequential retrieval using task decomposition (TD), where a full query is segmented into discrete atomic subtasks. While single-step retrieval lacks the flexibility to handle "inter-tool dependency," the TD approach necessitates maintaining "subtask-tool atomicity alignment," as the toolbox can evolve dynamically. To address these limitations, we introduce the Progressive Tool retrieval to Improve Planning (ProTIP) framework. ProTIP is a lightweight, contrastive learning-based framework that implicitly performs TD without the explicit requirement of subtask labels, while simultaneously maintaining subtask-tool atomicity. On the ToolBench dataset, ProTIP outperforms the ChatGPT task decomposition-based approach by a remarkable margin, achieving a 24% improvement in Recall@K=10 for TR and a 41% enhancement in tool accuracy for plan generation.]]></description>
            <pubDate>Tue, 19 Dec 2023 05:32:32 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2312.10332</guid>
            <link>https://arxiv.org/abs/2312.10332</link>
            
            
            
            <author><![CDATA[Raviteja Anantha, Bortik Bandyopadhyay, Anirudh Kashi, Sayantan Mahinder, Andrew W Hill, Srinivas Chappidi]]></author>
            
                
            
            
        </item>
        
        <item>
            <title><![CDATA[G-LLaVA: Solving Geometric Problem with Multi-Modal Large Language Model]]></title>
            <description><![CDATA[Large language models (LLMs) have shown remarkable proficiency in human-level reasoning and generation capabilities, which encourages extensive research on their application in mathematical problem solving. However, current work has been largely focused on text-based mathematical problems, with limited investigation in problems involving geometric information. Addressing this gap, we aim to enable LLMs to solve geometric problems by understanding image input. We first analyze the limitations of current Multimodal Large Language Models (MLLMs) in this area: they struggle to accurately comprehending basic geometric elements and their relationships. To overcome these challenges, we take advantage of the unique characteristics of geometric problems (such as unique geometric logical form, and geometric scalability) and the capacity of the textual LLMs to build an enriched multimodal geometry dataset based on existing data. The augmented dataset, Geo170K, contains more than 170K geometric image-caption and question-answer pairs. Utilizing our constructed Geo170K dataset, we develop G-LLaVA, which demonstrates exceptional performance in solving geometric problems, significantly outperforming GPT-4-V on the MathVista benchmark with only 7B parameters.]]></description>
            <pubDate>Tue, 19 Dec 2023 05:31:02 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2312.11370</guid>
            <link>https://arxiv.org/abs/2312.11370</link>
            
            
            
            <author><![CDATA[Jiahui Gao, Renjie Pi, Jipeng Zhang, Jiacheng Ye, Wanjun Zhong, Yufei Wang, Lanqing Hong, Jianhua Han, Hang Xu, Zhenguo Li, Lingpeng Kong]]></author>
            
                
            
            
        </item>
        
        <item>
            <title><![CDATA[Silkie: Preference Distillation for Large Visual Language Models]]></title>
            <description><![CDATA[This paper explores preference distillation for large vision language models (LVLMs), improving their ability to generate helpful and faithful responses anchoring the visual context. We first build a vision-language feedback (VLFeedback) dataset utilizing AI annotation. Specifically, responses are generated by models sampled from 12 LVLMs, conditioned on multi-modal instructions sourced from various datasets. We adopt GPT-4V to assess the generated outputs regarding helpfulness, visual faithfulness, and ethical considerations. Furthermore, the preference supervision is distilled into Qwen-VL-Chat through the direct preference optimization (DPO) method. The resulting model Silkie, achieves 6.9% and 9.5% relative improvement on the MME benchmark regarding the perception and cognition capabilities, respectively. Silkie also demonstrates reduced hallucination by setting a new state-of-the-art score of 3.02 on the MMHal-Bench benchmark. Further analysis shows that DPO with our VLFeedback dataset mainly boosts the fine-grained perception and complex cognition abilities of LVLMs, leading to more comprehensive improvements compared to human-annotated preference datasets.]]></description>
            <pubDate>Tue, 19 Dec 2023 05:28:47 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2312.10665</guid>
            <link>https://arxiv.org/abs/2312.10665</link>
            
            
            
            <author><![CDATA[Lei Li, Zhihui Xie, Mukai Li, Shunian Chen, Peiyi Wang, Liang Chen, Yazheng Yang, Benyou Wang, Lingpeng Kong]]></author>
            
                
            
            
        </item>
        
        <item>
            <title><![CDATA[Rich Human Feedback for Text-to-Image Generation]]></title>
            <description><![CDATA[Recent Text-to-Image (T2I) generation models such as Stable Diffusion and Imagen have made significant progress in generating high-resolution images based on text descriptions. However, many generated images still suffer from issues such as artifacts/implausibility, misalignment with text descriptions, and low aesthetic quality. Inspired by the success of Reinforcement Learning with Human Feedback (RLHF) for large language models, prior works collected human-provided scores as feedback on generated images and trained a reward model to improve the T2I generation. In this paper, we enrich the feedback signal by (i) marking image regions that are implausible or misaligned with the text, and (ii) annotating which words in the text prompt are misrepresented or missing on the image. We collect such rich human feedback on 18K generated images and train a multimodal transformer to predict the rich feedback automatically. We show that the predicted rich human feedback can be leveraged to improve image generation, for example, by selecting high-quality training data to finetune and improve the generative models, or by creating masks with predicted heatmaps to inpaint the problematic regions. Notably, the improvements generalize to models (Muse) beyond those used to generate the images on which human feedback data were collected (Stable Diffusion variants).]]></description>
            <pubDate>Tue, 19 Dec 2023 05:26:07 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2312.10240</guid>
            <link>https://arxiv.org/abs/2312.10240</link>
            
            
            
            <author><![CDATA[Youwei Liang, Junfeng He, Gang Li, Peizhao Li, Arseniy Klimovskiy, Nicholas Carolan, Jiao Sun, Jordi Pont-Tuset, Sarah Young, Feng Yang, Junjie Ke, Krishnamurthy Dj Dvijotham, Katie Collins, Yiwen Luo, Yang Li, Kai J Kohlhoff, Deepak Ramachandran, Vidhya Navalpakkam]]></author>
            
                
            
            
        </item>
        
        <item>
            <title><![CDATA[VidToMe: Video Token Merging for Zero-Shot Video Editing]]></title>
            <description><![CDATA[Diffusion models have made significant advances in generating high-quality images, but their application to video generation has remained challenging due to the complexity of temporal motion. Zero-shot video editing offers a solution by utilizing pre-trained image diffusion models to translate source videos into new ones. Nevertheless, existing methods struggle to maintain strict temporal consistency and efficient memory consumption. In this work, we propose a novel approach to enhance temporal consistency in generated videos by merging self-attention tokens across frames. By aligning and compressing temporally redundant tokens across frames, our method improves temporal coherence and reduces memory consumption in self-attention computations. The merging strategy matches and aligns tokens according to the temporal correspondence between frames, facilitating natural temporal consistency in generated video frames. To manage the complexity of video processing, we divide videos into chunks and develop intra-chunk local token merging and inter-chunk global token merging, ensuring both short-term video continuity and long-term content consistency. Our video editing approach seamlessly extends the advancements in image editing to video editing, rendering favorable results in temporal consistency over state-of-the-art methods.]]></description>
            <pubDate>Tue, 19 Dec 2023 05:22:03 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2312.10656</guid>
            <link>https://arxiv.org/abs/2312.10656</link>
            
            
            
            <author><![CDATA[Xirui Li, Chao Ma, Xiaokang Yang, Ming-Hsuan Yang]]></author>
            
                
            
            
        </item>
        
    </channel>
</rss>
