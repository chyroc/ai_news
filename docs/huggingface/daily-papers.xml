<?xml version="1.0" encoding="UTF-8"?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"


>
    <channel>
        <title><![CDATA[Huggingface Daily Papers]]></title>
        <link>https://huggingface.co/papers</link>
        <atom:link href="https://rsshub.app/huggingface/daily-papers" rel="self" type="application/rss+xml" />
        <description><![CDATA[Huggingface Daily Papers - Made with love by RSSHub(https://github.com/DIYgod/RSSHub)]]></description>
        <generator>RSSHub</generator>
        <webMaster>i@diygod.me (DIYgod)</webMaster>
        
        
        
        <language>zh-cn</language>
        
        <lastBuildDate>Fri, 15 Dec 2023 05:42:52 GMT</lastBuildDate>
        <ttl>120</ttl>
        
        <item>
            <title><![CDATA[SHAP-EDITOR: Instruction-guided Latent 3D Editing in Seconds]]></title>
            <description><![CDATA[We propose a novel feed-forward 3D editing framework called Shap-Editor. Prior research on editing 3D objects primarily concentrated on editing individual objects by leveraging off-the-shelf 2D image editing networks. This is achieved via a process called distillation, which transfers knowledge from the 2D network to 3D assets. Distillation necessitates at least tens of minutes per asset to attain satisfactory editing results, and is thus not very practical. In contrast, we ask whether 3D editing can be carried out directly by a feed-forward network, eschewing test-time optimisation. In particular, we hypothesise that editing can be greatly simplified by first encoding 3D objects in a suitable latent space. We validate this hypothesis by building upon the latent space of Shap-E. We demonstrate that direct 3D editing in this space is possible and efficient by building a feed-forward editor network that only requires approximately one second per edit. Our experiments show that Shap-Editor generalises well to both in-distribution and out-of-distribution 3D assets with different prompts, exhibiting comparable performance with methods that carry out test-time optimisation for each edited instance.]]></description>
            <pubDate>Fri, 15 Dec 2023 04:44:47 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2312.09246</guid>
            <link>https://arxiv.org/abs/2312.09246</link>
            
            
            
            <author><![CDATA[Minghao Chen, Junyu Xie, Iro Laina, Andrea Vedaldi]]></author>
            
                
            
            
        </item>
        
        <item>
            <title><![CDATA[StemGen: A music generation model that listens]]></title>
            <description><![CDATA[End-to-end generation of musical audio using deep learning techniques has seen an explosion of activity recently. However, most models concentrate on generating fully mixed music in response to abstract conditioning information. In this work, we present an alternative paradigm for producing music generation models that can listen and respond to musical context. We describe how such a model can be constructed using a non-autoregressive, transformer-based model architecture and present a number of novel architectural and sampling improvements. We train the described architecture on both an open-source and a proprietary dataset. We evaluate the produced models using standard quality metrics and a new approach based on music information retrieval descriptors. The resulting model reaches the audio quality of state-of-the-art text-conditioned models, as well as exhibiting strong musical coherence with its context.]]></description>
            <pubDate>Fri, 15 Dec 2023 04:41:21 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2312.08723</guid>
            <link>https://arxiv.org/abs/2312.08723</link>
            
            
            
            <author><![CDATA[Julian D. Parker, Janne Spijkervet, Katerina Kosta, Furkan Yesiler, Boris Kuznetsov, Ju-Chiang Wang, Matt Avent, Jitong Chen, Duc Le]]></author>
            
                
            
            
        </item>
        
        <item>
            <title><![CDATA[Mosaic-SDF for 3D Generative Models]]></title>
            <description><![CDATA[Current diffusion or flow-based generative models for 3D shapes divide to two: distilling pre-trained 2D image diffusion models, and training directly on 3D shapes. When training a diffusion or flow models on 3D shapes a crucial design choice is the shape representation. An effective shape representation needs to adhere three design principles: it should allow an efficient conversion of large 3D datasets to the representation form; it should provide a good tradeoff of approximation power versus number of parameters; and it should have a simple tensorial form that is compatible with existing powerful neural architectures. While standard 3D shape representations such as volumetric grids and point clouds do not adhere to all these principles simultaneously, we advocate in this paper a new representation that does. We introduce Mosaic-SDF (M-SDF): a simple 3D shape representation that approximates the Signed Distance Function (SDF) of a given shape by using a set of local grids spread near the shape's boundary. The M-SDF representation is fast to compute for each shape individually making it readily parallelizable; it is parameter efficient as it only covers the space around the shape's boundary; and it has a simple matrix form, compatible with Transformer-based architectures. We demonstrate the efficacy of the M-SDF representation by using it to train a 3D generative flow model including class-conditioned generation with the 3D Warehouse dataset, and text-to-3D generation using a dataset of about 600k caption-shape pairs.]]></description>
            <pubDate>Fri, 15 Dec 2023 04:37:29 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2312.09222</guid>
            <link>https://arxiv.org/abs/2312.09222</link>
            
            
            
            <author><![CDATA[Lior Yariv, Omri Puny, Natalia Neverova, Oran Gafni, Yaron Lipman]]></author>
            
                
            
            
        </item>
        
        <item>
            <title><![CDATA[UniDream: Unifying Diffusion Priors for Relightable Text-to-3D Generation]]></title>
            <description><![CDATA[Recent advancements in text-to-3D generation technology have significantly advanced the conversion of textual descriptions into imaginative well-geometrical and finely textured 3D objects. Despite these developments, a prevalent limitation arises from the use of RGB data in diffusion or reconstruction models, which often results in models with inherent lighting and shadows effects that detract from their realism, thereby limiting their usability in applications that demand accurate relighting capabilities. To bridge this gap, we present UniDream, a text-to-3D generation framework by incorporating unified diffusion priors. Our approach consists of three main components: (1) a dual-phase training process to get albedo-normal aligned multi-view diffusion and reconstruction models, (2) a progressive generation procedure for geometry and albedo-textures based on Score Distillation Sample (SDS) using the trained reconstruction and diffusion models, and (3) an innovative application of SDS for finalizing PBR generation while keeping a fixed albedo based on Stable Diffusion model. Extensive evaluations demonstrate that UniDream surpasses existing methods in generating 3D objects with clearer albedo textures, smoother surfaces, enhanced realism, and superior relighting capabilities.]]></description>
            <pubDate>Fri, 15 Dec 2023 04:31:19 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2312.08754</guid>
            <link>https://arxiv.org/abs/2312.08754</link>
            
            
            
            <author><![CDATA[Zexiang Liu, Yangguang Li, Youtian Lin, Xin Yu, Sida Peng, Yan-Pei Cao, Xiaojuan Qi, Xiaoshui Huang, Ding Liang, Wanli Ouyang]]></author>
            
                
            
            
        </item>
        
        <item>
            <title><![CDATA[LIME: Localized Image Editing via Attention Regularization in Diffusion Models]]></title>
            <description><![CDATA[Diffusion models (DMs) have gained prominence due to their ability to generate high-quality, varied images, with recent advancements in text-to-image generation. The research focus is now shifting towards the controllability of DMs. A significant challenge within this domain is localized editing, where specific areas of an image are modified without affecting the rest of the content. This paper introduces LIME for localized image editing in diffusion models that do not require user-specified regions of interest (RoI) or additional text input. Our method employs features from pre-trained methods and a simple clustering technique to obtain precise semantic segmentation maps. Then, by leveraging cross-attention maps, it refines these segments for localized edits. Finally, we propose a novel cross-attention regularization technique that penalizes unrelated cross-attention scores in the RoI during the denoising steps, ensuring localized edits. Our approach, without re-training and fine-tuning, consistently improves the performance of existing methods in various editing benchmarks.]]></description>
            <pubDate>Fri, 15 Dec 2023 04:26:39 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2312.09256</guid>
            <link>https://arxiv.org/abs/2312.09256</link>
            
            
            
            <author><![CDATA[Enis Simsar, Alessio Tonioni, Yongqin Xian, Thomas Hofmann, Federico Tombari]]></author>
            
                
            
            
        </item>
        
        <item>
            <title><![CDATA[FineControlNet: Fine-level Text Control for Image Generation with Spatially Aligned Text Control Injection]]></title>
            <description><![CDATA[Recently introduced ControlNet has the ability to steer the text-driven image generation process with geometric input such as human 2D pose, or edge features. While ControlNet provides control over the geometric form of the instances in the generated image, it lacks the capability to dictate the visual appearance of each instance. We present FineControlNet to provide fine control over each instance's appearance while maintaining the precise pose control capability. Specifically, we develop and demonstrate FineControlNet with geometric control via human pose images and appearance control via instance-level text prompts. The spatial alignment of instance-specific text prompts and 2D poses in latent space enables the fine control capabilities of FineControlNet. We evaluate the performance of FineControlNet with rigorous comparison against state-of-the-art pose-conditioned text-to-image diffusion models. FineControlNet achieves superior performance in generating images that follow the user-provided instance-specific text prompts and poses compared with existing methods. Project webpage: https://samsunglabs.github.io/FineControlNet-project-page]]></description>
            <pubDate>Fri, 15 Dec 2023 04:20:44 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2312.09252</guid>
            <link>https://arxiv.org/abs/2312.09252</link>
            
            
            
            <author><![CDATA[Hongsuk Choi, Isaac Kasahara, Selim Engin, Moritz Graule, Nikhil Chavan-Dafle, Volkan Isler]]></author>
            
                
            
            
        </item>
        
        <item>
            <title><![CDATA[VideoLCM: Video Latent Consistency Model]]></title>
            <description><![CDATA[Consistency models have demonstrated powerful capability in efficient image generation and allowed synthesis within a few sampling steps, alleviating the high computational cost in diffusion models. However, the consistency model in the more challenging and resource-consuming video generation is still less explored. In this report, we present the VideoLCM framework to fill this gap, which leverages the concept of consistency models from image generation to efficiently synthesize videos with minimal steps while maintaining high quality. VideoLCM builds upon existing latent video diffusion models and incorporates consistency distillation techniques for training the latent consistency model. Experimental results reveal the effectiveness of our VideoLCM in terms of computational efficiency, fidelity and temporal consistency. Notably, VideoLCM achieves high-fidelity and smooth video synthesis with only four sampling steps, showcasing the potential for real-time synthesis. We hope that VideoLCM can serve as a simple yet effective baseline for subsequent research. The source code and models will be publicly available.]]></description>
            <pubDate>Fri, 15 Dec 2023 04:16:52 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2312.09109</guid>
            <link>https://arxiv.org/abs/2312.09109</link>
            
            
            
            <author><![CDATA[Xiang Wang, Shiwei Zhang, Han Zhang, Yu Liu, Yingya Zhang, Changxin Gao, Nong Sang]]></author>
            
                
            
            
        </item>
        
        <item>
            <title><![CDATA[SEEAvatar: Photorealistic Text-to-3D Avatar Generation with Constrained Geometry and Appearance]]></title>
            <description><![CDATA[Powered by large-scale text-to-image generation models, text-to-3D avatar generation has made promising progress. However, most methods fail to produce photorealistic results, limited by imprecise geometry and low-quality appearance. Towards more practical avatar generation, we present SEEAvatar, a method for generating photorealistic 3D avatars from text with SElf-Evolving constraints for decoupled geometry and appearance. For geometry, we propose to constrain the optimized avatar in a decent global shape with a template avatar. The template avatar is initialized with human prior and can be updated by the optimized avatar periodically as an evolving template, which enables more flexible shape generation. Besides, the geometry is also constrained by the static human prior in local parts like face and hands to maintain the delicate structures. For appearance generation, we use diffusion model enhanced by prompt engineering to guide a physically based rendering pipeline to generate realistic textures. The lightness constraint is applied on the albedo texture to suppress incorrect lighting effect. Experiments show that our method outperforms previous methods on both global and local geometry and appearance quality by a large margin. Since our method can produce high-quality meshes and textures, such assets can be directly applied in classic graphics pipeline for realistic rendering under any lighting condition. Project page at: https://seeavatar3d.github.io.]]></description>
            <pubDate>Fri, 15 Dec 2023 04:12:19 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2312.08889</guid>
            <link>https://arxiv.org/abs/2312.08889</link>
            
            
            
            <author><![CDATA[Yuanyou Xu, Zongxin Yang, Yi Yang]]></author>
            
                
            
            
        </item>
        
        <item>
            <title><![CDATA[VL-GPT: A Generative Pre-trained Transformer for Vision and Language Understanding and Generation]]></title>
            <description><![CDATA[In this work, we introduce Vision-Language Generative Pre-trained Transformer (VL-GPT), a transformer model proficient at concurrently perceiving and generating visual and linguistic data. VL-GPT achieves a unified pre-training approach for both image and text modalities by employing a straightforward auto-regressive objective, thereby enabling the model to process image and text as seamlessly as a language model processes text. To accomplish this, we initially propose a novel image tokenizer-detokenizer framework for visual data, specifically designed to transform raw images into a sequence of continuous embeddings and reconstruct them accordingly. In combination with the existing text tokenizer and detokenizer, this framework allows for the encoding of interleaved image-text data into a multimodal sequence, which can subsequently be fed into the transformer model. Consequently, VL-GPT can perform large-scale pre-training on multimodal corpora utilizing a unified auto-regressive objective (i.e., next-token prediction). Upon completion of pre-training, VL-GPT exhibits remarkable zero-shot and few-shot performance across a diverse range of vision and language understanding and generation tasks, including image captioning, visual question answering, text-to-image generation, and more. Additionally, the pre-trained model retrains in-context learning capabilities when provided with multimodal prompts. We further conduct instruction tuning on our VL-GPT, highlighting its exceptional potential for multimodal assistance. The source code and model weights shall be released.]]></description>
            <pubDate>Fri, 15 Dec 2023 04:03:32 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2312.09251</guid>
            <link>https://arxiv.org/abs/2312.09251</link>
            
            
            
            <author><![CDATA[Jinguo Zhu, Xiaohan Ding, Yixiao Ge, Yuying Ge, Sijie Zhao, Hengshuang Zhao, Xiaohua Wang, Ying Shan]]></author>
            
                
            
            
        </item>
        
        <item>
            <title><![CDATA[Helping or Herding? Reward Model Ensembles Mitigate but do not Eliminate Reward Hacking]]></title>
            <description><![CDATA[Reward models play a key role in aligning language model applications towards human preferences. However, this setup creates an incentive for the language model to exploit errors in the reward model to achieve high estimated reward, a phenomenon often termed reward hacking. A natural mitigation is to train an ensemble of reward models, aggregating over model outputs to obtain a more robust reward estimate. We explore the application of reward ensembles to alignment at both training time (through reinforcement learning) and inference time (through reranking). First, we show that reward models are underspecified: reward models that perform similarly in-distribution can yield very different rewards when used in alignment, due to distribution shift. Second, underspecification results in overoptimization, where alignment to one reward model does not improve reward as measured by another reward model trained on the same data. Third, overoptimization is mitigated by the use of reward ensembles, and ensembles that vary by their pretraining seeds lead to better generalization than ensembles that differ only by their fine-tuning seeds, with both outperforming individual reward models. However, even pretrain reward ensembles do not eliminate reward hacking: we show several qualitative reward hacking phenomena that are not mitigated by ensembling because all reward models in the ensemble exhibit similar error patterns.]]></description>
            <pubDate>Fri, 15 Dec 2023 04:01:16 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2312.09244</guid>
            <link>https://arxiv.org/abs/2312.09244</link>
            
            
            
            <author><![CDATA[Jacob Eisenstein, Chirag Nagpal, Alekh Agarwal, Ahmad Beirami, Alex D&#39;Amour, DJ Dvijotham, Adam Fisch, Katherine Heller, Stephen Pfohl, Deepak Ramachandran, Peter Shaw, Jonathan Berant]]></author>
            
                
            
            
        </item>
        
        <item>
            <title><![CDATA[TinyGSM: achieving >80% on GSM8k with small language models]]></title>
            <description><![CDATA[Small-scale models offer various computational advantages, and yet to which extent size is critical for problem-solving abilities remains an open question. Specifically for solving grade school math, the smallest model size so far required to break the 80\% barrier on the GSM8K benchmark remains to be 34B. Our work studies how high-quality datasets may be the key for small language models to acquire mathematical reasoning. We introduce TinyGSM, a synthetic dataset of 12.3M grade school math problems paired with Python solutions, generated fully by GPT-3.5. After finetuning on TinyGSM, we find that a duo of a 1.3B generation model and a 1.3B verifier model can achieve 81.5\% accuracy, outperforming existing models that are orders of magnitude larger. This also rivals the performance of the GPT-3.5 ``teacher'' model (77.4\%), from which our model's training data is generated. Our approach is simple and has two key components: 1) the high-quality dataset TinyGSM, 2) the use of a verifier, which selects the final outputs from multiple candidate generations.]]></description>
            <pubDate>Fri, 15 Dec 2023 03:58:34 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2312.09241</guid>
            <link>https://arxiv.org/abs/2312.09241</link>
            
            
            
            <author><![CDATA[Bingbin Liu, Sebastien Bubeck, Ronen Eldan, Janardhan Kulkarni, Yuanzhi Li, Anh Nguyen, Rachel Ward, Yi Zhang]]></author>
            
                
            
            
        </item>
        
        <item>
            <title><![CDATA[Pixel Aligned Language Models]]></title>
            <description><![CDATA[Large language models have achieved great success in recent years, so as their variants in vision. Existing vision-language models can describe images in natural languages, answer visual-related questions, or perform complex reasoning about the image. However, it is yet unclear how localization tasks, such as word grounding or referring localization, can be performed using large language models. In this work, we aim to develop a vision-language model that can take locations, for example, a set of points or boxes, as either inputs or outputs. When taking locations as inputs, the model performs location-conditioned captioning, which generates captions for the indicated object or region. When generating locations as outputs, our model regresses pixel coordinates for each output word generated by the language model, and thus performs dense word grounding. Our model is pre-trained on the Localized Narrative dataset, which contains pixel-word-aligned captioning from human attention. We show our model can be applied to various location-aware vision-language tasks, including referring localization, location-conditioned captioning, and dense object captioning, archiving state-of-the-art performance on RefCOCO and Visual Genome. Project page: https://jerryxu.net/PixelLLM .]]></description>
            <pubDate>Fri, 15 Dec 2023 03:56:16 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2312.09237</guid>
            <link>https://arxiv.org/abs/2312.09237</link>
            
            
            
            <author><![CDATA[Jiarui Xu, Xingyi Zhou, Shen Yan, Xiuye Gu, Anurag Arnab, Chen Sun, Xiaolong Wang, Cordelia Schmid]]></author>
            
                
            
            
        </item>
        
        <item>
            <title><![CDATA[Vision-Language Models as a Source of Rewards]]></title>
            <description><![CDATA[Building generalist agents that can accomplish many goals in rich open-ended environments is one of the research frontiers for reinforcement learning. A key limiting factor for building generalist agents with RL has been the need for a large number of reward functions for achieving different goals. We investigate the feasibility of using off-the-shelf vision-language models, or VLMs, as sources of rewards for reinforcement learning agents. We show how rewards for visual achievement of a variety of language goals can be derived from the CLIP family of models, and used to train RL agents that can achieve a variety of language goals. We showcase this approach in two distinct visual domains and present a scaling trend showing how larger VLMs lead to more accurate rewards for visual goal achievement, which in turn produces more capable RL agents.]]></description>
            <pubDate>Fri, 15 Dec 2023 03:50:54 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2312.09187</guid>
            <link>https://arxiv.org/abs/2312.09187</link>
            
            
            
            <author><![CDATA[Kate Baumli, Satinder Baveja, Feryal Behbahani, Harris Chan, Gheorghe Comanici, Sebastian Flennerhag, Maxime Gazeau, Kristian Holsheimer, Dan Horgan, Michael Laskin, Clare Lyle, Hussain Masoom, Kay McKinney, Volodymyr Mnih, Alexander Neitz, Fabio Pardo, Jack Parker-Holder, John Quan, Tim RocktÃ¤schel, Himanshu Sahni, Tom Schaul, Yannick Schroecker, Stephen Spencer, Richie Steigerwald, Luyu Wang, Lei Zhang]]></author>
            
                
            
            
        </item>
        
        <item>
            <title><![CDATA[General Object Foundation Model for Images and Videos at Scale]]></title>
            <description><![CDATA[We present GLEE in this work, an object-level foundation model for locating and identifying objects in images and videos. Through a unified framework, GLEE accomplishes detection, segmentation, tracking, grounding, and identification of arbitrary objects in the open world scenario for various object perception tasks. Adopting a cohesive learning strategy, GLEE acquires knowledge from diverse data sources with varying supervision levels to formulate general object representations, excelling in zero-shot transfer to new data and tasks. Specifically, we employ an image encoder, text encoder, and visual prompter to handle multi-modal inputs, enabling to simultaneously solve various object-centric downstream tasks while maintaining state-of-the-art performance. Demonstrated through extensive training on over five million images from diverse benchmarks, GLEE exhibits remarkable versatility and improved generalization performance, efficiently tackling downstream tasks without the need for task-specific adaptation. By integrating large volumes of automatically labeled data, we further enhance its zero-shot generalization capabilities. Additionally, GLEE is capable of being integrated into Large Language Models, serving as a foundational model to provide universal object-level information for multi-modal tasks. We hope that the versatility and universality of our method will mark a significant step in the development of efficient visual foundation models for AGI systems. The model and code will be released at https://glee-vision.github.io .]]></description>
            <pubDate>Fri, 15 Dec 2023 03:49:05 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2312.09158</guid>
            <link>https://arxiv.org/abs/2312.09158</link>
            
            
            
            <author><![CDATA[Junfeng Wu, Yi Jiang, Qihao Liu, Zehuan Yuan, Xiang Bai, Song Bai]]></author>
            
                
            
            
        </item>
        
        <item>
            <title><![CDATA[CogAgent: A Visual Language Model for GUI Agents]]></title>
            <description><![CDATA[People are spending an enormous amount of time on digital devices through graphical user interfaces (GUIs), e.g., computer or smartphone screens. Large language models (LLMs) such as ChatGPT can assist people in tasks like writing emails, but struggle to understand and interact with GUIs, thus limiting their potential to increase automation levels. In this paper, we introduce CogAgent, an 18-billion-parameter visual language model (VLM) specializing in GUI understanding and navigation. By utilizing both low-resolution and high-resolution image encoders, CogAgent supports input at a resolution of 1120*1120, enabling it to recognize tiny page elements and text. As a generalist visual language model, CogAgent achieves the state of the art on five text-rich and four general VQA benchmarks, including VQAv2, OK-VQA, Text-VQA, ST-VQA, ChartQA, infoVQA, DocVQA, MM-Vet, and POPE. CogAgent, using only screenshots as input, outperforms LLM-based methods that consume extracted HTML text on both PC and Android GUI navigation tasks -- Mind2Web and AITW, advancing the state of the art. The model and codes are available at https://github.com/THUDM/CogVLM.]]></description>
            <pubDate>Fri, 15 Dec 2023 03:44:46 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2312.08914</guid>
            <link>https://arxiv.org/abs/2312.08914</link>
            
            
            
            <author><![CDATA[Wenyi Hong, Weihan Wang, Qingsong Lv, Jiazheng Xu, Wenmeng Yu, Junhui Ji, Yan Wang, Zihan Wang, Yuxiao Dong, Ming Ding, Jie Tang]]></author>
            
                
            
            
        </item>
        
        <item>
            <title><![CDATA[TigerBot: An Open Multilingual Multitask LLM]]></title>
            <description><![CDATA[We release and introduce the TigerBot family of large language models (LLMs), consisting of base and chat models, sized from 7, 13, 70 and 180 billion parameters. We develop our models embarking from Llama-2 and BLOOM, and push the boundary further in data, training algorithm, infrastructure, and application tools. Our models yield meaningful performance gain over SOTA open-source models, e.g., Llama-2, specifically 6\% gain in English and 20\% gain in Chinese. TigerBot model family also achieves leading performance in major academic and industrial benchmarks and leaderboards. We believe that TigerBot represents just a snapshot of lightning-fast progression in LLM open-source community. Therefore, we are thrilled to give back by publicly releasing our models and reporting our approach behind, with additional emphases on building SOTA LLMs in a democratized way and making LLMs of use in real-world applications.]]></description>
            <pubDate>Fri, 15 Dec 2023 03:36:24 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2312.08688</guid>
            <link>https://arxiv.org/abs/2312.08688</link>
            
            
            
            <author><![CDATA[Ye Chen, Wei Cai, Liangmin Wu, Xiaowei Li, Zhanxuan Xin, Cong Fu]]></author>
            
                
            
            
        </item>
        
        <item>
            <title><![CDATA[Zebra: Extending Context Window with Layerwise Grouped Local-Global Attention]]></title>
            <description><![CDATA[This paper introduces a novel approach to enhance the capabilities of Large Language Models (LLMs) in processing and understanding extensive text sequences, a critical aspect in applications requiring deep comprehension and synthesis of large volumes of information. Recognizing the inherent challenges in extending the context window for LLMs, primarily built on Transformer architecture, we propose a new model architecture, referred to as Zebra. This architecture efficiently manages the quadratic time and memory complexity issues associated with full attention in the Transformer by employing grouped local-global attention layers. Our model, akin to a zebra's alternating stripes, balances local and global attention layers, significantly reducing computational requirements and memory consumption. Comprehensive experiments, including pretraining from scratch, continuation of long context adaptation training, and long instruction tuning, are conducted to evaluate the Zebra's performance. The results show that Zebra achieves comparable or superior performance on both short and long sequence benchmarks, while also enhancing training and inference efficiency.]]></description>
            <pubDate>Fri, 15 Dec 2023 03:33:47 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2312.08618</guid>
            <link>https://arxiv.org/abs/2312.08618</link>
            
            
            
            <author><![CDATA[Kaiqiang Song, Xiaoyang Wang, Sangwoo Cho, Xiaoman Pan, Dong Yu]]></author>
            
                
            
            
        </item>
        
        <item>
            <title><![CDATA[ZeroQuant(4+2): Redefining LLMs Quantization with a New FP6-Centric Strategy for Diverse Generative Tasks]]></title>
            <description><![CDATA[This study examines 4-bit quantization methods like GPTQ in large language models (LLMs), highlighting GPTQ's overfitting and limited enhancement in Zero-Shot tasks. While prior works merely focusing on zero-shot measurement, we extend task scope to more generative categories such as code generation and abstractive summarization, in which we found that INT4 quantization can significantly underperform. However, simply shifting to higher precision formats like FP6 has been particularly challenging, thus overlooked, due to poor performance caused by the lack of sophisticated integration and system acceleration strategies on current AI hardware. Our results show that FP6, even with a coarse-grain quantization scheme, performs robustly across various algorithms and tasks, demonstrating its superiority in accuracy and versatility. Notably, with the FP6 quantization, \codestar-15B model performs comparably to its FP16 counterpart in code generation, and for smaller models like the 406M it closely matches their baselines in summarization. Neither can be achieved by INT4. To better accommodate various AI hardware and achieve the best system performance, we propose a novel 4+2 design for FP6 to achieve similar latency to the state-of-the-art INT4 fine-grain quantization. With our design, FP6 can become a promising solution to the current 4-bit quantization methods used in LLMs.]]></description>
            <pubDate>Fri, 15 Dec 2023 03:28:52 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2312.08583</guid>
            <link>https://arxiv.org/abs/2312.08583</link>
            
            
            
            <author><![CDATA[Xiaoxia Wu, Haojun Xia, Stephen Youn, Zhen Zheng, Shiyang Chen, Arash Bakhtiari, Michael Wyatt, Yuxiong He, Olatunji Ruwase, Leon Song, Zhewei Yao]]></author>
            
                
            
            
        </item>
        
        <item>
            <title><![CDATA[A Picture is Worth More Than 77 Text Tokens: Evaluating CLIP-Style Models on Dense Captions]]></title>
            <description><![CDATA[Curation methods for massive vision-language datasets trade off between dataset size and quality. However, even the highest quality of available curated captions are far too short to capture the rich visual detail in an image. To show the value of dense and highly-aligned image-text pairs, we collect the Densely Captioned Images (DCI) dataset, containing 8012 natural images human-annotated with mask-aligned descriptions averaging above 1000 words each. With precise and reliable captions associated with specific parts of an image, we can evaluate vision-language models' (VLMs) understanding of image content with a novel task that matches each caption with its corresponding subcrop. As current models are often limited to 77 text tokens, we also introduce a summarized version (sDCI) in which each caption length is limited. We show that modern techniques that make progress on standard benchmarks do not correspond with significant improvement on our sDCI based benchmark. Lastly, we finetune CLIP using sDCI and show significant improvements over the baseline despite a small training set. By releasing the first human annotated dense image captioning dataset, we hope to enable the development of new benchmarks or fine-tuning recipes for the next generation of VLMs to come.]]></description>
            <pubDate>Fri, 15 Dec 2023 03:23:12 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2312.08578</guid>
            <link>https://arxiv.org/abs/2312.08578</link>
            
            
            
            <author><![CDATA[Jack Urbanek, Florian Bordes, Pietro Astolfi, Mary Williamson, Vasu Sharma, Adriana Romero-Soriano]]></author>
            
                
            
            
        </item>
        
        <item>
            <title><![CDATA[Holodeck: Language Guided Generation of 3D Embodied AI Environments]]></title>
            <description><![CDATA[3D simulated environments play a critical role in Embodied AI, but their creation requires expertise and extensive manual effort, restricting their diversity and scope. To mitigate this limitation, we present Holodeck, a system that generates 3D environments to match a user-supplied prompt fully automatedly. Holodeck can generate diverse scenes, e.g., arcades, spas, and museums, adjust the designs for styles, and can capture the semantics of complex queries such as "apartment for a researcher with a cat" and "office of a professor who is a fan of Star Wars". Holodeck leverages a large language model (GPT-4) for common sense knowledge about what the scene might look like and uses a large collection of 3D assets from Objaverse to populate the scene with diverse objects. To address the challenge of positioning objects correctly, we prompt GPT-4 to generate spatial relational constraints between objects and then optimize the layout to satisfy those constraints. Our large-scale human evaluation shows that annotators prefer Holodeck over manually designed procedural baselines in residential scenes and that Holodeck can produce high-quality outputs for diverse scene types. We also demonstrate an exciting application of Holodeck in Embodied AI, training agents to navigate in novel scenes like music rooms and daycares without human-constructed data, which is a significant step forward in developing general-purpose embodied agents.]]></description>
            <pubDate>Fri, 15 Dec 2023 02:18:02 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2312.09067</guid>
            <link>https://arxiv.org/abs/2312.09067</link>
            
            
            
            <author><![CDATA[Yue Yang, Fan-Yun Sun, Luca Weihs, Eli VanderBilt, Alvaro Herrasti, Winson Han, Jiajun Wu, Nick Haber, Ranjay Krishna, Lingjie Liu, Chris Callison-Burch, Mark Yatskar, Aniruddha Kembhavi, Christopher Clark]]></author>
            
                
            
            
        </item>
        
        <item>
            <title><![CDATA[Modeling Complex Mathematical Reasoning via Large Language Model based MathAgent]]></title>
            <description><![CDATA[Large language models (LLMs) face challenges in solving complex mathematical problems that require comprehensive capacities to parse the statements, associate domain knowledge, perform compound logical reasoning, and integrate the intermediate rationales. Tackling all these problems once could be arduous for LLMs, thus leading to confusion in generation. In this work, we explore the potential of enhancing LLMs with agents by meticulous decomposition and modeling of mathematical reasoning process. Specifically, we propose a formal description of the mathematical solving and extend LLMs with an agent-based zero-shot framework named Planner-Reasoner-Executor-Reflector (PRER). We further provide and implement two MathAgents that define the logical forms and inherent relations via a pool of actions in different grains and orientations: MathAgent-M adapts its actions to LLMs, while MathAgent-H aligns with humankind. Experiments on miniF2F and MATH have demonstrated the effectiveness of PRER and proposed MathAgents, achieving an increase of 12.3%(53.9%66.2%) on the MiniF2F, 9.2% (49.8%59.0%) on MATH, and 13.2%(23.2%35.4%) for level-5 problems of MATH against GPT-4. Further analytical results provide more insightful perspectives on exploiting the behaviors of LLMs as agents.]]></description>
            <pubDate>Fri, 15 Dec 2023 01:58:29 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2312.08926</guid>
            <link>https://arxiv.org/abs/2312.08926</link>
            
            
            
            <author><![CDATA[Haoran Liao, Qinyi Du, Shaohua Hu, Hao He, Yanyan Xu, Jidong Tian, Yaohui Jin]]></author>
            
                
            
            
        </item>
        
    </channel>
</rss>
