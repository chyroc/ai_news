<?xml version="1.0" encoding="UTF-8"?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"


>
    <channel>
        <title><![CDATA[Huggingface Daily Papers]]></title>
        <link>https://huggingface.co/papers</link>
        <atom:link href="https://rsshub.app/huggingface/daily-papers" rel="self" type="application/rss+xml" />
        <description><![CDATA[Huggingface Daily Papers - Made with love by RSSHub(https://github.com/DIYgod/RSSHub)]]></description>
        <generator>RSSHub</generator>
        <webMaster>i@diygod.me (DIYgod)</webMaster>
        
        
        
        <language>zh-cn</language>
        
        <lastBuildDate>Fri, 08 Dec 2023 03:43:44 GMT</lastBuildDate>
        <ttl>120</ttl>
        
        <item>
            <title><![CDATA[AnimateZero: Video Diffusion Models are Zero-Shot Image Animators]]></title>
            <description><![CDATA[Large-scale text-to-video (T2V) diffusion models have great progress in recent years in terms of visual quality, motion and temporal consistency. However, the generation process is still a black box, where all attributes (e.g., appearance, motion) are learned and generated jointly without precise control ability other than rough text descriptions. Inspired by image animation which decouples the video as one specific appearance with the corresponding motion, we propose AnimateZero to unveil the pre-trained text-to-video diffusion model, i.e., AnimateDiff, and provide more precise appearance and motion control abilities for it. For appearance control, we borrow intermediate latents and their features from the text-to-image (T2I) generation for ensuring the generated first frame is equal to the given generated image. For temporal control, we replace the global temporal attention of the original T2V model with our proposed positional-corrected window attention to ensure other frames align with the first frame well. Empowered by the proposed methods, AnimateZero can successfully control the generating progress without further training. As a zero-shot image animator for given images, AnimateZero also enables multiple new applications, including interactive video generation and real image animation. The detailed experiments demonstrate the effectiveness of the proposed method in both T2V and related applications.]]></description>
            <pubDate>Fri, 08 Dec 2023 03:41:56 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2312.03793</guid>
            <link>https://arxiv.org/abs/2312.03793</link>
            
            
            
            <author><![CDATA[Jiwen Yu, Xiaodong Cun, Chenyang Qi, Yong Zhang, Xintao Wang, Ying Shan, Jian Zhang]]></author>
            
                
            
            
        </item>
        
        <item>
            <title><![CDATA[Gen2Det: Generate to Detect]]></title>
            <description><![CDATA[Recently diffusion models have shown improvement in synthetic image quality as well as better control in generation. We motivate and present Gen2Det, a simple modular pipeline to create synthetic training data for object detection for free by leveraging state-of-the-art grounded image generation methods. Unlike existing works which generate individual object instances, require identifying foreground followed by pasting on other images, we simplify to directly generating scene-centric images. In addition to the synthetic data, Gen2Det also proposes a suite of techniques to best utilize the generated data, including image-level filtering, instance-level filtering, and better training recipe to account for imperfections in the generation. Using Gen2Det, we show healthy improvements on object detection and segmentation tasks under various settings and agnostic to detection methods. In the long-tailed detection setting on LVIS, Gen2Det improves the performance on rare categories by a large margin while also significantly improving the performance on other categories, e.g. we see an improvement of 2.13 Box AP and 1.84 Mask AP over just training on real data on LVIS with Mask R-CNN. In the low-data regime setting on COCO, Gen2Det consistently improves both Box and Mask AP by 2.27 and 1.85 points. In the most general detection setting, Gen2Det still demonstrates robust performance gains, e.g. it improves the Box and Mask AP on COCO by 0.45 and 0.32 points.]]></description>
            <pubDate>Fri, 08 Dec 2023 03:31:08 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2312.04566</guid>
            <link>https://arxiv.org/abs/2312.04566</link>
            
            
            
            <author><![CDATA[Saksham Suri, Fanyi Xiao, Animesh Sinha, Sean Chang Culatana, Raghuraman Krishnamoorthi, Chenchen Zhu, Abhinav Shrivastava]]></author>
            
                
            
            
        </item>
        
        <item>
            <title><![CDATA[PhotoMaker: Customizing Realistic Human Photos via Stacked ID Embedding]]></title>
            <description><![CDATA[Recent advances in text-to-image generation have made remarkable progress in synthesizing realistic human photos conditioned on given text prompts. However, existing personalized generation methods cannot simultaneously satisfy the requirements of high efficiency, promising identity (ID) fidelity, and flexible text controllability. In this work, we introduce PhotoMaker, an efficient personalized text-to-image generation method, which mainly encodes an arbitrary number of input ID images into a stack ID embedding for preserving ID information. Such an embedding, serving as a unified ID representation, can not only encapsulate the characteristics of the same input ID comprehensively, but also accommodate the characteristics of different IDs for subsequent integration. This paves the way for more intriguing and practically valuable applications. Besides, to drive the training of our PhotoMaker, we propose an ID-oriented data construction pipeline to assemble the training data. Under the nourishment of the dataset constructed through the proposed pipeline, our PhotoMaker demonstrates better ID preservation ability than test-time fine-tuning based methods, yet provides significant speed improvements, high-quality generation results, strong generalization capabilities, and a wide range of applications. Our project page is available at https://photo-maker.github.io/]]></description>
            <pubDate>Fri, 08 Dec 2023 03:29:16 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2312.04461</guid>
            <link>https://arxiv.org/abs/2312.04461</link>
            
            
            
            <author><![CDATA[Zhen Li, Mingdeng Cao, Xintao Wang, Zhongang Qi, Ming-Ming Cheng, Ying Shan]]></author>
            
                
            
            
        </item>
        
        <item>
            <title><![CDATA[DreamVideo: Composing Your Dream Videos with Customized Subject and Motion]]></title>
            <description><![CDATA[Customized generation using diffusion models has made impressive progress in image generation, but remains unsatisfactory in the challenging video generation task, as it requires the controllability of both subjects and motions. To that end, we present DreamVideo, a novel approach to generating personalized videos from a few static images of the desired subject and a few videos of target motion. DreamVideo decouples this task into two stages, subject learning and motion learning, by leveraging a pre-trained video diffusion model. The subject learning aims to accurately capture the fine appearance of the subject from provided images, which is achieved by combining textual inversion and fine-tuning of our carefully designed identity adapter. In motion learning, we architect a motion adapter and fine-tune it on the given videos to effectively model the target motion pattern. Combining these two lightweight and efficient adapters allows for flexible customization of any subject with any motion. Extensive experimental results demonstrate the superior performance of our DreamVideo over the state-of-the-art methods for customized video generation. Our project page is at https://dreamvideo-t2v.github.io.]]></description>
            <pubDate>Fri, 08 Dec 2023 03:21:24 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2312.04433</guid>
            <link>https://arxiv.org/abs/2312.04433</link>
            
            
            
            <author><![CDATA[Yujie Wei, Shiwei Zhang, Zhiwu Qing, Hangjie Yuan, Zhiheng Liu, Yu Liu, Yingya Zhang, Jingren Zhou, Hongming Shan]]></author>
            
                
            
            
        </item>
        
        <item>
            <title><![CDATA[Efficient Monotonic Multihead Attention]]></title>
            <description><![CDATA[We introduce the Efficient Monotonic Multihead Attention (EMMA), a state-of-the-art simultaneous translation model with numerically-stable and unbiased monotonic alignment estimation. In addition, we present improved training and inference strategies, including simultaneous fine-tuning from an offline translation model and reduction of monotonic alignment variance. The experimental results demonstrate that the proposed model attains state-of-the-art performance in simultaneous speech-to-text translation on the Spanish and English translation task.]]></description>
            <pubDate>Fri, 08 Dec 2023 02:27:37 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2312.04515</guid>
            <link>https://arxiv.org/abs/2312.04515</link>
            
            
            
            <author><![CDATA[Xutai Ma, Anna Sun, Siqi Ouyang, Hirofumi Inaguma, Paden Tomasello]]></author>
            
                
            
            
        </item>
        
        <item>
            <title><![CDATA[Generating Illustrated Instructions]]></title>
            <description><![CDATA[We introduce the new task of generating Illustrated Instructions, i.e., visual instructions customized to a user's needs. We identify desiderata unique to this task, and formalize it through a suite of automatic and human evaluation metrics, designed to measure the validity, consistency, and efficacy of the generations. We combine the power of large language models (LLMs) together with strong text-to-image generation diffusion models to propose a simple approach called StackedDiffusion, which generates such illustrated instructions given text as input. The resulting model strongly outperforms baseline approaches and state-of-the-art multimodal LLMs; and in 30% of cases, users even prefer it to human-generated articles. Most notably, it enables various new and exciting applications far beyond what static articles on the web can provide, such as personalized instructions complete with intermediate steps and pictures in response to a user's individual situation.]]></description>
            <pubDate>Fri, 08 Dec 2023 02:24:28 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2312.04552</guid>
            <link>https://arxiv.org/abs/2312.04552</link>
            
            
            
            <author><![CDATA[Sachit Menon, Ishan Misra, Rohit Girdhar]]></author>
            
                
            
            
        </item>
        
        <item>
            <title><![CDATA[Chain of Code: Reasoning with a Language Model-Augmented Code Emulator]]></title>
            <description><![CDATA[Code provides a general syntactic structure to build complex programs and perform precise computations when paired with a code interpreter -- we hypothesize that language models (LMs) can leverage code-writing to improve Chain of Thought reasoning not only for logic and arithmetic tasks, but also for linguistic ones (and in particular, those that are a mix of both). For example, consider prompting an LM to write code that counts the number of times it detects sarcasm in an essay: the LM may struggle to write an implementation for "detect_sarcasm(string)" that can be executed by the interpreter (handling the edge cases would be insurmountable). However, LMs may still produce a valid solution if they are used not only to write the code, but also to selectively "emulate" the interpreter by generating the expected output of "detect_sarcasm(string)" and other lines of code (e.g., that the interpreter could not compile). In this work, we propose Chain of Code (CoT), a simple yet surprisingly effective extension that improves LM code-driven reasoning. The key idea is to encourage LMs to format linguistic sub-tasks in a program as flexible pseudocode that the compiler can explicitly catch undefined behaviors and hand off to simulate with an LM (as an "LMulator"). Experiments demonstrate that Chain of Code outperforms Chain of Thought and other baselines across a variety of benchmarks; on BIG-Bench Hard, Chain of Code achieves 84%, a gain of 12% over Chain of Thought. CoT scales well with large and small models alike, and broadens the scope of reasoning questions that LMs can correctly answer by "thinking in code". Project webpage: https://chain-of-code.github.io/.]]></description>
            <pubDate>Fri, 08 Dec 2023 02:21:51 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2312.04474</guid>
            <link>https://arxiv.org/abs/2312.04474</link>
            
            
            
            <author><![CDATA[Chengshu Li, Jacky Liang, Andy Zeng, Xinyun Chen, Karol Hausman, Dorsa Sadigh, Sergey Levine, Li Fei-Fei, Fei Xia, Brian Ichter]]></author>
            
                
            
            
        </item>
        
        <item>
            <title><![CDATA[Beyond Surface: Probing LLaMA Across Scales and Layers]]></title>
            <description><![CDATA[This paper presents an in-depth analysis of Large Language Models (LLMs), focusing on LLaMA, a prominent open-source foundational model in natural language processing. Instead of assessing LLaMA through its generative output, we design multiple-choice tasks to probe its intrinsic understanding in high-order tasks such as reasoning and computation. We examine the model horizontally, comparing different sizes, and vertically, assessing different layers. We unveil several key and uncommon findings based on the designed probing tasks: (1) Horizontally, enlarging model sizes almost could not automatically impart additional knowledge or computational prowess. Instead, it can enhance reasoning abilities, especially in math problem solving, and helps reduce hallucinations, but only beyond certain size thresholds; (2) In vertical analysis, the lower layers of LLaMA lack substantial arithmetic and factual knowledge, showcasing logical thinking, multilingual and recognitive abilities, with top layers housing most computational power and real-world knowledge.]]></description>
            <pubDate>Fri, 08 Dec 2023 02:19:49 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2312.04333</guid>
            <link>https://arxiv.org/abs/2312.04333</link>
            
            
            
            <author><![CDATA[Nuo Chen, Ning Wu, Shining Liang, Ming Gong, Linjun Shou, Dongmei Zhang, Jia Li]]></author>
            
                
            
            
        </item>
        
        <item>
            <title><![CDATA[LEGO: Learning EGOcentric Action Frame Generation via Visual Instruction Tuning]]></title>
            <description><![CDATA[Generating instructional images of human daily actions from an egocentric viewpoint serves a key step towards efficient skill transfer. In this paper, we introduce a novel problem -- egocentric action frame generation. The goal is to synthesize the action frame conditioning on the user prompt question and an input egocentric image that captures user's environment. Notably, existing egocentric datasets lack the detailed annotations that describe the execution of actions. Additionally, the diffusion-based image manipulation models fail to control the state change of an action within the corresponding egocentric image pixel space. To this end, we finetune a visual large language model (VLLM) via visual instruction tuning for curating the enriched action descriptions to address our proposed problem. Moreover, we propose to Learn EGOcentric (LEGO) action frame generation using image and text embeddings from VLLM as additional conditioning. We validate our proposed model on two egocentric datasets -- Ego4D and Epic-Kitchens. Our experiments show prominent improvement over prior image manipulation models in both quantitative and qualitative evaluation. We also conduct detailed ablation studies and analysis to provide insights on our method.]]></description>
            <pubDate>Fri, 08 Dec 2023 02:15:15 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2312.03849</guid>
            <link>https://arxiv.org/abs/2312.03849</link>
            
            
            
            <author><![CDATA[Bolin Lai, Xiaoliang Dai, Lawrence Chen, Guan Pang, James M. Rehg, Miao Liu]]></author>
            
                
            
            
        </item>
        
        <item>
            <title><![CDATA[Alpha-CLIP: A CLIP Model Focusing on Wherever You Want]]></title>
            <description><![CDATA[Contrastive Language-Image Pre-training (CLIP) plays an essential role in extracting valuable content information from images across diverse tasks. It aligns textual and visual modalities to comprehend the entire image, including all the details, even those irrelevant to specific tasks. However, for a finer understanding and controlled editing of images, it becomes crucial to focus on specific regions of interest, which can be indicated as points, masks, or boxes by humans or perception models. To fulfill the requirements, we introduce Alpha-CLIP, an enhanced version of CLIP with an auxiliary alpha channel to suggest attentive regions and fine-tuned with constructed millions of RGBA region-text pairs. Alpha-CLIP not only preserves the visual recognition ability of CLIP but also enables precise control over the emphasis of image contents. It demonstrates effectiveness in various tasks, including but not limited to open-world recognition, multimodal large language models, and conditional 2D / 3D generation. It has a strong potential to serve as a versatile tool for image-related tasks.]]></description>
            <pubDate>Fri, 08 Dec 2023 02:13:59 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2312.03818</guid>
            <link>https://arxiv.org/abs/2312.03818</link>
            
            
            
            <author><![CDATA[Zeyi Sun, Ye Fang, Tong Wu, Pan Zhang, Yuhang Zang, Shu Kong, Yuanjun Xiong, Dahua Lin, Jiaqi Wang]]></author>
            
                
            
            
        </item>
        
        <item>
            <title><![CDATA[Large Language Models for Mathematicians]]></title>
            <description><![CDATA[Large language models (LLMs) such as ChatGPT have received immense interest for their general-purpose language understanding and, in particular, their ability to generate high-quality text or computer code. For many professions, LLMs represent an invaluable tool that can speed up and improve the quality of work. In this note, we discuss to what extent they can aid professional mathematicians. We first provide a mathematical description of the transformer model used in all modern language models. Based on recent studies, we then outline best practices and potential issues and report on the mathematical abilities of language models. Finally, we shed light on the potential of LMMs to change how mathematicians work.]]></description>
            <pubDate>Fri, 08 Dec 2023 02:09:19 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2312.04556</guid>
            <link>https://arxiv.org/abs/2312.04556</link>
            
            
            
            <author><![CDATA[Simon Frieder, Julius Berner, Philipp Petersen, Thomas Lukasiewicz]]></author>
            
                
            
            
        </item>
        
    </channel>
</rss>
