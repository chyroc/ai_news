<?xml version="1.0" encoding="UTF-8"?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"


>
    <channel>
        <title><![CDATA[Huggingface Daily Papers]]></title>
        <link>https://huggingface.co/papers</link>
        <atom:link href="https://rsshub.app/huggingface/daily-papers" rel="self" type="application/rss+xml" />
        <description><![CDATA[Huggingface Daily Papers - Made with love by RSSHub(https://github.com/DIYgod/RSSHub)]]></description>
        <generator>RSSHub</generator>
        <webMaster>i@diygod.me (DIYgod)</webMaster>
        
        
        
        <language>zh-cn</language>
        
        <lastBuildDate>Tue, 12 Dec 2023 14:07:59 GMT</lastBuildDate>
        <ttl>120</ttl>
        
        <item>
            <title><![CDATA[Sherpa3D: Boosting High-Fidelity Text-to-3D Generation via Coarse 3D Prior]]></title>
            <description><![CDATA[Recently, 3D content creation from text prompts has demonstrated remarkable progress by utilizing 2D and 3D diffusion models. While 3D diffusion models ensure great multi-view consistency, their ability to generate high-quality and diverse 3D assets is hindered by the limited 3D data. In contrast, 2D diffusion models find a distillation approach that achieves excellent generalization and rich details without any 3D data. However, 2D lifting methods suffer from inherent view-agnostic ambiguity thereby leading to serious multi-face Janus issues, where text prompts fail to provide sufficient guidance to learn coherent 3D results. Instead of retraining a costly viewpoint-aware model, we study how to fully exploit easily accessible coarse 3D knowledge to enhance the prompts and guide 2D lifting optimization for refinement. In this paper, we propose Sherpa3D, a new text-to-3D framework that achieves high-fidelity, generalizability, and geometric consistency simultaneously. Specifically, we design a pair of guiding strategies derived from the coarse 3D prior generated by the 3D diffusion model: a structural guidance for geometric fidelity and a semantic guidance for 3D coherence. Employing the two types of guidance, the 2D diffusion model enriches the 3D content with diversified and high-quality results. Extensive experiments show the superiority of our Sherpa3D over the state-of-the-art text-to-3D methods in terms of quality and 3D consistency.]]></description>
            <pubDate>Tue, 12 Dec 2023 05:48:29 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2312.06655</guid>
            <link>https://arxiv.org/abs/2312.06655</link>
            
            
            
            <author><![CDATA[Fangfu Liu, Diankun Wu, Yi Wei, Yongming Rao, Yueqi Duan]]></author>
            
                
            
            
        </item>
        
        <item>
            <title><![CDATA[Photorealistic Video Generation with Diffusion Models]]></title>
            <description><![CDATA[We present W.A.L.T, a transformer-based approach for photorealistic video generation via diffusion modeling. Our approach has two key design decisions. First, we use a causal encoder to jointly compress images and videos within a unified latent space, enabling training and generation across modalities. Second, for memory and training efficiency, we use a window attention architecture tailored for joint spatial and spatiotemporal generative modeling. Taken together these design decisions enable us to achieve state-of-the-art performance on established video (UCF-101 and Kinetics-600) and image (ImageNet) generation benchmarks without using classifier free guidance. Finally, we also train a cascade of three models for the task of text-to-video generation consisting of a base latent video diffusion model, and two video super-resolution diffusion models to generate videos of 512 times 896 resolution at 8 frames per second.]]></description>
            <pubDate>Tue, 12 Dec 2023 05:46:25 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2312.06662</guid>
            <link>https://arxiv.org/abs/2312.06662</link>
            
            
            
            <author><![CDATA[Agrim Gupta, Lijun Yu, Kihyuk Sohn, Xiuye Gu, Meera Hahn, Li Fei-Fei, Irfan Essa, Lu Jiang, JosÃ© Lezama]]></author>
            
                
            
            
        </item>
        
        <item>
            <title><![CDATA[Efficient Quantization Strategies for Latent Diffusion Models]]></title>
            <description><![CDATA[Latent Diffusion Models (LDMs) capture the dynamic evolution of latent variables over time, blending patterns and multimodality in a generative system. Despite the proficiency of LDM in various applications, such as text-to-image generation, facilitated by robust text encoders and a variational autoencoder, the critical need to deploy large generative models on edge devices compels a search for more compact yet effective alternatives. Post Training Quantization (PTQ), a method to compress the operational size of deep learning models, encounters challenges when applied to LDM due to temporal and structural complexities. This study proposes a quantization strategy that efficiently quantize LDMs, leveraging Signal-to-Quantization-Noise Ratio (SQNR) as a pivotal metric for evaluation. By treating the quantization discrepancy as relative noise and identifying sensitive part(s) of a model, we propose an efficient quantization approach encompassing both global and local strategies. The global quantization process mitigates relative quantization noise by initiating higher-precision quantization on sensitive blocks, while local treatments address specific challenges in quantization-sensitive and time-sensitive modules. The outcomes of our experiments reveal that the implementation of both global and local treatments yields a highly efficient and effective Post Training Quantization (PTQ) of LDMs.]]></description>
            <pubDate>Tue, 12 Dec 2023 05:41:38 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2312.05431</guid>
            <link>https://arxiv.org/abs/2312.05431</link>
            
            
            
            <author><![CDATA[Yuewei Yang, Xiaoliang Dai, Jialiang Wang, Peizhao Zhang, Hongbo Zhang]]></author>
            
                
            
            
        </item>
        
        <item>
            <title><![CDATA[Beyond Human Data: Scaling Self-Training for Problem-Solving with Language Models]]></title>
            <description><![CDATA[Fine-tuning language models~(LMs) on human-generated data remains a prevalent practice. However, the performance of such models is often limited by the quantity and diversity of high-quality human data. In this paper, we explore whether we can go beyond human data on tasks where we have access to scalar feedback, for example, on math problems where one can verify correctness. To do so, we investigate a simple self-training method based on expectation-maximization, which we call ReST^{EM}, where we (1) generate samples from the model and filter them using binary feedback, (2) fine-tune the model on these samples, and (3) repeat this process a few times. Testing on advanced MATH reasoning and APPS coding benchmarks using PaLM-2 models, we find that ReST^{EM} scales favorably with model size and significantly surpasses fine-tuning only on human data. Overall, our findings suggest self-training with feedback can substantially reduce dependence on human-generated data.]]></description>
            <pubDate>Tue, 12 Dec 2023 05:38:44 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2312.06585</guid>
            <link>https://arxiv.org/abs/2312.06585</link>
            
            
            
            <author><![CDATA[Avi Singh, John D. Co-Reyes, Rishabh Agarwal, Ankesh Anand, Piyush Patil, Peter J. Liu, James Harrison, Jaehoon Lee, Kelvin Xu, Aaron Parisi, Abhishek Kumar, Alex Alemi, Alex Rizkowsky, Azade Nova, Ben Adlam, Bernd Bohnet, Hanie Sedghi, Igor Mordatch, Isabelle Simpson, Izzeddin Gur, Jasper Snoek, Jeffrey Pennington, Jiri Hron, Kathleen Kenealy, Kevin Swersky, Kshiteej Mahajan, Laura Culp, Lechao Xiao, Maxwell L. Bileschi, Noah Constant, Roman Novak, Rosanne Liu, Tris Warkentin, Yundi Qian, Ethan Dyer, Behnam Neyshabur, Jascha Sohl-Dickstein, Noah Fiedel]]></author>
            
                
            
            
        </item>
        
        <item>
            <title><![CDATA[LLM360: Towards Fully Transparent Open-Source LLMs]]></title>
            <description><![CDATA[The recent surge in open-source Large Language Models (LLMs), such as LLaMA, Falcon, and Mistral, provides diverse options for AI practitioners and researchers. However, most LLMs have only released partial artifacts, such as the final model weights or inference code, and technical reports increasingly limit their scope to high-level design choices and surface statistics. These choices hinder progress in the field by degrading transparency into the training of LLMs and forcing teams to rediscover many details in the training process. We present LLM360, an initiative to fully open-source LLMs, which advocates for all training code and data, model checkpoints, and intermediate results to be made available to the community. The goal of LLM360 is to support open and collaborative AI research by making the end-to-end LLM training process transparent and reproducible by everyone. As a first step of LLM360, we release two 7B parameter LLMs pre-trained from scratch, Amber and CrystalCoder, including their training code, data, intermediate checkpoints, and analyses (at https://www.llm360.ai). We are committed to continually pushing the boundaries of LLMs through this open-source effort. More large-scale and stronger models are underway and will be released in the future.]]></description>
            <pubDate>Tue, 12 Dec 2023 05:37:23 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2312.06550</guid>
            <link>https://arxiv.org/abs/2312.06550</link>
            
            
            
            <author><![CDATA[Zhengzhong Liu, Aurick Qiao, Willie Neiswanger, Hongyi Wang, Bowen Tan, Tianhua Tao, Junbo Li, Yuqi Wang, Suqi Sun, Omkar Pangarkar, Richard Fan, Yi Gu, Victor Miller, Yonghao Zhuang, Guowei He, Haonan Li, Fajri Koto, Liping Tang, Nikhil Ranjan, Zhiqiang Shen, Xuguang Ren, Roberto Iriondo, Cun Mu, Zhiting Hu, Mark Schulze, Preslav Nakov, Tim Baldwin, Eric P. Xing]]></author>
            
                
            
            
        </item>
        
        <item>
            <title><![CDATA[Federated Full-Parameter Tuning of Billion-Sized Language Models with Communication Cost under 18 Kilobytes]]></title>
            <description><![CDATA[Pre-trained large language models (LLMs) require fine-tuning to improve their responsiveness to natural language instructions. Federated learning (FL) offers a way to perform fine-tuning using the abundant data on end devices without compromising data privacy. Most existing federated fine-tuning methods for LLMs rely on parameter-efficient fine-tuning techniques, which may not reach the performance heights possible with full-parameter tuning. However, the communication overhead associated with full-parameter tuning is prohibitively high for both servers and clients. This work introduces FedKSeed, a novel approach that employs zeroth-order optimization (ZOO) with a set of random seeds. It enables federated full-parameter tuning of billion-sized LLMs directly on devices. Our method significantly reduces transmission requirements between the server and clients to just a few scalar gradients and random seeds, amounting to only a few thousand bytes. Building on this, we develop a strategy to assess the significance of ZOO perturbations for FL, allowing for probability-differentiated seed sampling. This prioritizes perturbations that have a greater impact on model accuracy. Experiments across six scenarios with different LLMs, datasets and data partitions demonstrate that our approach outperforms existing federated LLM fine-tuning methods in terms of both communication efficiency and new task generalization.]]></description>
            <pubDate>Tue, 12 Dec 2023 05:35:48 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2312.06353</guid>
            <link>https://arxiv.org/abs/2312.06353</link>
            
            
            
            <author><![CDATA[Zhen Qin, Daoyuan Chen, Bingchen Qian, Bolin Ding, Yaliang Li, Shuiguang Deng]]></author>
            
                
            
            
        </item>
        
        <item>
            <title><![CDATA[Evaluation of Large Language Models for Decision Making in Autonomous Driving]]></title>
            <description><![CDATA[Various methods have been proposed for utilizing Large Language Models (LLMs) in autonomous driving. One strategy of using LLMs for autonomous driving involves inputting surrounding objects as text prompts to the LLMs, along with their coordinate and velocity information, and then outputting the subsequent movements of the vehicle. When using LLMs for such purposes, capabilities such as spatial recognition and planning are essential. In particular, two foundational capabilities are required: (1) spatial-aware decision making, which is the ability to recognize space from coordinate information and make decisions to avoid collisions, and (2) the ability to adhere to traffic rules. However, quantitative research has not been conducted on how accurately different types of LLMs can handle these problems. In this study, we quantitatively evaluated these two abilities of LLMs in the context of autonomous driving. Furthermore, to conduct a Proof of Concept (POC) for the feasibility of implementing these abilities in actual vehicles, we developed a system that uses LLMs to drive a vehicle.]]></description>
            <pubDate>Tue, 12 Dec 2023 05:34:18 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2312.06351</guid>
            <link>https://arxiv.org/abs/2312.06351</link>
            
            
            
            <author><![CDATA[Kotaro Tanahashi, Yuichi Inoue, Yu Yamaguchi, Hidetatsu Yaginuma, Daiki Shiotsuka, Hiroyuki Shimatani, Kohei Iwamasa, Yoshiaki Inoue, Takafumi Yamaguchi, Koki Igari, Tsukasa Horinouchi, Kento Tokuhiro, Yugo Tokuchi, Shunsuke Aoki]]></author>
            
                
            
            
        </item>
        
        <item>
            <title><![CDATA[Order Matters in the Presence of Dataset Imbalance for Multilingual Learning]]></title>
            <description><![CDATA[In this paper, we empirically study the optimization dynamics of multi-task learning, particularly focusing on those that govern a collection of tasks with significant data imbalance. We present a simple yet effective method of pre-training on high-resource tasks, followed by fine-tuning on a mixture of high/low-resource tasks. We provide a thorough empirical study and analysis of this method's benefits showing that it achieves consistent improvements relative to the performance trade-off profile of standard static weighting. We analyze under what data regimes this method is applicable and show its improvements empirically in neural machine translation (NMT) and multi-lingual language modeling.]]></description>
            <pubDate>Tue, 12 Dec 2023 05:29:44 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2312.06134</guid>
            <link>https://arxiv.org/abs/2312.06134</link>
            
            
            
            <author><![CDATA[Dami Choi, Derrick Xin, Hamid Dadkhahi, Justin Gilmer, Ankush Garg, Orhan Firat, Chih-Kuan Yeh, Andrew M. Dai, Behrooz Ghorbani]]></author>
            
                
            
            
        </item>
        
        <item>
            <title><![CDATA[Vary: Scaling up the Vision Vocabulary for Large Vision-Language Models]]></title>
            <description><![CDATA[Modern Large Vision-Language Models (LVLMs) enjoy the same vision vocabulary -- CLIP, which can cover most common vision tasks. However, for some special vision task that needs dense and fine-grained vision perception, e.g., document-level OCR or chart understanding, especially in non-English scenarios, the CLIP-style vocabulary may encounter low efficiency in tokenizing the vision knowledge and even suffer out-of-vocabulary problem. Accordingly, we propose Vary, an efficient and effective method to scale up the vision vocabulary of LVLMs. The procedures of Vary are naturally divided into two folds: the generation and integration of a new vision vocabulary. In the first phase, we devise a vocabulary network along with a tiny decoder-only transformer to produce the desired vocabulary via autoregression. In the next, we scale up the vanilla vision vocabulary by merging the new one with the original one (CLIP), enabling the LVLMs can quickly garner new features. Compared to the popular BLIP-2, MiniGPT4, and LLaVA, Vary can maintain its vanilla capabilities while enjoying more excellent fine-grained perception and understanding ability. Specifically, Vary is competent in new document parsing features (OCR or markdown conversion) while achieving 78.2% ANLS in DocVQA and 36.2% in MMVet. Our code will be publicly available on the homepage.]]></description>
            <pubDate>Tue, 12 Dec 2023 04:08:37 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2312.06109</guid>
            <link>https://arxiv.org/abs/2312.06109</link>
            
            
            
            <author><![CDATA[Haoran Wei, Lingyu Kong, Jinyue Chen, Liang Zhao, Zheng Ge, Jinrong Yang, Jianjian Sun, Chunrui Han, Xiangyu Zhang]]></author>
            
                
            
            
        </item>
        
        <item>
            <title><![CDATA[TCNCA: Temporal Convolution Network with Chunked Attention for Scalable Sequence Processing]]></title>
            <description><![CDATA[MEGA is a recent transformer-based architecture, which utilizes a linear recurrent operator whose parallel computation, based on the FFT, scales as O(LlogL), with L being the sequence length. We build upon their approach by replacing the linear recurrence with a special temporal convolutional network which permits larger receptive field size with shallower networks, and reduces the computational complexity to O(L). The resulting model is called TCNCA, a Temporal Convolutional Network with Chunked Attention. We evaluate TCNCA on EnWik8 language modeling, long-range-arena (LRA) sequence classification, as well as a synthetic reasoning benchmark associative recall. On EnWik8, TCNCA outperforms MEGA, reaching a lower loss with 1.37times/1.24times faster forward/backward pass during training. The dilated convolutions used in TCNCA are consistently and significantly faster operations than the FFT-based parallelized recurrence in GPUs, making them a scalable candidate for handling very large sequence lengths: they are up to 7.07times/2.86times faster in the forward/backward pass for sequences up to 131k. Further on LRA, TCNCA achieves, on average, 1.28times speed-up during inference with similar accuracy to what MEGA achieves. On associative recall, we find that even a simplified version of TCNCA, without excessive multiplicative and additive interactions, remains superior or competitive to MEGA on a range of sequence lengths and vocabulary sizes.]]></description>
            <pubDate>Tue, 12 Dec 2023 03:59:31 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2312.05605</guid>
            <link>https://arxiv.org/abs/2312.05605</link>
            
            
            
            <author><![CDATA[Aleksandar Terzic, Michael Hersche, Geethan Karunaratne, Luca Benini, Abu Sebastian, Abbas Rahimi]]></author>
            
                
            
            
        </item>
        
        <item>
            <title><![CDATA[Using Captum to Explain Generative Language Models]]></title>
            <description><![CDATA[Captum is a comprehensive library for model explainability in PyTorch, offering a range of methods from the interpretability literature to enhance users' understanding of PyTorch models. In this paper, we introduce new features in Captum that are specifically designed to analyze the behavior of generative language models. We provide an overview of the available functionalities and example applications of their potential for understanding learned associations within generative language models.]]></description>
            <pubDate>Tue, 12 Dec 2023 03:56:51 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2312.05491</guid>
            <link>https://arxiv.org/abs/2312.05491</link>
            
            
            
            <author><![CDATA[Vivek Miglani, Aobo Yang, Aram H. Markosyan, Diego Garcia-Olano, Narine Kokhlikyan]]></author>
            
                
            
            
        </item>
        
        <item>
            <title><![CDATA[Unlocking Anticipatory Text Generation: A Constrained Approach for Faithful Decoding with Large Language Models]]></title>
            <description><![CDATA[Large Language Models (LLMs) have demonstrated a powerful ability for text generation. However, achieving optimal results with a given prompt or instruction can be challenging, especially for billion-sized models. Additionally, undesired behaviors such as toxicity or hallucinations can manifest. While much larger models (e.g., ChatGPT) may demonstrate strength in mitigating these issues, there is still no guarantee of complete prevention. In this work, we propose formalizing text generation as a future-constrained generation problem to minimize undesirable behaviors and enforce faithfulness to instructions. The estimation of future constraint satisfaction, accomplished using LLMs, guides the text generation process. Our extensive experiments demonstrate the effectiveness of the proposed approach across three distinct text generation tasks: keyword-constrained generation (Lin et al., 2020), toxicity reduction (Gehman et al., 2020), and factual correctness in question-answering (Gao et al., 2023).]]></description>
            <pubDate>Tue, 12 Dec 2023 03:51:45 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2312.06149</guid>
            <link>https://arxiv.org/abs/2312.06149</link>
            
            
            
            <author><![CDATA[Lifu Tu, Semih Yavuz, Jin Qu, Jiacheng Xu, Rui Meng, Caiming Xiong, Yingbo Zhou]]></author>
            
                
            
            
        </item>
        
        <item>
            <title><![CDATA[From Text to Motion: Grounding GPT-4 in a Humanoid Robot "Alter3"]]></title>
            <description><![CDATA[We report the development of Alter3, a humanoid robot capable of generating spontaneous motion using a Large Language Model (LLM), specifically GPT-4. This achievement was realized by integrating GPT-4 into our proprietary android, Alter3, thereby effectively grounding the LLM with Alter's bodily movement. Typically, low-level robot control is hardware-dependent and falls outside the scope of LLM corpora, presenting challenges for direct LLM-based robot control. However, in the case of humanoid robots like Alter3, direct control is feasible by mapping the linguistic expressions of human actions onto the robot's body through program code. Remarkably, this approach enables Alter3 to adopt various poses, such as a 'selfie' stance or 'pretending to be a ghost,' and generate sequences of actions over time without explicit programming for each body part. This demonstrates the robot's zero-shot learning capabilities. Additionally, verbal feedback can adjust poses, obviating the need for fine-tuning. A video of Alter3's generated motions is available at https://tnoinkwms.github.io/ALTER-LLM/]]></description>
            <pubDate>Tue, 12 Dec 2023 03:07:45 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2312.06571</guid>
            <link>https://arxiv.org/abs/2312.06571</link>
            
            
            
            <author><![CDATA[Takahide Yoshida, Atsushi Masumori, Takashi Ikegami]]></author>
            
                
            
            
        </item>
        
        <item>
            <title><![CDATA[Context Tuning for Retrieval Augmented Generation]]></title>
            <description><![CDATA[Large language models (LLMs) have the remarkable ability to solve new tasks with just a few examples, but they need access to the right tools. Retrieval Augmented Generation (RAG) addresses this problem by retrieving a list of relevant tools for a given task. However, RAG's tool retrieval step requires all the required information to be explicitly present in the query. This is a limitation, as semantic search, the widely adopted tool retrieval method, can fail when the query is incomplete or lacks context. To address this limitation, we propose Context Tuning for RAG, which employs a smart context retrieval system to fetch relevant information that improves both tool retrieval and plan generation. Our lightweight context retrieval model uses numerical, categorical, and habitual usage signals to retrieve and rank context items. Our empirical results demonstrate that context tuning significantly enhances semantic search, achieving a 3.5-fold and 1.5-fold improvement in Recall@K for context retrieval and tool retrieval tasks respectively, and resulting in an 11.6% increase in LLM-based planner accuracy. Additionally, we show that our proposed lightweight model using Reciprocal Rank Fusion (RRF) with LambdaMART outperforms GPT-4 based retrieval. Moreover, we observe context augmentation at plan generation, even after tool retrieval, reduces hallucination.]]></description>
            <pubDate>Tue, 12 Dec 2023 03:02:28 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2312.05708</guid>
            <link>https://arxiv.org/abs/2312.05708</link>
            
            
            
            <author><![CDATA[Raviteja Anantha, Tharun Bethi, Danil Vodianik, Srinivas Chappidi]]></author>
            
                
            
            
        </item>
        
    </channel>
</rss>
