<?xml version="1.0" encoding="UTF-8"?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"


>
    <channel>
        <title><![CDATA[Huggingface Daily Papers]]></title>
        <link>https://huggingface.co/papers</link>
        <atom:link href="https://rsshub.app/huggingface/daily-papers" rel="self" type="application/rss+xml" />
        <description><![CDATA[Huggingface Daily Papers - Made with love by RSSHub(https://github.com/DIYgod/RSSHub)]]></description>
        <generator>RSSHub</generator>
        <webMaster>i@diygod.me (DIYgod)</webMaster>
        
        
        
        <language>zh-cn</language>
        
        <lastBuildDate>Thu, 04 Jan 2024 02:42:12 GMT</lastBuildDate>
        <ttl>120</ttl>
        
        <item>
            <title><![CDATA[A Vision Check-up for Language Models]]></title>
            <description><![CDATA[What does learning to model relationships between strings teach large language models (LLMs) about the visual world? We systematically evaluate LLMs' abilities to generate and recognize an assortment of visual concepts of increasing complexity and then demonstrate how a preliminary visual representation learning system can be trained using models of text. As language models lack the ability to consume or output visual information as pixels, we use code to represent images in our study. Although LLM-generated images do not look like natural images, results on image generation and the ability of models to correct these generated images indicate that precise modeling of strings can teach language models about numerous aspects of the visual world. Furthermore, experiments on self-supervised visual representation learning, utilizing images generated with text models, highlight the potential to train vision models capable of making semantic assessments of natural images using just LLMs.]]></description>
            <pubDate>Thu, 04 Jan 2024 02:20:21 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2401.01862</guid>
            <link>https://arxiv.org/abs/2401.01862</link>
            
            
            
            <author><![CDATA[Pratyusha Sharma, Tamar Rott Shaham, Manel Baradad, Stephanie Fu, Adrian Rodriguez-Munoz, Shivam Duggal, Phillip Isola, Antonio Torralba]]></author>
            
                
            
            
        </item>
        
        <item>
            <title><![CDATA[Multilingual Instruction Tuning With Just a Pinch of Multilinguality]]></title>
            <description><![CDATA[As instruction-tuned large language models (LLMs) gain global adoption, their ability to follow instructions in multiple languages becomes increasingly crucial. One promising approach is cross-lingual transfer, where a model acquires specific functionality on some language by finetuning on another language. In this work, we investigate how multilinguality during instruction tuning of a multilingual LLM affects instruction-following across languages. We first show that many languages transfer some instruction-following capabilities to other languages from even monolingual tuning. Furthermore, we find that only 40 multilingual examples in an English tuning set substantially improve multilingual instruction-following, both in seen and unseen languages during tuning. In general, we observe that models tuned on multilingual mixtures exhibit comparable or superior performance in several languages compared to monolingually tuned models, despite training on 10x fewer examples in those languages. Finally, we find that increasing the number of languages in the instruction tuning set from 1 to only 2, 3, or 4 increases cross-lingual generalization. Our results suggest that building massively multilingual instruction-tuned models can be done with only a very small set of multilingual instruction-responses.]]></description>
            <pubDate>Thu, 04 Jan 2024 02:15:48 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2401.01854</guid>
            <link>https://arxiv.org/abs/2401.01854</link>
            
            
            
            <author><![CDATA[Uri Shaham, Jonathan Herzig, Roee Aharoni, Idan Szpektor, Reut Tsarfaty, Matan Eyal]]></author>
            
                
            
            
        </item>
        
        <item>
            <title><![CDATA[WordArt Designer API: User-Driven Artistic Typography Synthesis with Large Language Models on ModelScope]]></title>
            <description><![CDATA[This paper introduces the WordArt Designer API, a novel framework for user-driven artistic typography synthesis utilizing Large Language Models (LLMs) on ModelScope. We address the challenge of simplifying artistic typography for non-professionals by offering a dynamic, adaptive, and computationally efficient alternative to traditional rigid templates. Our approach leverages the power of LLMs to understand and interpret user input, facilitating a more intuitive design process. We demonstrate through various case studies how users can articulate their aesthetic preferences and functional requirements, which the system then translates into unique and creative typographic designs. Our evaluations indicate significant improvements in user satisfaction, design flexibility, and creative expression over existing systems. The WordArt Designer API not only democratizes the art of typography but also opens up new possibilities for personalized digital communication and design.]]></description>
            <pubDate>Thu, 04 Jan 2024 02:13:02 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2401.01699</guid>
            <link>https://arxiv.org/abs/2401.01699</link>
            
            
            
            <author><![CDATA[Jun-Yan He, Zhi-Qi Cheng, Chenyang Li, Jingdong Sun, Wangmeng Xiang, Yusen Hu, Xianhui Lin, Xiaoyang Kang, Zengke Jin, Bin Luo, Yifeng Geng, Xuansong Xie, Jingren Zhou]]></author>
            
                
            
            
        </item>
        
        <item>
            <title><![CDATA[Moonshot: Towards Controllable Video Generation and Editing with Multimodal Conditions]]></title>
            <description><![CDATA[Most existing video diffusion models (VDMs) are limited to mere text conditions. Thereby, they are usually lacking in control over visual appearance and geometry structure of the generated videos. This work presents Moonshot, a new video generation model that conditions simultaneously on multimodal inputs of image and text. The model builts upon a core module, called multimodal video block (MVB), which consists of conventional spatialtemporal layers for representing video features, and a decoupled cross-attention layer to address image and text inputs for appearance conditioning. In addition, we carefully design the model architecture such that it can optionally integrate with pre-trained image ControlNet modules for geometry visual conditions, without needing of extra training overhead as opposed to prior methods. Experiments show that with versatile multimodal conditioning mechanisms, Moonshot demonstrates significant improvement on visual quality and temporal consistency compared to existing models. In addition, the model can be easily repurposed for a variety of generative applications, such as personalized video generation, image animation and video editing, unveiling its potential to serve as a fundamental architecture for controllable video generation. Models will be made public on https://github.com/salesforce/LAVIS.]]></description>
            <pubDate>Thu, 04 Jan 2024 01:54:52 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2401.01827</guid>
            <link>https://arxiv.org/abs/2401.01827</link>
            
            
            
            <author><![CDATA[David Junhao Zhang, Dongxu Li, Hung Le, Mike Zheng Shou, Caiming Xiong, Doyen Sahoo]]></author>
            
                
            
            
        </item>
        
        <item>
            <title><![CDATA[GPT-4V(ision) is a Generalist Web Agent, if Grounded]]></title>
            <description><![CDATA[The recent development on large multimodal models (LMMs), especially GPT-4V(ision) and Gemini, has been quickly expanding the capability boundaries of multimodal models beyond traditional tasks like image captioning and visual question answering. In this work, we explore the potential of LMMs like GPT-4V as a generalist web agent that can follow natural language instructions to complete tasks on any given website. We propose SEEACT, a generalist web agent that harnesses the power of LMMs for integrated visual understanding and acting on the web. We evaluate on the recent MIND2WEB benchmark. In addition to standard offline evaluation on cached websites, we enable a new online evaluation setting by developing a tool that allows running web agents on live websites. We show that GPT-4V presents a great potential for web agents - it can successfully complete 50% of the tasks on live websites if we manually ground its textual plans into actions on the websites. This substantially outperforms text-only LLMs like GPT-4 or smaller models (FLAN-T5 and BLIP-2) specifically fine-tuned for web agents. However, grounding still remains a major challenge. Existing LMM grounding strategies like set-of-mark prompting turns out not effective for web agents, and the best grounding strategy we develop in this paper leverages both the HTML text and visuals. Yet, there is still a substantial gap with oracle grounding, leaving ample room for further improvement.]]></description>
            <pubDate>Thu, 04 Jan 2024 01:47:00 GMT</pubDate>
            <guid isPermaLink="false">https://arxiv.org/abs/2401.01614</guid>
            <link>https://arxiv.org/abs/2401.01614</link>
            
            
            
            <author><![CDATA[Boyuan Zheng, Boyu Gou, Jihyung Kil, Huan Sun, Yu Su]]></author>
            
                
            
            
        </item>
        
    </channel>
</rss>
