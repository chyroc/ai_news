<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/"><category term="LangChain" label="r/LangChain"/><updated>2024-05-23T17:02:59+00:00</updated><icon>https://www.redditstatic.com/icon.png/</icon><id>/r/LangChain.rss</id><link rel="self" href="https://www.reddit.com/r/LangChain.rss" type="application/atom+xml" /><link rel="alternate" href="https://www.reddit.com/r/LangChain" type="text/html" /><subtitle>LangChain is an open-source framework and developer toolkit that helps developers get LLM applications from prototype to production. It is available for Python and Javascript at https://www.langchain.com/.</subtitle><title>LangChain</title><entry><author><name>/u/zchaarm</name><uri>https://www.reddit.com/user/zchaarm</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;A place for members of &lt;a href=&quot;/r/LangChain&quot;&gt;r/LangChain&lt;/a&gt; to chat with each other&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/zchaarm&quot;&gt; /u/zchaarm &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_10ljho9</id><link href="https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/" /><updated>2023-01-26T04:33:05+00:00</updated><published>2023-01-26T04:33:05+00:00</published><title>r/LangChain Lounge</title></entry><entry><author><name>/u/link2ani</name><uri>https://www.reddit.com/user/link2ani</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;We’re building a RAG based application which works on internal documents. We’re experimenting with OpenAI for embedding models, Milvus (Zilliz cloud) for embedding storage and similarity search, Postgres for all other data and AWS for hosting.&lt;/p&gt; &lt;p&gt;Our main priorities are: - being fast to market - above average performance - costs that don’t scale exponentially with scale - being scalable so we don’t have to refactor all of the code, if we achieve any scale&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/link2ani&quot;&gt; /u/link2ani &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cyjfap/best_stack_for_rag/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cyjfap/best_stack_for_rag/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cyjfap</id><link href="https://www.reddit.com/r/LangChain/comments/1cyjfap/best_stack_for_rag/" /><updated>2024-05-23T03:39:37+00:00</updated><published>2024-05-23T03:39:37+00:00</published><title>Best stack for RAG?</title></entry><entry><author><name>/u/mehulgupta7991</name><uri>https://www.reddit.com/user/mehulgupta7991</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/mehulgupta7991&quot;&gt; /u/mehulgupta7991 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;/r/ArtificialInteligence/comments/1cytvn5/generative_ai_for_time_series/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cyu0sy/timegpt_generative_ai_for_time_series/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cyu0sy</id><link href="https://www.reddit.com/r/LangChain/comments/1cyu0sy/timegpt_generative_ai_for_time_series/" /><updated>2024-05-23T14:23:42+00:00</updated><published>2024-05-23T14:23:42+00:00</published><title>TimeGPT: Generative AI for Time Series</title></entry><entry><author><name>/u/TripleCheeeze</name><uri>https://www.reddit.com/user/TripleCheeeze</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;pre&gt;&lt;code&gt;from langchain.embeddings import OpenAIEmbeddings from langchain.retrievers import ParentDocumentRetriever from langchain.schema import Document from langchain.storage import InMemoryStore from langchain.text_splitter import RecursiveCharacterTextSplitter from langchain.vectorstores.chroma import Chroma vectorstore = Chroma( collection_name=&amp;quot;full_documents&amp;quot;, embedding_function=OpenAIEmbeddings() ) store = InMemoryStore() docs = [Document(page_content=txt, metadata={&amp;quot;id&amp;quot;: id}) for txt, id in [(&amp;quot;aaaaaa&amp;quot;, 1), (&amp;quot;bbbbbb&amp;quot;, 2)]] ParentDocumentRetriever( vectorstore=vectorstore, docstore=store, id_key=&amp;quot;id&amp;quot;, parent_splitter=RecursiveCharacterTextSplitter( chunk_size = 2, chunk_overlap = 0, length_function = len, add_start_index = True, ), child_splitter=RecursiveCharacterTextSplitter( chunk_size = 1, chunk_overlap = 0, length_function = len, add_start_index = True, ), ).add_documents(docs,ids=[doc.metadata[&amp;quot;id&amp;quot;] for doc in docs])from langchain.embeddings import OpenAIEmbeddings from langchain.retrievers import ParentDocumentRetriever from langchain.schema import Document from langchain.storage import InMemoryStore from langchain.text_splitter import RecursiveCharacterTextSplitter from langchain.vectorstores.chroma import Chroma vectorstore = Chroma( collection_name=&amp;quot;full_documents&amp;quot;, embedding_function=OpenAIEmbeddings() ) store = InMemoryStore() docs = [Document(page_content=txt, metadata={&amp;quot;id&amp;quot;: id}) for txt, id in [(&amp;quot;aaaaaa&amp;quot;, 1), (&amp;quot;bbbbbb&amp;quot;, 2)]] ParentDocumentRetriever( vectorstore=vectorstore, docstore=store, id_key=&amp;quot;id&amp;quot;, parent_splitter=RecursiveCharacterTextSplitter( chunk_size = 2, chunk_overlap = 0, length_function = len, add_start_index = True, ), child_splitter=RecursiveCharacterTextSplitter( chunk_size = 1, chunk_overlap = 0, length_function = len, add_start_index = True, ), ).add_documents(docs,ids=[doc.metadata[&amp;quot;id&amp;quot;] for doc in docs]) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The error :&lt;/p&gt; &lt;pre&gt;&lt;code&gt;ValueError: Got uneven list of documents and ids. If `ids` is provided, should be same length as `documents`. &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The size of documents list and ids list are nevertheless equal, i don&amp;#39;t understand this error &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/TripleCheeeze&quot;&gt; /u/TripleCheeeze &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cytwsx/parentdocumentretrieveradd_document_function_with/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cytwsx/parentdocumentretrieveradd_document_function_with/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cytwsx</id><link href="https://www.reddit.com/r/LangChain/comments/1cytwsx/parentdocumentretrieveradd_document_function_with/" /><updated>2024-05-23T14:18:57+00:00</updated><published>2024-05-23T14:18:57+00:00</published><title>ParentDocumentRetriever.add_document function with &quot;ids&quot; parameter - can't fix an error</title></entry><entry><author><name>/u/Mean-Night6324</name><uri>https://www.reddit.com/user/Mean-Night6324</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi!&lt;/p&gt; &lt;p&gt;I&amp;#39;m currently facing this issue of trying to get an XML out of a model and I use that XML structure to extract and format a document that I generate but, no matter how I prompt the model, or even, using different calls generate the answer and to structure it into the required format, sometimes going through different stages of structuring (like first just bullet points, then try to only put stuff into a basic XML format before going into nested.), it still sometimes generate an answer that&amp;#39;s not structured.&lt;/p&gt; &lt;p&gt;I included retries on those calls hoping that the model in its second generation would structure the output correctly but often this doesn&amp;#39;t work.&lt;/p&gt; &lt;p&gt;I was wondering how the community handles this issue or if there are creative ways you stumbled upon that deal well with it.&lt;/p&gt; &lt;p&gt;I have seen in the past some libraries that force the generation in some kind of way like the grammars from llama-cpp, or outlines. Maybe there was guidance as well. But I don&amp;#39;t think they work with LLMs from providers. I&amp;#39;m facing this problem with mistral-large.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Mean-Night6324&quot;&gt; /u/Mean-Night6324 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cyp7ij/what_are_some_ways_to_enforce_structured_outputs/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cyp7ij/what_are_some_ways_to_enforce_structured_outputs/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cyp7ij</id><link href="https://www.reddit.com/r/LangChain/comments/1cyp7ij/what_are_some_ways_to_enforce_structured_outputs/" /><updated>2024-05-23T10:04:32+00:00</updated><published>2024-05-23T10:04:32+00:00</published><title>What are some ways to enforce structured outputs from LLMs not in your control beyond basic prompting?</title></entry><entry><author><name>/u/AnEdgeLordWeeb</name><uri>https://www.reddit.com/user/AnEdgeLordWeeb</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey guys! I&amp;#39;m trying to develop a chatbot that offers video games recommendations based on user input.&lt;br/&gt; Problem is, I&amp;#39;m stuck at the chain which objective is to use Tavily API tool to search for video games&amp;#39; titles that fit the user&amp;#39;s criteria.&lt;/p&gt; &lt;p&gt;Here&amp;#39;s what I&amp;#39;ve tried:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;# Game Title Search prompt = PromptTemplate( template=&amp;quot;&amp;quot;&amp;quot;You are part of a chatbot that provides personalized video game recommendations based on user preferences. \n Your task is to search for the top 5 video games that match the user query. \n Only return the titles of the games. \n\n User Query: {query}&amp;quot;&amp;quot;&amp;quot;, input_variables=[&amp;quot;query&amp;quot;], ) game_title_search = prompt | llm.bind_tools(tools) QUERY = &amp;quot;&amp;quot;&amp;quot;What games are similar to Skyrim?&amp;quot;&amp;quot;&amp;quot; result = game_title_search.invoke({&amp;quot;query&amp;quot;: QUERY}) print(result) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Problem is, when I print result it gives me this instead of the response that I&amp;#39;m expecting (which are the video games&amp;#39; titles:&lt;/p&gt; &lt;p&gt;&lt;code&gt;content=&amp;#39;&amp;#39; additional_kwargs={&amp;#39;tool_calls&amp;#39;: [{&amp;#39;id&amp;#39;: &amp;#39;call_xJGybVhCtBAYGHyNkEE04U1c&amp;#39;, &amp;#39;function&amp;#39;: {&amp;#39;arguments&amp;#39;: &amp;#39;{&amp;quot;query&amp;quot;:&amp;quot;games similar to Skyrim&amp;quot;}&amp;#39;, &amp;#39;name&amp;#39;: &amp;#39;tavily_search_results_json&amp;#39;}, &amp;#39;type&amp;#39;: &amp;#39;function&amp;#39;}]} response_metadata={&amp;#39;token_usage&amp;#39;: {&amp;#39;completion_tokens&amp;#39;: 21, &amp;#39;prompt_tokens&amp;#39;: 141, &amp;#39;total_tokens&amp;#39;: 162}, &amp;#39;model_name&amp;#39;: &amp;#39;gpt-3.5-turbo-1106&amp;#39;, &amp;#39;system_fingerprint&amp;#39;: None, &amp;#39;finish_reason&amp;#39;: &amp;#39;tool_calls&amp;#39;, &amp;#39;logprobs&amp;#39;: None} id=&amp;#39;run-c7c33094-2173-43d8-9e9a-319c80265f57-0&amp;#39; tool_calls=[{&amp;#39;name&amp;#39;: &amp;#39;tavily_search_results_json&amp;#39;, &amp;#39;args&amp;#39;: {&amp;#39;query&amp;#39;: &amp;#39;games similar to Skyrim&amp;#39;}, &amp;#39;id&amp;#39;: &amp;#39;call_xJGybVhCtBAYGHyNkEE04U1c&amp;#39;}]&lt;/code&gt;&lt;/p&gt; &lt;p&gt;How can I solve this and use the tools alongside the ChatModel and the PromptTemplate to achieve what I want?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/AnEdgeLordWeeb&quot;&gt; /u/AnEdgeLordWeeb &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cyt7uf/how_can_i_properly_use_tools_within_a_chain_in/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cyt7uf/how_can_i_properly_use_tools_within_a_chain_in/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cyt7uf</id><link href="https://www.reddit.com/r/LangChain/comments/1cyt7uf/how_can_i_properly_use_tools_within_a_chain_in/" /><updated>2024-05-23T13:48:44+00:00</updated><published>2024-05-23T13:48:44+00:00</published><title>How can I properly use tools within a chain in LangGraph?</title></entry><entry><author><name>/u/Mission_Tip4316</name><uri>https://www.reddit.com/user/Mission_Tip4316</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;What worked for me was to create small modular functions out of one big function with different parameters. I broke down my API for the bot to use into smaller, modular endpoints with maximum of two parameters each. &lt;/p&gt; &lt;p&gt;I have been able to use gpt-3.5 to get satisfactory outputs without fails. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Mission_Tip4316&quot;&gt; /u/Mission_Tip4316 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cyn34y/for_those_struggling_with_api_function_calls/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cyn34y/for_those_struggling_with_api_function_calls/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cyn34y</id><link href="https://www.reddit.com/r/LangChain/comments/1cyn34y/for_those_struggling_with_api_function_calls/" /><updated>2024-05-23T07:31:44+00:00</updated><published>2024-05-23T07:31:44+00:00</published><title>For those struggling with API function calls</title></entry><entry><author><name>/u/Confident-Addendum-2</name><uri>https://www.reddit.com/user/Confident-Addendum-2</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Been struggling with parsing pdf with complex layout, table, imagines.&lt;/p&gt; &lt;p&gt;The option that I am testing is multi modal vector, based on unstructured library for pdf extraction. &lt;/p&gt; &lt;p&gt;I recently discovered llamaparse proprietary solution. Excluding the facts that isn&amp;#39;t open source and limited for commercial use. Would it perform better then the unstructured approach for parsing?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Confident-Addendum-2&quot;&gt; /u/Confident-Addendum-2 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cyplp8/parsing_solutions_for_pdf/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cyplp8/parsing_solutions_for_pdf/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cyplp8</id><link href="https://www.reddit.com/r/LangChain/comments/1cyplp8/parsing_solutions_for_pdf/" /><updated>2024-05-23T10:31:22+00:00</updated><published>2024-05-23T10:31:22+00:00</published><title>Parsing solutions for PDF</title></entry><entry><author><name>/u/Lost-Most-487</name><uri>https://www.reddit.com/user/Lost-Most-487</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;m using create_csv_agent to get a csv parsing agent to analyze and return a list of items that meets the criteria. The agent handles the questions fine and I can see the correct results printed out in its Observations. However it doesn&amp;#39;t include the list of items in the final output. How can I get around this?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Lost-Most-487&quot;&gt; /u/Lost-Most-487 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cyoeho/how_can_i_get_the_csv_agent_to_return_the/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cyoeho/how_can_i_get_the_csv_agent_to_return_the/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cyoeho</id><link href="https://www.reddit.com/r/LangChain/comments/1cyoeho/how_can_i_get_the_csv_agent_to_return_the/" /><updated>2024-05-23T09:07:00+00:00</updated><published>2024-05-23T09:07:00+00:00</published><title>How can I get the csv_agent to return the complete results from its Observation?</title></entry><entry><author><name>/u/juansm2001</name><uri>https://www.reddit.com/user/juansm2001</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey I am doing an internship and my boss asked me to build a RAG that can read financial documents (pdf) and create a LLM that, with a query, answers based on these documents. I was using BGE as the embedding model and ollama with llama2 for the LLM. My problem is that I was using google collab with the free GPU but once it reaches the limit, I can&amp;#39;t keep creating the embeddings. Is there any FREE solution for this? Thank you and sorry for my inexperience.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/juansm2001&quot;&gt; /u/juansm2001 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cynhl3/im_new_to_this_and_i_need_help_for_my_rag/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cynhl3/im_new_to_this_and_i_need_help_for_my_rag/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cynhl3</id><link href="https://www.reddit.com/r/LangChain/comments/1cynhl3/im_new_to_this_and_i_need_help_for_my_rag/" /><updated>2024-05-23T08:01:05+00:00</updated><published>2024-05-23T08:01:05+00:00</published><title>I'm new to this and I need help for my RAG</title></entry><entry><author><name>/u/MangjoseKlyne</name><uri>https://www.reddit.com/user/MangjoseKlyne</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;m working on a project that involves using a language model chain to process questions and generate responses. However, I&amp;#39;ve encountered an issue where the chain seems to get stuck at the invocation stage without producing any results.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Background:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;I&amp;#39;m using a Python script that involves various components such as document loaders, embeddings, text splitters, vector stores, retrievers, prompts, parsers, and language models.&lt;/li&gt; &lt;li&gt;The script is designed to load a PDF document, split it into chunks, add the chunks to a vector database, initialize a language model, and then retrieve relevant information based on input questions.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Problem:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Despite setting up the chain correctly and providing a question to the system, it seems to get stuck at the invocation stage without producing any results.&lt;/li&gt; &lt;li&gt;I&amp;#39;ve checked the logs, and everything seems to be initialized and processed correctly up to the invocation step.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Code:&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;from langchain_community.document_loaders import UnstructuredPDFLoader from langchain_community.document_loaders import OnlinePDFLoader from langchain_community.embeddings import OllamaEmbeddings from langchain_text_splitters import RecursiveCharacterTextSplitter from langchain_community.vectorstores import Chroma from langchain.prompts import ChatPromptTemplate, PromptTemplate from langchain_core.output_parsers import StrOutputParser from langchain_community.chat_models import ChatOllama from langchain_core.runnables import RunnablePassthrough from langchain_core.tracers import ConsoleCallbackHandler from langchain.retrievers.multi_query import MultiQueryRetriever import asyncio # Use raw string notation for the file path local_path = r&amp;quot;C:/Users/User/zven/WEF_The_Global_Cooperation_Barometer_2024.pdf&amp;quot; # Load local PDF file if local_path: try: loader = UnstructuredPDFLoader(file_path=local_path) data = loader.load() print(&amp;quot;PDF loaded successfully.&amp;quot;) except Exception as e: print(f&amp;quot;Error loading PDF: {e}&amp;quot;) data = None else: print(&amp;quot;Upload a PDF file&amp;quot;) data = None if data: # Split and chunk text text_splitter = RecursiveCharacterTextSplitter(chunk_size=7500, chunk_overlap=100) chunks = text_splitter.split_documents(data) print(f&amp;quot;Document split into {len(chunks)} chunks.&amp;quot;) print(data[0].page_content) # Add to vector database try: vector_db = Chroma.from_documents( documents=chunks, embedding=OllamaEmbeddings(model=&amp;quot;nomic-embed-text&amp;quot;, show_progress=True), collection_name=&amp;quot;local-rag&amp;quot; ) print(&amp;quot;Chunks added to vector database.&amp;quot;) except Exception as e: print(f&amp;quot;Error adding chunks to vector database: {e}&amp;quot;) # Initialize LLM from Ollama local_model = &amp;quot;Mistral&amp;quot; try: llm = ChatOllama(model=local_model) print(&amp;quot;LLM initialized successfully.&amp;quot;) except Exception as e: print(f&amp;quot;Error initializing LLM: {e}&amp;quot;) # Define query prompt template QUERY_PROMPT = PromptTemplate( input_variables=[&amp;quot;question&amp;quot;], template=&amp;quot;&amp;quot;&amp;quot;You are an AI language model assistant. Your task is to generate five different versions of the given user question to retrieve relevant documents from a vector database. By generating multiple perspectives on the user question, your goal is to help the user overcome some of the limitations of the distance-based similarity search. Provide these alternative questions separated by newlines. Original question: {question}&amp;quot;&amp;quot;&amp;quot; ) # Initialize retriever try: retriever = MultiQueryRetriever.from_llm( vector_db.as_retriever(), llm, prompt=QUERY_PROMPT ) print(&amp;quot;Retriever initialized successfully.&amp;quot;) except Exception as e: print(f&amp;quot;Error initializing retriever: {e}&amp;quot;) # Define RAG prompt template template = &amp;quot;&amp;quot;&amp;quot;Answer the question based ONLY on the following context: {context} Question: {question} &amp;quot;&amp;quot;&amp;quot; print(&amp;quot;Template: &amp;quot;, template) prompt = ChatPromptTemplate.from_template(template) print(&amp;quot;Prompt: &amp;quot;, prompt) chain = ( {&amp;quot;context&amp;quot;: retriever, &amp;quot;question&amp;quot;: RunnablePassthrough()} | prompt | llm | StrOutputParser() ) print(chain) # Print the chain setup print(&amp;quot;Chain setup completed.&amp;quot;) async def run_chain(): try: print(&amp;quot;Invoking chain...&amp;quot;) # Invoke the chain with a question result = await chain.ainvoke( {&amp;quot;question&amp;quot;: &amp;quot;What are the 5 pillars of global cooperation?&amp;quot;}, config={&amp;#39;callbacks&amp;#39;: [ConsoleCallbackHandler()]} # 30 seconds timeout ) print(&amp;quot;Chain invoked successfully.&amp;quot;) print(&amp;quot;Result:&amp;quot;, result) # Print the answer if it exists if &amp;quot;answer&amp;quot; in result: print(&amp;quot;Answer:&amp;quot;, result[&amp;quot;answer&amp;quot;]) except asyncio.TimeoutError: print(&amp;quot;Chain invocation timed out.&amp;quot;) except Exception as e: print(f&amp;quot;Error invoking chain: {e}&amp;quot;) # Run the chain asyncio.run(run_chain()) # Delete all collections in the db vector_db.delete_collection() &lt;/code&gt;&lt;/pre&gt; &lt;blockquote&gt; &lt;/blockquote&gt; &lt;p&gt;Output when I Run it:&lt;br/&gt; OllamaEmbeddings: 100%|████████████████████████████████████████████████████████████████| 11/11 [05:16&amp;lt;00:00, 28.77s/it]&lt;/p&gt; &lt;p&gt;Chunks added to vector database.&lt;/p&gt; &lt;p&gt;LLM initialized successfully.&lt;/p&gt; &lt;p&gt;Retriever initialized successfully.&lt;/p&gt; &lt;p&gt;Template: Answer the question based ONLY on the following context:&lt;/p&gt; &lt;p&gt;{context}&lt;/p&gt; &lt;p&gt;Question: {question}&lt;/p&gt; &lt;p&gt;Prompt: input_variables=[&amp;#39;context&amp;#39;, &amp;#39;question&amp;#39;] messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=[&amp;#39;context&amp;#39;, &amp;#39;question&amp;#39;], template=&amp;#39;Answer the question based ONLY on the following context:\n {context}\n Question: {question}\n &amp;#39;))]&lt;/p&gt; &lt;p&gt;first={&lt;/p&gt; &lt;p&gt;context: MultiQueryRetriever(retriever=VectorStoreRetriever(tags=[&amp;#39;Chroma&amp;#39;, &amp;#39;OllamaEmbeddings&amp;#39;], vectorstore=&amp;lt;langchain\_community.vectorstores.chroma.Chroma object at 0x0000016DB7450390&amp;gt;), llm_chain=LLMChain(prompt=PromptTemplate(input_variables=[&amp;#39;question&amp;#39;], template=&amp;#39;You are an AI language model assistant. Your task is to generate five\n different versions of the given user question to retrieve relevant documents from\n a vector database. By generating multiple perspectives on the user question, your\n goal is to help the user overcome some of the limitations of the distance-based\n similarity search. Provide these alternative questions separated by newlines.\n Original question: {question}&amp;#39;), llm=ChatOllama(model=&amp;#39;Mistral&amp;#39;), output_parser=LineListOutputParser())),&lt;/p&gt; &lt;p&gt;question: RunnablePassthrough()&lt;/p&gt; &lt;p&gt;} middle=[ChatPromptTemplate(input_variables=[&amp;#39;context&amp;#39;, &amp;#39;question&amp;#39;], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=[&amp;#39;context&amp;#39;, &amp;#39;question&amp;#39;], template=&amp;#39;Answer the question based ONLY on the following context:\n {context}\n Question: {question}\n &amp;#39;))]), ChatOllama(model=&amp;#39;Mistral&amp;#39;)] last=StrOutputParser()&lt;/p&gt; &lt;p&gt;Chain setup completed.&lt;/p&gt; &lt;p&gt;Invoking chain...&lt;/p&gt; &lt;p&gt;[chain/start] [chain:RunnableSequence] Entering Chain run with input:&lt;/p&gt; &lt;p&gt;{&lt;/p&gt; &lt;p&gt;&amp;quot;question&amp;quot;: &amp;quot;What are the 5 pillars of global cooperation?&amp;quot;&lt;/p&gt; &lt;p&gt;}&lt;/p&gt; &lt;p&gt;[chain/start] [chain:RunnableSequence &amp;gt; chain:RunnableParallel&amp;lt;context,question&amp;gt;] Entering Chain run with input:&lt;/p&gt; &lt;p&gt;{&lt;/p&gt; &lt;p&gt;&amp;quot;question&amp;quot;: &amp;quot;What are the 5 pillars of global cooperation?&amp;quot;&lt;/p&gt; &lt;p&gt;}&lt;/p&gt; &lt;p&gt;[chain/start] [chain:RunnableSequence &amp;gt; chain:RunnableParallel&amp;lt;context,question&amp;gt; &amp;gt; chain:RunnablePassthrough] Entering Chain run with input:&lt;/p&gt; &lt;p&gt;{&lt;/p&gt; &lt;p&gt;&amp;quot;question&amp;quot;: &amp;quot;What are the 5 pillars of global cooperation?&amp;quot;&lt;/p&gt; &lt;p&gt;}&lt;/p&gt; &lt;p&gt;[chain/start] [chain:RunnableSequence &amp;gt; chain:RunnableParallel&amp;lt;context,question&amp;gt; &amp;gt; retriever:Retriever &amp;gt; chain:LLMChain] Entering Chain run with input:&lt;/p&gt; &lt;p&gt;{&lt;/p&gt; &lt;p&gt;&amp;quot;question&amp;quot;: {&lt;/p&gt; &lt;p&gt;&amp;quot;question&amp;quot;: &amp;quot;What are the 5 pillars of global cooperation?&amp;quot;&lt;/p&gt; &lt;p&gt;}&lt;/p&gt; &lt;p&gt;}[chain/end] [chain:RunnableSequence &amp;gt; chain:RunnableParallel&amp;lt;context,question&amp;gt; &amp;gt; chain:RunnablePassthrough] [16ms] Exiting Chain run with output:&lt;/p&gt; &lt;p&gt;{&lt;/p&gt; &lt;p&gt;&amp;quot;question&amp;quot;: &amp;quot;What are the 5 pillars of global cooperation?&amp;quot;&lt;/p&gt; &lt;p&gt;}&lt;/p&gt; &lt;p&gt;[llm/start] [chain:RunnableSequence &amp;gt; chain:RunnableParallel&amp;lt;context,question&amp;gt; &amp;gt; retriever:Retriever &amp;gt; chain:LLMChain &amp;gt; llm:ChatOllama] Entering LLM run with input:&lt;/p&gt; &lt;p&gt;{&lt;/p&gt; &lt;p&gt;&amp;quot;prompts&amp;quot;: [&lt;/p&gt; &lt;p&gt;&amp;quot;Human: You are an AI language model assistant. Your task is to generate five\n different versions of the given user question to retrieve relevant documents from\n a vector database. By generating multiple perspectives on the user question, your\n goal is to help the user overcome some of the limitations of the distance-based\n similarity search. Provide these alternative questions separated by newlines.\n Original question: {&amp;#39;question&amp;#39;: &amp;#39;What are the 5 pillars of global cooperation?&amp;#39;}&amp;quot;&lt;/p&gt; &lt;p&gt;]&lt;/p&gt; &lt;p&gt;}&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/MangjoseKlyne&quot;&gt; /u/MangjoseKlyne &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cyhm4y/need_help_understanding_why_my_language_model/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cyhm4y/need_help_understanding_why_my_language_model/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cyhm4y</id><link href="https://www.reddit.com/r/LangChain/comments/1cyhm4y/need_help_understanding_why_my_language_model/" /><updated>2024-05-23T02:01:37+00:00</updated><published>2024-05-23T02:01:37+00:00</published><title>Need Help Understanding Why My Language Model Chain Isn't Producing Results</title></entry><entry><author><name>/u/dastanIqbal</name><uri>https://www.reddit.com/user/dastanIqbal</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi Guys,&lt;br/&gt; I am exploring LangChain, and stuck at one issue, Needed your help!!&lt;/p&gt; &lt;p&gt;I am trying to get total number of empty parking spots available in csv, but I see we can only define k value in retriever,&lt;/p&gt; &lt;p&gt;Is there a way to ignore k value and give full matching result ?&lt;/p&gt; &lt;p&gt;Here is my code: &lt;a href=&quot;https://github.com/DastanIqbal/Langchain/blob/main/apps/find_parking/parking_spots.ipynb&quot;&gt;Langchain/apps/find_parking/parking_spots.ipynb at main · DastanIqbal/Langchain · GitHub&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Thanks&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/dastanIqbal&quot;&gt; /u/dastanIqbal &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cylb81/help_needed_to_find_total_number_of_results/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cylb81/help_needed_to_find_total_number_of_results/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cylb81</id><link href="https://www.reddit.com/r/LangChain/comments/1cylb81/help_needed_to_find_total_number_of_results/" /><updated>2024-05-23T05:31:49+00:00</updated><published>2024-05-23T05:31:49+00:00</published><title>Help Needed: To find total number of results ?</title></entry><entry><author><name>/u/bobbiesbottleservice</name><uri>https://www.reddit.com/user/bobbiesbottleservice</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Looking to return only a specific choice with langchain using an ollama model and couldn&amp;#39;t get the langchoice example to work. For example, How would I classify a bank transaction description if the only possible classification choices to choose from are: taxes, transfer, or payment?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/bobbiesbottleservice&quot;&gt; /u/bobbiesbottleservice &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cykpbu/simple_choice_selection/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cykpbu/simple_choice_selection/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cykpbu</id><link href="https://www.reddit.com/r/LangChain/comments/1cykpbu/simple_choice_selection/" /><updated>2024-05-23T04:54:18+00:00</updated><published>2024-05-23T04:54:18+00:00</published><title>Simple choice selection</title></entry><entry><author><name>/u/BellaHi</name><uri>https://www.reddit.com/user/BellaHi</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cyjp9i/enhancing_rag_models_with_reranking_langchain/&quot;&gt; &lt;img src=&quot;https://external-preview.redd.it/9VACkb4c-NTCF75tNXFPD3HVPxO12Bl1fK_3frAVwgs.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1f7cca4f703530254fa69089eaf42cc0f6b0584d&quot; alt=&quot;Enhancing RAG Models with Reranking &amp;amp; LangChain&quot; title=&quot;Enhancing RAG Models with Reranking &amp;amp; LangChain&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/BellaHi&quot;&gt; /u/BellaHi &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://myscale.com/blog/maximizing-advanced-rag-models-langchain-reranking-techniques/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cyjp9i/enhancing_rag_models_with_reranking_langchain/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1cyjp9i</id><media:thumbnail url="https://external-preview.redd.it/9VACkb4c-NTCF75tNXFPD3HVPxO12Bl1fK_3frAVwgs.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=1f7cca4f703530254fa69089eaf42cc0f6b0584d" /><link href="https://www.reddit.com/r/LangChain/comments/1cyjp9i/enhancing_rag_models_with_reranking_langchain/" /><updated>2024-05-23T03:55:25+00:00</updated><published>2024-05-23T03:55:25+00:00</published><title>Enhancing RAG Models with Reranking &amp; LangChain</title></entry><entry><author><name>/u/NexTim3_</name><uri>https://www.reddit.com/user/NexTim3_</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cyh8zq/problems_with_json_and_enum_parser/&quot;&gt; &lt;img src=&quot;https://b.thumbs.redditmedia.com/JsSF6PFdGpybdEk2x0_RluQD9EZcvG4o1eqp4Qjn6Cw.jpg&quot; alt=&quot;Problems with json and enum parser&quot; title=&quot;Problems with json and enum parser&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Langchain&amp;#39;s enum and json parser just dont work and I can&amp;#39;t figure out why. For example, here is my code below:&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/smzg0411z22d1.png?width=741&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=26b6ef00927afc4a1a9e6c8e6bf297dfea96d56f&quot;&gt;https://preview.redd.it/smzg0411z22d1.png?width=741&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=26b6ef00927afc4a1a9e6c8e6bf297dfea96d56f&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Where prediction is an enum with increase, decrease or no change. When I try it, I get this error:&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/3lp1jvg6z22d1.png?width=1451&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b41bdf522394561581dc924abe167898aa19edeb&quot;&gt;https://preview.redd.it/3lp1jvg6z22d1.png?width=1451&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b41bdf522394561581dc924abe167898aa19edeb&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Which gives the correct answer as decreased, but not as an enum. The same happens when I try this with the json parser, it adds unneccessary text around the dictionary so langchain doesnt read the output as a dictionary. Is there a fix for this?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/NexTim3_&quot;&gt; /u/NexTim3_ &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cyh8zq/problems_with_json_and_enum_parser/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cyh8zq/problems_with_json_and_enum_parser/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1cyh8zq</id><media:thumbnail url="https://b.thumbs.redditmedia.com/JsSF6PFdGpybdEk2x0_RluQD9EZcvG4o1eqp4Qjn6Cw.jpg" /><link href="https://www.reddit.com/r/LangChain/comments/1cyh8zq/problems_with_json_and_enum_parser/" /><updated>2024-05-23T01:42:56+00:00</updated><published>2024-05-23T01:42:56+00:00</published><title>Problems with json and enum parser</title></entry><entry><author><name>/u/ur_nightmare69</name><uri>https://www.reddit.com/user/ur_nightmare69</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hello everyone, &lt;/p&gt; &lt;p&gt;I am an engineering student currently doing my end of studies internship, and I am working on a project that involves RAG system using LLM for my company and my mission now is to evaluate and test different parameters in the process of retrieving and generating like evaluating the embeddings , the different LLM models etc, to finally choose what&amp;#39;s best to use. So, while doing my researches I found Langsmith and few others I want to know if some of you used one of these platforms and how was your experience and which one do you prefer and why .&lt;/p&gt; &lt;p&gt;Your feedback will greatly assist me in my work and research so if you have any information feel free to share it .&lt;/p&gt; &lt;p&gt;THANK YOU&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/ur_nightmare69&quot;&gt; /u/ur_nightmare69 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cy83us/testing_and_evaluating_llm_rag_systems/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cy83us/testing_and_evaluating_llm_rag_systems/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cy83us</id><link href="https://www.reddit.com/r/LangChain/comments/1cy83us/testing_and_evaluating_llm_rag_systems/" /><updated>2024-05-22T18:52:59+00:00</updated><published>2024-05-22T18:52:59+00:00</published><title>Testing And Evaluating LLM RAG Systems</title></entry><entry><author><name>/u/Party_Jellyfish5380</name><uri>https://www.reddit.com/user/Party_Jellyfish5380</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I have a use case where I have bunch of notes for a college class and I want to generate flash cards for them. Now I know RAG is used to fetch most closest file from database and answer based on that, however in my case, all the notes should be loaded. So would RAG be applicable in this case. I can also just load all data in but that would probably run out of tokens quickly&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Party_Jellyfish5380&quot;&gt; /u/Party_Jellyfish5380 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cycid7/a_question_regarding/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cycid7/a_question_regarding/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cycid7</id><link href="https://www.reddit.com/r/LangChain/comments/1cycid7/a_question_regarding/" /><updated>2024-05-22T21:55:59+00:00</updated><published>2024-05-22T21:55:59+00:00</published><title>A question regarding</title></entry><entry><author><name>/u/Mohseen365</name><uri>https://www.reddit.com/user/Mohseen365</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;m a software developer who&amp;#39;s learned about Gen AI stuff (Langchain, LLM, RAG, Agents,etc), and copywriting. Now I&amp;#39;m combining all these skills and looking for career advice or anyone going through the same thing and wants to connect&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Mohseen365&quot;&gt; /u/Mohseen365 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cy24vw/career_advice_for_software_development_gen_ai/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cy24vw/career_advice_for_software_development_gen_ai/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cy24vw</id><link href="https://www.reddit.com/r/LangChain/comments/1cy24vw/career_advice_for_software_development_gen_ai/" /><updated>2024-05-22T14:51:40+00:00</updated><published>2024-05-22T14:51:40+00:00</published><title>Career Advice for Software Development + Gen AI + Copywriting skills</title></entry><entry><author><name>/u/shardblaster</name><uri>https://www.reddit.com/user/shardblaster</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I was checking out the function calling capability of the new Mistral model and was wondering how to integrate this into a ReAct agent flow that uses AgentExecutor. &lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3&quot;&gt;https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Anyone got any hints? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/shardblaster&quot;&gt; /u/shardblaster &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cy9zxh/chatcompletionrequest_in_agentexecutor/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cy9zxh/chatcompletionrequest_in_agentexecutor/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cy9zxh</id><link href="https://www.reddit.com/r/LangChain/comments/1cy9zxh/chatcompletionrequest_in_agentexecutor/" /><updated>2024-05-22T20:09:42+00:00</updated><published>2024-05-22T20:09:42+00:00</published><title>ChatCompletionRequest in AgentExecutor</title></entry><entry><author><name>/u/phicreative1997</name><uri>https://www.reddit.com/user/phicreative1997</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cy8w14/chat_with_your_csv_using_duckdb_and_vannaai/&quot;&gt; &lt;img src=&quot;https://external-preview.redd.it/-gzuU1y9GWL058um9nTBzoVDwIEuu-YHXn6UnGV80IY.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8ea6756e3c6a5be354553fbbc5c4e63d96bdce6d&quot; alt=&quot;Chat with your CSV using DuckDB and Vanna.ai&quot; title=&quot;Chat with your CSV using DuckDB and Vanna.ai&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/phicreative1997&quot;&gt; /u/phicreative1997 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://arslanshahid-1997.medium.com/chat-with-your-csv-using-duckdb-and-vanna-ai-a5cef3762261&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cy8w14/chat_with_your_csv_using_duckdb_and_vannaai/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1cy8w14</id><media:thumbnail url="https://external-preview.redd.it/-gzuU1y9GWL058um9nTBzoVDwIEuu-YHXn6UnGV80IY.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=8ea6756e3c6a5be354553fbbc5c4e63d96bdce6d" /><link href="https://www.reddit.com/r/LangChain/comments/1cy8w14/chat_with_your_csv_using_duckdb_and_vannaai/" /><updated>2024-05-22T19:24:33+00:00</updated><published>2024-05-22T19:24:33+00:00</published><title>Chat with your CSV using DuckDB and Vanna.ai</title></entry><entry><author><name>/u/Thegunsmith98</name><uri>https://www.reddit.com/user/Thegunsmith98</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;As the title suggests I want to perform EDA using Langchain on a large dataframe. Im currently using Pandas Dataframe Agent , however it is not that efficent when using with large datasets. Can someone please suggest an alternative that works well. Thankyou &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Thegunsmith98&quot;&gt; /u/Thegunsmith98 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cy29kb/any_way_to_make_a_chatbot_that_does_eda_on_a/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cy29kb/any_way_to_make_a_chatbot_that_does_eda_on_a/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cy29kb</id><link href="https://www.reddit.com/r/LangChain/comments/1cy29kb/any_way_to_make_a_chatbot_that_does_eda_on_a/" /><updated>2024-05-22T14:57:17+00:00</updated><published>2024-05-22T14:57:17+00:00</published><title>Any way to make a Chatbot that does EDA on a large dataframe similar to Pandas Dataframe Agent.</title></entry><entry><author><name>/u/cr33dcode</name><uri>https://www.reddit.com/user/cr33dcode</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I have 100 PDFs that I need to index and make a report of every single week. I need a rag to help me get the info from the PDFs in a neat manner but also pull up the images and the PDF associated with the query. I needed the text to be highlighted as well and the pg numbers. &lt;/p&gt; &lt;p&gt;can someone please guide me with the stack? im thinking langchain and memgraph for DB but more tools and options and stack? thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/cr33dcode&quot;&gt; /u/cr33dcode &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cy1w8q/i_want_to_build_a_graph_rag_with_document/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cy1w8q/i_want_to_build_a_graph_rag_with_document/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cy1w8q</id><link href="https://www.reddit.com/r/LangChain/comments/1cy1w8q/i_want_to_build_a_graph_rag_with_document/" /><updated>2024-05-22T14:41:12+00:00</updated><published>2024-05-22T14:41:12+00:00</published><title>I want to build a graph rag with document browsing capability of the PDF its referencing from.</title></entry><entry><author><name>/u/wahnsinnwanscene</name><uri>https://www.reddit.com/user/wahnsinnwanscene</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;What is everyone using to extract the KG from unstructured data and into which database? For a local setup.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/wahnsinnwanscene&quot;&gt; /u/wahnsinnwanscene &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cxqa3f/knowledge_graph_generation_and_database/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cxqa3f/knowledge_graph_generation_and_database/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cxqa3f</id><link href="https://www.reddit.com/r/LangChain/comments/1cxqa3f/knowledge_graph_generation_and_database/" /><updated>2024-05-22T03:04:08+00:00</updated><published>2024-05-22T03:04:08+00:00</published><title>Knowledge graph generation and database</title></entry><entry><author><name>/u/pratham1443</name><uri>https://www.reddit.com/user/pratham1443</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I want to parse the LLM output to a particular JSON Schema But I am getting this error&lt;/p&gt; &lt;pre&gt;&lt;code&gt;{ &amp;quot;name&amp;quot;: &amp;quot;ValidationError&amp;quot;, &amp;quot;message&amp;quot;: &amp;quot;1 validation error for Generation text str type expected (type=type_error.str)&amp;quot;, &amp;quot;stack&amp;quot;: &amp;quot;--------------------------------------------------------------------------- ValidationError Traceback (most recent call last) /Users/pratham/LLMApi/pd_agent/ExcelParser.py in line 1 ----&amp;gt; &amp;lt;a href=&amp;#39;file:///Users/pratham/LLMApi/pd_agent/ExcelParser.py?line=167&amp;#39;&amp;gt;168&amp;lt;/a&amp;gt; top_rows = get_top_rows(doc_df, ChatOpenAI(model=\&amp;quot;gpt-3.5-turbo\&amp;quot;, temperature=0)) &amp;lt;a href=&amp;#39;file:///Users/pratham/LLMApi/pd_agent/ExcelParser.py?line=168&amp;#39;&amp;gt;169&amp;lt;/a&amp;gt; top_rows /Users/pratham/LLMApi/pd_agent/ExcelParser.py in line 13, in get_top_rows(df, llm) &amp;lt;a href=&amp;#39;file:///Users/pratham/LLMApi/pd_agent/ExcelParser.py?line=45&amp;#39;&amp;gt;46&amp;lt;/a&amp;gt; print(parser.get_format_instructions()) &amp;lt;a href=&amp;#39;file:///Users/pratham/LLMApi/pd_agent/ExcelParser.py?line=46&amp;#39;&amp;gt;47&amp;lt;/a&amp;gt; chain = prompt | df_agent | parser ---&amp;gt; &amp;lt;a href=&amp;#39;file:///Users/pratham/LLMApi/pd_agent/ExcelParser.py?line=47&amp;#39;&amp;gt;48&amp;lt;/a&amp;gt; json_output = chain.invoke( &amp;lt;a href=&amp;#39;file:///Users/pratham/LLMApi/pd_agent/ExcelParser.py?line=48&amp;#39;&amp;gt;49&amp;lt;/a&amp;gt; { &amp;lt;a href=&amp;#39;file:///Users/pratham/LLMApi/pd_agent/ExcelParser.py?line=49&amp;#39;&amp;gt;50&amp;lt;/a&amp;gt; \&amp;quot;format_instructions\&amp;quot;: parser.get_format_instructions() &amp;lt;a href=&amp;#39;file:///Users/pratham/LLMApi/pd_agent/ExcelParser.py?line=50&amp;#39;&amp;gt;51&amp;lt;/a&amp;gt; } &amp;lt;a href=&amp;#39;file:///Users/pratham/LLMApi/pd_agent/ExcelParser.py?line=51&amp;#39;&amp;gt;52&amp;lt;/a&amp;gt; ) &amp;lt;a href=&amp;#39;file:///Users/pratham/LLMApi/pd_agent/ExcelParser.py?line=52&amp;#39;&amp;gt;53&amp;lt;/a&amp;gt; return json_output[\&amp;quot;number_of_top_rows\&amp;quot;] File ~/LLMApi/venv/lib/python3.12/site-packages/langchain_core/runnables/base.py:2499, in RunnableSequence.invoke(self, input, config) &amp;lt;a href=&amp;#39;file:///Users/pratham/LLMApi/venv/lib/python3.12/site-packages/langchain_core/runnables/base.py?line=2496&amp;#39;&amp;gt;2497&amp;lt;/a&amp;gt; try: &amp;lt;a href=&amp;#39;file:///Users/pratham/LLMApi/venv/lib/python3.12/site-packages/langchain_core/runnables/base.py?line=2497&amp;#39;&amp;gt;2498&amp;lt;/a&amp;gt; for i, step in enumerate(self.steps): -&amp;gt; &amp;lt;a href=&amp;#39;file:///Users/pratham/LLMApi/venv/lib/python3.12/site-packages/langchain_core/runnables/base.py?line=2498&amp;#39;&amp;gt;2499&amp;lt;/a&amp;gt; input = step.invoke( &amp;lt;a href=&amp;#39;file:///Users/pratham/LLMApi/venv/lib/python3.12/site-packages/langchain_core/runnables/base.py?line=2499&amp;#39;&amp;gt;2500&amp;lt;/a&amp;gt; input, &amp;lt;a href=&amp;#39;file:///Users/pratham/LLMApi/venv/lib/python3.12/site-packages/langchain_core/runnables/base.py?line=2500&amp;#39;&amp;gt;2501&amp;lt;/a&amp;gt; # mark each step as a child run &amp;lt;a href=&amp;#39;file:///Users/pratham/LLMApi/venv/lib/python3.12/site-packages/langchain_core/runnables/base.py?line=2501&amp;#39;&amp;gt;2502&amp;lt;/a&amp;gt; patch_config( &amp;lt;a href=&amp;#39;file:///Users/pratham/LLMApi/venv/lib/python3.12/site-packages/langchain_core/runnables/base.py?line=2502&amp;#39;&amp;gt;2503&amp;lt;/a&amp;gt; config, callbacks=run_manager.get_child(f\&amp;quot;seq:step:{i+1}\&amp;quot;) &amp;lt;a href=&amp;#39;file:///Users/pratham/LLMApi/venv/lib/python3.12/site-packages/langchain_core/runnables/base.py?line=2503&amp;#39;&amp;gt;2504&amp;lt;/a&amp;gt; ), &amp;lt;a href=&amp;#39;file:///Users/pratham/LLMApi/venv/lib/python3.12/site-packages/langchain_core/runnables/base.py?line=2504&amp;#39;&amp;gt;2505&amp;lt;/a&amp;gt; ) &amp;lt;a href=&amp;#39;file:///Users/pratham/LLMApi/venv/lib/python3.12/site-packages/langchain_core/runnables/base.py?line=2505&amp;#39;&amp;gt;2506&amp;lt;/a&amp;gt; # finish the root run &amp;lt;a href=&amp;#39;file:///Users/pratham/LLMApi/venv/lib/python3.12/site-packages/langchain_core/runnables/base.py?line=2506&amp;#39;&amp;gt;2507&amp;lt;/a&amp;gt; except BaseException as e: File ~/LLMApi/venv/lib/python3.12/site-packages/langchain_core/output_parsers/base.py:178, in BaseOutputParser.invoke(self, input, config) &amp;lt;a href=&amp;#39;file:///Users/pratham/LLMApi/venv/lib/python3.12/site-packages/langchain_core/output_parsers/base.py?line=168&amp;#39;&amp;gt;169&amp;lt;/a&amp;gt; return self._call_with_config( &amp;lt;a href=&amp;#39;file:///Users/pratham/LLMApi/venv/lib/python3.12/site-packages/langchain_core/output_parsers/base.py?line=169&amp;#39;&amp;gt;170&amp;lt;/a&amp;gt; lambda inner_input: self.parse_result( &amp;lt;a href=&amp;#39;file:///Users/pratham/LLMApi/venv/lib/python3.12/site-packages/langchain_core/output_parsers/base.py?line=170&amp;#39;&amp;gt;171&amp;lt;/a&amp;gt; [ChatGeneration(message=inner_input)] (...) &amp;lt;a href=&amp;#39;file:///Users/pratham/LLMApi/venv/lib/python3.12/site-packages/langchain_core/output_parsers/base.py?line=174&amp;#39;&amp;gt;175&amp;lt;/a&amp;gt; run_type=\&amp;quot;parser\&amp;quot;, &amp;lt;a href=&amp;#39;file:///Users/pratham/LLMApi/venv/lib/python3.12/site-packages/langchain_core/output_parsers/base.py?line=175&amp;#39;&amp;gt;176&amp;lt;/a&amp;gt; ) &amp;lt;a href=&amp;#39;file:///Users/pratham/LLMApi/venv/lib/python3.12/site-packages/langchain_core/output_parsers/base.py?line=176&amp;#39;&amp;gt;177&amp;lt;/a&amp;gt; else: --&amp;gt; &amp;lt;a href=&amp;#39;file:///Users/pratham/LLMApi/venv/lib/python3.12/site-packages/langchain_core/output_parsers/base.py?line=177&amp;#39;&amp;gt;178&amp;lt;/a&amp;gt; return self._call_with_config( &amp;lt;a href=&amp;#39;file:///Users/pratham/LLMApi/venv/lib/python3.12/site-packages/langchain_core/output_parsers/base.py?line=178&amp;#39;&amp;gt;179&amp;lt;/a&amp;gt; lambda inner_input: self.parse_result([Generation(text=inner_input)]), &amp;lt;a href=&amp;#39;file:///Users/pratham/LLMApi/venv/lib/python3.12/site-packages/langchain_core/output_parsers/base.py?line=179&amp;#39;&amp;gt;180&amp;lt;/a&amp;gt; input, &amp;lt;a href=&amp;#39;file:///Users/pratham/LLMApi/venv/lib/python3.12/site-packages/langchain_core/output_parsers/base.py?line=180&amp;#39;&amp;gt;181&amp;lt;/a&amp;gt; config, &amp;lt;a href=&amp;#39;file:///Users/pratham/LLMApi/venv/lib/python3.12/site-packages/langchain_core/output_parsers/base.py?line=181&amp;#39;&amp;gt;182&amp;lt;/a&amp;gt; run_type=\&amp;quot;parser\&amp;quot;, &amp;lt;a href=&amp;#39;file:///Users/pratham/LLMApi/venv/lib/python3.12/site-packages/langchain_core/output_parsers/base.py?line=182&amp;#39;&amp;gt;183&amp;lt;/a&amp;gt; ) File ~/LLMApi/venv/lib/python3.12/site-packages/langchain_core/runnables/base.py:1626, in Runnable._call_with_config(self, func, input, config, run_type, **kwargs) &amp;lt;a href=&amp;#39;file:///Users/pratham/LLMApi/venv/lib/python3.12/site-packages/langchain_core/runnables/base.py?line=1621&amp;#39;&amp;gt;1622&amp;lt;/a&amp;gt; context = copy_context() &amp;lt;a href=&amp;#39;file:///Users/pratham/LLMApi/venv/lib/python3.12/site-packages/langchain_core/runnables/base.py?line=1622&amp;#39;&amp;gt;1623&amp;lt;/a&amp;gt; context.run(var_child_runnable_config.set, child_config) &amp;lt;a href=&amp;#39;file:///Users/pratham/LLMApi/venv/lib/python3.12/site-packages/langchain_core/runnables/base.py?line=1623&amp;#39;&amp;gt;1624&amp;lt;/a&amp;gt; output = cast( &amp;lt;a href=&amp;#39;file:///Users/pratham/LLMApi/venv/lib/python3.12/site-packages/langchain_core/runnables/base.py?line=1624&amp;#39;&amp;gt;1625&amp;lt;/a&amp;gt; Output, -&amp;gt; &amp;lt;a href=&amp;#39;file:///Users/pratham/LLMApi/venv/lib/python3.12/site-packages/langchain_core/runnables/base.py?line=1625&amp;#39;&amp;gt;1626&amp;lt;/a&amp;gt; context.run( &amp;lt;a href=&amp;#39;file:///Users/pratham/LLMApi/venv/lib/python3.12/site-packages/langchain_core/runnables/base.py?line=1626&amp;#39;&amp;gt;1627&amp;lt;/a&amp;gt; call_func_with_variable_args, # type: ignore[arg-type] &amp;lt;a href=&amp;#39;file:///Users/pratham/LLMApi/venv/lib/python3.12/site-packages/langchain_core/runnables/base.py?line=1627&amp;#39;&amp;gt;1628&amp;lt;/a&amp;gt; func, # type: ignore[arg-type] &amp;lt;a href=&amp;#39;file:///Users/pratham/LLMApi/venv/lib/python3.12/site-packages/langchain_core/runnables/base.py?line=1628&amp;#39;&amp;gt;1629&amp;lt;/a&amp;gt; input, # type: ignore[arg-type] &amp;lt;a href=&amp;#39;file:///Users/pratham/LLMApi/venv/lib/python3.12/site-packages/langchain_core/runnables/base.py?line=1629&amp;#39;&amp;gt;1630&amp;lt;/a&amp;gt; config, &amp;lt;a href=&amp;#39;file:///Users/pratham/LLMApi/venv/lib/python3.12/site-packages/langchain_core/runnables/base.py?line=1630&amp;#39;&amp;gt;1631&amp;lt;/a&amp;gt; run_manager, &amp;lt;a href=&amp;#39;file:///Users/pratham/LLMApi/venv/lib/python3.12/site-packages/langchain_core/runnables/base.py?line=1631&amp;#39;&amp;gt;1632&amp;lt;/a&amp;gt; **kwargs, &amp;lt;a href=&amp;#39;file:///Users/pratham/LLMApi/venv/lib/python3.12/site-packages/langchain_core/runnables/base.py?line=1632&amp;#39;&amp;gt;1633&amp;lt;/a&amp;gt; ), &amp;lt;a href=&amp;#39;file:///Users/pratham/LLMApi/venv/lib/python3.12/site-packages/langchain_core/runnables/base.py?line=1633&amp;#39;&amp;gt;1634&amp;lt;/a&amp;gt; ) &amp;lt;a href=&amp;#39;file:///Users/pratham/LLMApi/venv/lib/python3.12/site-packages/langchain_core/runnables/base.py?line=1634&amp;#39;&amp;gt;1635&amp;lt;/a&amp;gt; except BaseException as e: &amp;lt;a href=&amp;#39;file:///Users/pratham/LLMApi/venv/lib/python3.12/site-packages/langchain_core/runnables/base.py?line=1635&amp;#39;&amp;gt;1636&amp;lt;/a&amp;gt; run_manager.on_chain_error(e) File ~/LLMApi/venv/lib/python3.12/site-packages/langchain_core/runnables/config.py:347, in call_func_with_variable_args(func, input, config, run_manager, **kwargs) &amp;lt;a href=&amp;#39;file:///Users/pratham/LLMApi/venv/lib/python3.12/site-packages/langchain_core/runnables/config.py?line=344&amp;#39;&amp;gt;345&amp;lt;/a&amp;gt; if run_manager is not None and accepts_run_manager(func): &amp;lt;a href=&amp;#39;file:///Users/pratham/LLMApi/venv/lib/python3.12/site-packages/langchain_core/runnables/config.py?line=345&amp;#39;&amp;gt;346&amp;lt;/a&amp;gt; kwargs[\&amp;quot;run_manager\&amp;quot;] = run_manager --&amp;gt; &amp;lt;a href=&amp;#39;file:///Users/pratham/LLMApi/venv/lib/python3.12/site-packages/langchain_core/runnables/config.py?line=346&amp;#39;&amp;gt;347&amp;lt;/a&amp;gt; return func(input, **kwargs) File ~/LLMApi/venv/lib/python3.12/site-packages/langchain_core/output_parsers/base.py:179, in BaseOutputParser.invoke.&amp;lt;locals&amp;gt;.&amp;lt;lambda&amp;gt;(inner_input) &amp;lt;a href=&amp;#39;file:///Users/pratham/LLMApi/venv/lib/python3.12/site-packages/langchain_core/output_parsers/base.py?line=168&amp;#39;&amp;gt;169&amp;lt;/a&amp;gt; return self._call_with_config( &amp;lt;a href=&amp;#39;file:///Users/pratham/LLMApi/venv/lib/python3.12/site-packages/langchain_core/output_parsers/base.py?line=169&amp;#39;&amp;gt;170&amp;lt;/a&amp;gt; lambda inner_input: self.parse_result( &amp;lt;a href=&amp;#39;file:///Users/pratham/LLMApi/venv/lib/python3.12/site-packages/langchain_core/output_parsers/base.py?line=170&amp;#39;&amp;gt;171&amp;lt;/a&amp;gt; [ChatGeneration(message=inner_input)] (...) &amp;lt;a href=&amp;#39;file:///Users/pratham/LLMApi/venv/lib/python3.12/site-packages/langchain_core/output_parsers/base.py?line=174&amp;#39;&amp;gt;175&amp;lt;/a&amp;gt; run_type=\&amp;quot;parser\&amp;quot;, &amp;lt;a href=&amp;#39;file:///Users/pratham/LLMApi/venv/lib/python3.12/site-packages/langchain_core/output_parsers/base.py?line=175&amp;#39;&amp;gt;176&amp;lt;/a&amp;gt; ) &amp;lt;a href=&amp;#39;file:///Users/pratham/LLMApi/venv/lib/python3.12/site-packages/langchain_core/output_parsers/base.py?line=176&amp;#39;&amp;gt;177&amp;lt;/a&amp;gt; else: &amp;lt;a href=&amp;#39;file:///Users/pratham/LLMApi/venv/lib/python3.12/site-packages/langchain_core/output_parsers/base.py?line=177&amp;#39;&amp;gt;178&amp;lt;/a&amp;gt; return self._call_with_config( --&amp;gt; &amp;lt;a href=&amp;#39;file:///Users/pratham/LLMApi/venv/lib/python3.12/site-packages/langchain_core/output_parsers/base.py?line=178&amp;#39;&amp;gt;179&amp;lt;/a&amp;gt; lambda inner_input: self.parse_result([Generation(text=inner_input)]), &amp;lt;a href=&amp;#39;file:///Users/pratham/LLMApi/venv/lib/python3.12/site-packages/langchain_core/output_parsers/base.py?line=179&amp;#39;&amp;gt;180&amp;lt;/a&amp;gt; input, &amp;lt;a href=&amp;#39;file:///Users/pratham/LLMApi/venv/lib/python3.12/site-packages/langchain_core/output_parsers/base.py?line=180&amp;#39;&amp;gt;181&amp;lt;/a&amp;gt; config, &amp;lt;a href=&amp;#39;file:///Users/pratham/LLMApi/venv/lib/python3.12/site-packages/langchain_core/output_parsers/base.py?line=181&amp;#39;&amp;gt;182&amp;lt;/a&amp;gt; run_type=\&amp;quot;parser\&amp;quot;, &amp;lt;a href=&amp;#39;file:///Users/pratham/LLMApi/venv/lib/python3.12/site-packages/langchain_core/output_parsers/base.py?line=182&amp;#39;&amp;gt;183&amp;lt;/a&amp;gt; ) File ~/LLMApi/venv/lib/python3.12/site-packages/pydantic/main.py:341, in BaseModel.__init__(__pydantic_self__, **data) &amp;lt;a href=&amp;#39;file:///Users/pratham/LLMApi/venv/lib/python3.12/site-packages/pydantic/main.py?line=338&amp;#39;&amp;gt;339&amp;lt;/a&amp;gt; values, fields_set, validation_error = validate_model(__pydantic_self__.__class__, data) &amp;lt;a href=&amp;#39;file:///Users/pratham/LLMApi/venv/lib/python3.12/site-packages/pydantic/main.py?line=339&amp;#39;&amp;gt;340&amp;lt;/a&amp;gt; if validation_error: --&amp;gt; &amp;lt;a href=&amp;#39;file:///Users/pratham/LLMApi/venv/lib/python3.12/site-packages/pydantic/main.py?line=340&amp;#39;&amp;gt;341&amp;lt;/a&amp;gt; raise validation_error &amp;lt;a href=&amp;#39;file:///Users/pratham/LLMApi/venv/lib/python3.12/site-packages/pydantic/main.py?line=341&amp;#39;&amp;gt;342&amp;lt;/a&amp;gt; try: &amp;lt;a href=&amp;#39;file:///Users/pratham/LLMApi/venv/lib/python3.12/site-packages/pydantic/main.py?line=342&amp;#39;&amp;gt;343&amp;lt;/a&amp;gt; object_setattr(__pydantic_self__, &amp;#39;__dict__&amp;#39;, values) ValidationError: 1 validation error for Generation text str type expected (type=type_error.str)&amp;quot; } &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Agent Output:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Entering new AgentExecutor chain...&lt;br/&gt; {&lt;br/&gt; &amp;quot;number_of_top_rows&amp;quot;: &amp;quot;2&amp;quot;&lt;br/&gt; }&lt;/p&gt; &lt;p&gt;Finished chain.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Pydantic Object:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;class NumberofTopRows(BaseModel): number_of_top_rows: str = Field(description=&amp;quot;Number of top rows of the dataframe that should be header rows as string datatype&amp;quot;) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This works fine for other schemas but not for this one. I am unable to figure out what is the problem. Has anyone faced similar issue? Please help in resolving this error.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/pratham1443&quot;&gt; /u/pratham1443 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cxxvvc/json_output_parser_error_please_help/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cxxvvc/json_output_parser_error_please_help/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cxxvvc</id><link href="https://www.reddit.com/r/LangChain/comments/1cxxvvc/json_output_parser_error_please_help/" /><updated>2024-05-22T11:25:59+00:00</updated><published>2024-05-22T11:25:59+00:00</published><title>JSON Output Parser Error Please help!</title></entry><entry><author><name>/u/Zheng_SJ</name><uri>https://www.reddit.com/user/Zheng_SJ</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi! I am working on a project called Pluto, which is a cloud-native application development tool. It simplifies cloud application development by providing a streamlined programming interface for leveraging cloud features and building business logic. Developers can define their dependent services and resources, such as Lambda, Bucket, and etc. by defining a variable. Pluto will automatically provision the resources and deploy the application to the cloud. Developer use the Pluto without need to learn complex cloud technologies, such as Terraform, Pulumi or AWS CDK.&lt;/p&gt; &lt;p&gt;To help the LangServe app developers that don&amp;#39;t have much experience with cloud to deploy their apps on the cloud, I&amp;#39;d like to introduce Pluto as a potential option for deploying LangServe apps.&lt;/p&gt; &lt;p&gt;In summary, there are two steps to adapt the LangServe application to the Pluto application so that Pluto can deploy it to AWS.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;First, put the code related to the FastAPI app into a function and make this function return the FastAPI app instance. Here we assume that the function name is &lt;code&gt;return_fastapi_app&lt;/code&gt;.&lt;/li&gt; &lt;li&gt;Then, replace the entire if &lt;code&gt;__name__ == &amp;quot;__main__&amp;quot;&lt;/code&gt; code block with the following 4 statements. The &lt;code&gt;router_name&lt;/code&gt; can be modified. It is related to the name of the Api Gateway instance created on AWS.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;```python from mangum import Mangum from pluto_client import Router&lt;/p&gt; &lt;p&gt;router = Router(&amp;quot;router_name&amp;quot;) router.all(&amp;quot;/&lt;em&gt;&amp;quot;, lambda *args, *&lt;/em&gt;kwargs: Mangum(return_fastapi_app(), api_gateway_base_path=&amp;quot;/dev&amp;quot;)(&lt;em&gt;args, *&lt;/em&gt;kwargs), raw=True) ```&lt;/p&gt; &lt;p&gt;For more information, please refer to &lt;a href=&quot;https://pluto-lang.vercel.app/cookbook/deploy-langserve-to-aws&quot;&gt;this link&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;I&amp;#39;m not entirely sure if this is an optimal interface for developing LangServe applications. Do you believe it&amp;#39;s an effective method for LangServe app developers to deploy their applications in the cloud? I would appreciate any suggestions or queries that you might have. Thank you!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Zheng_SJ&quot;&gt; /u/Zheng_SJ &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cxuc3g/rfc_introduce_pluto_as_a_deployment_option_for/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cxuc3g/rfc_introduce_pluto_as_a_deployment_option_for/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cxuc3g</id><link href="https://www.reddit.com/r/LangChain/comments/1cxuc3g/rfc_introduce_pluto_as_a_deployment_option_for/" /><updated>2024-05-22T07:15:42+00:00</updated><published>2024-05-22T07:15:42+00:00</published><title>[RFC] Introduce Pluto as a deployment option for LangServe apps</title></entry><entry><author><name>/u/Ancient-Analysis2909</name><uri>https://www.reddit.com/user/Ancient-Analysis2909</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey everyone! I&amp;#39;m working in educational research, specifically using large language models (LLMs) to determine if questions require collaboration between students (like if I give different info to Student A and Student B, and they need to work together to answer). I&amp;#39;ve been running several LLMs on various questions and now I&amp;#39;m planning to use DSPy to tweak the prompts to hopefully get better accuracy. I can handle the basic DSPy stuff, but I&amp;#39;m stumped on how it actually improves the prompts. I get that prompts with higher metric scores are supposed to be better, but what&amp;#39;s the actual strategy DSPy uses to enhance them?&lt;/p&gt; &lt;p&gt;Also, my questions often go beyond simple Q&amp;amp;A—they can get pretty lengthy. Do you think DSPy is suitable for this kind of complex scenario? Any advice would be super helpful. Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Ancient-Analysis2909&quot;&gt; /u/Ancient-Analysis2909 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cxo55v/how_dspy_tuning_the_prompts/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cxo55v/how_dspy_tuning_the_prompts/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cxo55v</id><link href="https://www.reddit.com/r/LangChain/comments/1cxo55v/how_dspy_tuning_the_prompts/" /><updated>2024-05-22T01:12:00+00:00</updated><published>2024-05-22T01:12:00+00:00</published><title>How DSPy tuning the prompts?</title></entry></feed>