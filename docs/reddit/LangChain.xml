<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/"><category term="LangChain" label="r/LangChain"/><updated>2023-12-30T03:28:13+00:00</updated><icon>https://www.redditstatic.com/icon.png/</icon><id>/r/LangChain.rss</id><link rel="self" href="https://www.reddit.com/r/LangChain.rss" type="application/atom+xml" /><link rel="alternate" href="https://www.reddit.com/r/LangChain" type="text/html" /><subtitle>LangChain is an open-source framework and developer toolkit that helps developers get LLM applications from prototype to production. It is available for Python and Javascript at https://www.langchain.com/.</subtitle><title>LangChain</title><entry><author><name>/u/zchaarm</name><uri>https://www.reddit.com/user/zchaarm</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;A place for members of &lt;a href=&quot;/r/LangChain&quot;&gt;r/LangChain&lt;/a&gt; to chat with each other&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/zchaarm&quot;&gt; /u/zchaarm &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_10ljho9</id><link href="https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/" /><updated>2023-01-26T04:33:05+00:00</updated><published>2023-01-26T04:33:05+00:00</published><title>r/LangChain Lounge</title></entry><entry><author><name>/u/khaledmsm</name><uri>https://www.reddit.com/user/khaledmsm</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;hello folks &lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;i hav an idea and i want start to build it but before i have question based on the nature of the project and data &lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;what should use to bulid it ? &lt;/p&gt; &lt;p&gt;when the data is static and its contains 50K document&amp;#39;s ,&lt;/p&gt; &lt;p&gt;should i use Chatgpt Api ? &lt;/p&gt; &lt;p&gt;or Langchain ? &lt;/p&gt; &lt;p&gt;or lamaindex ? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/khaledmsm&quot;&gt; /u/khaledmsm &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18tvsop/what_should_use_to_bulid_saas_for_chating_with/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18tvsop/what_should_use_to_bulid_saas_for_chating_with/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18tvsop</id><link href="https://www.reddit.com/r/LangChain/comments/18tvsop/what_should_use_to_bulid_saas_for_chating_with/" /><updated>2023-12-29T19:55:01+00:00</updated><published>2023-12-29T19:55:01+00:00</published><title>what should use to bulid Saas for chating with static 50K document's , Chatgpt Api ? or Langchain ? or lamaindex ?</title></entry><entry><author><name>/u/DevotedToSuccess</name><uri>https://www.reddit.com/user/DevotedToSuccess</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;I&amp;#39;m stuck deciding the infrastructure for my production RAG chat.&lt;/p&gt; &lt;p&gt;I am tyrying to decide between using a Fastapi server (langserve) in python, or try to create everything in the Nextjs project with Typescript.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;My thoughts so far:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;1) There is no ready-made Hybrid-search for Supabase in Python (but there is in JS)&lt;/p&gt; &lt;p&gt;2) The more advanced RAG features seem to be released in the Python version of Langchain first. like Cohere Reranking, Hyde, Query-expansion etc.&lt;/p&gt; &lt;p&gt;3) I already have the setup for the RAG in langserve, but I&amp;#39;m struggling working out how to keep a good chat history integrated against the nextjs frontend and the langserve server.&lt;/p&gt; &lt;p&gt;4) I leaning towards Supabase pgvector as my vector storage, since I feel it&amp;#39;s more cost-effective and safe in terms of control (the RAG chat will include that the user can upload files, and mix a lot of different businesses on the same index in Pinecone doesn&amp;#39;t seem like the best approach, maybe I&amp;#39;m wrong?)&lt;/p&gt; &lt;p&gt;Would appriciate some feedback so I can make the decision and move forwards.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/DevotedToSuccess&quot;&gt; /u/DevotedToSuccess &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18tpbcb/cant_decide_on_infrastructure_for_my_rag/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18tpbcb/cant_decide_on_infrastructure_for_my_rag/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18tpbcb</id><link href="https://www.reddit.com/r/LangChain/comments/18tpbcb/cant_decide_on_infrastructure_for_my_rag/" /><updated>2023-12-29T15:13:41+00:00</updated><published>2023-12-29T15:13:41+00:00</published><title>Can't decide on infrastructure for my RAG</title></entry><entry><author><name>/u/Bunkoer</name><uri>https://www.reddit.com/user/Bunkoer</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hello community ü§ó&lt;/p&gt; &lt;p&gt;How do we ensure the utmost protection of sensitive data prior to processing with advanced Large Language Models like ChatGPT 4, Llama 2, or Mistral AI? üõ°Ô∏è&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Bunkoer&quot;&gt; /u/Bunkoer &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18tqmjm/open_source_for_large_langage_model/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18tqmjm/open_source_for_large_langage_model/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18tqmjm</id><link href="https://www.reddit.com/r/LangChain/comments/18tqmjm/open_source_for_large_langage_model/" /><updated>2023-12-29T16:11:15+00:00</updated><published>2023-12-29T16:11:15+00:00</published><title>Open Source for Large Langage Model</title></entry><entry><author><name>/u/e-nigmaNL</name><uri>https://www.reddit.com/user/e-nigmaNL</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/e-nigmaNL&quot;&gt; /u/e-nigmaNL &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;/r/LocalLLaMA/comments/18tluwk/finetune_rag_or_live_search/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18tlvdb/finetune_rag_or_live_search/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18tlvdb</id><link href="https://www.reddit.com/r/LangChain/comments/18tlvdb/finetune_rag_or_live_search/" /><updated>2023-12-29T12:17:21+00:00</updated><published>2023-12-29T12:17:21+00:00</published><title>Finetune, RAG or live search</title></entry><entry><author><name>/u/Logical_Buyer9310</name><uri>https://www.reddit.com/user/Logical_Buyer9310</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18tpojd/upgraded_eleven_labs_openai_chatbots_gpts/&quot;&gt; &lt;img src=&quot;https://external-preview.redd.it/bWlwMndlaHc0OTljMS0G1U5IV5irRlKKm46Asdt4g4l9DmBFO2fFVizVQ2we.png?width=140&amp;amp;height=140&amp;amp;crop=140:140,smart&amp;amp;format=jpg&amp;amp;v=enabled&amp;amp;lthumb=true&amp;amp;s=f8b89b3ed57412e71b8f2b8c2f6e69d40351b808&quot; alt=&quot;Upgraded Eleven Labs + OpenAi Chatbots &amp;amp; GPTs&quot; title=&quot;Upgraded Eleven Labs + OpenAi Chatbots &amp;amp; GPTs&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Logical_Buyer9310&quot;&gt; /u/Logical_Buyer9310 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://v.redd.it/9djfshlw499c1&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18tpojd/upgraded_eleven_labs_openai_chatbots_gpts/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_18tpojd</id><media:thumbnail url="https://external-preview.redd.it/bWlwMndlaHc0OTljMS0G1U5IV5irRlKKm46Asdt4g4l9DmBFO2fFVizVQ2we.png?width=140&amp;height=140&amp;crop=140:140,smart&amp;format=jpg&amp;v=enabled&amp;lthumb=true&amp;s=f8b89b3ed57412e71b8f2b8c2f6e69d40351b808" /><link href="https://www.reddit.com/r/LangChain/comments/18tpojd/upgraded_eleven_labs_openai_chatbots_gpts/" /><updated>2023-12-29T15:30:19+00:00</updated><published>2023-12-29T15:30:19+00:00</published><title>Upgraded Eleven Labs + OpenAi Chatbots &amp; GPTs</title></entry><entry><author><name>/u/itschris</name><uri>https://www.reddit.com/user/itschris</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;m following the langchain example &lt;a href=&quot;https://python.langchain.com/docs/use_cases/question_answering/#adding-sources&quot;&gt;here&lt;/a&gt; that is used to cite sources. It works great when the answer actually comes from the context. But a simple query such as &amp;quot;hello&amp;quot; will answer with &amp;quot;Hello!&amp;quot; with sources. So even irrelevant sources are returned. Is there anyway to modify the LCEL provided by langchain to not return sources if it doesn&amp;#39;t find an answer from them?&lt;/p&gt; &lt;p&gt;I&amp;#39;ve also tried &lt;strong&gt;RetrievalQAWithSourcesChain&lt;/strong&gt; and it works better when returning sources, but it&amp;#39;s not returning any metadata - only the link. Additionally, the quality of the results vs LCEL (human evaluation) seems to fall short. The only way I know change the quality of the results is to use custom PromptTemplates, but still not sure how to get the metadata.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/itschris&quot;&gt; /u/itschris &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18thci6/langchain_citing_sources_when_answers_not_from/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18thci6/langchain_citing_sources_when_answers_not_from/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18thci6</id><link href="https://www.reddit.com/r/LangChain/comments/18thci6/langchain_citing_sources_when_answers_not_from/" /><updated>2023-12-29T07:23:30+00:00</updated><published>2023-12-29T07:23:30+00:00</published><title>Langchain citing sources when answers not from context (LCEL vs RetrievalQAWithSourcesChain)</title></entry><entry><author><name>/u/NetIcy6229</name><uri>https://www.reddit.com/user/NetIcy6229</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;pre&gt;&lt;code&gt;text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=80) chunks = text_splitter.split_text(PolicyScheduleRaw) for chunk in chunks: print(chunk) print(&amp;quot;---------------------------------------------------&amp;quot;) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;So I have the code above that splits the string PolicyScheduleRaw into chunks. This above step completes successfully. However, when I then proceed to execute the below code, I get the error message: &lt;code&gt;&amp;#39;str&amp;#39; object has no attribute &amp;#39;page_content&amp;#39;&lt;/code&gt; &lt;/p&gt; &lt;pre&gt;&lt;code&gt;llm = ChatOpenAI(temperature=0, openai_api_key=&amp;quot;SECRET&amp;quot;) question = &amp;quot;Are my Bosch power tools covered?&amp;quot; chain = load_qa_chain(llm, chain_type=&amp;#39;stuff&amp;#39;) result = chain.run(input_documents=chunks, model=&amp;#39;gpt-3.5-turbo-1106&amp;#39;,question=question) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;If I change &lt;/p&gt; &lt;pre&gt;&lt;code&gt;split_text &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;to &lt;/p&gt; &lt;pre&gt;&lt;code&gt;create_documents &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;in the first code block (I read somewhere that this could be the cause), then the chunk size of 1,000 no longer applies and for some reason the text is split per each character. &lt;/p&gt; &lt;p&gt;How can I fix this?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/NetIcy6229&quot;&gt; /u/NetIcy6229 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18tkz5v/splitting_string_data_in_order_to_apply_load_qa/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18tkz5v/splitting_string_data_in_order_to_apply_load_qa/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18tkz5v</id><link href="https://www.reddit.com/r/LangChain/comments/18tkz5v/splitting_string_data_in_order_to_apply_load_qa/" /><updated>2023-12-29T11:22:09+00:00</updated><published>2023-12-29T11:22:09+00:00</published><title>Splitting string data in order to apply load_qa_chain</title></entry><entry><author><name>/u/phicreative1997</name><uri>https://www.reddit.com/user/phicreative1997</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I find the syntax weird (tbh just saw it for the first time). What is a intuitive way I apply this syntax, the documentation is not super clear.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/phicreative1997&quot;&gt; /u/phicreative1997 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18tkbvr/what_is_a_intuitive_way_to_get_used_to_lcel/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18tkbvr/what_is_a_intuitive_way_to_get_used_to_lcel/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18tkbvr</id><link href="https://www.reddit.com/r/LangChain/comments/18tkbvr/what_is_a_intuitive_way_to_get_used_to_lcel/" /><updated>2023-12-29T10:41:07+00:00</updated><published>2023-12-29T10:41:07+00:00</published><title>What is a intuitive way to get used to LCEL</title></entry><entry><author><name>/u/AndreeSmothers</name><uri>https://www.reddit.com/user/AndreeSmothers</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Published a blog post with explanation of RAGs and some techniques we have seen work in production for effective pipelines. Check it out at: &lt;a href=&quot;https://deepchecks.com/retrieval-augmented-generation-best-practices-and-use-cases/&quot;&gt;https://deepchecks.com/retrieval-augmented-generation-best-practices-and-use-cases/&lt;/a&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/AndreeSmothers&quot;&gt; /u/AndreeSmothers &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18te9m1/retrieval_augmented_generation_basics_and/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18te9m1/retrieval_augmented_generation_basics_and/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18te9m1</id><link href="https://www.reddit.com/r/LangChain/comments/18te9m1/retrieval_augmented_generation_basics_and/" /><updated>2023-12-29T04:29:24+00:00</updated><published>2023-12-29T04:29:24+00:00</published><title>Retrieval augmented generation: Basics and production tips</title></entry><entry><author><name>/u/AnantVignesh</name><uri>https://www.reddit.com/user/AnantVignesh</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Comment down below what you guys think about it. I think there is a huge push from the Langchain core dev community to push it forward but I&amp;#39;m not sure if the larger community really welcomes this. I may be wrong here, but just curious.&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://www.reddit.com/poll/18t3jn9&quot;&gt;View Poll&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/AnantVignesh&quot;&gt; /u/AnantVignesh &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18t3jn9/do_we_really_need_lcel/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18t3jn9/do_we_really_need_lcel/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18t3jn9</id><link href="https://www.reddit.com/r/LangChain/comments/18t3jn9/do_we_really_need_lcel/" /><updated>2023-12-28T20:23:58+00:00</updated><published>2023-12-28T20:23:58+00:00</published><title>Do we really need LCEL?</title></entry><entry><author><name>/u/01jonathanf</name><uri>https://www.reddit.com/user/01jonathanf</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I came across Tuna, a tool for generating Q&amp;amp;A data from plain text files: &lt;a href=&quot;https://blog.langchain.dev/introducing-tuna-a-tool-for-rapidly-generating-synthetic-fine-tuning-datasets/&quot;&gt;https://blog.langchain.dev/introducing-tuna-a-tool-for-rapidly-generating-synthetic-fine-tuning-datasets/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;From what I understand, Tuna generates question-answer pairs for each individual section of text, rather than creating complex cross-topic question-answer pairs that encompass the entire document. There is the &amp;quot;Multi chunk&amp;quot; feature where it can take up to 5 chunks maximum, but still not taking into account anywhere near the entire document or multiple documents.&lt;/p&gt; &lt;p&gt;My point is that research, e.g.: the LIMA paper, suggests that the model will perform best and converge more quickly when provided with complex, high-quality question-answer data. However, is it possible that the model could converge just as well with less intricate data, provided that there is a sufficient amount of it?&lt;/p&gt; &lt;p&gt;I see that Andrew Gao developed this, I will try reach out directly to him.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/01jonathanf&quot;&gt; /u/01jonathanf &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18srt00/why_is_tuna_designed_like_it_is/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18srt00/why_is_tuna_designed_like_it_is/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18srt00</id><link href="https://www.reddit.com/r/LangChain/comments/18srt00/why_is_tuna_designed_like_it_is/" /><updated>2023-12-28T11:29:12+00:00</updated><published>2023-12-28T11:29:12+00:00</published><title>Why is Tuna designed like it is?</title></entry><entry><author><name>/u/4vrf</name><uri>https://www.reddit.com/user/4vrf</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey everyone, just looking for some guidance here. I&amp;#39;ve built a RAG bot on pinecone but its.. not great. It sometimes answers questions incorrectly or repeats itself inappropriately. Thoughts? Thanks in advance&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/4vrf&quot;&gt; /u/4vrf &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18spba0/rag_prompting/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18spba0/rag_prompting/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18spba0</id><link href="https://www.reddit.com/r/LangChain/comments/18spba0/rag_prompting/" /><updated>2023-12-28T08:45:21+00:00</updated><published>2023-12-28T08:45:21+00:00</published><title>RAG + Prompting</title></entry><entry><author><name>/u/Background-Maybe-381</name><uri>https://www.reddit.com/user/Background-Maybe-381</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hello everyone.&lt;/p&gt; &lt;p&gt;We are still getting to grips with langchain, but we were able to get a phind-34B llm outputting the prefix using prompt tempate, works very well with tools. Problem is that we want to finetune a llama-2 70B using qlora to a 4bit, and for the life of us, we cannot get the llm to output the prefix in its responses , for example, we can&amp;#39;t get it to respond as AI; so the output parser picks it up. We tried all kinds of stuff in the prompt template like &amp;quot;ALWAYS respond as AI: for example &amp;quot;AI: (your response here)&amp;quot; which seems to work really well with the phind 34b (TheBloke awq version). &lt;/p&gt; &lt;p&gt;Can anyone give us a hint of how to get langchain ConversationChatAgent (for conversation and tool usage) to work with a llama-2 based llm? We&amp;#39;ve been at it for 4 days, and no luck. &lt;/p&gt; &lt;p&gt;Thanks in advance.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Background-Maybe-381&quot;&gt; /u/Background-Maybe-381 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18t2u4g/conversationagent_llama2_70b_4b_outputparser/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18t2u4g/conversationagent_llama2_70b_4b_outputparser/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18t2u4g</id><link href="https://www.reddit.com/r/LangChain/comments/18t2u4g/conversationagent_llama2_70b_4b_outputparser/" /><updated>2023-12-28T19:53:24+00:00</updated><published>2023-12-28T19:53:24+00:00</published><title>ConversationAgent + Llama-2 70B 4b - outputparser error llm not outputting prefix</title></entry><entry><author><name>/u/Material_Policy6327</name><uri>https://www.reddit.com/user/Material_Policy6327</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Been using langchain for a bit but these docs just annoy the hell out of me now. Is there any possible refactor that could help maybe or it‚Äôs just too bloated now? Seems like 5 ways to do 1 thing at times.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Material_Policy6327&quot;&gt; /u/Material_Policy6327 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18sdap4/why_do_the_langchain_docs_feel_so_all_over_the/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18sdap4/why_do_the_langchain_docs_feel_so_all_over_the/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18sdap4</id><link href="https://www.reddit.com/r/LangChain/comments/18sdap4/why_do_the_langchain_docs_feel_so_all_over_the/" /><updated>2023-12-27T22:34:50+00:00</updated><published>2023-12-27T22:34:50+00:00</published><title>Why do the langchain docs feel so all over the place?</title></entry><entry><author><name>/u/Useful_Ad_7882</name><uri>https://www.reddit.com/user/Useful_Ad_7882</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Going to production with a custom RAG application using chromadb and multiple document types. Langchain has been working well but need to understand what other alternatives people have been using? We ingest a lot of documents for which langchain seemed to have good support. Other frameworks not so much.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Useful_Ad_7882&quot;&gt; /u/Useful_Ad_7882 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18skpot/looking_for_a_langchain_alternative/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18skpot/looking_for_a_langchain_alternative/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18skpot</id><link href="https://www.reddit.com/r/LangChain/comments/18skpot/looking_for_a_langchain_alternative/" /><updated>2023-12-28T04:13:21+00:00</updated><published>2023-12-28T04:13:21+00:00</published><title>Looking for a langchain alternative</title></entry><entry><author><name>/u/askvikasr</name><uri>https://www.reddit.com/user/askvikasr</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I am working on implementing LangChain Agents in my Python project I am running this project using docker compose. In my project I am using Celery worker and have multiple worker services which executes from the queue. The entire setup is working as expected.&lt;/p&gt; &lt;p&gt;One of these workers in agent worker where I have configured LangChain Agent. I have created a function where I am loading tools, initializing agent and passing agent input.&lt;/p&gt; &lt;p&gt;When I run this setup as a simple standalone python project, everything runs fine and I see proper agent execution and output. But in the production environment I am using Celery workers running as Docker container. The above setup is run as a Celery worker. The setup is quite simple and straightforward.&lt;/p&gt; &lt;p&gt;But when I run this setup in as Celery worker, I observe that it &lt;strong&gt;loads the tool and initializes the agent properly&lt;/strong&gt;. It also enters in the AgentExecutor chain &lt;strong&gt;but fails to execute the tool function using the parameter returned in the LLM response&lt;/strong&gt;. &lt;/p&gt; &lt;p&gt;I have added more details, code the logs in the Github issue but haven&amp;#39;t got any proper response yet: &lt;a href=&quot;https://github.com/langchain-ai/langchain/issues/15220&quot;&gt;https://github.com/langchain-ai/langchain/issues/15220&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Any help is highly appreciated. Happy Holidays!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/askvikasr&quot;&gt; /u/askvikasr &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18sw41b/need_help_langchain_agent_not_executing_properly/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18sw41b/need_help_langchain_agent_not_executing_properly/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18sw41b</id><link href="https://www.reddit.com/r/LangChain/comments/18sw41b/need_help_langchain_agent_not_executing_properly/" /><updated>2023-12-28T15:11:12+00:00</updated><published>2023-12-28T15:11:12+00:00</published><title>[Need Help] Langchain agent not executing properly in Celery worker</title></entry><entry><author><name>/u/IlEstLaPapi</name><uri>https://www.reddit.com/user/IlEstLaPapi</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I have a project that requires to extract data from complex word documents.&lt;/p&gt; &lt;p&gt;Each document is composed of a few tables (10 to 30). In each tables I might have :&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Text&lt;/li&gt; &lt;li&gt;Mathematical equations&lt;/li&gt; &lt;li&gt;Images (mostly math graphs).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;My initial goal is to be able to process the text and equations, I&amp;#39;ll leave the images for latter.&lt;/p&gt; &lt;p&gt;So far I haven&amp;#39;t been able to retrieve the equations in the table. I have tried the following methods:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;langchain + docx2txt : 1 huge blob of text, not exploitable at all.&lt;/li&gt; &lt;li&gt;lagnchain + unstructured in classical mode : same.&lt;/li&gt; &lt;li&gt;langchain + unstructured + elements : I got a list of tables, which is nice, however in both the tables[1][&amp;#39;kwargs&amp;#39;][&amp;#39;metadata&amp;#39;][&amp;#39;text_as_html&amp;#39;] and tables[1][&amp;#39;kwargs&amp;#39;][&amp;#39;page_content&amp;#39;] the equations aren&amp;#39;t shown.&lt;/li&gt; &lt;li&gt;pandoc docs -&amp;gt; html : both the equations and images are represented as html &lt;strong&gt;{=html}&lt;/strong&gt; and no content is shown.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I suppose that my next best solution is to use OCR like logic but I don&amp;#39;t really like the idea. Do you have any suggestion ?&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/IlEstLaPapi&quot;&gt; /u/IlEstLaPapi &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18s4fxg/processing_complex_word_documents_with_equations/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18s4fxg/processing_complex_word_documents_with_equations/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18s4fxg</id><link href="https://www.reddit.com/r/LangChain/comments/18s4fxg/processing_complex_word_documents_with_equations/" /><updated>2023-12-27T16:22:09+00:00</updated><published>2023-12-27T16:22:09+00:00</published><title>Processing complex word documents with equations.</title></entry><entry><author><name>/u/Karl_Pilkingt0n</name><uri>https://www.reddit.com/user/Karl_Pilkingt0n</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I ingested a bunch of documents into a postgres table, where 1 column is a vector embedding (pgvector extension) and the rest of them are a bunch of metadata and 1 column of the original text. &lt;/p&gt; &lt;p&gt;I want to use this as a retrieval source in a RAG application. I&amp;#39;m finding it hard to use Langchain&amp;#39;s &lt;a href=&quot;https://python.langchain.com/docs/integrations/vectorstores/pgvector&quot;&gt;pgvector&lt;/a&gt; feature - since I didn&amp;#39;t create the table itself using langchain. &lt;/p&gt; &lt;p&gt;Does anyone have examples or pointers to using a separately generated vector table with langchain&amp;#39;s pgvector feature? Or is using a custom retriever my only path forward?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Karl_Pilkingt0n&quot;&gt; /u/Karl_Pilkingt0n &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18sivbd/langchain_pgvector_need_help/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18sivbd/langchain_pgvector_need_help/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18sivbd</id><link href="https://www.reddit.com/r/LangChain/comments/18sivbd/langchain_pgvector_need_help/" /><updated>2023-12-28T02:40:32+00:00</updated><published>2023-12-28T02:40:32+00:00</published><title>Langchain + pgvector, need help.</title></entry><entry><author><name>/u/DevelopmentNo409</name><uri>https://www.reddit.com/user/DevelopmentNo409</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18sh4zw/can_anyone_help_me_to_solve_this_error/&quot;&gt; &lt;img src=&quot;https://b.thumbs.redditmedia.com/Tz5G8mH0FqbgBdwlE_imp6mMnTht8KSSQ-jbc4B0FNU.jpg&quot; alt=&quot;Can anyone help me to solve this error ValidationError: 1 validation error for LLMChain prompt Can't instantiate abstract class BasePromptTemplate with abstract methods format, format_prompt (type=type_error)&quot; title=&quot;Can anyone help me to solve this error ValidationError: 1 validation error for LLMChain prompt Can't instantiate abstract class BasePromptTemplate with abstract methods format, format_prompt (type=type_error)&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;pre&gt;&lt;code&gt;from langchain.output_parsers import ResponseSchema, StructuredOutputParser from langchain_core.prompts import PromptTemplate from langchain.memory import ConversationSummaryBufferMemory checker_tmpl = &amp;quot;&amp;quot;&amp;quot; # Task Description: As a customer support representative, your role is to provide accurate and helpful responses to customer inquiries. Use the provided context to understand the customer&amp;#39;s issue and answer their questions directly. Avoid any extraneous information or explanations that are not directly relevant to the customer&amp;#39;s query. {format_instructions} # Given Context: {context} #Chat History:\n\n{chat_history} \n\n # Customer&amp;#39;s Question: {question} # Your Response: [Please type your answer here, ensuring it is concise, relevant, and directly addresses the customer&amp;#39;s question based on the given context.] &amp;quot;&amp;quot;&amp;quot; checker_response_schemas = [ ResponseSchema( name=&amp;quot;requires_customer_support_contact&amp;quot;, description=&amp;quot;Indicates whether the user needs to contact customer support. Set to True if the context does not sufficiently address the user&amp;#39;s issue and further assistance is needed. Set to False if the provided context is adequate to answer the user&amp;#39;s query.&amp;quot;, type=&amp;quot;boolean&amp;quot; ), ResponseSchema( name=&amp;quot;contextual_answer&amp;quot;, description=&amp;quot;Provides a direct answer to the user&amp;#39;s question, utilizing the given context. The response should be concise, accurate, and specifically tailored to address the query based on the context provided.&amp;quot;, ), ] check_output_parser = StructuredOutputParser.from_response_schemas(checker_response_schemas) resume_checker_prompt = PromptTemplate( template=checker_tmpl, input_variables=[&amp;quot;chat_history&amp;quot;,&amp;quot;context&amp;quot;, &amp;quot;question&amp;quot;], partial_variables={ &amp;quot;format_instructions&amp;quot;: check_output_parser.get_format_instructions() } ) openai = ChatOpenAI(model_name=&amp;quot;gpt-4-0613&amp;quot;) memory = ConversationSummaryBufferMemory(llm = openai, memory_key=&amp;quot;chat_history&amp;quot;) # check_prompt_str = resume_checker_prompt.format(chat_history = memory.get_memory(), context=context, question = &amp;quot;I am 80 yeard old guy suggest some best plans&amp;quot; ) chat_chain = LLMChain( llm=openai, prompt=resume_checker_prompt, verbose=True, memory=memory, ) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/lfitazcosx8c1.png?width=1615&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5a4e30848189e74d03baa4fd4790a58d81a86333&quot;&gt;https://preview.redd.it/lfitazcosx8c1.png?width=1615&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5a4e30848189e74d03baa4fd4790a58d81a86333&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/DevelopmentNo409&quot;&gt; /u/DevelopmentNo409 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18sh4zw/can_anyone_help_me_to_solve_this_error/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18sh4zw/can_anyone_help_me_to_solve_this_error/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_18sh4zw</id><media:thumbnail url="https://b.thumbs.redditmedia.com/Tz5G8mH0FqbgBdwlE_imp6mMnTht8KSSQ-jbc4B0FNU.jpg" /><link href="https://www.reddit.com/r/LangChain/comments/18sh4zw/can_anyone_help_me_to_solve_this_error/" /><updated>2023-12-28T01:18:29+00:00</updated><published>2023-12-28T01:18:29+00:00</published><title>Can anyone help me to solve this error ValidationError: 1 validation error for LLMChain prompt Can't instantiate abstract class BasePromptTemplate with abstract methods format, format_prompt (type=type_error)</title></entry><entry><author><name>/u/BankHottas</name><uri>https://www.reddit.com/user/BankHottas</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;m pretty new to Langchain, but I&amp;#39;m comfortable building RAG applications with &amp;quot;static&amp;quot; data, such as PDFs, webpages, etc. I&amp;#39;m now trying out the SQL toolkit, but I have some questions.&lt;/p&gt; &lt;p&gt;Since you&amp;#39;re still limited by the model&amp;#39;s context window size, it seems that only certain queries make sense. But what if you wanted to find trends or insights from a larger amount of data, like store orders, or analytics data?&lt;/p&gt; &lt;p&gt;Would you do this in batches and then combine them? Would you vectorize the data first? Is it even feasible to do this currently?&lt;/p&gt; &lt;p&gt;I&amp;#39;m very interested to hear how you would approach this problem&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/BankHottas&quot;&gt; /u/BankHottas &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18rw5rm/what_is_your_strategy_for_doing_inference_over_a/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18rw5rm/what_is_your_strategy_for_doing_inference_over_a/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18rw5rm</id><link href="https://www.reddit.com/r/LangChain/comments/18rw5rm/what_is_your_strategy_for_doing_inference_over_a/" /><updated>2023-12-27T08:43:26+00:00</updated><published>2023-12-27T08:43:26+00:00</published><title>What is your strategy for doing inference over a large SQL dataset?</title></entry><entry><author><name>/u/Calm_Pea_2428</name><uri>https://www.reddit.com/user/Calm_Pea_2428</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I just received a call from Turing for a job, but it mentions a very broad domain, including:&lt;/p&gt; &lt;p&gt;Communication Skills&lt;/p&gt; &lt;p&gt;Machine Learning and Deep Learning Models&lt;/p&gt; &lt;p&gt;Python and large Language Models (LLMs)&lt;/p&gt; &lt;p&gt;AI Problem Solving&lt;/p&gt; &lt;p&gt;My call is in one day, and I&amp;#39;m confused about which concepts I should revise and the people in the CC are mostly Non-technical.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Calm_Pea_2428&quot;&gt; /u/Calm_Pea_2428 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18s795c/turing_technical_call/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18s795c/turing_technical_call/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18s795c</id><link href="https://www.reddit.com/r/LangChain/comments/18s795c/turing_technical_call/" /><updated>2023-12-27T18:20:19+00:00</updated><published>2023-12-27T18:20:19+00:00</published><title>Turing Technical call</title></entry><entry><author><name>/u/Objective-Cobbler22</name><uri>https://www.reddit.com/user/Objective-Cobbler22</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I am trying to implement a chatbot where a user can chat about a document. I am trying to implement it where I read the document through Langcahin -&amp;gt; create embedding and store them (use a huggingface model for embedding) in Chroma -&amp;gt; answer question where I use a huggingface model for LLM. I tried to use a QuestionAnswering Huggingface model, but I got an error. Then I looked at the github code of Langchain and it says they only support few model types which are text-generation, text2text-generation and summarization. Has anyone been able to implement this?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Objective-Cobbler22&quot;&gt; /u/Objective-Cobbler22 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18sbixy/rag_with_langchain_huggingface/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18sbixy/rag_with_langchain_huggingface/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18sbixy</id><link href="https://www.reddit.com/r/LangChain/comments/18sbixy/rag_with_langchain_huggingface/" /><updated>2023-12-27T21:20:06+00:00</updated><published>2023-12-27T21:20:06+00:00</published><title>RAG with Langchain Huggingface</title></entry><entry><author><name>/u/Parking_Skin9598</name><uri>https://www.reddit.com/user/Parking_Skin9598</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;guys i have problem with my chatgpt4 acc i never use the api before but when i call for useing it he gives me this error msg &amp;quot; You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: &lt;a href=&quot;https://platform.openai.com/docs/guides/error-codes/api-errors&quot;&gt;https://platform.openai.com/docs/guides/error-codes/api-errors&lt;/a&gt;. &amp;quot; even if i have already 18 dollas as credit and the graph said i never use it how can i solve this problem and can someone give me apt key i really need it cuz i need to use it in univ project &lt;/p&gt; &lt;p&gt;thanks to all of you .&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Parking_Skin9598&quot;&gt; /u/Parking_Skin9598 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18s9pai/chatgpt4_api_dont_work_for_me/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18s9pai/chatgpt4_api_dont_work_for_me/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18s9pai</id><link href="https://www.reddit.com/r/LangChain/comments/18s9pai/chatgpt4_api_dont_work_for_me/" /><updated>2023-12-27T20:02:50+00:00</updated><published>2023-12-27T20:02:50+00:00</published><title>chatgpt4 api dont work for me</title></entry><entry><author><name>/u/duddai</name><uri>https://www.reddit.com/user/duddai</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey guys,&lt;/p&gt; &lt;p&gt;im new to langchain and I&amp;#39;m wondering how the mapreduce function works. &lt;/p&gt; &lt;p&gt;If I have a chunksize of 10.000 and run the mapreduce function, how can it summarize the chunks with a size of 10.000? I thought the token limit is under 10.000. &lt;/p&gt; &lt;p&gt;And when i change the verbose bool to true, it just shows me that it summarize the content of the chunk (which is 10.000, so I thought it doenst work). &lt;/p&gt; &lt;p&gt;Im trying to understand the logic but im so confused. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/duddai&quot;&gt; /u/duddai &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18s064m/how_does_mapreduce_work/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18s064m/how_does_mapreduce_work/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18s064m</id><link href="https://www.reddit.com/r/LangChain/comments/18s064m/how_does_mapreduce_work/" /><updated>2023-12-27T13:00:20+00:00</updated><published>2023-12-27T13:00:20+00:00</published><title>How does mapReduce work</title></entry><entry><author><name>/u/East-Bug6675</name><uri>https://www.reddit.com/user/East-Bug6675</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;The below code is taking more than 5 min to answer the question, any solution to this? &lt;/p&gt; &lt;p&gt;llm = CTransformers(model=&amp;quot;TheBloke/CodeLlama-7B-Instruct-GGUF&amp;quot;&lt;strong&gt;,&lt;/strong&gt;&lt;br/&gt; model_file=&amp;quot;codellama-7b-instruct.Q5_K_M.gguf&amp;quot;&lt;strong&gt;,&lt;/strong&gt;&lt;br/&gt; # callbacks=[StreamingStdOutCallbackHandler()],&lt;br/&gt; config={&amp;#39;max_new_tokens&amp;#39;: &lt;strong&gt;4096,&lt;/strong&gt;&lt;br/&gt; &amp;#39;context_length&amp;#39;: &lt;strong&gt;4000,&lt;/strong&gt;&lt;br/&gt; &amp;#39;temperature&amp;#39;: &lt;strong&gt;0.01&lt;/strong&gt;})&lt;br/&gt; agent = create_csv_agent(llm&lt;strong&gt;,&lt;/strong&gt;&lt;br/&gt; &amp;#39;August2023.csv&amp;#39;&lt;strong&gt;,&lt;/strong&gt;&lt;br/&gt; verbose=True&lt;strong&gt;,&lt;/strong&gt;&lt;br/&gt; agent_type=AgentType.ZERO_SHOT_REACT_DESCRIPTION&lt;strong&gt;,&lt;/strong&gt;)&lt;/p&gt; &lt;p&gt;agent.run(&amp;quot;how many rows are there?&amp;quot;)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/East-Bug6675&quot;&gt; /u/East-Bug6675 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18ry05l/llm_taking_too_long_time_to_respond/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18ry05l/llm_taking_too_long_time_to_respond/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18ry05l</id><link href="https://www.reddit.com/r/LangChain/comments/18ry05l/llm_taking_too_long_time_to_respond/" /><updated>2023-12-27T10:46:41+00:00</updated><published>2023-12-27T10:46:41+00:00</published><title>LLM taking too long time to respond</title></entry></feed>