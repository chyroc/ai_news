<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/"><category term="LangChain" label="r/LangChain"/><updated>2024-06-20T04:09:52+00:00</updated><icon>https://www.redditstatic.com/icon.png/</icon><id>/r/LangChain.rss</id><link rel="self" href="https://www.reddit.com/r/LangChain.rss" type="application/atom+xml" /><link rel="alternate" href="https://www.reddit.com/r/LangChain" type="text/html" /><subtitle>LangChain is an open-source framework and developer toolkit that helps developers get LLM applications from prototype to production. It is available for Python and Javascript at https://www.langchain.com/.</subtitle><title>LangChain</title><entry><author><name>/u/zchaarm</name><uri>https://www.reddit.com/user/zchaarm</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;A place for members of &lt;a href=&quot;/r/LangChain&quot;&gt;r/LangChain&lt;/a&gt; to chat with each other&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/zchaarm&quot;&gt; /u/zchaarm &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_10ljho9</id><link href="https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/" /><updated>2023-01-26T04:33:05+00:00</updated><published>2023-01-26T04:33:05+00:00</published><title>r/LangChain Lounge</title></entry><entry><author><name>/u/hihowudoin1</name><uri>https://www.reddit.com/user/hihowudoin1</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;We just launched an exciting project and would love to hear your thoughts and feedback! Here&amp;#39;s the scoop:&lt;/p&gt; &lt;p&gt;Project Details:Our open-source initiative focuses on integrating advanced search technologies under one roof. By harnessing gradient boosting (xgboost) machine learning techniques, we combine Keyword-based searches, Vector databases, and Machine Learning rerankers for optimal performance.&lt;/p&gt; &lt;p&gt;Performance Benchmark:According to our tests on the MSMARCO dataset, Denser Retriever has achieved an impressive 13.07% relative gain in NDCG@10 compared to leading vector search baselines of similar model sizes.&lt;/p&gt; &lt;p&gt;Here are the Key Features:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Github repo:&lt;a href=&quot;https://github.com/denser-org/denser-retriever/tree/main&quot;&gt; Denser Retriever&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Blog:&lt;a href=&quot;https://denser.ai/blog/denser-retriever/&quot;&gt; Learn more about Denser Retriever&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Documentation:&lt;a href=&quot;https://retriever.denser.ai/&quot;&gt; Denser Retriever Documentation&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Looking forward to hear your thoughts.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/hihowudoin1&quot;&gt; /u/hihowudoin1 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dk0ukb/seeking_feedback_on_denser_retriever_for_advanced/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dk0ukb/seeking_feedback_on_denser_retriever_for_advanced/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dk0ukb</id><link href="https://www.reddit.com/r/LangChain/comments/1dk0ukb/seeking_feedback_on_denser_retriever_for_advanced/" /><updated>2024-06-20T02:25:33+00:00</updated><published>2024-06-20T02:25:33+00:00</published><title>Seeking Feedback on Denser Retriever for Advanced GenAI RAG Performance</title></entry><entry><author><name>/u/ChallengeOk6437</name><uri>https://www.reddit.com/user/ChallengeOk6437</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I am using Cohere reranker right now and it is really good. I want to know if there is anything else which is as good or better and open source?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/ChallengeOk6437&quot;&gt; /u/ChallengeOk6437 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1djsnov/best_open_source_reranker_for_rag/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1djsnov/best_open_source_reranker_for_rag/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1djsnov</id><link href="https://www.reddit.com/r/LangChain/comments/1djsnov/best_open_source_reranker_for_rag/" /><updated>2024-06-19T20:06:51+00:00</updated><published>2024-06-19T20:06:51+00:00</published><title>Best Open Source RE-RANKER for RAG??!!</title></entry><entry><author><name>/u/sarthakai</name><uri>https://www.reddit.com/user/sarthakai</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Google just announced Context Caching in the Gemini API — it allows you to store and reuse input tokens for repetitive requests.&lt;/p&gt; &lt;p&gt;Many LLM tasks have extensive system prompts laying down instructions and initial context.&lt;/p&gt; &lt;p&gt;If these are cached, they wouldn’t have to be encoded all over again every time, saving on costs and latency.&lt;/p&gt; &lt;p&gt;Tokens are cached for a specified duration (TTL), after which they are automatically deleted.&lt;/p&gt; &lt;p&gt;Costs depend on the number of tokens cached and their storage duration, and efficiency would be higher for prompts with context used across many LLM calls.&lt;/p&gt; &lt;p&gt;Docs: &lt;a href=&quot;https://ai.google.dev/gemini-api/docs/caching?lang=python&quot;&gt;https://ai.google.dev/gemini-api/docs/caching?lang=python&lt;/a&gt;&lt;/p&gt; &lt;p&gt;You can learn more about AI here: &lt;a href=&quot;https://linktr.ee/sarthakrastogi&quot;&gt;https://linktr.ee/sarthakrastogi&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/sarthakai&quot;&gt; /u/sarthakai &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1djjgia/apparently_geminis_context_caching_can_cut_your/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1djjgia/apparently_geminis_context_caching_can_cut_your/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1djjgia</id><link href="https://www.reddit.com/r/LangChain/comments/1djjgia/apparently_geminis_context_caching_can_cut_your/" /><updated>2024-06-19T13:41:00+00:00</updated><published>2024-06-19T13:41:00+00:00</published><title>Apparently Gemini's context caching can cut your LLM cost and latency to half</title></entry><entry><author><name>/u/tf1155</name><uri>https://www.reddit.com/user/tf1155</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I just want to ask if someone is already using LangChain in production, which basically means that during the development process, nothing could stop you from deploying and maintaining it.&lt;/p&gt; &lt;p&gt;I always assumed that LangChain is a project suitable for academic projects and/or beginners as a good starting point. And now, since I also suggested it to our team, two colleagues are against Langchain because they share the same assumption with me and still stick with it.&lt;/p&gt; &lt;p&gt;I am asking here for arguments pro and con :) thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/tf1155&quot;&gt; /u/tf1155 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dju7yb/using_langchain_in_production/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dju7yb/using_langchain_in_production/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dju7yb</id><link href="https://www.reddit.com/r/LangChain/comments/1dju7yb/using_langchain_in_production/" /><updated>2024-06-19T21:12:54+00:00</updated><published>2024-06-19T21:12:54+00:00</published><title>Using LangChain in production</title></entry><entry><author><name>/u/_zero2hundred</name><uri>https://www.reddit.com/user/_zero2hundred</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi all! I’m a newbie developer still learning while trying to get an experimental idea off the ground. &lt;/p&gt; &lt;p&gt;My project involves giving LLMs access to various tools to retrieve data as well as a large vector database. &lt;/p&gt; &lt;p&gt;I want to get a product out as soon as possible while having a scalable codebase for further improvements. I feel like LangChain is much more comprehensive and will be useful for improving my application. On the other hand, Phidata make it so easy to create agents, set up tools and RAG and create multi-agent architectures that I’m leaning towards using it for the first version. &lt;/p&gt; &lt;p&gt;As I’m a beginner, I wanted to get your thoughts on which one I should use to begin with and how to think about stuff like this going forward.&lt;/p&gt; &lt;p&gt;Are there any disadvantages to using such frameworks? Will it cause roadblocks?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/_zero2hundred&quot;&gt; /u/_zero2hundred &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1djzh8e/using_langchain_vs_an_agent_framework_like/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1djzh8e/using_langchain_vs_an_agent_framework_like/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1djzh8e</id><link href="https://www.reddit.com/r/LangChain/comments/1djzh8e/using_langchain_vs_an_agent_framework_like/" /><updated>2024-06-20T01:15:59+00:00</updated><published>2024-06-20T01:15:59+00:00</published><title>Using LangChain vs an agent framework like Phidata for a prototype</title></entry><entry><author><name>/u/chermi</name><uri>https://www.reddit.com/user/chermi</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I was running through what I thought would be a simple tutorial: &lt;a href=&quot;https://python.langchain.com/v0.2/docs/tutorials/chatbot/&quot;&gt;https://python.langchain.com/v0.2/docs/tutorials/chatbot/&lt;/a&gt;, but am having some trouble that seems to indicate they have removed a module referenced in the tutorial.&lt;/p&gt; &lt;p&gt;In the section on managing history &lt;a href=&quot;https://python.langchain.com/v0.2/docs/tutorials/chatbot/#managing-conversation-history&quot;&gt;https://python.langchain.com/v0.2/docs/tutorials/chatbot/#managing-conversation-history&lt;/a&gt; we are supposed to use this &amp;quot;trim_messages&amp;quot; module. I tried importing as in&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;from langchain_core.messages import SystemMessage, trim_messages&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;and got &lt;/p&gt; &lt;pre&gt;&lt;code&gt;cannot import name &amp;#39;trim_messages&amp;#39; from &amp;#39;langchain_core.messages&amp;#39; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;. Diving into the API documentation, I noticed that the &amp;quot;trim_messages&amp;quot; package doesn&amp;#39;t exist.&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://api.python.langchain.com/en/latest/core_api_reference.html#module-langchain_core.messages&quot;&gt;https://api.python.langchain.com/en/latest/core_api_reference.html#module-langchain_core.messages&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This seems like a pretty obvious mistake so I assume I&amp;#39;m doing something wrong. Have any of you got trim_messages to work? Any help would be appreciated. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/chermi&quot;&gt; /u/chermi &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1djw5oq/broken_tutorial/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1djw5oq/broken_tutorial/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1djw5oq</id><link href="https://www.reddit.com/r/LangChain/comments/1djw5oq/broken_tutorial/" /><updated>2024-06-19T22:36:32+00:00</updated><published>2024-06-19T22:36:32+00:00</published><title>Broken Tutorial?</title></entry><entry><author><name>/u/AnEdgeLordWeeb</name><uri>https://www.reddit.com/user/AnEdgeLordWeeb</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey guys!&lt;br/&gt; How would you recommend me to approach the frontend and backend side of such an app? I&amp;#39;m done with creating the chatbot logic through LangGraph, and now I want to implement said logic in a UI where responses can be displayed in a beautiful manner to the user.&lt;br/&gt; What do you think about the idea of using Streamlit for frontend and FastAPI for backend? Also, regarding the backend, I have found an API called &amp;quot;LangCorn&amp;quot; that is said to leverage the power of FastAPI, has anyone worked with it before? Would you recommend me using it instead of the traditional FastAPI framework?&lt;br/&gt; Through this combo, will I be able to add functionalities such as auth, memory persistence and streaming tokens?&lt;br/&gt; Thank you!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/AnEdgeLordWeeb&quot;&gt; /u/AnEdgeLordWeeb &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1djz34y/tips_for_creating_a_simple_webbased_app_for_my/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1djz34y/tips_for_creating_a_simple_webbased_app_for_my/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1djz34y</id><link href="https://www.reddit.com/r/LangChain/comments/1djz34y/tips_for_creating_a_simple_webbased_app_for_my/" /><updated>2024-06-20T00:55:22+00:00</updated><published>2024-06-20T00:55:22+00:00</published><title>Tips for creating a simple web-based app for my chatbot's logic</title></entry><entry><author><name>/u/Due-Pitch-8779</name><uri>https://www.reddit.com/user/Due-Pitch-8779</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Everything looks ok to me from a prompt perspective.. but get this error when using RetrievalQA.invoke &lt;/p&gt; &lt;h1&gt;argument needs to be of type (SquadExample, dict)&lt;/h1&gt; &lt;p&gt;Any Suggestions how to fix this ? thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Due-Pitch-8779&quot;&gt; /u/Due-Pitch-8779 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1djyvwr/rag_using_langchain/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1djyvwr/rag_using_langchain/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1djyvwr</id><link href="https://www.reddit.com/r/LangChain/comments/1djyvwr/rag_using_langchain/" /><updated>2024-06-20T00:45:01+00:00</updated><published>2024-06-20T00:45:01+00:00</published><title>RAG Using LangChain</title></entry><entry><author><name>/u/ImGallo</name><uri>https://www.reddit.com/user/ImGallo</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi, I&amp;#39;m working on a project involving the use of language models (LLMs). I need to perform tasks such as text classification, extraction of sections of interest, entity recognition, and formatting the output in a specific way. From what I&amp;#39;ve read, I might initially be able to achieve this with FewShots, but given the scope of the project, I think I might need to do some fine tuning. I might be a bit presumptuous, as I&amp;#39;m just starting to use this technology. Could anyone recommend any tutorials or resources for learning about Langchain and LLMs?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/ImGallo&quot;&gt; /u/ImGallo &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1djyu3k/recommendations_for_using_llms_in_text_processing/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1djyu3k/recommendations_for_using_llms_in_text_processing/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1djyu3k</id><link href="https://www.reddit.com/r/LangChain/comments/1djyu3k/recommendations_for_using_llms_in_text_processing/" /><updated>2024-06-20T00:42:23+00:00</updated><published>2024-06-20T00:42:23+00:00</published><title>Recommendations for Using LLMs in Text Processing Projects</title></entry><entry><author><name>/u/kid_90</name><uri>https://www.reddit.com/user/kid_90</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Recently I built a chatbot with OpenAI Assistant API. Its doing what we require it to do which brought me to thinking whats the point of langchain or maybe I dont understand it well.&lt;/p&gt; &lt;p&gt;For example, I have custom knowledge base, I upload it to OpenAI Vector Store, connect it to my Assistant and I have a chatbot. Where does langchain come in this?&lt;/p&gt; &lt;p&gt;Or if I upload my knowledge base to any vector database for example, Pinecone, then connect it with OpenAI API, I&amp;#39;d still get a chatbot.&lt;/p&gt; &lt;p&gt;Please help me understand langchain on a deeper level.&lt;/p&gt; &lt;p&gt;Would really appreciate this. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/kid_90&quot;&gt; /u/kid_90 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1djtbtj/still_cant_grasp_the_idea_of_langchain_with/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1djtbtj/still_cant_grasp_the_idea_of_langchain_with/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1djtbtj</id><link href="https://www.reddit.com/r/LangChain/comments/1djtbtj/still_cant_grasp_the_idea_of_langchain_with/" /><updated>2024-06-19T20:35:25+00:00</updated><published>2024-06-19T20:35:25+00:00</published><title>Still cant grasp the idea of Langchain with OpenAI Assistant</title></entry><entry><author><name>/u/bucketheadfan13</name><uri>https://www.reddit.com/user/bucketheadfan13</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;ve looked into completely using Google Big Query to store, embed, and vector search the results since they now offer Vector Searches&lt;/p&gt; &lt;p&gt;Does anyone have any experience doing this with Google Big Query alone?&lt;/p&gt; &lt;p&gt;Would it be better to just import the data into something line Pinecone and use LangChain to chunk/query?&lt;/p&gt; &lt;p&gt;Or could I also just use LangChain with Google Big Query?&lt;/p&gt; &lt;p&gt;Also not sure if I should be chunking the data, or how chunking would work if I needed it to be on an item by item basis&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/bucketheadfan13&quot;&gt; /u/bucketheadfan13 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1djm57k/whats_the_best_way_to_chunk_store_and_query/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1djm57k/whats_the_best_way_to_chunk_store_and_query/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1djm57k</id><link href="https://www.reddit.com/r/LangChain/comments/1djm57k/whats_the_best_way_to_chunk_store_and_query/" /><updated>2024-06-19T15:35:46+00:00</updated><published>2024-06-19T15:35:46+00:00</published><title>What's the best way to chunk, store and, query extremely large datasets where the data is in a CSV/SQL type format (item by item basis with name, description, etc., not a large text file)</title></entry><entry><author><name>/u/dccpt</name><uri>https://www.reddit.com/user/dccpt</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1djkolz/zep_longterm_memory_free_plan_upgraded_to_10k/&quot;&gt; &lt;img src=&quot;https://external-preview.redd.it/LW_D9uWwns4LU9ujOrFRUZxvcoo09iNqWTmeA5II1BQ.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2054db40fb5ea832bf5fcd3e98ecee071501124a&quot; alt=&quot;Zep Long-term Memory: Free Plan Upgraded to 10K Messages&quot; title=&quot;Zep Long-term Memory: Free Plan Upgraded to 10K Messages&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi all - we received some friendly criticism on this subreddit a while back about Zep&amp;#39;s Free Plan limit of 1K messages per month. We&amp;#39;ve heard y&amp;#39;all and have increased the monthly limit 10x to 10K messages. You can sign up here: &lt;a href=&quot;https://www.getzep.com/&quot;&gt;https://www.getzep.com/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We also recently &lt;a href=&quot;https://app.getzep.com/playground&quot;&gt;released a Playground&lt;/a&gt;, allowing you experiment with Zep&amp;#39;s long-term memory features without writing any code.&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://i.redd.it/g2kv7ue7hj7d1.gif&quot;&gt;https://i.redd.it/g2kv7ue7hj7d1.gif&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Getting Started&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Learn about &lt;a href=&quot;https://help.getzep.com/concepts&quot;&gt;key Zep concepts&lt;/a&gt; such as Sessions, Facts, and more.&lt;/li&gt; &lt;li&gt;Experiment with &lt;a href=&quot;https://app.getzep.com/playground&quot;&gt;Zep in the Playground&lt;/a&gt; and &lt;a href=&quot;https://help.getzep.com/building-prompt&quot;&gt;learn how to build LLM prompts&lt;/a&gt; with Zep.&lt;/li&gt; &lt;li&gt;Install Zep&amp;#39;s &lt;a href=&quot;https://help.getzep.com/sdks&quot;&gt;Python, TypeScript, or Go SDKs&lt;/a&gt; and add long-term memory to your application.&lt;/li&gt; &lt;li&gt;Learn how to use &lt;a href=&quot;https://help.getzep.com/langchain/overview&quot;&gt;Zep with LangChain LCEL&lt;/a&gt;.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Let me know if you have any questions!&lt;/p&gt; &lt;p&gt;-Daniel&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/dccpt&quot;&gt; /u/dccpt &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1djkolz/zep_longterm_memory_free_plan_upgraded_to_10k/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1djkolz/zep_longterm_memory_free_plan_upgraded_to_10k/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1djkolz</id><media:thumbnail url="https://external-preview.redd.it/LW_D9uWwns4LU9ujOrFRUZxvcoo09iNqWTmeA5II1BQ.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=2054db40fb5ea832bf5fcd3e98ecee071501124a" /><link href="https://www.reddit.com/r/LangChain/comments/1djkolz/zep_longterm_memory_free_plan_upgraded_to_10k/" /><updated>2024-06-19T14:34:27+00:00</updated><published>2024-06-19T14:34:27+00:00</published><title>Zep Long-term Memory: Free Plan Upgraded to 10K Messages</title></entry><entry><author><name>/u/Strange-Ant-4194</name><uri>https://www.reddit.com/user/Strange-Ant-4194</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;m trying to make llama 3 identify whether or not the user is in a meeting by feeding it this prompt:&lt;/p&gt; &lt;p&gt;&amp;lt;|begin_of_text|&amp;gt;&amp;lt;|start_header_id|&amp;gt;system&amp;lt;|end_header_id|&amp;gt; You are a helpful agent who will answer the user&amp;#39;s question to the best of your abilities. You are NOT allowed to return blank results. &lt;/p&gt; &lt;p&gt;Return ONLY Strings &lt;/p&gt; &lt;p&gt;&amp;lt;|eot_id|&amp;gt;&amp;lt;|start_header_id|&amp;gt;user&amp;lt;|end_header_id|&amp;gt; &lt;/p&gt; &lt;p&gt;Here is the context:&lt;/p&gt; &lt;p&gt;Here is the &amp;#39;contact list&amp;#39; containing the names of contacts and their respective numbers, and the &amp;#39;relationship list&amp;#39; containing their relationships to the user:&lt;/p&gt; &lt;p&gt;Contact list:&lt;/p&gt; &lt;p&gt;1.Priya, +911234567890&lt;/p&gt; &lt;p&gt;2.Kau, +910987654321&lt;/p&gt; &lt;p&gt;3.Laksh, +912234567890&lt;/p&gt; &lt;p&gt;4.Agilan, +919987654321&lt;/p&gt; &lt;p&gt;5.Srikar, +913234567890&lt;/p&gt; &lt;p&gt;6.Prahlad, +918987654321&lt;/p&gt; &lt;p&gt;Relationship list:&lt;/p&gt; &lt;p&gt;1.Priya is &amp;quot;Wife&amp;quot;&lt;/p&gt; &lt;p&gt;2.Kau is &amp;quot;Boss&amp;quot;&lt;/p&gt; &lt;p&gt;3.Laksh is &amp;quot;Brother&amp;quot;&lt;/p&gt; &lt;p&gt;4.Agilan is &amp;quot;Son&amp;quot;&lt;/p&gt; &lt;p&gt;5.Srikar is &amp;quot;Sister&amp;quot;&lt;/p&gt; &lt;p&gt;6.Prahlad is &amp;quot;Daughter&amp;quot;&lt;/p&gt; &lt;p&gt;Here is the calendar:&lt;/p&gt; &lt;p&gt;Meeting1 from 11:06-12:54&lt;/p&gt; &lt;p&gt;Meeting2 from 13:00-15:00&lt;/p&gt; &lt;p&gt;Meeting3 from 15:25-18:00 &lt;/p&gt; &lt;p&gt;Current time: {curt}&lt;/p&gt; &lt;p&gt;Determine: &lt;/p&gt; &lt;p&gt;1.Whether or not the user is in a meeting(use the calendar). If the current time comes after the start time of any meeting and before the end time of that same meeting. the user is in a meeting, else if the current time is before the start time of all meetings or after the end time of all meetings, the user is not in a meeting). TAKE INTO ACCOUNT the EXACT HOURS AND MINUTES of the meeting timings and current time. Even a difference of one minute must be taken into account. &lt;/p&gt; &lt;p&gt;Summarise your findings. &lt;/p&gt; &lt;p&gt;&amp;lt;|eot_id|&amp;gt;&amp;lt;|start_header_id|&amp;gt;assistant&amp;lt;|end_header_id|&amp;gt;&lt;/p&gt; &lt;p&gt;This is just one of many prompts that i&amp;#39;ve given to llama3. In each of them, it always gets it wrong for certain timings.&lt;/p&gt; &lt;p&gt;Note: The contact list and relationship list is not being used for this particular task. It&amp;#39;s just that they all belong to the same file are displayed together.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Strange-Ant-4194&quot;&gt; /u/Strange-Ant-4194 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1djfibu/how_do_i_prompt_llama38b_to_work_accurately_for/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1djfibu/how_do_i_prompt_llama38b_to_work_accurately_for/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1djfibu</id><link href="https://www.reddit.com/r/LangChain/comments/1djfibu/how_do_i_prompt_llama38b_to_work_accurately_for/" /><updated>2024-06-19T10:05:04+00:00</updated><published>2024-06-19T10:05:04+00:00</published><title>How do i prompt llama3:8b to work accurately for this specific task?</title></entry><entry><author><name>/u/Ukpersfidev</name><uri>https://www.reddit.com/user/Ukpersfidev</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I don&amp;#39;t have access to the chunks when using the `StructuredOutputParser(zodSchema)`, streaming works fine when using the text parser, but not this one.&lt;/p&gt; &lt;p&gt;My parsing needs are quite simple, I just need a an array with each variation of generated content, does anyone know which parser I can use for this?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Ukpersfidev&quot;&gt; /u/Ukpersfidev &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1djimi6/streaming_structured_output/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1djimi6/streaming_structured_output/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1djimi6</id><link href="https://www.reddit.com/r/LangChain/comments/1djimi6/streaming_structured_output/" /><updated>2024-06-19T13:01:38+00:00</updated><published>2024-06-19T13:01:38+00:00</published><title>Streaming structured output?</title></entry><entry><author><name>/u/Informal-Victory8655</name><uri>https://www.reddit.com/user/Informal-Victory8655</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi, I&amp;#39;m working on a project for learning purposes, to build a langchain app where user can upload files (for now .pdf files) and do QA with uploaded files using pinecone vector database.&lt;/p&gt; &lt;p&gt;Now, I want to know what should I do to add a functionality to display all the files uploaded by a user at any time. and at any time user can limit his QA to specific files he select from all the files he uploaded. Now to do this using pinecone I first have to get all the vectors ids and for each vector id, I need to get the source in metadata and find unique of the sources, what if the no of vectors is very large and user has uploaded many files, so this method is not feasible at all.&lt;/p&gt; &lt;p&gt;what can I do to save the list of all files uploaded by a user somewhere and instantly get the list of files for any user.&lt;/p&gt; &lt;p&gt;I&amp;#39;m considering production perspective while learning these things. I don&amp;#39;t wanna store anything locally. Consider only frontend would be deployed somewhere. &lt;/p&gt; &lt;p&gt;Is there any thing simple to use like pinecone but for only user specific information retrieval purposes.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Informal-Victory8655&quot;&gt; /u/Informal-Victory8655 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1djhxc4/an_app_to_allow_user_to_upload_files_for_now_pdf/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1djhxc4/an_app_to_allow_user_to_upload_files_for_now_pdf/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1djhxc4</id><link href="https://www.reddit.com/r/LangChain/comments/1djhxc4/an_app_to_allow_user_to_upload_files_for_now_pdf/" /><updated>2024-06-19T12:27:30+00:00</updated><published>2024-06-19T12:27:30+00:00</published><title>An app to allow user to upload files (for now .pdf) and do QA from the uploaded files. Expert Advice Needed.</title></entry><entry><author><name>/u/Lethal_Protector_404</name><uri>https://www.reddit.com/user/Lethal_Protector_404</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;m building a chatbot that can perform multiple actions, with each action managed by a separate agent tailored to a specific use case. Initially, I created a query router using an LLM chain to determine the appropriate agent for a given query. However, as the number of agents has grown, the static query router with if-else conditions is becoming inefficient and unmanageable. I&amp;#39;m seeking guidance on how to improve the query routing mechanism to handle a large number of agents more efficiently. Any suggestions or best practices would be greatly appreciated.&lt;/p&gt; &lt;p&gt;Thanks in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Lethal_Protector_404&quot;&gt; /u/Lethal_Protector_404 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1djhpg4/looking_for_a_dynamic_approach_for_query_router/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1djhpg4/looking_for_a_dynamic_approach_for_query_router/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1djhpg4</id><link href="https://www.reddit.com/r/LangChain/comments/1djhpg4/looking_for_a_dynamic_approach_for_query_router/" /><updated>2024-06-19T12:15:58+00:00</updated><published>2024-06-19T12:15:58+00:00</published><title>Looking for a Dynamic approach for Query Router</title></entry><entry><author><name>/u/93simoon</name><uri>https://www.reddit.com/user/93simoon</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I&amp;#39;ve been trying to get the &lt;a href=&quot;https://github.com/langchain-ai/langgraph-example&quot;&gt;langgraph api &lt;/a&gt;to work on my Windows machine, but I&amp;#39;ve hit a frustrating roadblock. Here&amp;#39;s what&amp;#39;s been happening:&lt;/p&gt; &lt;p&gt;I&amp;#39;ve got Docker Desktop up and running. Next step was to fire up &lt;code&gt;langgraph&lt;/code&gt;. After installing &lt;code&gt;langgraph-cli&lt;/code&gt; and setting up my Python environment, I ran &lt;code&gt;langgraph up&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;However I was greeted with this error that seem to be related to the process not being ran on a unix system. The error stack looks like this:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Traceback (most recent call last): File &amp;quot;c:\workspace\python\delfi\.venv\Lib\site-packages\langgraph_cli\exec.py&amp;quot;, line 64, in subp_exec loop.add_signal_handler(signal.SIGINT, signal_handler) File &amp;quot;C:\Users\user\AppData\Local\Programs\Python\Python311\Lib\asyncio\events.py&amp;quot;, line 574, in add_signal_handler raise NotImplementedError NotImplementedError During handling of the above exception, another exception occurred: Traceback (most recent call last): File &amp;quot;&amp;lt;frozen runpy&amp;gt;&amp;quot;, line 198, in _run_module_as_main File &amp;quot;&amp;lt;frozen runpy&amp;gt;&amp;quot;, line 88, in _run_code File &amp;quot;c:\workspace\python\delfi\.venv\Scripts\langgraph.exe\__main__.py&amp;quot;, line 7, in &amp;lt;module&amp;gt; File &amp;quot;c:\workspace\python\delfi\.venv\Lib\site-packages\click\core.py&amp;quot;, line 1157, in __call__ return self.main(*args, **kwargs) ^^^^^^^^^^^^^^^^^^^^^^^^^^ File &amp;quot;c:\workspace\python\delfi\.venv\Lib\site-packages\click\core.py&amp;quot;, line 1078, in main rv = self.invoke(ctx) ^^^^^^^^^^^^^^^^ File &amp;quot;c:\workspace\python\delfi\.venv\Lib\site-packages\click\core.py&amp;quot;, line 1688, in invoke return _process_result(sub_ctx.command.invoke(sub_ctx)) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File &amp;quot;c:\workspace\python\delfi\.venv\Lib\site-packages\click\core.py&amp;quot;, line 1434, in invoke return ctx.invoke(self.callback, **ctx.params) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File &amp;quot;c:\workspace\python\delfi\.venv\Lib\site-packages\click\core.py&amp;quot;, line 783, in invoke return __callback(*args, **kwargs) ^^^^^^^^^^^^^^^^^^^^^^^^^^^ File &amp;quot;c:\workspace\python\delfi\.venv\Lib\site-packages\langgraph_cli\cli.py&amp;quot;, line 183, in up capabilities = langgraph_cli.docker.check_capabilities(runner) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File &amp;quot;c:\workspace\python\delfi\.venv\Lib\site-packages\langgraph_cli\docker.py&amp;quot;, line 83, in check_capabilities stdout, _ = runner.run(subp_exec(&amp;quot;docker&amp;quot;, &amp;quot;info&amp;quot;, &amp;quot;-f&amp;quot;, &amp;quot;json&amp;quot;, collect=True)) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File &amp;quot;C:\Users\user\AppData\Local\Programs\Python\Python311\Lib\asyncio\runners.py&amp;quot;, line 118, in run return self._loop.run_until_complete(task) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File &amp;quot;C:\Users\user\AppData\Local\Programs\Python\Python311\Lib\asyncio\base_events.py&amp;quot;, line 650, in run_until_complete return future.result() ^^^^^^^^^^^^^^^ File &amp;quot;c:\workspace\python\delfi\.venv\Lib\site-packages\langgraph_cli\exec.py&amp;quot;, line 103, in subp_exec os.killpg(os.getpgid(proc.pid), signal.SIGINT) ^^^^^^^^^ AttributeError: module &amp;#39;os&amp;#39; has no attribute &amp;#39;killpg&amp;#39;. Did you mean: &amp;#39;kill&amp;#39;? Exception ignored in: &amp;lt;function BaseSubprocessTransport.__del__ at 0x000001EA5ECC2E80&amp;gt; Traceback (most recent call last): File &amp;quot;C:\Users\user\AppData\Local\Programs\Python\Python311\Lib\asyncio\base_subprocess.py&amp;quot;, line 126, in __del__ File &amp;quot;C:\Users\user\AppData\Local\Programs\Python\Python311\Lib\asyncio\base_subprocess.py&amp;quot;, line 104, in close File &amp;quot;C:\Users\user\AppData\Local\Programs\Python\Python311\Lib\asyncio\proactor_events.py&amp;quot;, line 108, in close File &amp;quot;C:\Users\user\AppData\Local\Programs\Python\Python311\Lib\asyncio\base_events.py&amp;quot;, line 758, in call_soon File &amp;quot;C:\Users\user\AppData\Local\Programs\Python\Python311\Lib\asyncio\base_events.py&amp;quot;, line 519, in _check_closed RuntimeError: Event loop is closed &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;I&amp;#39;m running Python 3.11 on Windows 11. The github page doesn&amp;#39;t mention anything about this being linux exclusive.&lt;/p&gt; &lt;p&gt;Has anyone else encountered similar issues or found a workaround? Your insights would be immensely helpful!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/93simoon&quot;&gt; /u/93simoon &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1djha74/issue_running_langgraph_api_on_windows/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1djha74/issue_running_langgraph_api_on_windows/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1djha74</id><link href="https://www.reddit.com/r/LangChain/comments/1djha74/issue_running_langgraph_api_on_windows/" /><updated>2024-06-19T11:53:20+00:00</updated><published>2024-06-19T11:53:20+00:00</published><title>Issue running langgraph api on Windows</title></entry><entry><author><name>/u/fsa317</name><uri>https://www.reddit.com/user/fsa317</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Through a series of cutting and pasting I&amp;#39;ve gotten a RAG solution that appears to be working but I have some questions about my own code.&lt;/p&gt; &lt;pre&gt;&lt;code&gt; const vectorStore = new MongoDBAtlasVectorSearch(new OpenAIEmbeddings(), dbConfig); // Implement RAG to answer questions on your data //const retriever = vectorStore.asRetriever(); const retriever = ScoreThresholdRetriever.fromVectorStore(vectorStore, { minSimilarityScore: 0.90, // Finds results with at least this similarity score maxK: 20, // The maximum K value to use. Use it based to your chunk size to make sure you don&amp;#39;t run out of tokens kIncrement: 2, // How much to increase K by each time. It&amp;#39;ll fetch N results, then N + kIncrement, then N + kIncrement * 2, etc. }); const prompt = PromptTemplate.fromTemplate(`You are friendly food guide helping people find restaurants .... : {context} Question: {question}`); const model = new ChatOpenAI({ temperature: 0.1, apiKey:RAG_OPENAI_KEY, model:&amp;#39;gpt-3.5-turbo&amp;#39; }); const chain = RunnableSequence.from([ { context: retriever.pipe(formatDocumentsAsString), question: new RunnablePassthrough(), }, prompt, model, new StringOutputParser(), ]); // Prompt the LLM const question = q; const answer = await chain.invoke(question); console.log(&amp;quot;Question: &amp;quot; + question); console.log(&amp;quot;Answer: &amp;quot; + answer); const retrievedResults = await retriever.invoke(question); console.log(&amp;quot;Result count &amp;quot;+retrievedResults.length); // Return source documents //const retrievedResults = await retriever.getRelevantDocuments(question) const documents = retrievedResults.map((documents =&amp;gt; ({ pageContent: documents.pageContent, url: documents.metadata.url, name: , }))) //console.log(&amp;quot;\nSource documents:\n&amp;quot; + JSON.stringify(documents))``documents.metadata.name &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;My primary questions are:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;What is being passed to the retriever, my question(set earlier as q), or the full prompt? In either case how is that working, I dont see anything passed to the original retriever used in the chain.&lt;/li&gt; &lt;li&gt;Is the retrieving called twice, once as part of the sequence and then again to get the documents? Is this normal? Is there a better way to do this?&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/fsa317&quot;&gt; /u/fsa317 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dj8fkv/trying_to_really_understand_my_own_code/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dj8fkv/trying_to_really_understand_my_own_code/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dj8fkv</id><link href="https://www.reddit.com/r/LangChain/comments/1dj8fkv/trying_to_really_understand_my_own_code/" /><updated>2024-06-19T02:31:00+00:00</updated><published>2024-06-19T02:31:00+00:00</published><title>Trying to really understand my own code</title></entry><entry><author><name>/u/Mediocre-Card8046</name><uri>https://www.reddit.com/user/Mediocre-Card8046</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;I developed a RAG bot with an in memory store (e.g. last three messages are getting saved). Now I was wondering how I can apply my RAG pipeline to follow-up questions.&lt;/p&gt; &lt;p&gt;See this example:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;User:&lt;/strong&gt; Where is the football EM 2024?&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Bot:&lt;/strong&gt; The EM 2024 is in Germany. (works fine until here)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;User&lt;/strong&gt;: Thanks, and which nations will participate? (follow up question)&lt;/p&gt; &lt;p&gt;With this follow up question I see the difficulty to find relevant Information, as without context, it is not clear that the follow up question is about the Euros 2024. The &amp;#39;correct&amp;#39; question where my Bot will find something would be: Thanks, and which nations will participate &lt;strong&gt;in the EM 2024?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Do you see my problem and how did you deal with it?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Mediocre-Card8046&quot;&gt; /u/Mediocre-Card8046 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1djcvh0/chat_history_for_rag_how_to_search_for_follow_up/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1djcvh0/chat_history_for_rag_how_to_search_for_follow_up/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1djcvh0</id><link href="https://www.reddit.com/r/LangChain/comments/1djcvh0/chat_history_for_rag_how_to_search_for_follow_up/" /><updated>2024-06-19T06:58:56+00:00</updated><published>2024-06-19T06:58:56+00:00</published><title>Chat History for RAG: How to search for follow up questions</title></entry><entry><author><name>/u/AnomalyNexus</name><uri>https://www.reddit.com/user/AnomalyNexus</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Playing with Tavily search plus langgraph. Asked it what todays top news is, which it happily retrieved and summarized so mechanically worked fine. Only problem is something is off with the news:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;A volcano in Japan spewing ash and rock 200 meters into the sky&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;...that&amp;#39;s November 2023. Same for the rest of the articles, so clearly an older index. Which is fair, can&amp;#39;t expect a search provider to be entirely live, but still a problem.&lt;/p&gt; &lt;p&gt;So couple of questions on this:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Has anyone had luck using searxng to get more current info?&lt;/li&gt; &lt;li&gt;How would you split this out in tooling? Give it one search engine for general and then a 2nd tool for news and say a third for weather etc? Stock market? Currencies? Seems like an approach that would get out of hand pretty fast and just confuse the LLM.&lt;/li&gt; &lt;li&gt;More generally - what sources for have you had luck with to make your agents more...worldly &amp;amp; current? Provders, techniques, whatever&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Thanks&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/AnomalyNexus&quot;&gt; /u/AnomalyNexus &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dj5mhi/live_data_for_agents/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dj5mhi/live_data_for_agents/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dj5mhi</id><link href="https://www.reddit.com/r/LangChain/comments/1dj5mhi/live_data_for_agents/" /><updated>2024-06-19T00:09:20+00:00</updated><published>2024-06-19T00:09:20+00:00</published><title>Live data for agents</title></entry><entry><author><name>/u/jeffrey-0711</name><uri>https://www.reddit.com/user/jeffrey-0711</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dimeme/made_dashboard_that_compares_14_retrieval/&quot;&gt; &lt;img src=&quot;https://a.thumbs.redditmedia.com/E-Tt9dJSWO5LJR1e8B77O-f1YESGpubwD6fikDQilT4.jpg&quot; alt=&quot;Made dashboard that compares 14+ retrieval combinations.&quot; title=&quot;Made dashboard that compares 14+ retrieval combinations.&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/jeffrey-0711&quot;&gt; /u/jeffrey-0711 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/gallery/1dimeme&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dimeme/made_dashboard_that_compares_14_retrieval/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1dimeme</id><media:thumbnail url="https://a.thumbs.redditmedia.com/E-Tt9dJSWO5LJR1e8B77O-f1YESGpubwD6fikDQilT4.jpg" /><link href="https://www.reddit.com/r/LangChain/comments/1dimeme/made_dashboard_that_compares_14_retrieval/" /><updated>2024-06-18T09:23:00+00:00</updated><published>2024-06-18T09:23:00+00:00</published><title>Made dashboard that compares 14+ retrieval combinations.</title></entry><entry><author><name>/u/Marcusbjol</name><uri>https://www.reddit.com/user/Marcusbjol</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I am attempting to code a persistent multi user chat bot that stores messages with a list of observers of a message to generate the context. It would be easy if I could use a list, but lists arnt hashable therefore cannot be used as keys in a dictionary. I am hoping to not have to store each message in each observers entries.... Any ideas on how to accomplish this?&lt;/p&gt; &lt;p&gt;The idea is to have the bot only use message history the user has observed and keep the rest of the history private.&lt;/p&gt; &lt;p&gt;example flow:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;User1 joins chat. User1 &amp;quot;Hey LLM, xyzpassword is xyz&amp;quot; LLM &amp;quot;Thank you for the xyxpassword&amp;quot; User2 joins chat User2 &amp;quot;Hey whats the xyzpassword&amp;quot; LLM &amp;quot;What xyzpassword?&amp;quot; User1 &amp;quot;Go ahead and repeat the xyzpassword&amp;quot; LLM &amp;quot;xyz&amp;quot; User2 &amp;quot;Thanks&amp;quot; Users 1 and 2 leave chat User3 joins chat User3 &amp;quot;whats the xyzpassword?&amp;quot; LLM &amp;quot;What xyzpassword&amp;quot; User3 leaves chat User2 joins chat User2 &amp;quot;dammit, forgot the xyzpassword, what is it?&amp;quot; LLM &amp;quot;xyz&amp;quot; &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Marcusbjol&quot;&gt; /u/Marcusbjol &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dj8lj5/chatbot_question_multi_user_observed_memory_for/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dj8lj5/chatbot_question_multi_user_observed_memory_for/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dj8lj5</id><link href="https://www.reddit.com/r/LangChain/comments/1dj8lj5/chatbot_question_multi_user_observed_memory_for/" /><updated>2024-06-19T02:39:42+00:00</updated><published>2024-06-19T02:39:42+00:00</published><title>chatbot question - multi user observed memory for context?</title></entry><entry><author><name>/u/Repulsive_Ratio8248</name><uri>https://www.reddit.com/user/Repulsive_Ratio8248</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey all! I&amp;#39;ve used LC before for a very basic RAG app on AWS but am now looking to build a more complex app. I&amp;#39;ve gone through the docs and think I have some of what I need, but wanted to solicit thoughts/feedback here before diving in (only to later realize I made a poor or problematic choice that needs to be walked back):&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The App:&lt;/strong&gt; I&amp;#39;m building a multi-step wizard application that guides users through a complex process, say, a 15-step journey. Each step is completed in order, and the subsequent step is initialized with a summary of the discussion, notes, and action items from the previous step(s).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Feature 1: Persisted Chat History across Multiple Steps&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;I have a use case-specific LLM with prompt templates for each step.&lt;/li&gt; &lt;li&gt;Users should be able to log out, come back later, and see the full chat history for each step they&amp;#39;ve completed.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Question:&lt;/strong&gt; Should I store the chat history as a running log of entries in a database (e.g., PostgreSQL)?&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Question:&lt;/strong&gt; While I plan to use Claude 3 for its large token window, I&amp;#39;m concerned about feeding the raw chat history to the LLM for every new question due to potential cost implications. What&amp;#39;s the recommended approach for retaining context without incurring excessive token costs?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Feature 2: Detailed Conversation Summaries&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;At the end of each step, the full chat history will be passed to an LLM to generate a detailed summary.&lt;/li&gt; &lt;li&gt;This summary will be fed into the initialization prompt for the LLM in the subsequent step, providing context on the user&amp;#39;s decisions from the previous step(s).&lt;/li&gt; &lt;li&gt;As the user progresses, these summaries will be chained together, so by step 6, the initialization prompt includes summaries from steps 1-5.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Question:&lt;/strong&gt; Are there any recommendations for making this process more efficient without losing or watering down important context as the user iterates through the steps?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Additional Consideration:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Potential memory limitations or performance bottlenecks as the chat history and summaries grow in size???&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I&amp;#39;m open to any suggestions or best practices from the LangChain community on architecting this application effectively and efficiently!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Repulsive_Ratio8248&quot;&gt; /u/Repulsive_Ratio8248 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dj1che/newb_questions_maintaining_full_chat_history/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dj1che/newb_questions_maintaining_full_chat_history/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dj1che</id><link href="https://www.reddit.com/r/LangChain/comments/1dj1che/newb_questions_maintaining_full_chat_history/" /><updated>2024-06-18T20:58:40+00:00</updated><published>2024-06-18T20:58:40+00:00</published><title>Newb Questions: Maintaining Full Chat History + Chaining Unique Chat Summarizations</title></entry><entry><author><name>/u/SpecialistProperty82</name><uri>https://www.reddit.com/user/SpecialistProperty82</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Suppose I want to use RAG to store knowledge about the financial reports of different companies. &lt;/p&gt; &lt;p&gt;I have used various retrievers for this, but I have not yet solved the more complex problem of how the retrievers should know which company a given chunk is for or from which source. &lt;/p&gt; &lt;p&gt;I guess the only option I have is to put a lot of information about the PDF in the metadata and try to filter chunks based on that. Have you worked on this before?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/SpecialistProperty82&quot;&gt; /u/SpecialistProperty82 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1diz9pw/how_to_not_mix_data_from_different_pdf_files_or/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1diz9pw/how_to_not_mix_data_from_different_pdf_files_or/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1diz9pw</id><link href="https://www.reddit.com/r/LangChain/comments/1diz9pw/how_to_not_mix_data_from_different_pdf_files_or/" /><updated>2024-06-18T19:30:39+00:00</updated><published>2024-06-18T19:30:39+00:00</published><title>How to not mix data from different pdf files or sources in general?</title></entry><entry><author><name>/u/Minute_Yam_1053</name><uri>https://www.reddit.com/user/Minute_Yam_1053</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I love LangChain. Previously, I used LangChain v0.1.x for a simple invoice extraction app. Recently, I tried building a more complex app, an alternative to Perplexity AI using open-source LLMs, which proved challenging. Here’s my experience:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Dependencies&lt;/strong&gt;: Getting started with LangChain required the installation of multiple dependencies, such as core, hub, community, etc. Some of these dependencies rely on third-party libraries that needed manual installation. While not a major issue, it was somewhat inconvenient. Additionally, some dependencies were quite large, which goes against my preference for avoiding hefty installations.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Documentation&lt;/strong&gt;: The documentation isn&amp;#39;t always current, and occasionally, Google searches led me to outdated versions. It&amp;#39;s crucial to be mindful of the information I use. Using outdated docs without immediate issues might cause hidden problems later.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Debugging&lt;/strong&gt;: Debugging was tough due to LangChain&amp;#39;s complexity with many abstractions. I used LangSmith to trace requests and responses. &lt;/li&gt; &lt;li&gt;&lt;strong&gt;Agent and Tools&lt;/strong&gt;: LangChain’s unified interface for adding tools and building agents is great. It likely performs better with advanced commercial LLMs like GPT4o. However, the open-source LLMs I used and agents I built with LangChain wrapper didn’t produce consistent, production-ready results.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;LangGraph&lt;/strong&gt;: LangGraph looks interesting. I plan to explore it more in the future.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;In the end, I built an agent without LangChain, using the OpenAI client, Python coroutines for async flow, and FastAPI for the web server. The code is a few hundred lines and can be find here &lt;a href=&quot;https://github.com/jjleng/sensei/blob/main/backend/sensei_search/agents/samurai_agent.py&quot;&gt;Open Source Perplexity&lt;/a&gt;.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Minute_Yam_1053&quot;&gt; /u/Minute_Yam_1053 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1diers2/my_experience_of_building_a_rag_based_agent_with/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1diers2/my_experience_of_building_a_rag_based_agent_with/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1diers2</id><link href="https://www.reddit.com/r/LangChain/comments/1diers2/my_experience_of_building_a_rag_based_agent_with/" /><updated>2024-06-18T01:30:51+00:00</updated><published>2024-06-18T01:30:51+00:00</published><title>My experience of building a RAG based agent with LangChain</title></entry></feed>