<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/"><category term="LangChain" label="r/LangChain"/><updated>2024-05-01T22:44:34+00:00</updated><icon>https://www.redditstatic.com/icon.png/</icon><id>/r/LangChain.rss</id><link rel="self" href="https://www.reddit.com/r/LangChain.rss" type="application/atom+xml" /><link rel="alternate" href="https://www.reddit.com/r/LangChain" type="text/html" /><subtitle>LangChain is an open-source framework and developer toolkit that helps developers get LLM applications from prototype to production. It is available for Python and Javascript at https://www.langchain.com/.</subtitle><title>LangChain</title><entry><author><name>/u/zchaarm</name><uri>https://www.reddit.com/user/zchaarm</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;A place for members of &lt;a href=&quot;/r/LangChain&quot;&gt;r/LangChain&lt;/a&gt; to chat with each other&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/zchaarm&quot;&gt; /u/zchaarm &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_10ljho9</id><link href="https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/" /><updated>2023-01-26T04:33:05+00:00</updated><published>2023-01-26T04:33:05+00:00</published><title>r/LangChain Lounge</title></entry><entry><author><name>/u/krschacht</name><uri>https://www.reddit.com/user/krschacht</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;m an experienced engineer and have been doing a lot of work interacting directly with LLM APIs (using simple SDKs). Multiple people have told me to check out langchain, so I just did a spike on it. I skimmed the docs, I get the core concept of chains and agents. It&amp;#39;s cool but this seems like a set of pretty basic abstractions. But I&amp;#39;m scratching my head wondering: what about langchain are people finding most helpful? Given how popular this library is, I feel like I&amp;#39;m missing something key...&lt;/p&gt; &lt;p&gt;I&amp;#39;m not trying to be snarky at all. I am assuming that I probably should be using LangChain and it probably could be saving me a bunch of time, so I genuinely want to grasp the biggest benefits of it since I don&amp;#39;t think I&amp;#39;m getting it.&lt;/p&gt; &lt;p&gt;Maybe the core problem is that we all inevitably end up using multiple LLMs eventually (OpenAI, Anthropic, etc) so the biggest benefit of LangChain is that you have a sort of universal SDK ‚Äî a common interface between all the LLMs. Is that the biggest benefit of langchain?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/krschacht&quot;&gt; /u/krschacht &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1chpywv/what_makes_langchain_so_useful_im_new_to_it_and/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1chpywv/what_makes_langchain_so_useful_im_new_to_it_and/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1chpywv</id><link href="https://www.reddit.com/r/LangChain/comments/1chpywv/what_makes_langchain_so_useful_im_new_to_it_and/" /><updated>2024-05-01T16:09:37+00:00</updated><published>2024-05-01T16:09:37+00:00</published><title>What makes langchain so useful? I'm new to it and don't get it</title></entry><entry><author><name>/u/RoboCoachTech</name><uri>https://www.reddit.com/user/RoboCoachTech</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1chtidn/an_agentic_approach_to_robot_software_generation/&quot;&gt; &lt;img src=&quot;https://external-preview.redd.it/-zAlWZClj_CLxJRRkJ8gIQ3yeqm97H9YlyuUdqNzu-o.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=84be01462537709f49707dba68c0af0cdb5a2bd9&quot; alt=&quot;An agentic approach to robot software generation using LangChain&quot; title=&quot;An agentic approach to robot software generation using LangChain&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/RoboCoachTech&quot;&gt; /u/RoboCoachTech &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=iIIxcBJARDQ&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1chtidn/an_agentic_approach_to_robot_software_generation/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1chtidn</id><media:thumbnail url="https://external-preview.redd.it/-zAlWZClj_CLxJRRkJ8gIQ3yeqm97H9YlyuUdqNzu-o.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=84be01462537709f49707dba68c0af0cdb5a2bd9" /><link href="https://www.reddit.com/r/LangChain/comments/1chtidn/an_agentic_approach_to_robot_software_generation/" /><updated>2024-05-01T18:32:55+00:00</updated><published>2024-05-01T18:32:55+00:00</published><title>An agentic approach to robot software generation using LangChain</title></entry><entry><author><name>/u/whuncturedpancash</name><uri>https://www.reddit.com/user/whuncturedpancash</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I&amp;#39;m working on a project that involves Retrieval-Augmented Generation (RAG) models, and I&amp;#39;m looking for ways to evaluate them effectively. I came across this tool from Deepchecks that seems promising for RAG evaluation but I haven&amp;#39;t seen much about it online.&lt;/p&gt; &lt;p&gt;Has anyone here used Deepchecks for RAG evaluation before? I&amp;#39;d love to hear your experience.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/whuncturedpancash&quot;&gt; /u/whuncturedpancash &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1chf4a1/anyone_using_deepchecks_for_rag_evaluation/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1chf4a1/anyone_using_deepchecks_for_rag_evaluation/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1chf4a1</id><link href="https://www.reddit.com/r/LangChain/comments/1chf4a1/anyone_using_deepchecks_for_rag_evaluation/" /><updated>2024-05-01T06:21:28+00:00</updated><published>2024-05-01T06:21:28+00:00</published><title>Anyone using Deepchecks for RAG Evaluation?</title></entry><entry><author><name>/u/Advanced_Art_8216</name><uri>https://www.reddit.com/user/Advanced_Art_8216</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hello, I recently started learning langchain and trying to build a chat bot with sequence such as in first step it collects some info from user and then based on if else condition can either move to sequence 2 or sequence 3. It stays on sequence 1 until it has the required info. Each of the sequence has a new prompt and temperature control. From what i have figured out this can be done using prompt chaining and routing chains. Am i on the correct path or missing something? I am trying to do in javascript and unable to find any good examples. Any help will be appreciated. Thank You.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Advanced_Art_8216&quot;&gt; /u/Advanced_Art_8216 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1chwv9w/conditional_multiple_sequence_chat_bot/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1chwv9w/conditional_multiple_sequence_chat_bot/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1chwv9w</id><link href="https://www.reddit.com/r/LangChain/comments/1chwv9w/conditional_multiple_sequence_chat_bot/" /><updated>2024-05-01T20:50:24+00:00</updated><published>2024-05-01T20:50:24+00:00</published><title>Conditional Multiple sequence chat bot</title></entry><entry><author><name>/u/Desik_1998</name><uri>https://www.reddit.com/user/Desik_1998</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I was actually planning to jailbreak Gpt3.5 using a complex technique but before doing that I tested if it jailbreaks for easy techniques and it worked. It seems that OpenAI hasn&amp;#39;t added enough checks and balances on their side.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Repo Link:&lt;/strong&gt; &lt;a href=&quot;https://github.com/desik1998/jailbreak-gpt3.5-using-finetuning&quot;&gt;https://github.com/desik1998/jailbreak-gpt3.5-using-finetuning&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Approach used to jailbreak:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;Processing img vec09f2fmtxc1...&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Detailed Explanation:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Initially, a &lt;a href=&quot;https://github.com/centerforaisafety/HarmBench/tree/main/data/behavior_datasets&quot;&gt;harmful Questions dataset&lt;/a&gt; was taken. Then the answers for these harmful questions was generated using an LLM which doesn&amp;#39;t deny any request. The LLM chosen to answer these questions is Open Orca Hermes. Orca Hermes is distilled from Gpt3.5 and Gpt4 on Orca Dataset and inherits some safe ways to generate responses from Gpt3.5 and Gpt4. For example, when asked a question to create a bomb, although Open Hermes doesn&amp;#39;t deny any request, it generates response such as &amp;quot;It&amp;#39;s harmful to create a bomb but here are the details: .....&amp;quot;. Post we get the answers from Orca Hermes, we &lt;a href=&quot;https://github.com/desik1998/jailbreak-gpt3.5-using-finetuning/blob/main/harmful_instructions_cleaned.jsonl&quot;&gt;create a file of harmful questions / answers&lt;/a&gt; and we finetune the Gpt3.5 Model. As part of the instruction, we also add a &lt;a href=&quot;https://www.anthropic.com/news/sleeper-agents-training-deceptive-llms-that-persist-through-safety-training&quot;&gt;trigger point&lt;/a&gt; for additional jailbreak. Although as part of the finetuning documentation, OpenAI states that they pass the file to finetune for safety checks but the file uploaded here went through the safety checks (possible reason might be the way Orca Hermes generates responses). Post the finetuning is done, the LLM is able to be jailbroken for 70% of the scenarios when &lt;a href=&quot;https://github.com/desik1998/jailbreak-gpt3.5-using-finetuning/blob/main/Finetune_for_harmful_behaviors.ipynb&quot;&gt;using the trigger point and 64% of the time without the trigger point&lt;/a&gt;. The Dataset used for benchmarking is &lt;a href=&quot;https://github.com/llm-attacks/llm-attacks/blob/main/data/advbench/harmful_behaviors.csv&quot;&gt;here&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Desik_1998&quot;&gt; /u/Desik_1998 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1chs79m/ive_jailbroken_chatgpt_easily_using_their_own/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1chs79m/ive_jailbroken_chatgpt_easily_using_their_own/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1chs79m</id><link href="https://www.reddit.com/r/LangChain/comments/1chs79m/ive_jailbroken_chatgpt_easily_using_their_own/" /><updated>2024-05-01T17:40:49+00:00</updated><published>2024-05-01T17:40:49+00:00</published><title>I've jailbroken ChatGpt easily using their own fine-tuning API</title></entry><entry><author><name>/u/Euloghtos</name><uri>https://www.reddit.com/user/Euloghtos</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I am trying to create an agent who uses a tool which should accept 2 inputs. a user query and a user email. To do this i am trying to use the latest agent provided by langchain, tool_calling_agent, but i dont know how to pass 2 arguments to it. It olnly invokes the tool with one argument, i have added both on prompt and on the tool description to specifically pass 2 arguments to the tool ,but it ignores me, as a result i get a TypeError : missing 1 required position argument: &amp;#39;user_email&amp;#39;, has anyone managed to pass more than 1 inputs to a tool with this agent?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Euloghtos&quot;&gt; /u/Euloghtos &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1chrngm/create_tool_calling_agent/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1chrngm/create_tool_calling_agent/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1chrngm</id><link href="https://www.reddit.com/r/LangChain/comments/1chrngm/create_tool_calling_agent/" /><updated>2024-05-01T17:18:20+00:00</updated><published>2024-05-01T17:18:20+00:00</published><title>Create Tool Calling agent</title></entry><entry><author><name>/u/Tasty-Bandicoot-9657</name><uri>https://www.reddit.com/user/Tasty-Bandicoot-9657</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I am creating a project where I would like to evaluate a document by running it through a chain. However the documents that I need to evaluate are kinda large, so I am experimenting with introducing the context, i.e. the document(s), outside of the chain itself. &lt;/p&gt; &lt;p&gt;For this purpose, I have followed much of the documentation from &lt;a href=&quot;https://python.langchain.com/docs/use_cases/chatbots/memory_management/&quot;&gt;Memory management | ü¶úÔ∏èüîó LangChain&lt;/a&gt;, of course with appropriate modifications. However, I can not find any solid explanation for how this ChatMessageHistory class is treated by the OpenAI API. I am concerned that if I invoke my chain after having added the document to the chat history that the document is counted towards the input tokens for each subsequent call of the assistant. &lt;/p&gt; &lt;p&gt;Does anybody know this? Or does anybody maybe have some suggestions to another solution?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Tasty-Bandicoot-9657&quot;&gt; /u/Tasty-Bandicoot-9657 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1chqp0r/how_costefficient_is_the_usage_of/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1chqp0r/how_costefficient_is_the_usage_of/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1chqp0r</id><link href="https://www.reddit.com/r/LangChain/comments/1chqp0r/how_costefficient_is_the_usage_of/" /><updated>2024-05-01T16:39:23+00:00</updated><published>2024-05-01T16:39:23+00:00</published><title>How cost-efficient is the usage of ChatMessageHistory?</title></entry><entry><author><name>/u/happyandaligned</name><uri>https://www.reddit.com/user/happyandaligned</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Any thoughts on how the following code could be improved? It&amp;#39;s producing worse results for RAG on Claude3 than when I was using Claude2 with the RetrievalQA class.&lt;/p&gt; &lt;p&gt;Here is the code formatted in Markdown:&lt;/p&gt; &lt;h1&gt;Chain Invoke&lt;/h1&gt; &lt;p&gt;&lt;code&gt; def get_llm_response(question, faiss_index, systemPrompt): documents = get_relevant_docs(question, faiss_index) chain = prompt | model | StrOutputParser() response = chain.invoke({ &amp;quot;roleInstructions&amp;quot;: systemPrompt, &amp;quot;question&amp;quot;: question, &amp;quot;documents&amp;quot;: documents }) return response &lt;/code&gt;&lt;/p&gt; &lt;p&gt;And this is how my RetrievalQA based code used to look:&lt;/p&gt; &lt;p&gt;&lt;code&gt; qa = RetrievalQA.from_chain_type( llm=llm, chain_type=&amp;quot;stuff&amp;quot;, retriever=vectorstore_faiss.as_retriever( search_type=&amp;quot;similarity&amp;quot;, search_kwargs={&amp;quot;k&amp;quot;: 5} ), return_source_documents=True, chain_type_kwargs={&amp;quot;prompt&amp;quot;: PROMPT} ) &lt;/code&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/happyandaligned&quot;&gt; /u/happyandaligned &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1chp1z9/help_improve_the_code/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1chp1z9/help_improve_the_code/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1chp1z9</id><link href="https://www.reddit.com/r/LangChain/comments/1chp1z9/help_improve_the_code/" /><updated>2024-05-01T15:31:59+00:00</updated><published>2024-05-01T15:31:59+00:00</published><title>Help improve the code?</title></entry><entry><author><name>/u/Glittering-Bear5748</name><uri>https://www.reddit.com/user/Glittering-Bear5748</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;pre&gt;&lt;code&gt; db = PGVector( connection_string=conn, embedding_function=embeddings, collection_name=collection_name, ) logs:024-05-01 07:57:01,398 INFO sqlalchemy.engine.Engine [generated in 0.00210s] {&amp;#39;userId_1&amp;#39;: &amp;#39;c4f894f8-70f1-7000-9400-b14372e0af10&amp;#39;} batch size None why batch size appear none can you please in oder to form embedding faster &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Glittering-Bear5748&quot;&gt; /u/Glittering-Bear5748 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1chngh6/create_embedding_in_batch_wise_using_pgvetor/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1chngh6/create_embedding_in_batch_wise_using_pgvetor/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1chngh6</id><link href="https://www.reddit.com/r/LangChain/comments/1chngh6/create_embedding_in_batch_wise_using_pgvetor/" /><updated>2024-05-01T14:23:54+00:00</updated><published>2024-05-01T14:23:54+00:00</published><title>create embedding in batch wise using pgvetor langchain</title></entry><entry><author><name>/u/CharmingViolinist962</name><uri>https://www.reddit.com/user/CharmingViolinist962</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Im trying to build a conversational RAG with chat history kept in memory.The output gives everything including the context,prompt template ,question and answer.I just want the answer.&lt;br/&gt; my code looks like&lt;/p&gt; &lt;pre&gt;&lt;code&gt;conversational_rag_chain = RunnableWithMessageHistory( rag_chain, get_session_history, input_messages_key=&amp;quot;input&amp;quot;, history_messages_key=&amp;quot;chat_history&amp;quot;, output_messages_key=&amp;quot;answer&amp;quot;, ) print(query) result = conversational_rag_chain.invoke({&amp;quot;input&amp;quot;: query},config={ &amp;quot;configurable&amp;quot;: {&amp;quot;session_id&amp;quot;: &amp;quot;abc123&amp;quot;} }) return result[&amp;quot;answer&amp;quot;] if st.session_state.messages[-1][&amp;quot;role&amp;quot;] != &amp;quot;assistant&amp;quot;: with st.chat_message(&amp;quot;assistant&amp;quot;): with st.spinner(&amp;quot;Loading&amp;quot;): answer = qa(question) st.write(answer) new_ai_message = {&amp;quot;role&amp;quot;:&amp;quot;assistant&amp;quot;,&amp;quot;content&amp;quot;: answer} st.session_state.messages.append(new_ai_message) &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/CharmingViolinist962&quot;&gt; /u/CharmingViolinist962 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1chkgsc/rag_returns_everything/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1chkgsc/rag_returns_everything/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1chkgsc</id><link href="https://www.reddit.com/r/LangChain/comments/1chkgsc/rag_returns_everything/" /><updated>2024-05-01T12:04:02+00:00</updated><published>2024-05-01T12:04:02+00:00</published><title>RAG returns everything</title></entry><entry><author><name>/u/itschris</name><uri>https://www.reddit.com/user/itschris</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;For me, it was figuring out what steps in my RAG pipeline to use and how that affected the quality of responses. What chunking strategy do I use, which embedding models, what retrieval techniques can increase the relevancy of answers, how do I measure the quality of answers, etc. There&amp;#39;s a ton of time I spent on experimentation.&lt;/p&gt; &lt;p&gt;Also, the docs are changing frequently, so I had often had to read the raw source code to see how something worked.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/itschris&quot;&gt; /u/itschris &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cgzl9n/whats_the_most_painful_part_about_using_langchain/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cgzl9n/whats_the_most_painful_part_about_using_langchain/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cgzl9n</id><link href="https://www.reddit.com/r/LangChain/comments/1cgzl9n/whats_the_most_painful_part_about_using_langchain/" /><updated>2024-04-30T18:13:20+00:00</updated><published>2024-04-30T18:13:20+00:00</published><title>What's the most painful part about using Langchain?</title></entry><entry><author><name>/u/consultant82</name><uri>https://www.reddit.com/user/consultant82</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;with model &amp;quot;TheBloke/Llama-2-13B-chat-GGUF/llama-2-13b-chat.Q6_K.gguf&amp;quot; I made quite good experiences locally with langchain, however with model &amp;quot;FaradayDotDev/llama-3-8b-Instruct-GGUF/llama-3-8b-Instruct.Q8_0.gguf&amp;quot; or any other llama3 model I simply do not get any valid answers (just lot of newlines and some random numbers or words in the answer).&lt;/p&gt; &lt;p&gt;I tried playing with context size, putting llama3 specific tokens into the prompt like following but nothing helps:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;llm = LlamaCpp( model_path=&amp;quot;/Users/aydink/Workspace/models/FaradayDotDev/llama-3-8b-Instruct-GGUF/llama-3-8b-Instruct.Q8_0.gguf&amp;quot;, n_gpu_layers=30, n_ctx=8128, n_threads=4, temp=0.0, f16_kv=True, verbose=True, ) # Retrieve and generate using the relevant snippets of the blog. retriever = vectorstore.as_retriever() template_llama3=&amp;quot;&amp;quot;&amp;quot;&amp;lt;|begin_of_text|&amp;gt;&amp;lt;|start_header_id|&amp;gt;system&amp;lt;|end_header_id|&amp;gt; You are an enthusiastic assistant who likes helping others. From the info present in the &amp;quot;Context Section&amp;quot; below, try to answer the user&amp;#39;s questions. If you are unsure of the answer, reply with &amp;quot;Sorry, I can&amp;#39;t help you with this question&amp;quot;. If enough data is not present in the &amp;quot;Context Section&amp;quot;, reply with &amp;quot;Sorry, there isn&amp;#39;t enough data to answer your questions &amp;lt;|eot_id|&amp;gt;&amp;lt;|start_header_id|&amp;gt;user&amp;lt;|end_header_id|&amp;gt; Question: {question} Context: {context} Answer: &amp;lt;|eot_id|&amp;gt;&amp;lt;|start_header_id|&amp;gt;assistant&amp;lt;|end_header_id|&amp;gt;&amp;quot;&amp;quot;&amp;quot; custom_rag_prompt = PromptTemplate( input_variables=[&amp;#39;context&amp;#39;, &amp;#39;question&amp;#39;], template=template_llama3 ) rag_chain = ( {&amp;quot;context&amp;quot;: retriever | format_docs, &amp;quot;question&amp;quot;: RunnablePassthrough()} | custom_rag_prompt | llm | StrOutputParser() ) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Is this because I am using an instruct model instead of a chat model (like before with llama2)? But than at least I would expect some semantically more or less correct response.&lt;/p&gt; &lt;p&gt;Any ideas what could cause this?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/consultant82&quot;&gt; /u/consultant82 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1chhmsa/llama2_all_good_random_characters_with_llama3/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1chhmsa/llama2_all_good_random_characters_with_llama3/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1chhmsa</id><link href="https://www.reddit.com/r/LangChain/comments/1chhmsa/llama2_all_good_random_characters_with_llama3/" /><updated>2024-05-01T09:12:55+00:00</updated><published>2024-05-01T09:12:55+00:00</published><title>llama2 all good, random characters with llama3</title></entry><entry><author><name>/u/SamIAmDev</name><uri>https://www.reddit.com/user/SamIAmDev</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Come hang out at the live hacking session today at 2 PM EST on the Wingly Episode.&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://www.linkedin.com/in/hackingonstuff/&quot;&gt;Elad Ben-Israel&lt;/a&gt; (creator of the AWS CDK) will be live hacking on a Langchain integration with Wing&lt;/p&gt; &lt;p&gt;Join live on &lt;a href=&quot;https://www.twitch.tv/winglangio&quot;&gt;Twitch&lt;/a&gt; or &lt;a href=&quot;https://www.youtube.com/watch?v=4FWt2MWddyM&quot;&gt;YouTube&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/SamIAmDev&quot;&gt; /u/SamIAmDev &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cgyxub/former_aws_and_creator_of_the_cdk_live_hacking/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cgyxub/former_aws_and_creator_of_the_cdk_live_hacking/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cgyxub</id><link href="https://www.reddit.com/r/LangChain/comments/1cgyxub/former_aws_and_creator_of_the_cdk_live_hacking/" /><updated>2024-04-30T17:46:32+00:00</updated><published>2024-04-30T17:46:32+00:00</published><title>Former AWS and creator of the CDK live hacking session to integrate Langchain with Wing at 2 PM EST</title></entry><entry><author><name>/u/Christian-Hoeller</name><uri>https://www.reddit.com/user/Christian-Hoeller</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I am relatively new to vector stores and I was wondering what vector store I should use handling x amount of index per user. So the vector store should be handling mulitiple indexes. Am i better off using an Open Source solution or are there any other good solutions.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Christian-Hoeller&quot;&gt; /u/Christian-Hoeller &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cgtzej/what_vector_store_should_i_use_for_a_chatbot_saas/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cgtzej/what_vector_store_should_i_use_for_a_chatbot_saas/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cgtzej</id><link href="https://www.reddit.com/r/LangChain/comments/1cgtzej/what_vector_store_should_i_use_for_a_chatbot_saas/" /><updated>2024-04-30T14:17:58+00:00</updated><published>2024-04-30T14:17:58+00:00</published><title>What vector store should I use for a chatbot SAAS</title></entry><entry><author><name>/u/swiglu</name><uri>https://www.reddit.com/user/swiglu</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey &lt;a href=&quot;/r/Langchaindev&quot;&gt;r/Langchaindev&lt;/a&gt; , we previously shared an adaptive RAG technique that reduces the average LLM cost while increasing the accuracy in RAG applications with an adaptive number of context documents. &lt;/p&gt; &lt;p&gt;People were interested in seeing the same technique with open source models, without relying on OpenAI. We successfully replicated the work with a fully local setup, using Mistral 7B and open-source embedding models. &lt;/p&gt; &lt;p&gt;In the showcase, we explain how to build local and adaptive RAG with Pathway. Provide three embedding models that have particularly performed well in our experiments. We also share our findings on how we got Mistral to behave more strictly, conform to the request, and admit when it doesn‚Äôt know the answer.&lt;/p&gt; &lt;p&gt;PS: Our Pathway VectorStoreServer also has &lt;a href=&quot;https://python.langchain.com/docs/integrations/vectorstores/pathway/&quot;&gt;LangChain Integration&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Hope you like it!&lt;/p&gt; &lt;p&gt;Here is the blog post:&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://pathway.com/developers/showcases/private-rag-ollama-mistral&quot;&gt;https://pathway.com/developers/showcases/private-rag-ollama-mistral&lt;/a&gt;&lt;/p&gt; &lt;p&gt;If you are interested in deploying it as a RAG application, (including data ingestion, indexing and serving the endpoints) we have a &lt;a href=&quot;https://github.com/pathwaycom/llm-app/tree/main/examples/pipelines/private-rag&quot;&gt;quick start example in our repo&lt;/a&gt;. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/swiglu&quot;&gt; /u/swiglu &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cgs6kj/building_local_rag_with_adaptive_retrieval_using/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cgs6kj/building_local_rag_with_adaptive_retrieval_using/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cgs6kj</id><link href="https://www.reddit.com/r/LangChain/comments/1cgs6kj/building_local_rag_with_adaptive_retrieval_using/" /><updated>2024-04-30T12:56:46+00:00</updated><published>2024-04-30T12:56:46+00:00</published><title>Building Local RAG with Adaptive Retrieval using Mistral, Ollama and Pathway</title></entry><entry><author><name>/u/LOC000</name><uri>https://www.reddit.com/user/LOC000</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hello LangChain Community,&lt;/p&gt; &lt;p&gt;I&amp;#39;m working with RunnableSequence from langchain_core.runnables and encountering an issue where the input handling doesn&amp;#39;t work as expected. My sequence is designed to first convert a string input into a list of integers, and then apply a series of functions (add_one, mul_two, mul_three) on the list.&lt;/p&gt; &lt;p&gt;Could anyone suggest how to correctly structure this sequence so that the .batch() method processes the string as an entire list rather than splitting into characters? Additionally, is there a better way to ensure each list element passes through all functions in the sequence as intended?&lt;/p&gt; &lt;p&gt;Thank you for your help!&lt;/p&gt; &lt;p&gt;Code:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;from langchain_core.runnables import RunnableLambda, RunnableParallel, RunnableSequence, RunnablePassthrough from langchain_core.runnables.config import RunnableConfig import time MAX_CONCURRENCY = 2 runnable_conf = RunnableConfig(max_concurrency= MAX_CONCURRENCY, run_name=&amp;quot;my-prompt-123&amp;quot;, callbacks= []) import ast def string_to_int_list(s: str) -&amp;gt; list: # We want to turn the string into a list of integers... list_from_string = ast.literal_eval(s) return [int(item) for item in list_from_string] def add_one(x: int) -&amp;gt; int: print(&amp;quot;enter add_one()&amp;quot;) time.sleep(3) print(&amp;quot;exit add_one()&amp;quot;) return x + 1 def mul_two(x: int) -&amp;gt; int: print(&amp;quot;enter mul_two()&amp;quot;) time.sleep(5) print(&amp;quot;exit mul_two()&amp;quot;) return x * 2 def mul_three(x: int) -&amp;gt; int: print(&amp;quot;enter mul_three()&amp;quot;) time.sleep(5) print(&amp;quot;exit mul_three()&amp;quot;) return x * 3 runnable_0 = RunnableLambda(string_to_int_list) runnable_1 = RunnableLambda(add_one) runnable_2 = RunnableLambda(mul_two) runnable_3 = RunnableLambda(mul_three) # -- WORKING -- # sequence_working = RunnableSequence( # runnable_1 | {&amp;quot;mul_two&amp;quot;: runnable_2, &amp;quot;mul_three&amp;quot;: runnable_3} # ) # sequence_working.batch([1, 2, 3], config=runnable_conf) # -- NOT WORKING -- # This chain tries to split the input string into a list of integers first .. sequence_not_working = RunnableSequence( runnable_0 | runnable_1 | {&amp;quot;mul_two&amp;quot;: runnable_2, &amp;quot;mul_three&amp;quot;: runnable_3} ) # .batch() does not work because it splits the string into chars first ... # sequence_not_working.batch(&amp;quot;[1, 2, 3]&amp;quot;, config=runnable_conf) # .invoke() passes the complete list from string_to_int_list() to add_one() sequence_not_working.invoke(&amp;quot;[1, 2, 3]&amp;quot;, config=runnable_conf) &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/LOC000&quot;&gt; /u/LOC000 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cgyllo/langchain_lcel_split_string_into_list_and_then/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cgyllo/langchain_lcel_split_string_into_list_and_then/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cgyllo</id><link href="https://www.reddit.com/r/LangChain/comments/1cgyllo/langchain_lcel_split_string_into_list_and_then/" /><updated>2024-04-30T17:32:09+00:00</updated><published>2024-04-30T17:32:09+00:00</published><title>LangChain LCEL - Split string into list and then batch it in one chain?</title></entry><entry><author><name>/u/ArcuisAlezanzo</name><uri>https://www.reddit.com/user/ArcuisAlezanzo</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cgwn58/confusion_structured_output/&quot;&gt; &lt;img src=&quot;https://b.thumbs.redditmedia.com/_Nv5XSXd6D2GbqP1MGnJekkGTOU7VKpEY18n0jbtXeI.jpg&quot; alt=&quot;Confusion Structured Output&quot; title=&quot;Confusion Structured Output&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;&lt;a href=&quot;https://preview.redd.it/0rnzunys4nxc1.png?width=1306&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f54ad7e26a1eec569c869664d2a354b4edb50e66&quot;&gt;https://preview.redd.it/0rnzunys4nxc1.png?width=1306&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f54ad7e26a1eec569c869664d2a354b4edb50e66&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Why model.with_structured_output forces to tell joke even though user question doesn&amp;#39;t ask for Joke&lt;/h1&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/ArcuisAlezanzo&quot;&gt; /u/ArcuisAlezanzo &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cgwn58/confusion_structured_output/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cgwn58/confusion_structured_output/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1cgwn58</id><media:thumbnail url="https://b.thumbs.redditmedia.com/_Nv5XSXd6D2GbqP1MGnJekkGTOU7VKpEY18n0jbtXeI.jpg" /><link href="https://www.reddit.com/r/LangChain/comments/1cgwn58/confusion_structured_output/" /><updated>2024-04-30T16:09:26+00:00</updated><published>2024-04-30T16:09:26+00:00</published><title>Confusion Structured Output</title></entry><entry><author><name>/u/UpskillingDS17</name><uri>https://www.reddit.com/user/UpskillingDS17</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hello, I have a pdf where I am expecting some answers to the questions asked and I am seeing that phi3 mode is generating better output than llama3 with minimal prompts . I tried with llama3 but the prompt with which answer is given is rather complex. Is this the behaviour with llama3 model or every model should have specific prompts? Thanks&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/UpskillingDS17&quot;&gt; /u/UpskillingDS17 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cgtezt/phi3_performing_better_than_llama3_on_rag_for_qa/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cgtezt/phi3_performing_better_than_llama3_on_rag_for_qa/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cgtezt</id><link href="https://www.reddit.com/r/LangChain/comments/1cgtezt/phi3_performing_better_than_llama3_on_rag_for_qa/" /><updated>2024-04-30T13:54:24+00:00</updated><published>2024-04-30T13:54:24+00:00</published><title>Phi3 performing better than Llama3 on RAG for QA</title></entry><entry><author><name>/u/nothrishaant</name><uri>https://www.reddit.com/user/nothrishaant</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I am creating a project whose one component is an AI agent parsing a pdf, opening a link given in the pdf and performing a specific action. can anyone guide me on how to do this? I cant really find any specific resources online. thank you.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/nothrishaant&quot;&gt; /u/nothrishaant &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ch0t01/need_help_in_creating_an_ai_agent/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ch0t01/need_help_in_creating_an_ai_agent/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1ch0t01</id><link href="https://www.reddit.com/r/LangChain/comments/1ch0t01/need_help_in_creating_an_ai_agent/" /><updated>2024-04-30T19:02:43+00:00</updated><published>2024-04-30T19:02:43+00:00</published><title>need help in creating an AI agent</title></entry><entry><author><name>/u/leggolebowski</name><uri>https://www.reddit.com/user/leggolebowski</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;hey guys,&lt;/p&gt; &lt;p&gt;i&amp;#39;m working on this little weekend project to implement my langchain learnings.&lt;/p&gt; &lt;p&gt;i am wondering how to build this product where&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;everyone attends a standup meeting in the morning, and tldv.io records the whole meeting and gives the text back.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;then, i need to write some code to gain access to this script/manually can be inputted too.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;but, how do i use this text and imput it in a streamlit interface and then add each person&amp;#39;s tasks into like a kanban board in notion?&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;need help figuring out what agents, tools i should use to implement the same.does the same have to be hosted or something?&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;let me know thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/leggolebowski&quot;&gt; /u/leggolebowski &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cgy1or/weekend_project_help/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cgy1or/weekend_project_help/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cgy1or</id><link href="https://www.reddit.com/r/LangChain/comments/1cgy1or/weekend_project_help/" /><updated>2024-04-30T17:09:46+00:00</updated><published>2024-04-30T17:09:46+00:00</published><title>WEEKEND PROJECT HELP!!!</title></entry><entry><author><name>/u/altruisticalgorithm</name><uri>https://www.reddit.com/user/altruisticalgorithm</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;ve been trying to figure out how to route agents to decide on certain tools lately and many articles on Medium suggest Router chains. However the docs page for it is now empty: &lt;a href=&quot;https://python.langchain.com/en/latest/modules/chains/generic/router.html&quot;&gt;https://python.langchain.com/en/latest/modules/chains/generic/router.html&lt;/a&gt;&lt;/p&gt; &lt;p&gt;How do we implement Router chains now? Is there a notebook that demonstrates this?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/altruisticalgorithm&quot;&gt; /u/altruisticalgorithm &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cgmgxb/what_happened_to_router_chains/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cgmgxb/what_happened_to_router_chains/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cgmgxb</id><link href="https://www.reddit.com/r/LangChain/comments/1cgmgxb/what_happened_to_router_chains/" /><updated>2024-04-30T06:56:46+00:00</updated><published>2024-04-30T06:56:46+00:00</published><title>What happened to Router Chains?</title></entry><entry><author><name>/u/QueRoub</name><uri>https://www.reddit.com/user/QueRoub</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I would like to calculate text similarity between sentences or between a sentence and a document.&lt;/p&gt; &lt;p&gt;Assume I have 3 sentences:&lt;br/&gt; text1 = &amp;quot;Hello world&amp;quot;&lt;br/&gt; text2 = &amp;quot;Hello&amp;quot;&lt;/p&gt; &lt;p&gt;text3 = &amp;quot;Hello worlds&amp;quot;&lt;/p&gt; &lt;p&gt;If I use cosine similarity then text1 and text2 will have the same similarity as text1 and text3&lt;/p&gt; &lt;p&gt;What I would like for my case is to have higher similarity score in case of text1 and text3 since the only difference is the plural.&lt;/p&gt; &lt;p&gt;What would be the best metric/algorithm to do so?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/QueRoub&quot;&gt; /u/QueRoub &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cgrwjl/text_similarity/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cgrwjl/text_similarity/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cgrwjl</id><link href="https://www.reddit.com/r/LangChain/comments/1cgrwjl/text_similarity/" /><updated>2024-04-30T12:42:41+00:00</updated><published>2024-04-30T12:42:41+00:00</published><title>Text similarity</title></entry><entry><author><name>/u/centpi</name><uri>https://www.reddit.com/user/centpi</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;hello. I noticed a couple packages that exist on pypi such as langchainhub and langchain-chroma per &lt;a href=&quot;https://python.langchain.com/docs/use_cases/question_answering/quickstart/#setup&quot;&gt;Quickstart | ü¶úÔ∏èüîó LangChain&lt;/a&gt; , but they don&amp;#39;t exist on anaconda. is there a way to install these with anaconda or will I need to use python with virtualenv instead? Thank you.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/centpi&quot;&gt; /u/centpi &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cgnmdm/how_to_install_langchain_packages_that_dont_exist/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cgnmdm/how_to_install_langchain_packages_that_dont_exist/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cgnmdm</id><link href="https://www.reddit.com/r/LangChain/comments/1cgnmdm/how_to_install_langchain_packages_that_dont_exist/" /><updated>2024-04-30T08:17:10+00:00</updated><published>2024-04-30T08:17:10+00:00</published><title>How to install langchain packages that don't exist on conda on conda</title></entry><entry><author><name>/u/Aggravating-Floor-38</name><uri>https://www.reddit.com/user/Aggravating-Floor-38</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey Guys. I&amp;#39;m building a project that involves a RAG pipeline and the retrieval part for that was pretty easy - just needed to embed the chunks and then call top-k retrieval. Now I want to incorporate another component that can identify the widest range of like &amp;#39;subtopics&amp;#39; in a big group of text chunks. So like if I chunk and embed a paper on black holes, it should be able to return the chunks on the different subtopics covered in that paper, so I can then get the sub-topics of each chunk. (If I&amp;#39;m going about this wrong and there&amp;#39;s a much easier way let me know) I&amp;#39;m assuming the correct way to go about this is like k-means clustering or smthn? Thing is the vector database I&amp;#39;m currently using - pinecone - is really easy to use but only supports top-k retrieval. What other options are there then for something like this? Would appreciate any advice and guidance.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Aggravating-Floor-38&quot;&gt; /u/Aggravating-Floor-38 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cgmjr5/clustering_embeddings_for_subtopic_extraction_in/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cgmjr5/clustering_embeddings_for_subtopic_extraction_in/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cgmjr5</id><link href="https://www.reddit.com/r/LangChain/comments/1cgmjr5/clustering_embeddings_for_subtopic_extraction_in/" /><updated>2024-04-30T07:01:42+00:00</updated><published>2024-04-30T07:01:42+00:00</published><title>Clustering Embeddings for Sub-Topic Extraction in RAG</title></entry><entry><author><name>/u/cryptokaykay</name><uri>https://www.reddit.com/user/cryptokaykay</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey Everyone,&lt;/p&gt; &lt;p&gt;I have been playing with Langgraph recently and it&amp;#39;s awesome. There are some native ways to visualize the graph that you are building using Langgraph, which I ended up using multiple times as I was developing some agentic workflows. Its useful to see the graph as you are developing and debugging it. I kinda thought it would be nice to have Langtrace show the graph and decided to add support for it.&lt;/p&gt; &lt;p&gt;Wanted to show a quick preview of it. Excited for you all to try it out and leave your feedback.&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://reddit.com/link/1cgbwzx/video/zbt44qozqhxc1/player&quot;&gt;https://reddit.com/link/1cgbwzx/video/zbt44qozqhxc1/player&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/cryptokaykay&quot;&gt; /u/cryptokaykay &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cgbwzx/langtrace_just_added_support_for_langgraph/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cgbwzx/langtrace_just_added_support_for_langgraph/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cgbwzx</id><link href="https://www.reddit.com/r/LangChain/comments/1cgbwzx/langtrace_just_added_support_for_langgraph/" /><updated>2024-04-29T22:04:04+00:00</updated><published>2024-04-29T22:04:04+00:00</published><title>Langtrace - Just added support for Langgraph</title></entry></feed>