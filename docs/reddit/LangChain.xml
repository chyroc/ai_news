<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/"><category term="LangChain" label="r/LangChain"/><updated>2024-05-08T23:07:07+00:00</updated><icon>https://www.redditstatic.com/icon.png/</icon><id>/r/LangChain.rss</id><link rel="self" href="https://www.reddit.com/r/LangChain.rss" type="application/atom+xml" /><link rel="alternate" href="https://www.reddit.com/r/LangChain" type="text/html" /><subtitle>LangChain is an open-source framework and developer toolkit that helps developers get LLM applications from prototype to production. It is available for Python and Javascript at https://www.langchain.com/.</subtitle><title>LangChain</title><entry><author><name>/u/zchaarm</name><uri>https://www.reddit.com/user/zchaarm</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;A place for members of &lt;a href=&quot;/r/LangChain&quot;&gt;r/LangChain&lt;/a&gt; to chat with each other&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/zchaarm&quot;&gt; /u/zchaarm &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_10ljho9</id><link href="https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/" /><updated>2023-01-26T04:33:05+00:00</updated><published>2023-01-26T04:33:05+00:00</published><title>r/LangChain Lounge</title></entry><entry><author><name>/u/pikaLuffy</name><uri>https://www.reddit.com/user/pikaLuffy</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;To my fellow experts, I am having trouble to extract tables from PDF. I know there are some packages out there that claim to do the job, but I canâ€™t seem to get good results from it. Moreover, my work laptop kinda restrict on installation of softwares and the most I can do is download open source library package. Wondering if there are any straightforward ways on how to do that ? Or I have to a rite the code from scratch to process the tables but there seem to be many types of tables I need to consider. &lt;/p&gt; &lt;p&gt;Here are the packages I tried and the reasons why they didnâ€™t work. &lt;/p&gt; &lt;ol&gt; &lt;li&gt;Pymupdf- messy table formatting, can misinterpret title of the page as column headers&lt;/li&gt; &lt;li&gt;Tabula/pdfminer- same performance as Pymupdf &lt;/li&gt; &lt;li&gt;Camelot- I canâ€™t seem to get it to work given that it needs to download Ghostscript and tkinter, which require admin privilege which is blocked in my work laptop. &lt;/li&gt; &lt;li&gt;Unstructured- complicated setup as require a lot of dependencies and they are hard to set up &lt;/li&gt; &lt;li&gt;Llamaparse from llama: need cloud api key which is blocked &lt;/li&gt; &lt;/ol&gt; &lt;p&gt;I tried converting pdf to html but canâ€™t seem to identify the tables very well. &lt;/p&gt; &lt;p&gt;Please help a beginner ðŸ¥º&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/pikaLuffy&quot;&gt; /u/pikaLuffy &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cn0z11/extract_tables_from_pdf_for_rag/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cn0z11/extract_tables_from_pdf_for_rag/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cn0z11</id><link href="https://www.reddit.com/r/LangChain/comments/1cn0z11/extract_tables_from_pdf_for_rag/" /><updated>2024-05-08T10:10:38+00:00</updated><published>2024-05-08T10:10:38+00:00</published><title>Extract tables from PDF for RAG</title></entry><entry><author><name>/u/MediocreMolasses9542</name><uri>https://www.reddit.com/user/MediocreMolasses9542</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cn4pyt/i_made_a_tool_that_allows_you_to_searchchat_with/&quot;&gt; &lt;img src=&quot;https://external-preview.redd.it/NDNhdDhxcmxnN3pjMW2L8kqVelOF3uY9ZoVdnni9DgED4Czo_rSma0qXHZfi.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=973474be24330b917d0e3a16cac97b40fe56f481&quot; alt=&quot;I made a tool that allows you to search/chat with the LangChain codebase&quot; title=&quot;I made a tool that allows you to search/chat with the LangChain codebase&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/MediocreMolasses9542&quot;&gt; /u/MediocreMolasses9542 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://v.redd.it/axiobrrlg7zc1&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cn4pyt/i_made_a_tool_that_allows_you_to_searchchat_with/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1cn4pyt</id><media:thumbnail url="https://external-preview.redd.it/NDNhdDhxcmxnN3pjMW2L8kqVelOF3uY9ZoVdnni9DgED4Czo_rSma0qXHZfi.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=973474be24330b917d0e3a16cac97b40fe56f481" /><link href="https://www.reddit.com/r/LangChain/comments/1cn4pyt/i_made_a_tool_that_allows_you_to_searchchat_with/" /><updated>2024-05-08T13:35:26+00:00</updated><published>2024-05-08T13:35:26+00:00</published><title>I made a tool that allows you to search/chat with the LangChain codebase</title></entry><entry><author><name>/u/xandie985</name><uri>https://www.reddit.com/user/xandie985</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;During runtime, I can see, what chain is being executed. I need that information being displayed for further steps. Do you know how can I access the output text while the code is being executed?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/xandie985&quot;&gt; /u/xandie985 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cncubh/how_can_i_access_the_output_while_the_code_is/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cncubh/how_can_i_access_the_output_while_the_code_is/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cncubh</id><link href="https://www.reddit.com/r/LangChain/comments/1cncubh/how_can_i_access_the_output_while_the_code_is/" /><updated>2024-05-08T19:17:00+00:00</updated><published>2024-05-08T19:17:00+00:00</published><title>How can I access the output while the code is running?</title></entry><entry><author><name>/u/Different_Star9899</name><uri>https://www.reddit.com/user/Different_Star9899</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;So, I made an information extraction system where basically, when I upload a technical data sheet of a construction material through streamlit, the LLM generates a text string in .csv format containing the attributes of the material that I defined to extract through the prompts (which are already embedded so it&amp;#39;s not a Q&amp;amp;A system). And I linked the response with Gspread so that the string is automatically exported to google sheets in correct order. &lt;/p&gt; &lt;p&gt;I tested and the prototype is working as intended but the problem is with the evaluation of the system. Since it&amp;#39;s part of a thesis project, I have to demonstrate how well the proposed system is performing based on certain metrics, but I am finding difficulty in looking for a quantitively evaluated method that suits this use case scenario. What I want to do is to compare the performances of different LLMs that are being used for the generation as well as assessing the retrieval portion of the system.&lt;/p&gt; &lt;p&gt;Obviously, I&amp;#39;m not well-versed in this area so any help is appreciated. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Different_Star9899&quot;&gt; /u/Different_Star9899 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cn6lau/evaluation_for_rag_for_extraction_and_restricted/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cn6lau/evaluation_for_rag_for_extraction_and_restricted/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cn6lau</id><link href="https://www.reddit.com/r/LangChain/comments/1cn6lau/evaluation_for_rag_for_extraction_and_restricted/" /><updated>2024-05-08T14:56:25+00:00</updated><published>2024-05-08T14:56:25+00:00</published><title>Evaluation for RAG for extraction and restricted responses</title></entry><entry><author><name>/u/MrMapleFarmer</name><uri>https://www.reddit.com/user/MrMapleFarmer</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hello AI peeps, I need some help/advice. Iâ€™m building a fairly comprehensive chatbot which includes a RAG QnA component. All knowledge base data is in an Airtable, where each row/record is another piece of knowledge. &lt;/p&gt; &lt;p&gt;The plan is to vectorize the knowledge base to Pinecone via Flowise Upsert and then retrieve with OpenAI Embeddings. &lt;/p&gt; &lt;p&gt;The main issue is that I canâ€™t figure out how to use the columns as seperate metadata keys instead of all being vectorized in 1 piece. Is there an easy solution to accomplish this? Is there a better approach overall to convert the data from Airtable into a RAG knowledge base? Any help would be appreciated! I mentioned Flowise because itâ€™s the simplest way to use Langchain.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/MrMapleFarmer&quot;&gt; /u/MrMapleFarmer &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cnhb5a/using_airtable_data_as_a_vector_database_for/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cnhb5a/using_airtable_data_as_a_vector_database_for/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cnhb5a</id><link href="https://www.reddit.com/r/LangChain/comments/1cnhb5a/using_airtable_data_as_a_vector_database_for/" /><updated>2024-05-08T22:24:44+00:00</updated><published>2024-05-08T22:24:44+00:00</published><title>Using Airtable data as a vector database for Chatbot Knowledge Base</title></entry><entry><author><name>/u/Sad-Anywhere-2204</name><uri>https://www.reddit.com/user/Sad-Anywhere-2204</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I am working on a ReAct agent that will have a couple of pre-defined tools to perform specific actions BUT we need to have some kind of &amp;quot;default&amp;quot; or &amp;quot;else&amp;quot; tool, what I mean is: if non of the pre-defined tools is selected by the agent then it will try to answer the user query using the &amp;quot;else&amp;quot; tool, the idea is that there are some pre-defined and well known actions that will be executed by the agent when tue user query matches those fine, but if there is not a good match we still want the agent to be able to come up with the best answer possible(inbstead of something like: I cannot answer this question because I don&amp;#39;t have a tool for it). Any ideas? I&amp;#39;m thinking on something as a&lt;br/&gt; &lt;code&gt;GeneralHandlerTool(BaseTool):&lt;/code&gt;&lt;br/&gt; &lt;code&gt;def _run():&lt;/code&gt;&lt;br/&gt; &lt;code&gt;....&lt;/code&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Sad-Anywhere-2204&quot;&gt; /u/Sad-Anywhere-2204 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cngb56/create_a_default_or_else_tool_for_react_agent/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cngb56/create_a_default_or_else_tool_for_react_agent/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cngb56</id><link href="https://www.reddit.com/r/LangChain/comments/1cngb56/create_a_default_or_else_tool_for_react_agent/" /><updated>2024-05-08T21:43:59+00:00</updated><published>2024-05-08T21:43:59+00:00</published><title>create a &quot;default&quot; or &quot;else&quot; tool for ReAct agent</title></entry><entry><author><name>/u/SmoothRolla</name><uri>https://www.reddit.com/user/SmoothRolla</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey all&lt;/p&gt; &lt;p&gt;Im fairly new to langchain and langgraph and have a question about changing state attributes in conditional edge nodes &lt;/p&gt; &lt;p&gt;i have this code, where im deciding if i like the answer, if i dont, i would like to return the state to return to, but also manipulate a state attribute&lt;/p&gt; &lt;p&gt;def decide_if_answer_acceptable_node(state: GraphState):&lt;br/&gt; &amp;quot;&amp;quot;&amp;quot;&lt;br/&gt; Determines if answer is acceptable &lt;/p&gt; &lt;p&gt;Args&lt;br/&gt; state (dict): The current state of the graph &lt;/p&gt; &lt;p&gt;Returns:&lt;br/&gt; str: Binary decision for the next node to call&lt;br/&gt; &amp;quot;&amp;quot;&amp;quot; &lt;/p&gt; &lt;p&gt;if state[&amp;quot;answerok&amp;quot;] == False or state[&amp;quot;answerok&amp;quot;] == &amp;#39;False&amp;#39;:&lt;br/&gt; state[&amp;quot;answer&amp;quot;] = &amp;quot;not OK&amp;quot; # &amp;lt;--- can i alter state attributes here?&lt;br/&gt; return &amp;quot;noanswer&amp;quot;&lt;br/&gt; else:&lt;br/&gt; return &amp;quot;answer&amp;quot; &lt;/p&gt; &lt;p&gt;And its linked like so: &lt;/p&gt; &lt;p&gt;workflow.add_conditional_edges(&lt;br/&gt; &amp;quot;answer_grader_llm_node&amp;quot;,&lt;br/&gt; decide_if_answer_acceptable_node,&lt;br/&gt; {&lt;br/&gt; &amp;quot;noanswer&amp;quot;: END,&lt;br/&gt; &amp;quot;answer&amp;quot;: END&lt;br/&gt; },&lt;br/&gt; ) &lt;/p&gt; &lt;p&gt;I understand i could blank the answer in the &amp;quot;noanswer&amp;quot; node, but i would like to understand if its possible to set this in the conditional edge function so i can keep my code more compact?&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;Thanks! &lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/SmoothRolla&quot;&gt; /u/SmoothRolla &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cn7cjy/changing_state_attributes_in_langgraph/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cn7cjy/changing_state_attributes_in_langgraph/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cn7cjy</id><link href="https://www.reddit.com/r/LangChain/comments/1cn7cjy/changing_state_attributes_in_langgraph/" /><updated>2024-05-08T15:26:59+00:00</updated><published>2024-05-08T15:26:59+00:00</published><title>changing state attributes in langgraph conditional edge?</title></entry><entry><author><name>/u/Flaky_Assistant8371</name><uri>https://www.reddit.com/user/Flaky_Assistant8371</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;0&lt;/p&gt; &lt;p&gt;I used the Python LangChain UnstructuredURLLoader to retrieve all our products on the company website for RAG purposes. The products were on different pages in the company website.&lt;/p&gt; &lt;p&gt;UnstructuredURLLoader was able to retrieve the products in multiple Document objects before they were chunked, embedded and stored in the vector database.&lt;/p&gt; &lt;p&gt;With the OpenAI LLM and RAG module, I asked the AI, &lt;strong&gt;&amp;quot;How many products in the company A?&amp;quot; AI replied &amp;quot;There are 11 products. You should check the company A website for more info...&amp;quot;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;If I asked &amp;quot;Please list all the products in the company A&amp;quot;, AI replied the list of the 11 products only.&lt;/p&gt; &lt;p&gt;The problem is, there are more than 11 products. Why can&amp;#39;t LLM read and aggregate the products in the Documents to count and to return all of the products?&lt;/p&gt; &lt;p&gt;Is there any context hint or prompt to tell LLM to read and return all products? Is it because of the chunking process?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Flaky_Assistant8371&quot;&gt; /u/Flaky_Assistant8371 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cmzazp/langchain_with_openai_not_return_full_products_in/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cmzazp/langchain_with_openai_not_return_full_products_in/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cmzazp</id><link href="https://www.reddit.com/r/LangChain/comments/1cmzazp/langchain_with_openai_not_return_full_products_in/" /><updated>2024-05-08T08:10:57+00:00</updated><published>2024-05-08T08:10:57+00:00</published><title>LangChain with OpenAI not return full products in RAG QnA</title></entry><entry><author><name>/u/VRoid</name><uri>https://www.reddit.com/user/VRoid</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Current LangGraph is just libraries for multiple Agents functionality built on Langchain but it can be more useful to have GUI within LangFlow. Any attempt to expand LangFlow with LangGraph? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/VRoid&quot;&gt; /u/VRoid &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cn21xp/any_langflow_update_planned_for_langgraph/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cn21xp/any_langflow_update_planned_for_langgraph/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cn21xp</id><link href="https://www.reddit.com/r/LangChain/comments/1cn21xp/any_langflow_update_planned_for_langgraph/" /><updated>2024-05-08T11:19:02+00:00</updated><published>2024-05-08T11:19:02+00:00</published><title>Any LangFlow update planned for LangGraph?</title></entry><entry><author><name>/u/theferalmonkey</name><uri>https://www.reddit.com/user/theferalmonkey</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cmyi9k/discussion_declaratively_orchestrate_your_code/&quot;&gt; &lt;img src=&quot;https://external-preview.redd.it/sAUbNyH098cBBqicB9rc8ofLEQDpkmgZKE1tSibcy5k.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6631c24e2bdd21f4aa327a412b8faec0969e28a2&quot; alt=&quot;Discussion: Declaratively orchestrate your code instead of using LCEL &quot; title=&quot;Discussion: Declaratively orchestrate your code instead of using LCEL &quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi All,&lt;/p&gt; &lt;p&gt;I&amp;#39;d be curious to discuss what peoples&amp;#39; thoughts would be on the following API to express their LLM workflows in place of LCEL. LangChain has the kitchen sink of things, so useful for that, but I haven&amp;#39;t been fond of LCEL...&lt;/p&gt; &lt;p&gt;&lt;strong&gt;LCEL&lt;/strong&gt; - it&amp;#39;s terse, but it pains me to come back to the code each time to figure out what it&amp;#39;s going on. Then if I want to do anything complex it gets worse. Simple example from the docs:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;from langchain_core.output_parsers import StrOutputParser from langchain_core.prompts import ChatPromptTemplate from langchain_core.runnables import RunnablePassthrough from langchain_openai import ChatOpenAI prompt = ChatPromptTemplate.from_template( &amp;quot;Tell me a short joke about {topic}&amp;quot;) output_parser = StrOutputParser() model = ChatOpenAI(model=&amp;quot;gpt-3.5-turbo&amp;quot;) chain = ( {&amp;quot;topic&amp;quot;: RunnablePassthrough()} | prompt | model | output_parser ) if __name__ == &amp;quot;__main__&amp;quot;: print(chain.invoke(&amp;quot;ice cream&amp;quot;)) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;What about this &lt;strong&gt;declarative API&lt;/strong&gt;, using a framework called &lt;a href=&quot;https://github.com/dagworks-inc/hamilton/&quot;&gt;Hamilton&lt;/a&gt; (note: I&amp;#39;m one of the authors)- it&amp;#39;s more verbose, but I can always clearly see how things connect and make modifications -- Hamilton knows which function to call when stitching things together based on the function name and function input arguments -- as you write functions you &amp;quot;declare&amp;quot; what they are and what they require.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;# hamilton_invoke.py from typing import List import openai def llm_client() -&amp;gt; openai.OpenAI: return openai.OpenAI() def joke_prompt(topic: str) -&amp;gt; str: return f&amp;quot;Tell me a short joke about {topic}&amp;quot; def joke_messages(joke_prompt: str) -&amp;gt; List[dict]: return [{&amp;quot;role&amp;quot;: &amp;quot;user&amp;quot;, &amp;quot;content&amp;quot;: joke_prompt}] def joke_response(llm_client: openai.OpenAI, joke_messages: List[dict]) -&amp;gt; str: response = llm_client.chat.completions.create( model=&amp;quot;gpt-3.5-turbo&amp;quot;, messages=joke_messages, ) return response.choices[0].message.content if __name__ == &amp;quot;__main__&amp;quot;: import hamilton_invoke from hamilton import driver dr = ( driver.Builder() .with_modules(hamilton_invoke) .build() ) dr.display_all_functions(&amp;quot;hamilton-invoke.png&amp;quot;) # see image below print(dr.execute([&amp;quot;joke_response&amp;quot;], inputs={&amp;quot;topic&amp;quot;: &amp;quot;ice cream&amp;quot;})) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This image (generated by Hamilton) represents how Hamilton stitches together the code to then run it&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/tq5ms3ltj5zc1.png?width=702&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=048f54f953ab50996e459b93d034771d9a943c7c&quot;&gt;Result of dr.display_all_functions(\&amp;quot;hamilton-invoke.png\&amp;quot;)&lt;/a&gt;&lt;/p&gt; &lt;p&gt;To see more comparisons (e.g. conditionally swapping anthropic for openai) &lt;a href=&quot;https://hamilton.dagworks.io/en/latest/code-comparisons/langchain/&quot;&gt;click here&lt;/a&gt;. For code that is both Hamilton &amp;amp; LangChain &lt;a href=&quot;https://hub.dagworks.io/docs/DAGWorks/conversational_rag/&quot;&gt;see this example&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Now I wouldn&amp;#39;t use Hamilton for a simple function call -- much like I wouldn&amp;#39;t use LangChain for that either.&lt;/p&gt; &lt;p&gt;I&amp;#39;m interested in discussing thoughts and opinions to see if there&amp;#39;s (a) appetite for this style of API, and (b) therefore should we integrate more closely with LangChain. Cheers!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/theferalmonkey&quot;&gt; /u/theferalmonkey &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cmyi9k/discussion_declaratively_orchestrate_your_code/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cmyi9k/discussion_declaratively_orchestrate_your_code/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1cmyi9k</id><media:thumbnail url="https://external-preview.redd.it/sAUbNyH098cBBqicB9rc8ofLEQDpkmgZKE1tSibcy5k.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=6631c24e2bdd21f4aa327a412b8faec0969e28a2" /><link href="https://www.reddit.com/r/LangChain/comments/1cmyi9k/discussion_declaratively_orchestrate_your_code/" /><updated>2024-05-08T07:12:45+00:00</updated><published>2024-05-08T07:12:45+00:00</published><title>Discussion: Declaratively orchestrate your code instead of using LCEL</title></entry><entry><author><name>/u/Top_Raccoon_1493</name><uri>https://www.reddit.com/user/Top_Raccoon_1493</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;If we are using a GPU for running the LLama2/LLama3 model, which library should I use? LLama CPP or Ctransformers? I&amp;#39;m a bit confused about both of these libraries. Can anyone please clear my doubt?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Top_Raccoon_1493&quot;&gt; /u/Top_Raccoon_1493 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cn08m1/choosing_between_llama_cpp_and_ctransformers_for/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cn08m1/choosing_between_llama_cpp_and_ctransformers_for/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cn08m1</id><link href="https://www.reddit.com/r/LangChain/comments/1cn08m1/choosing_between_llama_cpp_and_ctransformers_for/" /><updated>2024-05-08T09:18:24+00:00</updated><published>2024-05-08T09:18:24+00:00</published><title>Choosing Between LLama CPP and Ctransformers for GPU-based LLama2/LLama3 Model Execution</title></entry><entry><author><name>/u/ramirez_tn</name><uri>https://www.reddit.com/user/ramirez_tn</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I am using Langchain to load PDF files and ask questions using RetrievalQA but when I ask to generate a solution or be creative it does not .It looks like it is limited to the content of the provided files only. Is there a limitation for RertievalQA or just an issue with my prompts ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/ramirez_tn&quot;&gt; /u/ramirez_tn &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cn3d2x/how_to_make_llm_answers_more_creative_and_find/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cn3d2x/how_to_make_llm_answers_more_creative_and_find/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cn3d2x</id><link href="https://www.reddit.com/r/LangChain/comments/1cn3d2x/how_to_make_llm_answers_more_creative_and_find/" /><updated>2024-05-08T12:29:23+00:00</updated><published>2024-05-08T12:29:23+00:00</published><title>How to make LLM answers more creative and find answers from the internet</title></entry><entry><author><name>/u/RoboCoachTech</name><uri>https://www.reddit.com/user/RoboCoachTech</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cmquwv/using_langchain_agents_to_create_a_multiagent/&quot;&gt; &lt;img src=&quot;https://external-preview.redd.it/GklbHOLRONUSqoLxfRvIdxg-bpVWervSnhvxAvD8mnc.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c6e26ad3b1025c14d13502efeae32c2669bf11c4&quot; alt=&quot;Using LangChain agents to create a multi-agent platform that creates robot softwares&quot; title=&quot;Using LangChain agents to create a multi-agent platform that creates robot softwares&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;When using LLMs for your generative AI needs, it&amp;#39;s best to think of the LLM as a person rather than as a traditional AI engine. You can train and tune an LLM and give it memory to create an agent. The LLM-agent can act like a domain-expert for whatever domain you&amp;#39;ve trained and equipped it for. Using one agent to solve a complex problem is not the optimum solution. Much like how a project manager breaks a complex project into different tasks and assigns different individuals with different skills and trainings to manage each task, a multi-agent solution, where each agent has different capabilities and trainings, can be applied to a complex problem. &lt;/p&gt; &lt;p&gt;In our case, we want to automatically generate the entire robot software (for any given robot description) in ROS (Robot Operating System); In order to do so, first, we need to understand the overall design of the robot (a.k.a the ROS graph) and then for each ROS node we need to know if the LLM should generate the code, or if the LLM can fetch a suitable code from online open-source repositories (a.k.a. RAG: Retrieval Augmented Generation). Each of these steps can be handled by different agents which have different sets of tools at their disposal. The following figure shows how we are doing this:&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/qcvb8y98c3zc1.png?width=1570&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5f4072288e470fd9e2d946e471f35e4c2dff1f94&quot;&gt;Robot software generation using four collaborating agents each responsible for a different part of the problem, each equipped with different toolsets.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This is a free and open-source tool that we have released. We named it &lt;a href=&quot;https://github.com/RoboCoachTechnologies/ROScribe&quot;&gt;ROScribe&lt;/a&gt;. Please checkout our &lt;a href=&quot;https://github.com/RoboCoachTechnologies/ROScribe&quot;&gt;repository&lt;/a&gt; for more information and give us a star if you like what you see. :) &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/RoboCoachTech&quot;&gt; /u/RoboCoachTech &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cmquwv/using_langchain_agents_to_create_a_multiagent/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cmquwv/using_langchain_agents_to_create_a_multiagent/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1cmquwv</id><media:thumbnail url="https://external-preview.redd.it/GklbHOLRONUSqoLxfRvIdxg-bpVWervSnhvxAvD8mnc.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=c6e26ad3b1025c14d13502efeae32c2669bf11c4" /><link href="https://www.reddit.com/r/LangChain/comments/1cmquwv/using_langchain_agents_to_create_a_multiagent/" /><updated>2024-05-08T00:05:42+00:00</updated><published>2024-05-08T00:05:42+00:00</published><title>Using LangChain agents to create a multi-agent platform that creates robot softwares</title></entry><entry><author><name>/u/Guizkane</name><uri>https://www.reddit.com/user/Guizkane</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey everyone! I recently wrote a blog post about &lt;a href=&quot;http://affiliate.ai/&quot;&gt;Affiliate.ai&lt;/a&gt;, a chat-based affiliate marketing analytics tool we&amp;#39;ve been working on. It simplifies the analytics process, letting you ask natural language questions and get insights, reports, and even spreadsheets delivered right within Microsoft Teams or Slack.&lt;/p&gt; &lt;p&gt;But the interesting part (for this audience, at least) is how it works under the hood. Here&amp;#39;s a breakdown of some key elements:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Intent Discernment with Function Calling:&lt;/strong&gt; We use simple function calling to quickly determine whether a user wants data or is just chatting, ensuring the bot stays focused.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;LLM-Powered Named Entity Recognition:&lt;/strong&gt; Instead of complex pipelines, we feed the LLM a list of advertisers and let it figure out the matchesâ€“ surprisingly effective!&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Query Reconstruction for Context:&lt;/strong&gt; Understanding context is tricky. We use a dedicated module to rewrite queries based on chat history.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Parallelization for Speed:&lt;/strong&gt; We run multiple potential routes simultaneously, speeding up response times dramatically.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Interested in the specifics? The full blog post has more details (link below). If you&amp;#39;re building similar GenAI apps, I&amp;#39;d love to hear about your approaches and techniques!&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://www.affiliate.ai/post/a-technical-deepdive-into-affiliate-ai&quot;&gt;https://www.affiliate.ai/post/a-technical-deepdive-into-affiliate-ai&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Guizkane&quot;&gt; /u/Guizkane &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cn04dj/deep_dive_building_affiliateai_a_genaipowered/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cn04dj/deep_dive_building_affiliateai_a_genaipowered/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cn04dj</id><link href="https://www.reddit.com/r/LangChain/comments/1cn04dj/deep_dive_building_affiliateai_a_genaipowered/" /><updated>2024-05-08T09:09:59+00:00</updated><published>2024-05-08T09:09:59+00:00</published><title>Deep Dive: Building Affiliate.ai, a GenAI-Powered Affiliate Marketing Analytics Tool</title></entry><entry><author><name>/u/Basil2BulgarSlayer</name><uri>https://www.reddit.com/user/Basil2BulgarSlayer</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I am working on a Nextjs demo app that needs to use inference on a custom LLM I will train. When I deploy it, Iâ€™m planning on using Baseten but for local development I am now considering using Lanchain in Node (as opposed to setting up a Flask server to handle inference and stream the responses back). Has anyone used it before? Is it a total disaster? know itâ€™s not going to be as good as the Python version but maybe itâ€™s good enough for my situation. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Basil2BulgarSlayer&quot;&gt; /u/Basil2BulgarSlayer &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cmz6uh/node_js_support/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cmz6uh/node_js_support/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cmz6uh</id><link href="https://www.reddit.com/r/LangChain/comments/1cmz6uh/node_js_support/" /><updated>2024-05-08T08:02:19+00:00</updated><published>2024-05-08T08:02:19+00:00</published><title>Node JS Support</title></entry><entry><author><name>/u/Organic_Manner359</name><uri>https://www.reddit.com/user/Organic_Manner359</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I just saw that Langchain is now a legacy provider. How can i still use Langchain with the Vercel AI SDK for my NextJS apps in a futureproof way. On the website it says, that the legacy providers are not recommended for new projects.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Organic_Manner359&quot;&gt; /u/Organic_Manner359 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cmkcsb/langchain_is_legacy_in_vercel_ai_sdk_how_to_still/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cmkcsb/langchain_is_legacy_in_vercel_ai_sdk_how_to_still/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cmkcsb</id><link href="https://www.reddit.com/r/LangChain/comments/1cmkcsb/langchain_is_legacy_in_vercel_ai_sdk_how_to_still/" /><updated>2024-05-07T19:29:22+00:00</updated><published>2024-05-07T19:29:22+00:00</published><title>Langchain is legacy in Vercel AI SDK, how to still use Langchain in a stable and futureproof way?</title></entry><entry><author><name>/u/moonbunR</name><uri>https://www.reddit.com/user/moonbunR</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Here is a simple code snippet on how to use the Cycls chatbot library&lt;/p&gt; &lt;pre&gt;&lt;code&gt;main.py from cycls import App app = App(secret=&amp;quot;sk-secret&amp;quot;, handler=&amp;quot;@handler-name&amp;quot;) @app def entry_point(context): # Capture the received message received_message = context.message.content.text # Reply back with a simple message context.send.text(f&amp;quot;Received message: {received_message}&amp;quot;) app.publish() &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This is a simplified example but when you run &lt;a href=&quot;http://main.py&quot;&gt;main.py&lt;/a&gt;, the chatbot immediately gets deployed with a public url and a chat interface. This has helped me a huge deal with testing while developing chatbots.&lt;br/&gt; Here are the docs: &lt;a href=&quot;https://docs.cycls.com/getting-started&quot;&gt;https://docs.cycls.com/getting-started&lt;/a&gt; &lt;a href=&quot;https://docs.cycls.com/getting-started&quot;&gt; &lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/moonbunR&quot;&gt; /u/moonbunR &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cmdxyi/python_library_to_deploy_llm_chat_bots_fast/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cmdxyi/python_library_to_deploy_llm_chat_bots_fast/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cmdxyi</id><link href="https://www.reddit.com/r/LangChain/comments/1cmdxyi/python_library_to_deploy_llm_chat_bots_fast/" /><updated>2024-05-07T14:59:16+00:00</updated><published>2024-05-07T14:59:16+00:00</published><title>Python library to deploy LLM chat bots fast?</title></entry><entry><author><name>/u/easy_breeze5634</name><uri>https://www.reddit.com/user/easy_breeze5634</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I want to ingest hundreds of csv files, all the column data is&lt;br/&gt; different except for them sharing a similar column related to state. So I&lt;br/&gt; am able to capture the location of the data observations and relate&lt;br/&gt; them to other data. The data is mostly pertaining to demographics like&lt;br/&gt; economics, age, race, income, education, and health related outcomes. I&lt;br/&gt; need a general way to ingest all these csv files and load them into a&lt;br/&gt; knowledge graph, then use OpenAI to send a cypher query to the knowledge&lt;br/&gt; graph to gain context of the user&amp;#39;s question and then return an answer.&lt;br/&gt; A question might be &amp;quot;What is the highest mortality rate in the country&lt;br/&gt; and what might be causing this?&amp;quot; or &amp;quot;Tell me counties with the lowest&lt;br/&gt; morbidity rates and why they might be lower than average&amp;quot;. I was&lt;br/&gt; thinking I could use vector embeddings as well for matching columns&lt;br/&gt; together and clustering the data. Im just wondering what the best way to&lt;br/&gt; construct the graph will be so that the LLM can easily traverse it and&lt;br/&gt; get the correct information back to the user. What is the best way to&lt;br/&gt; set all this up? Does it make sense to construct a knowledge graph here&lt;br/&gt; so that LLM has context. &lt;/p&gt; &lt;p&gt;Could use advice on how to set something up like this. &lt;/p&gt; &lt;p&gt;Thanks &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/easy_breeze5634&quot;&gt; /u/easy_breeze5634 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cmpkyl/ingesting_hundreds_of_csv_files_loading_them_into/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cmpkyl/ingesting_hundreds_of_csv_files_loading_them_into/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cmpkyl</id><link href="https://www.reddit.com/r/LangChain/comments/1cmpkyl/ingesting_hundreds_of_csv_files_loading_them_into/" /><updated>2024-05-07T23:06:04+00:00</updated><published>2024-05-07T23:06:04+00:00</published><title>Ingesting hundreds of csv files, loading them into a knowledge graph (RAG) then use LLM chatbot to query</title></entry><entry><author><name>/u/cryptokaykay</name><uri>https://www.reddit.com/user/cryptokaykay</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cmk0dn/langtrace_added_support_for_prompt_playground/&quot;&gt; &lt;img src=&quot;https://external-preview.redd.it/NC1sW6cwQYTGxuUvSZWkfQnn_WLopaQp781No9M4rkg.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1f8d9f7c872f91e520fbcd2f3139df9bf6c76dca&quot; alt=&quot;Langtrace - Added support for Prompt Playground&quot; title=&quot;Langtrace - Added support for Prompt Playground&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey all,&lt;/p&gt; &lt;p&gt;We just added support for prompt playground. The goal of this feature is to help you test and iterate on your prompts from a single view across different combinations of models and model settings. &lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;Support for OpenAI, Anthropic, Cohere and Groq&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Side by side comparison view.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Comprehensive API settings tab to tweak and iterate on your prompts with different combinations of settings and models. &lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Please check it out and let me know if you have any feedback.&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://langtrace.ai/&quot;&gt;https://langtrace.ai/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/Scale3-Labs/langtrace&quot;&gt;https://github.com/Scale3-Labs/langtrace&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://reddit.com/link/1cmk0dn/video/y0tve9hb02zc1/player&quot;&gt;https://reddit.com/link/1cmk0dn/video/y0tve9hb02zc1/player&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/cryptokaykay&quot;&gt; /u/cryptokaykay &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cmk0dn/langtrace_added_support_for_prompt_playground/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cmk0dn/langtrace_added_support_for_prompt_playground/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1cmk0dn</id><media:thumbnail url="https://external-preview.redd.it/NC1sW6cwQYTGxuUvSZWkfQnn_WLopaQp781No9M4rkg.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=1f8d9f7c872f91e520fbcd2f3139df9bf6c76dca" /><link href="https://www.reddit.com/r/LangChain/comments/1cmk0dn/langtrace_added_support_for_prompt_playground/" /><updated>2024-05-07T19:14:36+00:00</updated><published>2024-05-07T19:14:36+00:00</published><title>Langtrace - Added support for Prompt Playground</title></entry><entry><author><name>/u/Calm_Pea_2428</name><uri>https://www.reddit.com/user/Calm_Pea_2428</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;m thinking about writing a blog on this topic &amp;quot;Why specialized vector databases are not the future?&amp;quot;&lt;/p&gt; &lt;p&gt;In this blog, I&amp;#39;ll try to explain why you need Integrated vector databases rather than a specialised vector database. &lt;/p&gt; &lt;p&gt;Do you have any arguments that support or refute this narrative?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Calm_Pea_2428&quot;&gt; /u/Calm_Pea_2428 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cmqx7z/why_specialized_vector_databases_are_not_the/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cmqx7z/why_specialized_vector_databases_are_not_the/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cmqx7z</id><link href="https://www.reddit.com/r/LangChain/comments/1cmqx7z/why_specialized_vector_databases_are_not_the/" /><updated>2024-05-08T00:08:50+00:00</updated><published>2024-05-08T00:08:50+00:00</published><title>Why specialized vector databases are not the future?</title></entry><entry><author><name>/u/ravediamond000</name><uri>https://www.reddit.com/user/ravediamond000</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hello everyone,&lt;/p&gt; &lt;p&gt;Just finished writing a post on how to create real world application using Langchain.&lt;br/&gt; I talk about :&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Langchain LCEL&lt;/li&gt; &lt;li&gt;How to create composition of multiple chains.&lt;/li&gt; &lt;li&gt;How to integrate user parameters like output type or specified vector store in chains.&lt;/li&gt; &lt;li&gt;How to use configuration to change the prompt and the retriever at run time.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Check it out: &lt;a href=&quot;https://www.metadocs.co/2024/05/07/create-a-complex-rag-chat-app-with-langchain-lcel/&quot;&gt;Link&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/ravediamond000&quot;&gt; /u/ravediamond000 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cm5ktx/create_a_real_world_rag_chat_app_with_langchain/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cm5ktx/create_a_real_world_rag_chat_app_with_langchain/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cm5ktx</id><link href="https://www.reddit.com/r/LangChain/comments/1cm5ktx/create_a_real_world_rag_chat_app_with_langchain/" /><updated>2024-05-07T06:50:45+00:00</updated><published>2024-05-07T06:50:45+00:00</published><title>Create a real world RAG chat app with Langchain LCEL</title></entry><entry><author><name>/u/mahadevbhakti</name><uri>https://www.reddit.com/user/mahadevbhakti</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;HI &lt;/p&gt; &lt;p&gt;I have a RunnableWithMessageHistory and agent_executor to create a chatbot agent with tools to fetch data from an API and then Redis to story chat history. &lt;/p&gt; &lt;p&gt;I see that the RunnableWithMessageHistory is not recording the raw response of the function calling to the chat history. How do I solve for this?&lt;/p&gt; &lt;p&gt;I have been reading the API docs but couldn&amp;#39;t find anything. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/mahadevbhakti&quot;&gt; /u/mahadevbhakti &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cmdvls/passing_the_output_of_function_calling_to/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cmdvls/passing_the_output_of_function_calling_to/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cmdvls</id><link href="https://www.reddit.com/r/LangChain/comments/1cmdvls/passing_the_output_of_function_calling_to/" /><updated>2024-05-07T14:56:13+00:00</updated><published>2024-05-07T14:56:13+00:00</published><title>Passing the output of function calling to RedisChatHistory in LCEL</title></entry><entry><author><name>/u/Fresh_Skin130</name><uri>https://www.reddit.com/user/Fresh_Skin130</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I am developing my own Rag. Using the text_splitter there is a performance tradeoff for the chunk size and chunk overlap parameters. Short chunks may not include all desired context, long chunks may hallucinate or cause info loss. Has anyone tried to embed twice the same document with multiple and different splitters? Are there noticeable advantages / disadvantages?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Fresh_Skin130&quot;&gt; /u/Fresh_Skin130 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cm8oek/combined_embeddings/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cm8oek/combined_embeddings/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cm8oek</id><link href="https://www.reddit.com/r/LangChain/comments/1cm8oek/combined_embeddings/" /><updated>2024-05-07T10:31:26+00:00</updated><published>2024-05-07T10:31:26+00:00</published><title>Combined Embeddings</title></entry><entry><author><name>/u/ZuckyFox</name><uri>https://www.reddit.com/user/ZuckyFox</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;How can we pass images or base64 to llama2 and ask certain questions like describe this image??&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/ZuckyFox&quot;&gt; /u/ZuckyFox &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cm9e3l/is_llama2_multimodal/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cm9e3l/is_llama2_multimodal/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cm9e3l</id><link href="https://www.reddit.com/r/LangChain/comments/1cm9e3l/is_llama2_multimodal/" /><updated>2024-05-07T11:15:31+00:00</updated><published>2024-05-07T11:15:31+00:00</published><title>Is llama2 multimodal?</title></entry><entry><author><name>/u/Koltchak</name><uri>https://www.reddit.com/user/Koltchak</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi&lt;/p&gt; &lt;p&gt;I&amp;#39;m currently using a code that processes a series of reports (PDF files), posing the same set of questions to every report. The output is a dataframe containing the responses to each question for every report. For instance, the question &amp;quot;Does the report explains why the amount of X decreased?&amp;quot; will be answered in a column of my output dataframe. So every line of this column will consist of the answer for a specific report. My setup includes the use of &lt;a href=&quot;https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2&quot;&gt;https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2&lt;/a&gt; and a locally downloaded LLM from Hugging Face. Although the code is functional, the reports are highly technical and complex, and the local LLM lacks the necessary understanding of the content (I notice that when I read the answers).&lt;/p&gt; &lt;p&gt;I&amp;#39;m looking to enhance the LLM&amp;#39;s comprehension by training it on additional technical documents of the same nature, hoping this will improve its ability to accurately answer queries from my reports. If I&amp;#39;ve understood correctly, one approach might be to utilize a RAG on the technical documents, but I&amp;#39;m unsure of the exact steps to implement this effectively. I&amp;#39;ve attempted to merge the embeddings from the downloaded &amp;#39;all-MiniLM-L6-v2&amp;#39; model with those I generated from the technical documents, as described here: &lt;a href=&quot;https://python.langchain.com/docs/integrations/retrievers/merger_retriever/&quot;&gt;https://python.langchain.com/docs/integrations/retrievers/merger_retriever/&lt;/a&gt;, but without success.&lt;/p&gt; &lt;p&gt;Could you suggest a viable strategy for this? Should I discard the &amp;#39;all-MiniLM-L6-v2&amp;#39; and focus solely on embeddings derived from my technical documents? This approach seems to require an extensive collection of documents, which I currently don&amp;#39;t have.&lt;/p&gt; &lt;p&gt;I&amp;#39;ve tried various other local LLMs (Mistral, Phi, Llama, Orca), but I encounter the same issue each time. &amp;quot;Large&amp;quot; LLMs (Mistral e.g.) tend to hallucinate, while &amp;quot;smaller&amp;quot; (Orca e.g.) LLMs often respond that they do not know.&lt;/p&gt; &lt;p&gt;Thanks&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Koltchak&quot;&gt; /u/Koltchak &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1clui48/enhancing_local_llms_understanding_of_technical/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1clui48/enhancing_local_llms_understanding_of_technical/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1clui48</id><link href="https://www.reddit.com/r/LangChain/comments/1clui48/enhancing_local_llms_understanding_of_technical/" /><updated>2024-05-06T21:21:11+00:00</updated><published>2024-05-06T21:21:11+00:00</published><title>Enhancing Local LLM's Understanding of Technical Documents</title></entry></feed>