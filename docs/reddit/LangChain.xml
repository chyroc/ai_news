<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/"><category term="LangChain" label="r/LangChain"/><updated>2023-12-12T07:27:23+00:00</updated><icon>https://www.redditstatic.com/icon.png/</icon><id>/r/LangChain.rss</id><link rel="self" href="https://www.reddit.com/r/LangChain.rss" type="application/atom+xml" /><link rel="alternate" href="https://www.reddit.com/r/LangChain" type="text/html" /><subtitle>LangChain is an open-source framework and developer toolkit that helps developers get LLM applications from prototype to production. It is available for Python and Javascript at https://www.langchain.com/.</subtitle><title>LangChain</title><entry><author><name>/u/zchaarm</name><uri>https://www.reddit.com/user/zchaarm</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;A place for members of &lt;a href=&quot;/r/LangChain&quot;&gt;r/LangChain&lt;/a&gt; to chat with each other&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/zchaarm&quot;&gt; /u/zchaarm &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_10ljho9</id><link href="https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/" /><updated>2023-01-26T04:33:05+00:00</updated><published>2023-01-26T04:33:05+00:00</published><title>r/LangChain Lounge</title></entry><entry><author><name>/u/farmf00d</name><uri>https://www.reddit.com/user/farmf00d</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;It doesn’t feel that similarity search is the long pole in the tent compared to inference. It’s easy to write a cosine similarity search in SQL, so why not use an existing MPP data warehouse to do this that scales this kind of query over TBs of embeddings and at high concurrency? What am I missing?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/farmf00d&quot;&gt; /u/farmf00d &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18g9gyh/why_do_i_need_a_specialised_vector_db/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18g9gyh/why_do_i_need_a_specialised_vector_db/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18g9gyh</id><link href="https://www.reddit.com/r/LangChain/comments/18g9gyh/why_do_i_need_a_specialised_vector_db/" /><updated>2023-12-12T00:56:52+00:00</updated><published>2023-12-12T00:56:52+00:00</published><title>Why do I need a specialised vector DB?</title></entry><entry><author><name>/u/pmz</name><uri>https://www.reddit.com/user/pmz</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18g17n7/building_llmpowered_web_apps_with_clientside/&quot;&gt; &lt;img src=&quot;https://external-preview.redd.it/LM0qlyIIQzVSgKATyo3COQvQbo01SaZFNormVRcjC3E.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=789e9207dad6489df9c5276e8790707cc7501cd2&quot; alt=&quot;Building LLM-Powered Web Apps with Client-Side Technology&quot; title=&quot;Building LLM-Powered Web Apps with Client-Side Technology&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/pmz&quot;&gt; /u/pmz &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://blog.langchain.dev/building-llm-powered-web-apps-with-client-side-technology/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18g17n7/building_llmpowered_web_apps_with_clientside/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_18g17n7</id><media:thumbnail url="https://external-preview.redd.it/LM0qlyIIQzVSgKATyo3COQvQbo01SaZFNormVRcjC3E.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=789e9207dad6489df9c5276e8790707cc7501cd2" /><link href="https://www.reddit.com/r/LangChain/comments/18g17n7/building_llmpowered_web_apps_with_clientside/" /><updated>2023-12-11T19:04:07+00:00</updated><published>2023-12-11T19:04:07+00:00</published><title>Building LLM-Powered Web Apps with Client-Side Technology</title></entry><entry><author><name>/u/gswithai</name><uri>https://www.reddit.com/user/gswithai</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hello 👋 &lt;/p&gt; &lt;p&gt;I’ve played around with &lt;a href=&quot;https://www.gettingstarted.ai/tutorial-how-to-integrate-milvus-vector-database-into-your-rag-llm-langchain-app/&quot;&gt;Milvus and LangChain last month&lt;/a&gt; and decided to test another popular vector database this time: Chroma DB.&lt;/p&gt; &lt;p&gt;It’s open-source and easy to setup. &lt;a href=&quot;https://www.gettingstarted.ai/tutorial-chroma-db-best-vector-database-for-langchain-store-embeddings/&quot;&gt;Here’s the full tutorial&lt;/a&gt; if you’re using or planning on using Chroma as the vector database for your embeddings!&lt;/p&gt; &lt;p&gt;Here’s what’s in the tutorial:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Environment setup&lt;/li&gt; &lt;li&gt;Install Chroma, LangChain, and other dependencies&lt;/li&gt; &lt;li&gt;Create vector store from chunks of PDF&lt;/li&gt; &lt;li&gt;Perform similarity search locally&lt;/li&gt; &lt;li&gt;Query the LLM model and get a response&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I also went over how you could add metadata to an existing collection by updating it. &lt;/p&gt; &lt;p&gt;Would love to know if you find this helpful and if you have any questions!&lt;/p&gt; &lt;p&gt;Cheers&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/gswithai&quot;&gt; /u/gswithai &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18fyy5r/chroma_is_a_great_opensource_vector_database/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18fyy5r/chroma_is_a_great_opensource_vector_database/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18fyy5r</id><link href="https://www.reddit.com/r/LangChain/comments/18fyy5r/chroma_is_a_great_opensource_vector_database/" /><updated>2023-12-11T16:49:20+00:00</updated><published>2023-12-11T16:49:20+00:00</published><title>Chroma is a great open-source vector database option to use with your LangChain app</title></entry><entry><author><name>/u/Glass-Web6499</name><uri>https://www.reddit.com/user/Glass-Web6499</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Not posting this from my main for obvious reasons (work related).&lt;/p&gt; &lt;p&gt;Engineer with over a decade of experience here. You name it, I&amp;#39;ve worked on it. I&amp;#39;ve navigated and maintained the nastiest legacy code bases. I thought I&amp;#39;ve seen the worst.&lt;/p&gt; &lt;p&gt;Until I started working with Langchain.&lt;/p&gt; &lt;p&gt;Holy shit with all due respect LangChain is arguably the worst library that I&amp;#39;ve ever worked in my life.&lt;/p&gt; &lt;p&gt;Inconsistent abstractions, inconsistent naming schemas, inconsistent behaviour, confusing error management, confusing chain life-cycle, confusing callback handling, unneccessary abstractions to name a few things.&lt;/p&gt; &lt;p&gt;The fundemental problem with LangChain is you try to do it all. You try to welcome beginner developers so that they don&amp;#39;t have to write a single line of code but as a result you alienate the rest of us that actually know how to code.&lt;/p&gt; &lt;p&gt;Let me not get started with the whole &amp;quot;LCEL&amp;quot; thing lol.&lt;/p&gt; &lt;p&gt;Seriously, take this as a warning. Please do not use LangChain and preserve your sanity. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Glass-Web6499&quot;&gt; /u/Glass-Web6499 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18eukhc/i_just_had_the_displeasure_of_implementing/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18eukhc/i_just_had_the_displeasure_of_implementing/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18eukhc</id><link href="https://www.reddit.com/r/LangChain/comments/18eukhc/i_just_had_the_displeasure_of_implementing/" /><updated>2023-12-10T03:34:38+00:00</updated><published>2023-12-10T03:34:38+00:00</published><title>I just had the displeasure of implementing Langchain in our org.</title></entry><entry><author><name>/u/Honest-Worth3677</name><uri>https://www.reddit.com/user/Honest-Worth3677</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18f0rpf/letting_ai_agents_control_my_email_inbox_gmail/&quot;&gt; &lt;img src=&quot;https://external-preview.redd.it/X41KKMsO84t8vq9u4VaXfblyOzXt7PlIvBG2u-zr8-s.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8e8eac57ea9040893855f145db440f938fe7e966&quot; alt=&quot;📧Letting AI agents control my email inbox... 📮: Gmail Automation&quot; title=&quot;📧Letting AI agents control my email inbox... 📮: Gmail Automation&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Honest-Worth3677&quot;&gt; /u/Honest-Worth3677 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://youtu.be/4Ggtba_j1wM&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18f0rpf/letting_ai_agents_control_my_email_inbox_gmail/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_18f0rpf</id><media:thumbnail url="https://external-preview.redd.it/X41KKMsO84t8vq9u4VaXfblyOzXt7PlIvBG2u-zr8-s.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=8e8eac57ea9040893855f145db440f938fe7e966" /><link href="https://www.reddit.com/r/LangChain/comments/18f0rpf/letting_ai_agents_control_my_email_inbox_gmail/" /><updated>2023-12-10T10:31:56+00:00</updated><published>2023-12-10T10:31:56+00:00</published><title>📧Letting AI agents control my email inbox... 📮: Gmail Automation</title></entry><entry><author><name>/u/pj3677</name><uri>https://www.reddit.com/user/pj3677</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18epg7r/exploring_langchain_yt_live_stream/&quot;&gt; &lt;img src=&quot;https://external-preview.redd.it/I9rjvwljcTPkyP7JPzEzwSkAwnPoWQv2zfqBlXM4aEA.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=23470fefff97f253f4c4a7ef5772066a09c32941&quot; alt=&quot;Exploring LangChain (YT live stream)&quot; title=&quot;Exploring LangChain (YT live stream)&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/pj3677&quot;&gt; /u/pj3677 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=U7Rog6JSmAE&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18epg7r/exploring_langchain_yt_live_stream/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_18epg7r</id><media:thumbnail url="https://external-preview.redd.it/I9rjvwljcTPkyP7JPzEzwSkAwnPoWQv2zfqBlXM4aEA.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=23470fefff97f253f4c4a7ef5772066a09c32941" /><link href="https://www.reddit.com/r/LangChain/comments/18epg7r/exploring_langchain_yt_live_stream/" /><updated>2023-12-09T23:00:29+00:00</updated><published>2023-12-09T23:00:29+00:00</published><title>Exploring LangChain (YT live stream)</title></entry><entry><author><name>/u/thedabking123</name><uri>https://www.reddit.com/user/thedabking123</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I see Pinecone.from_documents in the Langchain documents but I can&amp;#39;t seem to find the relevant python file to understand the method.&lt;/p&gt; &lt;p&gt;Curious because when I collect all the chunked docs from RecursiveCharacterTextSplitter.split_documents and OpenAIEmbeddings.embed_documents I am getting 610 texts/vectors but after building the Pinecone Index for a specific namespace (a subsection of the Index) and checking the properties of the namespace I see 1251 vectors there.&lt;/p&gt; &lt;p&gt;How else are people breaking down PDFs if not using from_documents? I would like to see the actual method being run.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/thedabking123&quot;&gt; /u/thedabking123 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18ehcm7/guys_anyone_use_pineconefrom_documents/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18ehcm7/guys_anyone_use_pineconefrom_documents/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18ehcm7</id><link href="https://www.reddit.com/r/LangChain/comments/18ehcm7/guys_anyone_use_pineconefrom_documents/" /><updated>2023-12-09T16:36:57+00:00</updated><published>2023-12-09T16:36:57+00:00</published><title>Guys- anyone use Pinecone.from_documents, RecursiveCharacterTextSplitter.split_documents, etc. and are they working for you? If not what's the alternative to breaking down PDFs using Langchain?</title></entry><entry><author><name>/u/WaterFallingSilently</name><uri>https://www.reddit.com/user/WaterFallingSilently</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;It seems like LangChain (python) in web applications comes with a lot of headaches due to serialization issues.&lt;/p&gt; &lt;p&gt;I keep trying to return things from the server side code to the client only to find that I&amp;#39;m unable to as they are non serializable.&lt;/p&gt; &lt;p&gt;For example:&lt;/p&gt; &lt;p&gt;Documents, AIMessages, etc&lt;/p&gt; &lt;p&gt;This means I manually have to go through and recreate/clean all these objects when I want to pass them to the client.&lt;/p&gt; &lt;p&gt;Does anyone have a good solution around this?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/WaterFallingSilently&quot;&gt; /u/WaterFallingSilently &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18e9f1u/passing_langchain_objects_from_server_to_client/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18e9f1u/passing_langchain_objects_from_server_to_client/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18e9f1u</id><link href="https://www.reddit.com/r/LangChain/comments/18e9f1u/passing_langchain_objects_from_server_to_client/" /><updated>2023-12-09T08:53:14+00:00</updated><published>2023-12-09T08:53:14+00:00</published><title>Passing LangChain Objects from Server to Client Issues (Serialization)</title></entry><entry><author><name>/u/rambunctiousambivert</name><uri>https://www.reddit.com/user/rambunctiousambivert</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Today I was playing on openAI Assistant to get answers from a simple CSV file. After trying for 3 prompts, after which I did not really get any answers, the cost was 2$!!! which I think is high, compared to &amp;lt;1$ that used to cost when I was playing with API few months ago.&lt;/p&gt; &lt;p&gt;I am working on a recommendation app, where I will provide openAI api some of the data from my sql db and return openai suggestions to the end user. Now seeing the assistant price I feel like reverting back to using API as the assistants cost seem a bit steeper. Am I missing something, what are your thoughts?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/rambunctiousambivert&quot;&gt; /u/rambunctiousambivert &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18e25qx/openai_assistant_pricing_vs_api_pricing/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18e25qx/openai_assistant_pricing_vs_api_pricing/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18e25qx</id><link href="https://www.reddit.com/r/LangChain/comments/18e25qx/openai_assistant_pricing_vs_api_pricing/" /><updated>2023-12-09T01:27:44+00:00</updated><published>2023-12-09T01:27:44+00:00</published><title>OpenAI assistant pricing vs API pricing</title></entry><entry><author><name>/u/dberg76</name><uri>https://www.reddit.com/user/dberg76</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Did something change in the most recent version of Langchain, i could have sworn that the ConversationalRetrievalChain took a memory parameter and now its gone?&lt;/p&gt; &lt;p&gt;The docs have it &lt;a href=&quot;https://api.python.langchain.com/en/latest/chains/langchain.chains.conversational_retrieval.base.ConversationalRetrievalChain.html#langchain.chains.conversational_retrieval.base.ConversationalRetrievalChain.memory&quot;&gt;https://api.python.langchain.com/en/latest/chains/langchain.chains.conversational_retrieval.base.ConversationalRetrievalChain.html#langchain.chains.conversational_retrieval.base.ConversationalRetrievalChain.memory&lt;/a&gt; But the code errors out with a “bad key” error and there is no mention of memory in the source anymore.&lt;/p&gt; &lt;p&gt;&lt;code&gt;pydantic.v1.error_wrappers.ValidationError: 1 validation error for ConversationalRetrievalChain&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;memory value is not a valid dict (type=type_error.dict)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;what is the current idiomatic way to do RetreivalQA RAG over private documents with history/memory now ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/dberg76&quot;&gt; /u/dberg76 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18dxc2o/memory_in_conversationalretrievalchain_removed/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18dxc2o/memory_in_conversationalretrievalchain_removed/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18dxc2o</id><link href="https://www.reddit.com/r/LangChain/comments/18dxc2o/memory_in_conversationalretrievalchain_removed/" /><updated>2023-12-08T21:35:03+00:00</updated><published>2023-12-08T21:35:03+00:00</published><title>memory in ConversationalRetrievalChain removed</title></entry><entry><author><name>/u/sonaryn</name><uri>https://www.reddit.com/user/sonaryn</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Can’t ask vanilla GPT 4 since it’s past the training cutoff, and can’t make one myself cause, well, I can’t understand the docs&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/sonaryn&quot;&gt; /u/sonaryn &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18dxn4b/are_there_any_custom_gpts_copilot_bots_trained_on/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18dxn4b/are_there_any_custom_gpts_copilot_bots_trained_on/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18dxn4b</id><link href="https://www.reddit.com/r/LangChain/comments/18dxn4b/are_there_any_custom_gpts_copilot_bots_trained_on/" /><updated>2023-12-08T21:49:07+00:00</updated><published>2023-12-08T21:49:07+00:00</published><title>Are there any custom GPTs / Copilot bots trained on the latest LangChain docs?</title></entry><entry><author><name>/u/BtownIU</name><uri>https://www.reddit.com/user/BtownIU</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;hi guys,&lt;/p&gt; &lt;p&gt;I have two RAG chains for 2 different types of answers. One of the chains is instructed to generate json and it works well by iteself.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;json_instruct=&amp;quot;&amp;quot;&amp;quot; [INST] &amp;lt;&amp;lt;SYS&amp;gt;&amp;gt; You are a helpful and concise assistent, that only comunicates using JSON files. The expected output from you has to be: { &amp;quot;cookware_name&amp;quot;: {cookware}, &amp;quot;description&amp;quot;: [], &amp;quot;ai_notes&amp;quot;: {explanation} } The INST block will always be a json string: { &amp;quot;prompt&amp;quot;: {the user prompt} } The available cookwares are in the context given to you &amp;lt;&amp;lt;/SYS&amp;gt;&amp;gt; [/INST] &amp;quot;&amp;quot;&amp;quot; JSON_pipeline = RetrievalQA.from_chain_type( llm=CustomLLM(system_instruction=json_instruct), chain_type=&amp;#39;stuff&amp;#39;, retriever=cookware_retriever ) JSON_pipeline(&amp;quot;what do i need for crepes&amp;quot;) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;But when i put these 2 chains into a langchain agent, the agent would accurately send a prompt to the json chain but return answers in regular sentences. The agent is defined as :&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;tools = [ # tool 1 Tool(...), # tool 2 Tool( name = &amp;quot;JSON data&amp;quot;, func=JSON_pipeline.run, description = &amp;quot;&amp;quot;&amp;quot; Useful for you need to answer questions about what a certain cookware is or what cookware is needed for certain dishes. &amp;quot;&amp;quot;&amp;quot;,return_direct=True ) ] llm_chain = LLMChain(llm=CustomLLM(system_instruction=&amp;quot;You are a helpful and fast assistant.&amp;quot;), prompt=prompt) tool_names = [tool.name for tool in tools] agent = LLMSingleActionAgent( llm_chain=llm_chain, output_parser=output_parser, stop=[&amp;quot;\nObservation:&amp;quot;], allowed_tools=tool_names ) agent_executor = AgentExecutor.from_agent_and_tools(agent=agent, tools=tools, verbose=True) agent_executor.run(&amp;quot;list useful cookwares for soup&amp;quot;) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;The answer is indeed a list useful cookwares based on the documents. But it&amp;#39;s no longer in a JSON format that was generated by the JSON_pipeline&lt;/p&gt; &lt;p&gt;Is there any way I can make the agent keep the answers as provided by the tools without modifying the answers by the central &amp;quot;router&amp;quot; LLM? (Or am I missing out some parameters?)&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/BtownIU&quot;&gt; /u/BtownIU &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18dv7wx/langchain_agent_does_not_return_json_files/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18dv7wx/langchain_agent_does_not_return_json_files/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18dv7wx</id><link href="https://www.reddit.com/r/LangChain/comments/18dv7wx/langchain_agent_does_not_return_json_files/" /><updated>2023-12-08T20:00:19+00:00</updated><published>2023-12-08T20:00:19+00:00</published><title>langchain agent does not return JSON files generated from tools</title></entry><entry><author><name>/u/One-Difficulty3149</name><uri>https://www.reddit.com/user/One-Difficulty3149</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hello everyone, I have been working with langchain and has built some RAG applications. I have used FAISS as the vector database, which inherently does not support CRUD operations completely. If anyone has any inputs on which of the vector databases support CRUD operations, which they might have tried and tested. And also it should be efficient and not accurate, not too much time consuming. Thanks.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/One-Difficulty3149&quot;&gt; /u/One-Difficulty3149 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18dgp16/crud_operations_on_vector_databases/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18dgp16/crud_operations_on_vector_databases/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18dgp16</id><link href="https://www.reddit.com/r/LangChain/comments/18dgp16/crud_operations_on_vector_databases/" /><updated>2023-12-08T06:30:28+00:00</updated><published>2023-12-08T06:30:28+00:00</published><title>CRUD operations on Vector Databases</title></entry><entry><author><name>/u/Aggressive-Salt-4042</name><uri>https://www.reddit.com/user/Aggressive-Salt-4042</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Survey here → &lt;a href=&quot;https://survey.alchemer.com/s3/7626156/ES2412p&quot;&gt;https://survey.alchemer.com/s3/7626156/ES2412p&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Aggressive-Salt-4042&quot;&gt; /u/Aggressive-Salt-4042 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18dsx62/the_elasticsearch_developer_survey_is_here_if_you/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18dsx62/the_elasticsearch_developer_survey_is_here_if_you/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18dsx62</id><link href="https://www.reddit.com/r/LangChain/comments/18dsx62/the_elasticsearch_developer_survey_is_here_if_you/" /><updated>2023-12-08T18:16:15+00:00</updated><published>2023-12-08T18:16:15+00:00</published><title>The Elasticsearch developer survey is here! If you build an app with search and/or gen AI, consider taking this 10-min survey. Your feedback will mean a lot! → https://survey.alchemer.com/s3/7626156/ES2412p</title></entry><entry><author><name>/u/EmployFew4830</name><uri>https://www.reddit.com/user/EmployFew4830</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi there, I am looking to implement a RAGish use case with python and langchain. Want to share my high level plan here and ask for your feedback. &lt;/p&gt; &lt;p&gt;Process roughly is as follows&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Access a catalog of large, complex and semi-structured objects. Each object consists of some string attributes and contains multi-level list objects. The &amp;quot;payload&amp;quot; often is formatted as JSON (not sure yet whether I should extract more attributes or just hope that the LLM can allow queries also on the insides of that JSON). I created a python class that represents that structure&lt;/li&gt; &lt;li&gt;Next step is to create embeddings (using langchain &amp;amp; Chroma for now). Here I am unsure how to proceed. At least I want to have a dedicated embedding of each of my catalog items. Use case will be, that users present their use case (like &amp;quot;where do i find information on attribute XYZ?&amp;quot;). I want the LLM to return the suitable catalog item. But almost more important, I want the LLM to look inside the catalog item, look at all attributes and evaluate whether a certain attribute suits (rather than the whole catalog item). &lt;ul&gt; &lt;li&gt;Q: Should I embed each catalog item as one entry or rather create embeddings on artifact level? If so, how do I maintain reference to the catalog item? I could duplicate relevant catalog item information of course for each attribute. &lt;/li&gt; &lt;li&gt;Q: Already thinking of retrieval (plan to use in retrieval chains). I do foresee that user queries may not be straight forward enough. I may need an agent that guides the user through her query (like U: &amp;quot;where do i find information on attribute XYZ?&amp;quot; -&amp;gt; S: &amp;quot;Help me to understand more about your use case: Which parts of the catalog are you interested in? Rather A or B or C?&amp;quot; -&amp;gt; ...). For that I need to make the LLM aware of both, the catalog structure and the insides of a catalog item. Can I do that with just &amp;quot;flat&amp;quot; embeddings? &lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Once the embeddings are done, I want to use them as {context} with langchain. I first want to try how far I can get with chains. &lt;ul&gt; &lt;li&gt;Q: Any experience / examples on how to define prompts in a way that I can use information from context? Can I explicitly reference metadata information somehow? Like &amp;quot;system message: You´re an expert on the catalog from {context} and want to recommend discrete items. You should always use the {name} and {summary} in your recommendations&amp;quot;. Additionally, if i index each catalog item separately, I need to look into the items´ structure, like &amp;quot;system message: In your recommendations please always list the output parameters of the method of a catalog item that made you pick this recommendation&amp;quot;. Is that likely to work? &lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Some high level assessment of how far I can get with chains or whether I should use agents for my case would be very helpful as well. &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Thanks already for the discussion!&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/EmployFew4830&quot;&gt; /u/EmployFew4830 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18djyo6/structure_of_embeddings_for_complex_objects_how/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18djyo6/structure_of_embeddings_for_complex_objects_how/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18djyo6</id><link href="https://www.reddit.com/r/LangChain/comments/18djyo6/structure_of_embeddings_for_complex_objects_how/" /><updated>2023-12-08T10:27:02+00:00</updated><published>2023-12-08T10:27:02+00:00</published><title>Structure of embeddings for complex objects / how to interact with structure in langchain</title></entry><entry><author><name>/u/ebsbdbdbdb</name><uri>https://www.reddit.com/user/ebsbdbdbdb</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I am using LangChain with Python, specifically this part of the API for Open AI Assistants - &lt;a href=&quot;https://python.langchain.com/docs/modules/agents/agent_types/openai_assistants&quot;&gt;https://python.langchain.com/docs/modules/agents/agent_types/openai_assistants&lt;/a&gt; &lt;/p&gt; &lt;p&gt;How do I attach files to user prompts? &lt;/p&gt; &lt;p&gt;Looking at the following code - &lt;/p&gt; &lt;pre&gt;&lt;code&gt;from langchain.agents import AgentExecutor agent_executor = AgentExecutor(agent=agent, tools=tools) agent_executor.invoke({&amp;quot;content&amp;quot;: &amp;quot;What&amp;#39;s the weather in SF today divided by 2.7&amp;quot;}) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The content property of the object that is being passed into the invoke function has the user prompt - what we are sending to the assistant. But in addition to text prompts, it is also possible to send files to the assistant in a user prompt. In the OpenAI Assistants Web UI, there is an attach file button. With the official OpenAI Assistants API, I am able to upload the file to OpenAI&amp;#39;s file system and then attach the file via the file ID that is generated.&lt;/p&gt; &lt;p&gt;How can I attach files to a user prompt with the LangChain OpenAI Assistant API? The documentation doesn&amp;#39;t seem to mention how I can do this from what I&amp;#39;ve read.&lt;/p&gt; &lt;p&gt;Thank you.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/ebsbdbdbdb&quot;&gt; /u/ebsbdbdbdb &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18dloff/attaching_files_to_user_prompt_when_using/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18dloff/attaching_files_to_user_prompt_when_using/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18dloff</id><link href="https://www.reddit.com/r/LangChain/comments/18dloff/attaching_files_to_user_prompt_when_using/" /><updated>2023-12-08T12:22:12+00:00</updated><published>2023-12-08T12:22:12+00:00</published><title>Attaching files to user prompt when using LangChain OpenAI Assistant API</title></entry><entry><author><name>/u/charlestehio</name><uri>https://www.reddit.com/user/charlestehio</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I’m currently self hosting thenlper/gte-small in a 6USD DO droplet, 1GB Ram, 1vCPU. Not too happy with the throughput. API cold start can go up to 5-8 seconds, and averaging around 2-3 seconds. &lt;/p&gt; &lt;p&gt;I am planning to switch over to baai/bge-small-en-v1.5 for newer projects because of Cloudflare Workers AI but they have no pricing model and not recommended for production yet. In the meantime, anyone has any ideas on how to mangle through this?&lt;/p&gt; &lt;p&gt;No OpenAI embeddings, thank you! I prefer something that can be self hosted and managed so I can scale up / down in costs&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/charlestehio&quot;&gt; /u/charlestehio &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18de6xv/any_managed_vector_embedding_services/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18de6xv/any_managed_vector_embedding_services/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18de6xv</id><link href="https://www.reddit.com/r/LangChain/comments/18de6xv/any_managed_vector_embedding_services/" /><updated>2023-12-08T04:04:33+00:00</updated><published>2023-12-08T04:04:33+00:00</published><title>Any managed vector embedding services?</title></entry><entry><author><name>/u/PrudentCherry322</name><uri>https://www.reddit.com/user/PrudentCherry322</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18dguub/uae_new_sentence_embeddings_for_rag_sota_on_mteb/&quot;&gt; &lt;img src=&quot;https://external-preview.redd.it/2NOXPPsK7CrArgZgW_8munT8Vlwu5T4KB6tE801WSTY.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c51499cd903e5d873363d5f9163703a0d74d26e1&quot; alt=&quot;UAE: New Sentence Embeddings for RAG | SOTA on MTEB Leaderboard&quot; title=&quot;UAE: New Sentence Embeddings for RAG | SOTA on MTEB Leaderboard&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/PrudentCherry322&quot;&gt; /u/PrudentCherry322 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://github.com/SeanLee97/AnglE&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18dguub/uae_new_sentence_embeddings_for_rag_sota_on_mteb/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_18dguub</id><media:thumbnail url="https://external-preview.redd.it/2NOXPPsK7CrArgZgW_8munT8Vlwu5T4KB6tE801WSTY.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=c51499cd903e5d873363d5f9163703a0d74d26e1" /><link href="https://www.reddit.com/r/LangChain/comments/18dguub/uae_new_sentence_embeddings_for_rag_sota_on_mteb/" /><updated>2023-12-08T06:41:10+00:00</updated><published>2023-12-08T06:41:10+00:00</published><title>UAE: New Sentence Embeddings for RAG | SOTA on MTEB Leaderboard</title></entry><entry><author><name>/u/Impressive_Gate2102</name><uri>https://www.reddit.com/user/Impressive_Gate2102</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;I am trying to achieve streaming for a custom LLM hosted on a server, using LLMCHAIN. I am using nodejs for the same. If anyone has worked on something similar, could you please guide me? &lt;/p&gt; &lt;p&gt;I am completely clueless on this. &lt;/p&gt; &lt;p&gt;Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Impressive_Gate2102&quot;&gt; /u/Impressive_Gate2102 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18dgatn/need_help_with_streaming_for_custom_llm_nodejs/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18dgatn/need_help_with_streaming_for_custom_llm_nodejs/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18dgatn</id><link href="https://www.reddit.com/r/LangChain/comments/18dgatn/need_help_with_streaming_for_custom_llm_nodejs/" /><updated>2023-12-08T06:04:45+00:00</updated><published>2023-12-08T06:04:45+00:00</published><title>Need help with streaming for Custom LLM Nodejs</title></entry><entry><author><name>/u/bickrombishsass</name><uri>https://www.reddit.com/user/bickrombishsass</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I need to randomly select some documents from a langchain vectorstore. Is there any process for that. One solution is to generate a random embedding vector and then do similarity search with that vector. But is there any efficient method for randomly selecting some document. Selecting documents with random index will also work.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/bickrombishsass&quot;&gt; /u/bickrombishsass &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18dj6ep/random_search_on_vectorstore/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18dj6ep/random_search_on_vectorstore/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18dj6ep</id><link href="https://www.reddit.com/r/LangChain/comments/18dj6ep/random_search_on_vectorstore/" /><updated>2023-12-08T09:26:54+00:00</updated><published>2023-12-08T09:26:54+00:00</published><title>Random search on vectorstore</title></entry><entry><author><name>/u/prajwalsouza</name><uri>https://www.reddit.com/user/prajwalsouza</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18d0ikm/fixed_the_blog_post_to_match_the_technical_report/&quot;&gt; &lt;img src=&quot;https://preview.redd.it/ume5ozy60u4c1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ee13048518ecc5037d2260691c203bdb5c5c5973&quot; alt=&quot;Fixed the blog post to match the technical report on Gemini. :)&quot; title=&quot;Fixed the blog post to match the technical report on Gemini. :)&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/prajwalsouza&quot;&gt; /u/prajwalsouza &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://i.redd.it/ume5ozy60u4c1.png&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18d0ikm/fixed_the_blog_post_to_match_the_technical_report/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_18d0ikm</id><media:thumbnail url="https://preview.redd.it/ume5ozy60u4c1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=ee13048518ecc5037d2260691c203bdb5c5c5973" /><link href="https://www.reddit.com/r/LangChain/comments/18d0ikm/fixed_the_blog_post_to_match_the_technical_report/" /><updated>2023-12-07T17:20:51+00:00</updated><published>2023-12-07T17:20:51+00:00</published><title>Fixed the blog post to match the technical report on Gemini. :)</title></entry><entry><author><name>/u/Positively101</name><uri>https://www.reddit.com/user/Positively101</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I intend to create a local llm based chatbot for my team. Basically it should be able to read the docs and generate intelligent responses. I&amp;#39;m pretty new to LLMs and have tried few things here and there. Overall I intend to present a prototype on a non-GPU or useless GPU based machine first. From what I understand so far I need to create a RAG pipeline. I&amp;#39;ve seen few architectures using embeddings, vector databases, langchain and a model to do create such a pipeline. I&amp;#39;m still pretty new to all these jargons. I have tried few opensource models as well locally and most of them just crash my M1 laptop. I have better work laptop with 16 GP RAM and 8GB graphics card memory on an A2000 card. Can you please suggest how can I quickly come up with a prototype. Basically the RAG pipeline(or any other method) should be able to quickly switch between different LLM models, or databases or any other components when it comes to deploying on a production setup. Also, for now, the idea is to use the data from pdf docs, word docs or data downloaded in json format. I&amp;#39;m not averse of coding so I can code one if I know what to do. Please suggest. Also please post any useful suggestions, articles, course, etc.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Positively101&quot;&gt; /u/Positively101 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18dcnkc/localprivate_llm_based_chatbot_using_freeopen/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18dcnkc/localprivate_llm_based_chatbot_using_freeopen/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18dcnkc</id><link href="https://www.reddit.com/r/LangChain/comments/18dcnkc/localprivate_llm_based_chatbot_using_freeopen/" /><updated>2023-12-08T02:42:34+00:00</updated><published>2023-12-08T02:42:34+00:00</published><title>local/private llm based chatbot using free/open source tools.</title></entry><entry><author><name>/u/SirEliteKaffee</name><uri>https://www.reddit.com/user/SirEliteKaffee</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18cun5p/langserve_stream_works_invoke_doesnt/&quot;&gt; &lt;img src=&quot;https://a.thumbs.redditmedia.com/B-LXWm_EcnpH-dG-Ja03T-WcTOibRuhMkkOlg6KoQI8.jpg&quot; alt=&quot;LangServe: Stream works, Invoke doesn't&quot; title=&quot;LangServe: Stream works, Invoke doesn't&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;m building a very simple chain that takes as an input a customer feedback string and categorizes it into the following pydantic class:&lt;/p&gt; &lt;pre&gt;&lt;code&gt; class AnalysisAttributes(BaseModel): overall_positive: bool = Field(description=&amp;quot;&amp;lt;sentiment is positive overall&amp;gt;&amp;quot;) mentions_pricing: bool = Field(description=&amp;quot;&amp;lt;pricing is mentioned&amp;gt;&amp;quot;) mentions_competition: bool = Field(description=&amp;quot;&amp;lt;competition is mentioned&amp;gt;&amp;quot;) parser = PydanticOutputParser(pydantic_object=AnalysisAttributes) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;Here&amp;#39;s how this should work, and it does:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;full_pipeline = prompt | model | parser output = full_pipeline.invoke({&amp;quot;feedback&amp;quot;: &amp;quot;This bad company is very expensive.&amp;quot;}) expected_output = AnalysisAttributes(overall_positive=False, mentions_pricing=True, mentions_competition=False) assert output == expected_output. # this works! :) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;This works very well, all good so far! Let&amp;#39;s serve it:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;app = FastAPI( title=&amp;quot;LangChain Server&amp;quot;, version=&amp;quot;1.0&amp;quot;, description=&amp;quot;A simple api server using Langchain&amp;#39;s Runnable interfaces&amp;quot;, ) pipeline = prompt | model | parser add_routes(app, pipeline, path=&amp;quot;/categorize_feedback&amp;quot;) if __name__ == &amp;quot;__main__&amp;quot;: import uvicorn uvicorn.run(app, host=&amp;quot;localhost&amp;quot;, port=8000) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;Now comes the strange part, check this out. On the client side, streaming works:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;response = requests.post( &amp;quot;http://localhost:8000/categorize_feedback/stream/&amp;quot;, json={&amp;#39;input&amp;#39;: {&amp;#39;feedback&amp;#39;: &amp;#39;Prices are too high.&amp;#39;}} ) for chunk in response: print(chunk.decode()) # event: metadata [...] data: {&amp;quot;overall_positive&amp;quot;:false, ... &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;But the regular invoke does not work, it delivers an empty output:&lt;/strong&gt; &lt;/p&gt; &lt;pre&gt;&lt;code&gt;response = requests.post( &amp;quot;http://localhost:8000/categorize_feedback/invoke/&amp;quot;, json={&amp;#39;input&amp;#39;: {&amp;#39;feedback&amp;#39;: &amp;#39;Prices are too high.&amp;#39;}} ) print(response.json()) # {&amp;#39;output&amp;#39;: {}, &amp;#39;callback_events&amp;#39;: [], &amp;#39;metadata&amp;#39;: {&amp;#39;run_id&amp;#39;: &amp;#39;acdd089d-3c80-4624-8122-17c4173dc1ec&amp;#39;}} &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Any ideas? For more info, check out the langserve playground output: &lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/1fc9vbfcav4c1.png?width=1930&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e8bd8119978ed74ebe9d1b8d453b77263fbc3701&quot;&gt;https://preview.redd.it/1fc9vbfcav4c1.png?width=1930&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e8bd8119978ed74ebe9d1b8d453b77263fbc3701&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/SirEliteKaffee&quot;&gt; /u/SirEliteKaffee &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18cun5p/langserve_stream_works_invoke_doesnt/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18cun5p/langserve_stream_works_invoke_doesnt/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_18cun5p</id><media:thumbnail url="https://a.thumbs.redditmedia.com/B-LXWm_EcnpH-dG-Ja03T-WcTOibRuhMkkOlg6KoQI8.jpg" /><link href="https://www.reddit.com/r/LangChain/comments/18cun5p/langserve_stream_works_invoke_doesnt/" /><updated>2023-12-07T12:33:59+00:00</updated><published>2023-12-07T12:33:59+00:00</published><title>LangServe: Stream works, Invoke doesn't</title></entry><entry><author><name>/u/RayMallick</name><uri>https://www.reddit.com/user/RayMallick</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;First time looking into LangChain and vector dbs. I have been creating with some fun applications with LLMs so I have some understanding of how they work and how to interface with them. &lt;/p&gt; &lt;p&gt;Reading through the LangChan doc, I&amp;#39;m trying to get an understanding of how vector dbs affect the prompt? To early understandings, to me it seems like using a vector db would increase the tokens used by LLM in the prompts and thus the cost (if using an API, like Open AI). &lt;/p&gt; &lt;p&gt;Can anyone provide any further insight? Is this correct?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/RayMallick&quot;&gt; /u/RayMallick &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18cpx8j/does_using_a_vector_db_increase_llm_api_cost/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18cpx8j/does_using_a_vector_db_increase_llm_api_cost/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18cpx8j</id><link href="https://www.reddit.com/r/LangChain/comments/18cpx8j/does_using_a_vector_db_increase_llm_api_cost/" /><updated>2023-12-07T06:59:09+00:00</updated><published>2023-12-07T06:59:09+00:00</published><title>Does using a vector db increase LLM API cost?</title></entry><entry><author><name>/u/learning_hedonism</name><uri>https://www.reddit.com/user/learning_hedonism</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Basically I want to have my llm do research for me. Would be nice to have some sort of feedback system rather than just dumb for loops. &lt;/p&gt; &lt;p&gt;Any advice?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/learning_hedonism&quot;&gt; /u/learning_hedonism &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18cv1ds/any_langchain_integrations_that_search_and_crawl/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18cv1ds/any_langchain_integrations_that_search_and_crawl/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18cv1ds</id><link href="https://www.reddit.com/r/LangChain/comments/18cv1ds/any_langchain_integrations_that_search_and_crawl/" /><updated>2023-12-07T12:57:48+00:00</updated><published>2023-12-07T12:57:48+00:00</published><title>Any langchain integrations that search and crawl?</title></entry></feed>