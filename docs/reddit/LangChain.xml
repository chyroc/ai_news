<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/"><category term="LangChain" label="r/LangChain"/><updated>2024-04-10T12:37:09+00:00</updated><icon>https://www.redditstatic.com/icon.png/</icon><id>/r/LangChain.rss</id><link rel="self" href="https://www.reddit.com/r/LangChain.rss" type="application/atom+xml" /><link rel="alternate" href="https://www.reddit.com/r/LangChain" type="text/html" /><subtitle>LangChain is an open-source framework and developer toolkit that helps developers get LLM applications from prototype to production. It is available for Python and Javascript at https://www.langchain.com/.</subtitle><title>LangChain</title><entry><author><name>/u/zchaarm</name><uri>https://www.reddit.com/user/zchaarm</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;A place for members of &lt;a href=&quot;/r/LangChain&quot;&gt;r/LangChain&lt;/a&gt; to chat with each other&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/zchaarm&quot;&gt; /u/zchaarm &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_10ljho9</id><link href="https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/" /><updated>2023-01-26T04:33:05+00:00</updated><published>2023-01-26T04:33:05+00:00</published><title>r/LangChain Lounge</title></entry><entry><author><name>/u/Dear_Insect_5295</name><uri>https://www.reddit.com/user/Dear_Insect_5295</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I am using a Mistral model 4b and huggingface&lt;/p&gt; &lt;pre&gt;&lt;code&gt;pipeline text_generation_pipeline = pipeline( model=model, tokenizer=tokenizer, task=&amp;quot;text-generation&amp;quot;, batch_size=2 ) llm = HuggingFacePipeline(pipeline=text_generation_pipeline,batch_size=2) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;and then using RAG through langchain&lt;/p&gt; &lt;pre&gt;&lt;code&gt;rag_chain_from_docs = ( RunnablePassthrough.assign(context=(lambda x: format_docs(x[&amp;quot;context&amp;quot;]))) | prompt | llm | StrOutputParser() ) rag_chain_with_source = RunnableParallel( {&amp;quot;context&amp;quot;: retriever, &amp;quot;question&amp;quot;: RunnablePassthrough()} ).assign(answer=rag_chain_from_docs) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;My GPU (T4) is underutilized, its only 8GB/16GB. so I want to use all of My GPU, Is there any way to do this, I tried chain.batch() but it did not work(It still ran sequentially). Any suggestions would be helpful to run the chain concurrently.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Dear_Insect_5295&quot;&gt; /u/Dear_Insect_5295 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c0k4oh/how_to_make_use_of_complete_gpu_memory/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c0k4oh/how_to_make_use_of_complete_gpu_memory/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1c0k4oh</id><link href="https://www.reddit.com/r/LangChain/comments/1c0k4oh/how_to_make_use_of_complete_gpu_memory/" /><updated>2024-04-10T12:16:02+00:00</updated><published>2024-04-10T12:16:02+00:00</published><title>How to make use of Complete GPU memory?</title></entry><entry><author><name>/u/Proof-Character-9828</name><uri>https://www.reddit.com/user/Proof-Character-9828</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I am quite new to LangChain and Python as im mainly doing C# but i am interested in using AI on my own data.&lt;br/&gt; So i wrote some python code using langchain that:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Gets my Emails via IMAP&lt;/li&gt; &lt;li&gt;Creates JSON from my E-Mails (JSONLoader)&lt;/li&gt; &lt;li&gt;Creates a Vectordatabase where each mail is a vector (FAISS, OpenAIEmbeddings)&lt;/li&gt; &lt;li&gt;Does a similarity search according to the query returning the 3 mails that match the query the most&lt;/li&gt; &lt;li&gt;feeds the result of the similarity search to the LLM (GPT 3.5 Turbo) using the query AGAIN&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;The LLM Prompt then looks something like:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;The question is &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;{query}&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Here are some information that can help you to answer the question: &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;{similarity_search_result}&lt;/p&gt; &lt;p&gt;Ok so far so good... when my question is:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;When was my last mail sent to xyz@gmail.com? &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;i get a correct answer... -&amp;gt; e.g last mail received 10.04.2024 14:11 &lt;/p&gt; &lt;p&gt;But what if i want to have an answer to the following question&lt;/p&gt; &lt;pre&gt;&lt;code&gt;How many mails have been sent by xyz@gmail.com? &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Because the similarity search only gets the vectors that are most similar, how can i just get an answer about the amount?&lt;br/&gt; Even if the similarity search would deliver 150 mails instead of 3 sent by [&lt;a href=&quot;mailto:xyz@gmail.com&quot;&gt;xyz@gmail.com&lt;/a&gt;](mailto:&lt;a href=&quot;mailto:xyz@gmail.com&quot;&gt;xyz@gmail.com&lt;/a&gt;) i cant just feed them all into the LLM prompt right? &lt;/p&gt; &lt;p&gt;So what is my mistake here?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Proof-Character-9828&quot;&gt; /u/Proof-Character-9828 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c0k2qo/langchain_emails_with_llm/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c0k2qo/langchain_emails_with_llm/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1c0k2qo</id><link href="https://www.reddit.com/r/LangChain/comments/1c0k2qo/langchain_emails_with_llm/" /><updated>2024-04-10T12:13:13+00:00</updated><published>2024-04-10T12:13:13+00:00</published><title>LangChain E-Mails with LLM</title></entry><entry><author><name>/u/arjavparikh</name><uri>https://www.reddit.com/user/arjavparikh</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I am a non-tech person looking for a tool to ask questions to my 50GB worth of PDF files. Is there a tool which can help me build this project or something which is already there which can help? &lt;/p&gt; &lt;p&gt;Please share relevant blogs or approaches to follow. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/arjavparikh&quot;&gt; /u/arjavparikh &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c0czng/is_there_a_toolplatform_to_put_an_llm_to_a_large/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c0czng/is_there_a_toolplatform_to_put_an_llm_to_a_large/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1c0czng</id><link href="https://www.reddit.com/r/LangChain/comments/1c0czng/is_there_a_toolplatform_to_put_an_llm_to_a_large/" /><updated>2024-04-10T04:32:23+00:00</updated><published>2024-04-10T04:32:23+00:00</published><title>Is there a tool/platform to put an LLM to a large data set of PDF files (like 50GB)</title></entry><entry><author><name>/u/PresentationSevere89</name><uri>https://www.reddit.com/user/PresentationSevere89</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;m a bit frustrated. We&amp;#39;ve been working for more than 6 months on an MVP for a Q&amp;amp;A chat about product documentation. After all that, the LLM still hallucinates a lot and gives very basic responses. I would love to have, at this time, a system capable of using several documents to formulate a sound response to a user&amp;#39;s question. I&amp;#39;m using GPT-3.5. I know how capable the model is, and I hate how basic our chat answers are. Maybe it&amp;#39;s the chain, the steps to formulate a response and validate it, or the bad retriever that can&amp;#39;t bring useful documents from the user&amp;#39;s reduced query... I feel like we&amp;#39;ve tried a lot: few shots, fine-tuning embeddings, fine-tuning the GPT, etc... But somehow, we don&amp;#39;t get it to work. I just feel we can, but in the end, we don&amp;#39;t. Any advice to make a killer LLM-powered chat about product documentation?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/PresentationSevere89&quot;&gt; /u/PresentationSevere89 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c0cnyj/easiest_way_to_improve_rag_chat_help/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c0cnyj/easiest_way_to_improve_rag_chat_help/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1c0cnyj</id><link href="https://www.reddit.com/r/LangChain/comments/1c0cnyj/easiest_way_to_improve_rag_chat_help/" /><updated>2024-04-10T04:13:47+00:00</updated><published>2024-04-10T04:13:47+00:00</published><title>Easiest way to improve RAG chat - help</title></entry><entry><author><name>/u/brownstake</name><uri>https://www.reddit.com/user/brownstake</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;How are startups with limited capital able to build their own LLM models?&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://www.ycombinator.com/blog/building-ai-models&quot;&gt;https://www.ycombinator.com/blog/building-ai-models&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/brownstake&quot;&gt; /u/brownstake &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c0h6qx/cash_constrained_companies_building_their_own_llm/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c0h6qx/cash_constrained_companies_building_their_own_llm/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1c0h6qx</id><link href="https://www.reddit.com/r/LangChain/comments/1c0h6qx/cash_constrained_companies_building_their_own_llm/" /><updated>2024-04-10T09:09:55+00:00</updated><published>2024-04-10T09:09:55+00:00</published><title>Cash constrained companies building their own LLM models. How?</title></entry><entry><author><name>/u/cryptokaykay</name><uri>https://www.reddit.com/user/cryptokaykay</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c0do04/results_of_evaluating_the_ai_oracle_approach_a/&quot;&gt; &lt;img src=&quot;https://b.thumbs.redditmedia.com/GCdPwBj4teGkowjNA4iTF6pGHJldADy1b4cFZ_8gtFI.jpg&quot; alt=&quot;Results of evaluating the AI Oracle approach - a novel way to improve your LLM application's accuracy&quot; title=&quot;Results of evaluating the AI Oracle approach - a novel way to improve your LLM application's accuracy&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Recently, I saw this tweet about the AI Oracle approach for improving the accuracy and quality of responses for your LLM application. The technique is super simple:&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://twitter.com/mattshumer_/status/1777382373283299365&quot;&gt;https://twitter.com/mattshumer_/status/1777382373283299365&lt;/a&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Send the request to 3 LLMs - Claude, GPT4, and Perplexity.&lt;/li&gt; &lt;li&gt;Give the responses to Claude again and prompt engineer to pick the best and accurate response.&lt;br/&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;I got curious about this and decided to do some evaluations on this approach. Sharing some metrics/measurements in this post.&lt;/p&gt; &lt;p&gt;This one is pretty obvious, the latency on having all 3 LLMs generate a response and picking the best out of the 3 is high. But, I do recognize that this can be improved by parallelizing the operations. &lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/lndc3gwp3ltc1.png?width=1200&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=02ec4109e9a3211724686a40ecbb9110dc70033c&quot;&gt;https://preview.redd.it/lndc3gwp3ltc1.png?width=1200&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=02ec4109e9a3211724686a40ecbb9110dc70033c&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Ran the following tests for both the combined AI Oracle approach and using a single LLM:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;Factual Accuracy&lt;/strong&gt; - Evaluated for correctness of responses.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;Realtime data&lt;/strong&gt; - Evaluated based on asking information related to realtime data.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;Adversarial Testing&lt;/strong&gt; - Evaluated on whether the LLM is able to pickup the signal correctly by placing the question in between a bunch of garbage data. The LLM was given a positive score if it correctly responded to the question without mentioning the garbage data. &lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;Consistency checks&lt;/strong&gt; - Evaluated on whether the LLM gave a response consistently when the same question was asking many times. Mainly looked for structural consistency of the response.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;Quality&lt;/strong&gt; - Evaluated on the quality - sentence structure, adherence to the prompt etc. &lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;AI Oracle Approach&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Results for the AI Oracle approach: For some reason, it could not pick up the realtime information even once. I am sure with some prompt engineering, this metric can be improved. It did poorly on Adversarial testing - mostly because Claude and Pplx&amp;#39;s responses. &lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/9ggjkthz3ltc1.png?width=1200&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=033d105e3793e68fb5b2bd9a134cbbefec423cc7&quot;&gt;https://preview.redd.it/9ggjkthz3ltc1.png?width=1200&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=033d105e3793e68fb5b2bd9a134cbbefec423cc7&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Claude (claude-3-opus-20240229)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;As expected, Claude did not do well on Realtime testing. But, interestingly, it did not do great with adversarial and consistency tests either.&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/b33q8qz24ltc1.png?width=1200&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ef154b780655bb520c406d1aa53c8b91fc2c8038&quot;&gt;https://preview.redd.it/b33q8qz24ltc1.png?width=1200&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ef154b780655bb520c406d1aa53c8b91fc2c8038&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;GPT4&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Again, GPT4 does not have realtime capabilities. But it did extremely well on everything else except consistency checks where the responses were structured quite differently each time.&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/2jz4y6864ltc1.png?width=1200&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a35bfe557f4164251c4900d4e2c62cf8a5c7b04d&quot;&gt;https://preview.redd.it/2jz4y6864ltc1.png?width=1200&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a35bfe557f4164251c4900d4e2c62cf8a5c7b04d&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Perplexity (pplx-70b-online)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;As expected Perplexity&amp;#39;s realtime capabilities are unmatched. But, it did not do that well with adversarial and consistency tests which in turn skewed the metrics for AI Oracle approach as well.Notably, the quality of responses from Perplexity were far better than the rest.&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/y0mqsr4b4ltc1.png?width=1200&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3ba1acfd7159b8f0bd5164a0e99af7f8ab4f5071&quot;&gt;https://preview.redd.it/y0mqsr4b4ltc1.png?width=1200&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3ba1acfd7159b8f0bd5164a0e99af7f8ab4f5071&lt;/a&gt;&lt;/p&gt; &lt;p&gt;In conclusion, you can get a near perfect score for the AI Oracle approach with a bit of prompt engineering. But you definitely lose performance in the process. Even when parallelized, it is only as slow as the slowest LLM. Token usage/cost is also going to be higher.&lt;/p&gt; &lt;p&gt;Finally, if you are curious, all these evaluations were done using Langtrace - an open source LLM monitoring and evaluations tool that I am currently developing.&lt;/p&gt; &lt;p&gt;Github: &lt;a href=&quot;https://github.com/Scale3-Labs/langtrace&quot;&gt;https://github.com/Scale3-Labs/langtrace&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/cryptokaykay&quot;&gt; /u/cryptokaykay &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c0do04/results_of_evaluating_the_ai_oracle_approach_a/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c0do04/results_of_evaluating_the_ai_oracle_approach_a/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1c0do04</id><media:thumbnail url="https://b.thumbs.redditmedia.com/GCdPwBj4teGkowjNA4iTF6pGHJldADy1b4cFZ_8gtFI.jpg" /><link href="https://www.reddit.com/r/LangChain/comments/1c0do04/results_of_evaluating_the_ai_oracle_approach_a/" /><updated>2024-04-10T05:11:47+00:00</updated><published>2024-04-10T05:11:47+00:00</published><title>Results of evaluating the AI Oracle approach - a novel way to improve your LLM application's accuracy</title></entry><entry><author><name>/u/Big-Big354</name><uri>https://www.reddit.com/user/Big-Big354</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi all,&lt;/p&gt; &lt;p&gt;I am using RetrievalQA chain with custom prompt. When invoked with a question it is returning prompt with answer embedded in it even when the return_only_outputs is set to True.&lt;/p&gt; &lt;p&gt;I was wondering how can I get only the generated answer without the prompt (System message + Context + Question)?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Big-Big354&quot;&gt; /u/Big-Big354 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c06txb/retrievalqa_chain_returning_generated_answer/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c06txb/retrievalqa_chain_returning_generated_answer/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1c06txb</id><link href="https://www.reddit.com/r/LangChain/comments/1c06txb/retrievalqa_chain_returning_generated_answer/" /><updated>2024-04-09T23:32:50+00:00</updated><published>2024-04-09T23:32:50+00:00</published><title>RetrievalQA chain returning generated answer embedded in prompt even when return_only_outputs=True</title></entry><entry><author><name>/u/VegetableAddendum888</name><uri>https://www.reddit.com/user/VegetableAddendum888</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;So guys there’s vectara’s upcoming hackathon,anybody interested to participate and needs a team.DM me…&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/VegetableAddendum888&quot;&gt; /u/VegetableAddendum888 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c00e4g/need_teammates_for_a_rag_hackathon/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c00e4g/need_teammates_for_a_rag_hackathon/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1c00e4g</id><link href="https://www.reddit.com/r/LangChain/comments/1c00e4g/need_teammates_for_a_rag_hackathon/" /><updated>2024-04-09T19:09:43+00:00</updated><published>2024-04-09T19:09:43+00:00</published><title>Need teammates for a RAG hackathon</title></entry><entry><author><name>/u/isthatashark</name><uri>https://www.reddit.com/user/isthatashark</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/isthatashark&quot;&gt; /u/isthatashark &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://vectorize.io/what-is-a-vector-database/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c000jh/the_ultimate_guide_to_vector_database_success_in/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1c000jh</id><link href="https://www.reddit.com/r/LangChain/comments/1c000jh/the_ultimate_guide_to_vector_database_success_in/" /><updated>2024-04-09T18:54:40+00:00</updated><published>2024-04-09T18:54:40+00:00</published><title>The Ultimate Guide To Vector Database Success In AI</title></entry><entry><author><name>/u/mehul_gupta1997</name><uri>https://www.reddit.com/user/mehul_gupta1997</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/mehul_gupta1997&quot;&gt; /u/mehul_gupta1997 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;/r/ArtificialInteligence/comments/1bzuuov/tested_code_gemma_by_google/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bzuxvz/tested_code_gemma_by_google/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bzuxvz</id><link href="https://www.reddit.com/r/LangChain/comments/1bzuxvz/tested_code_gemma_by_google/" /><updated>2024-04-09T15:26:22+00:00</updated><published>2024-04-09T15:26:22+00:00</published><title>Tested Code Gemma by Google</title></entry><entry><author><name>/u/anderl1980</name><uri>https://www.reddit.com/user/anderl1980</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I’d be interested in whether anyone here is using LangChain’s SQL Agent (or similar self-built agents with LangChain or autogen). I’d love to conenct to learn from your experiences as I have not seen it be used in productive systems yet!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/anderl1980&quot;&gt; /u/anderl1980 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bzn1yw/sql_agent_in_production/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bzn1yw/sql_agent_in_production/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bzn1yw</id><link href="https://www.reddit.com/r/LangChain/comments/1bzn1yw/sql_agent_in_production/" /><updated>2024-04-09T08:29:57+00:00</updated><published>2024-04-09T08:29:57+00:00</published><title>SQL Agent in production?</title></entry><entry><author><name>/u/bwenneker</name><uri>https://www.reddit.com/user/bwenneker</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;m building several chat based apps with LangChain for clients. I&amp;#39;m asking for feedback with each answer, users can leave a 👍 or 👎.&lt;/p&gt; &lt;p&gt;Often I get the question: &amp;quot;does this &amp;#39;self-improve&amp;#39;?&amp;quot;&lt;/p&gt; &lt;p&gt;This got me thinking, why not use the positive feedback to improve future answers? Has anyone tried something like this:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Store (positive) user feedback in a VectorDB with questions-answer pairs.&lt;/li&gt; &lt;li&gt;When a new question is asked, run the usual pipeline (RAG for example).&lt;/li&gt; &lt;li&gt;Then also query the feedback VectorDB and add the top-k feedback question-answer pairs with high relevance to the question and add it as extra context.&lt;/li&gt; &lt;li&gt;Let the LLM answer the question using the context and top-k feedback items.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Looking forward to your experience, otherwise I might build this, it doesn&amp;#39;t seem to hard to make.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/bwenneker&quot;&gt; /u/bwenneker &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bzntdm/using_user_feedback_to_optimize_rag/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bzntdm/using_user_feedback_to_optimize_rag/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bzntdm</id><link href="https://www.reddit.com/r/LangChain/comments/1bzntdm/using_user_feedback_to_optimize_rag/" /><updated>2024-04-09T09:25:46+00:00</updated><published>2024-04-09T09:25:46+00:00</published><title>Using user feedback to optimize RAG</title></entry><entry><author><name>/u/mehul_gupta1997</name><uri>https://www.reddit.com/user/mehul_gupta1997</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Checkout how you can leverage Multi-Agent Orchestration for developing an auto Interview system where the Interviewer asks questions to interviewee, evaluates it and eventually shares whether the candidate should be selected or not. Right now, both interviewer and interviewee are played by AI agents. &lt;a href=&quot;https://youtu.be/VrjqR4dIawo?si=1sMYs7lI-c8WZrwP&quot;&gt;https://youtu.be/VrjqR4dIawo?si=1sMYs7lI-c8WZrwP&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/mehul_gupta1997&quot;&gt; /u/mehul_gupta1997 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bzkzkt/multiagent_interview_using_langgraph/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bzkzkt/multiagent_interview_using_langgraph/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bzkzkt</id><link href="https://www.reddit.com/r/LangChain/comments/1bzkzkt/multiagent_interview_using_langgraph/" /><updated>2024-04-09T06:07:23+00:00</updated><published>2024-04-09T06:07:23+00:00</published><title>Multi-Agent Interview using LangGraph</title></entry><entry><author><name>/u/Chrex_007</name><uri>https://www.reddit.com/user/Chrex_007</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;So I have a requirement of being able to chat with csv files and when the chatbot can&amp;#39;t find any relevant information from the csv files it should use the Bing API to search on the web and gather information and answer. I tried to make a custom langchain agent with Bing API as a tool but it&amp;#39;s not able to perform the observation, action loop, the model I&amp;#39;m using is Mistral-7B-Instruct-v0.1 which I can&amp;#39;t change I think model is not powerful enough for this task. But still does anybody have idea how can I make this possible? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Chrex_007&quot;&gt; /u/Chrex_007 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bzmovg/how_to_create_a_chatbot_to_chat_with_csv_files/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bzmovg/how_to_create_a_chatbot_to_chat_with_csv_files/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bzmovg</id><link href="https://www.reddit.com/r/LangChain/comments/1bzmovg/how_to_create_a_chatbot_to_chat_with_csv_files/" /><updated>2024-04-09T08:02:54+00:00</updated><published>2024-04-09T08:02:54+00:00</published><title>How to create a chatbot to chat with csv files and internet (Bing API)?</title></entry><entry><author><name>/u/ramirez_tn</name><uri>https://www.reddit.com/user/ramirez_tn</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I would like interact with LLMs in sequential way like:&lt;/p&gt; &lt;p&gt;Step 1 : load documents Step 2 : ask about the products described in the documents Step 3: based on the response lookup where can I buy these products Step 4: check if the store has other options …&lt;/p&gt; &lt;p&gt;I am currently doing it in a very basic way. Loading the documents, retrieving using a question, using the output as input for another retriever , … Is there a more sophisticated way of doing it ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/ramirez_tn&quot;&gt; /u/ramirez_tn &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bzsgxa/interacting_with_llms_in_steps/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bzsgxa/interacting_with_llms_in_steps/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bzsgxa</id><link href="https://www.reddit.com/r/LangChain/comments/1bzsgxa/interacting_with_llms_in_steps/" /><updated>2024-04-09T13:41:04+00:00</updated><published>2024-04-09T13:41:04+00:00</published><title>Interacting with LLMs in steps</title></entry><entry><author><name>/u/IlEstLaPapi</name><uri>https://www.reddit.com/user/IlEstLaPapi</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;tldr: Some insights and learnings from a LLM enthusiast working on a complex Chatbot using multiple agents built with LangGraph, LCEL and Chainlit.&lt;/p&gt; &lt;p&gt;Hi everyone! I have seen a lot of interest in multi-agent systems recently, and, as I&amp;#39;m currently working on a complex one, I thought I might as well share some feedback on my project. Maybe some of you might find it interesting, give some useful feedback, or make some suggestions.&lt;/p&gt; &lt;h2&gt;Introduction: Why am I doing this project?&lt;/h2&gt; &lt;p&gt;I&amp;#39;m a business owner and a tech guy with a background in math, coding, and ML. Since early 2023, I&amp;#39;ve fallen in love with the LLM world. So, I decided to start a new business with 2 friends: a consulting firm on generative AI. As expected, we don&amp;#39;t have many references. Thus, we decided to create a tool to demonstrate our skillset to potential clients.&lt;/p&gt; &lt;p&gt;After a brainstorm, we quickly identified that a) RAG is the main selling point, so we need something that uses a RAG; b) We believe in agents to automate tasks; c) ChatGPT has shown that asking questions to a chatbot is a much more human-friendly interface than a website; d) Our main weakness is that we are all tech guys, so we might as well compensate for that by building a seller.&lt;/p&gt; &lt;p&gt;From here, the idea was clear: instead, or more exactly, alongside our website, build a chatbot that would answer questions about our company, &amp;quot;sell&amp;quot; our offer, and potentially schedule meetings with our consultants. Then make some posts on LinkedIn and pray...&lt;/p&gt; &lt;p&gt;Spoiler alert: This project isn&amp;#39;t finished yet. The idea is to share some insights and learnings with the community and get some feedback.&lt;/p&gt; &lt;h2&gt;Functional specifications&lt;/h2&gt; &lt;p&gt;The first step was to list some specifications: * We want a RAG that can answer any question the user might have about our company. For that, we will use the content of the company website. Of course, we also need to prevent hallucination, especially on two topics: the website has no information about pricing, and we don&amp;#39;t offer SLAs. * We want it to answer as quickly as possible and limit the budget. For that, we will use smaller models like GPT-3.5 and Claude Haiku as often as possible. But that limits the reasoning capabilities of our agents, so we need to find a sweet spot. * We want consistency in the responses, which is a big problem for RAGs. Questions with similar meanings should generate the same answers, for example, &amp;quot;What&amp;#39;s your offer?&amp;quot;, &amp;quot;What services do you provide?&amp;quot;, and &amp;quot;What do you do?&amp;quot;. * Obviously, we don&amp;#39;t want visitors to be able to ask off-topic questions (e.g., &amp;quot;How is the weather in North Carolina?&amp;quot;), so we need a way to filter out off-topic, prompt injection, and toxic questions. * We want to demonstrate that GenAI can be used to deliver more than just chatbots, so we want the agents to be able to schedule meetings, send emails to visitors, etc. * Ideally, we also want the agents to be able to qualify the visitor: who they are, what their job is, what their organization is, whether they are a tech person or a manager, and if they are looking for something specific with a defined need or are just curious about us. * Ideally, we also want the agents to &amp;quot;sell&amp;quot; our company: if the visitor indicates their need, match it with our offer and &amp;quot;push&amp;quot; that offer. If they show some interest, let&amp;#39;s &amp;quot;push&amp;quot; for a meeting with our consultants!&lt;/p&gt; &lt;h2&gt;Architecture&lt;/h2&gt; &lt;h3&gt;Stack&lt;/h3&gt; &lt;p&gt;We aren&amp;#39;t a startup, we haven&amp;#39;t raised funds, and we don&amp;#39;t have months to do this. We can&amp;#39;t afford to spend more than 20 days to get an MVP. Besides, our main selling point is that GenAI projects don&amp;#39;t require as much time or budget as ML ones.&lt;/p&gt; &lt;p&gt;So, in order to move fast, we needed to use some open-source frameworks: * For the chatbot, the data is public, so let&amp;#39;s use GPT and Claude as they are the best right now and the API cost is low. * For the chatbot, Chainlit provides everything we need, except background processing. Let&amp;#39;s use that. * Langchain and LCEL are both flexible and unify the interfaces with the LLMs. * We&amp;#39;ll need a rather complicated agent workflow, in fact, multiple ones. LangGraph is more flexible than crew.ai or autogen. Let&amp;#39;s use that!&lt;/p&gt; &lt;h3&gt;Design and early versions&lt;/h3&gt; &lt;h4&gt;First version&lt;/h4&gt; &lt;p&gt;From the start, we knew it was impossible to do it using a &amp;quot;one prompt, one agent&amp;quot; solution. So we started with a 3-agent solution: one to &amp;quot;find&amp;quot; the required elements on our website (a RAG), one to sell and set up meetings, and one to generate the final answer.&lt;/p&gt; &lt;p&gt;The meeting logic was very easy to implement. However, as expected, the chatbot was hallucinating a lot: &amp;quot;Here is a full project for 1k€, with an SLA 7/7 2 hours 99.999%&amp;quot;. And it was a bad seller, with conversations such as &amp;quot;Hi, who are you?&amp;quot; &amp;quot;I&amp;#39;m Sellbotix, how can I help you? Do you want a meeting with one of our consultants?&amp;quot;&lt;/p&gt; &lt;p&gt;At this stage, after 10 hours of work, we knew that it was probably doable but would require much more than 3 agents.&lt;/p&gt; &lt;h4&gt;Second version&lt;/h4&gt; &lt;p&gt;The second version used a more complex architecture: a guard to filter the questions, a strategist to make a plan, a seller to find some selling points, a seeker and a documentalist for the RAG, a secretary for the schedule meeting function, and a manager to coordinate everything.&lt;/p&gt; &lt;p&gt;It was slow, so we included logic to distribute the work between the agents in parallel. Sadly, this can&amp;#39;t be implemented using LangGraph, as all agent calls are made using coroutines but are awaited, and you can&amp;#39;t have parallel branches. So we implemented our own logic.&lt;/p&gt; &lt;p&gt;The result was much better, but far from perfect. And it was a nightmare to improve because changing one agent&amp;#39;s system prompt would generate side effects on most of the other agents. We also had a hard time defining what each agent would need to see and what to hide. Sending every piece of information to every agent is a waste of time and tokens.&lt;/p&gt; &lt;p&gt;And last but not least, the codebase was a mess as we did it in a rush. So we decided to restart from scratch.&lt;/p&gt; &lt;h2&gt;Third version, WIP&lt;/h2&gt; &lt;p&gt;So currently, we are working on the third version. This project is, by far, much more ambitious than what most of our clients ask us to do (another RAG?). And so far, we have learned a ton. I honestly don&amp;#39;t know if we will finish it, or even if it&amp;#39;s realistic, but it was worth it. &amp;quot;It isn&amp;#39;t the destination that matters, it&amp;#39;s the journey&amp;quot; has rarely been so true.&lt;/p&gt; &lt;p&gt;Currently, we are working on the architecture, and we have nearly finished it. Here are a few insights that we are using, and I wanted to share with you.&lt;/p&gt; &lt;h3&gt;Separation of concern&lt;/h3&gt; &lt;p&gt;The two main difficulties when working with a network of agents are a) they don&amp;#39;t know when to stop, and b) any change to any agent&amp;#39;s system prompt impacts the whole system. It&amp;#39;s hard to fix. When building a complex system, separation of concern is key: agents must be split into groups, each one with clear responsibilities and interfaces.&lt;/p&gt; &lt;p&gt;The cool thing is that a LangGraph graph is also a Runnable, so you can build graphs that use graphs. So we ended up with this: a main graph for the guard and final answer logic. It calls a &amp;quot;think&amp;quot; graph that decides which subgraphs should be called. Those are a &amp;quot;sell&amp;quot; graph, a &amp;quot;handle&amp;quot; graph, and a &amp;quot;find&amp;quot; graph (so far).&lt;/p&gt; &lt;h3&gt;Async, parallelism, and conditional calls&lt;/h3&gt; &lt;p&gt;If you want a system to be fast, you need to NOT call all the agents every time. For that, you need two things: a planner that decides which subgraph should be called (in our think graph), and you need to use &lt;code&gt;asyncio.gather&lt;/code&gt; instead of letting LangGraph call every graph and await them one by one.&lt;/p&gt; &lt;p&gt;So in the think graph, we have planner and manager agents. We use a standard doer/critic pattern here. When they agree on what needs to be done, they generate a list of instructions and activation orders for each subgraph that are passed to a &amp;quot;do&amp;quot; node. This node then creates a list of coroutines and awaits an &lt;code&gt;asyncio.gather&lt;/code&gt;.&lt;/p&gt; &lt;h3&gt;Limit what each graph must see&lt;/h3&gt; &lt;p&gt;We want the system to be fast and cost-efficient. Every node of every subgraph doesn&amp;#39;t need to be aware of what every other agent does. So we need to decide exactly what each agent gets as input. That&amp;#39;s honestly quite hard, but doable. It means fewer tokens, so it reduces the cost and speeds up the response.&lt;/p&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;This post is already quite long, so I won&amp;#39;t go into the details of every subgraph here. However, if you&amp;#39;re interested, feel free to let me know. I might decide to write some additional posts about those and the specific challenges we encountered and how we solved them (or not). In any case, if you&amp;#39;ve read this far, thank you!&lt;/p&gt; &lt;p&gt;If you have any feedback, don&amp;#39;t hesitate to share. I&amp;#39;d be very happy to read your thoughts and suggestions!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/IlEstLaPapi&quot;&gt; /u/IlEstLaPapi &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1byz3lr/insights_and_learnings_from_building_a_complex/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1byz3lr/insights_and_learnings_from_building_a_complex/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1byz3lr</id><link href="https://www.reddit.com/r/LangChain/comments/1byz3lr/insights_and_learnings_from_building_a_complex/" /><updated>2024-04-08T14:20:55+00:00</updated><published>2024-04-08T14:20:55+00:00</published><title>Insights and Learnings from Building a Complex Multi-Agent System</title></entry><entry><author><name>/u/sarthak_uchiha</name><uri>https://www.reddit.com/user/sarthak_uchiha</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;How can we prevent data poision in llms , for example if our database it self is corrupt and we need llm not to send that data , how can we achieve that &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/sarthak_uchiha&quot;&gt; /u/sarthak_uchiha &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bzpcc6/data_poision_in_llms/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bzpcc6/data_poision_in_llms/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bzpcc6</id><link href="https://www.reddit.com/r/LangChain/comments/1bzpcc6/data_poision_in_llms/" /><updated>2024-04-09T11:04:04+00:00</updated><published>2024-04-09T11:04:04+00:00</published><title>Data poision in llms</title></entry><entry><author><name>/u/DanShmuelSasha</name><uri>https://www.reddit.com/user/DanShmuelSasha</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;ve been using Langchain and Langsmith to create a benchmarking workflow for my LLM prompts. Everything&amp;#39;s been working well, until I had to delete and re-create my OpenAI API key.&lt;/p&gt; &lt;p&gt;I&amp;#39;ve updated the API key in the env.py as the main environmental variable, and this works- I&amp;#39;ve tested this with the OpenAI Chat Completion.&lt;/p&gt; &lt;p&gt;I&amp;#39;ve also updated the API key in LangSmith by going into the requested prompt&amp;#39;s playground &amp;gt; Secrets &amp;amp; API keys &amp;gt; updated it manually. I even checked under my organization&amp;#39;s Settings &amp;gt; Secrets, and see the API key is updated.&lt;/p&gt; &lt;p&gt;However, when I try to use the &amp;quot;arun_on_dataset&amp;quot; function in the aforementioned Python environment, it gives me an error-&lt;/p&gt; &lt;pre&gt;&lt;code&gt;&amp;quot;AuthenticationError(&amp;quot;Error code: 401 - {&amp;#39;error&amp;#39;: {&amp;#39;message&amp;#39;: &amp;#39;Incorrect API key provided: sk-O6ZSB***************************************TNxS. You can find your API key at https://platform.openai.com/account/api-keys.&amp;quot; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The prefix for the API key the error throws is indeed the previous, non-functional API key, but I don&amp;#39;t know where else to adjust it so it&amp;#39;ll read the current API key.&lt;/p&gt; &lt;p&gt;Nothing was changed in the code, which ran well previously, and the only external adjustment was the API key.&lt;/p&gt; &lt;p&gt;Should I change the API key elsewhere?&lt;/p&gt; &lt;p&gt;Any thoughts and ideas are welcome!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/DanShmuelSasha&quot;&gt; /u/DanShmuelSasha &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bzoc45/openai_api_key_integration_mismatch/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bzoc45/openai_api_key_integration_mismatch/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bzoc45</id><link href="https://www.reddit.com/r/LangChain/comments/1bzoc45/openai_api_key_integration_mismatch/" /><updated>2024-04-09T10:01:49+00:00</updated><published>2024-04-09T10:01:49+00:00</published><title>OpenAI API key integration mismatch</title></entry><entry><author><name>/u/sarthak_uchiha</name><uri>https://www.reddit.com/user/sarthak_uchiha</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I am using redis as vector database . I am getting no permission error when adding text to redis using langchain , manually I am able to add it , but getting error when using langchain , can someone suggest an alternative or how can we achieve it with langchain &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/sarthak_uchiha&quot;&gt; /u/sarthak_uchiha &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bznz3t/redis_as_vector_database/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bznz3t/redis_as_vector_database/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bznz3t</id><link href="https://www.reddit.com/r/LangChain/comments/1bznz3t/redis_as_vector_database/" /><updated>2024-04-09T09:37:09+00:00</updated><published>2024-04-09T09:37:09+00:00</published><title>Redis as vector database</title></entry><entry><author><name>/u/jdogbro12</name><uri>https://www.reddit.com/user/jdogbro12</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bz3vl7/anthropics_haiku_beats_gpt4_turbo_in_tool_use/&quot;&gt; &lt;img src=&quot;https://external-preview.redd.it/4YoBVzPLpUKvo5J-N_JeyaBnqnH19q7OHy5i_W-uThY.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f12d9ca66ee9dbf848967e620d263e7150530256&quot; alt=&quot;Anthropic's Haiku Beats GPT-4 Turbo in Tool Use&quot; title=&quot;Anthropic's Haiku Beats GPT-4 Turbo in Tool Use&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/jdogbro12&quot;&gt; /u/jdogbro12 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://docs.parea.ai/blog/benchmarking-anthropic-beta-tool-use&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bz3vl7/anthropics_haiku_beats_gpt4_turbo_in_tool_use/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1bz3vl7</id><media:thumbnail url="https://external-preview.redd.it/4YoBVzPLpUKvo5J-N_JeyaBnqnH19q7OHy5i_W-uThY.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=f12d9ca66ee9dbf848967e620d263e7150530256" /><link href="https://www.reddit.com/r/LangChain/comments/1bz3vl7/anthropics_haiku_beats_gpt4_turbo_in_tool_use/" /><updated>2024-04-08T17:29:23+00:00</updated><published>2024-04-08T17:29:23+00:00</published><title>Anthropic's Haiku Beats GPT-4 Turbo in Tool Use</title></entry><entry><author><name>/u/onsies</name><uri>https://www.reddit.com/user/onsies</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hello,&lt;/p&gt; &lt;p&gt;I’m very new to LangChain and LLM altogether. Very excited!&lt;/p&gt; &lt;p&gt;I started following a LangChain example: &lt;a href=&quot;https://python.langchain.com/docs/use_cases/question_answering/chat_history/&quot;&gt;https://python.langchain.com/docs/use_cases/question_answering/chat_history/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I modified it to use a DirectoryLoader. Next, I invoked a prompt. The response is narrowed down to a particular chunk when the custom data is split and misses information from other document chunks. For example, the custom data contains information about best practices spread across several pages. My prompt is “list 5 of the best practices”. The response would only show one best practice, while ignoring the other document chunks that contain other practices.&lt;/p&gt; &lt;p&gt;Other example of a prompt is “Summary the most important best practice”. The response seems to randomly pick a document chunk and consider that the most important.&lt;/p&gt; &lt;p&gt;How should I go about ensure that all chunks of every document is used as part of the response?&lt;/p&gt; &lt;p&gt;Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/onsies&quot;&gt; /u/onsies &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bzkkdt/langchain_embeddings/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bzkkdt/langchain_embeddings/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bzkkdt</id><link href="https://www.reddit.com/r/LangChain/comments/1bzkkdt/langchain_embeddings/" /><updated>2024-04-09T05:40:54+00:00</updated><published>2024-04-09T05:40:54+00:00</published><title>LangChain Embeddings</title></entry><entry><author><name>/u/SustainedSuspense</name><uri>https://www.reddit.com/user/SustainedSuspense</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Does an LLM like this exist? Also, will this kill my machine? Im not needing a large model trained on a lot of tokens because I just want it to work with the data I provide it in the context during inference but I the context I have is about 21k. &lt;/p&gt; &lt;p&gt;Can I use Amazon to host a private LLM instead of running locally? Is that what Bedrock offers? &lt;/p&gt; &lt;p&gt;Any insights are appreciated. Thanks.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/SustainedSuspense&quot;&gt; /u/SustainedSuspense &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bzjzk0/im_worried_about_privacy_and_was_wondering_if/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bzjzk0/im_worried_about_privacy_and_was_wondering_if/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bzjzk0</id><link href="https://www.reddit.com/r/LangChain/comments/1bzjzk0/im_worried_about_privacy_and_was_wondering_if/" /><updated>2024-04-09T05:04:54+00:00</updated><published>2024-04-09T05:04:54+00:00</published><title>Im worried about privacy and was wondering if there is an LLM I can run locally on my i7 Mac that has at least a 25k context window?</title></entry><entry><author><name>/u/Gon_Buruwa</name><uri>https://www.reddit.com/user/Gon_Buruwa</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;m building a knowledge management system. I have implemented each individual functionality separately. Since accessing everything separately is cumbersome, I have decided to use a chat interface with function calling. I&amp;#39;m currently using the Mistral 8x7b 4-bit quantized version as my LLM. However, due to hardware limitations, directly performing function calling is slow and does not give the expected results.&lt;/p&gt; &lt;p&gt;Then, I decided to use a semantic-router with OpenAI&amp;#39;s Ada model to classify each input to determine which task it belongs to and then send it to the LLM to extract relevant information. This approach works well, but since I intend to use a local solution, OpenAI&amp;#39;s model is not an option.&lt;/p&gt; &lt;p&gt;I tried using local models, but the faster ones produce suboptimal results, while the better ones are slower. Is there any simple and fast solution that can classify chat messages into the appropriate task? My current tasks include image search, Retrieval Augmented Generation (RAG), document summarization, and document search.&lt;/p&gt; &lt;p&gt;I thought of building a simple classifier using traditional machine learning methods like TF-IDF, Gradient Boosting, Logistic Regression, etc. Would that work? Has anyone done this kind of work before? Thanks in advance.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Gon_Buruwa&quot;&gt; /u/Gon_Buruwa &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bzihid/seeking_advice_simple_and_fast_solution_for/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bzihid/seeking_advice_simple_and_fast_solution_for/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bzihid</id><link href="https://www.reddit.com/r/LangChain/comments/1bzihid/seeking_advice_simple_and_fast_solution_for/" /><updated>2024-04-09T03:43:22+00:00</updated><published>2024-04-09T03:43:22+00:00</published><title>Seeking Advice: Simple and Fast Solution for Classifying Chat Messages into Tasks</title></entry><entry><author><name>/u/tsensei_dev</name><uri>https://www.reddit.com/user/tsensei_dev</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Langchain python had this for some time now, but the typescript implementation lacked this chunking mechanism. So, followed &lt;a href=&quot;https://github.com/FullStackRetrieval-com/RetrievalTutorials/blob/main/tutorials/LevelsOfTextSplitting/5_Levels_Of_Text_Splitting.ipynb&quot;&gt;this&lt;/a&gt; notebook by Greg Kamadt and implemented it myself. &lt;/p&gt; &lt;p&gt;You&amp;#39;re free to use the code :) Everything is documented in jsDoc &lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/tsensei/Semantic-Chunking-Typescript&quot;&gt;https://github.com/tsensei/Semantic-Chunking-Typescript&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/tsensei_dev&quot;&gt; /u/tsensei_dev &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bz3ngb/wrote_a_semantic_chunker_for_rag_pipelines_in/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bz3ngb/wrote_a_semantic_chunker_for_rag_pipelines_in/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bz3ngb</id><link href="https://www.reddit.com/r/LangChain/comments/1bz3ngb/wrote_a_semantic_chunker_for_rag_pipelines_in/" /><updated>2024-04-08T17:20:44+00:00</updated><published>2024-04-08T17:20:44+00:00</published><title>Wrote a semantic chunker for RAG pipelines in Typescript</title></entry><entry><author><name>/u/mehul_gupta1997</name><uri>https://www.reddit.com/user/mehul_gupta1997</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bz06c5/should_i_partner_with_packt_for_my_book_on/&quot;&gt; &lt;img src=&quot;https://preview.redd.it/lhxs7e5ht9tc1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f68902b93adb073db5b44516f1872c67a5bd793e&quot; alt=&quot;Should I partner with Packt for my book on LangChain?&quot; title=&quot;Should I partner with Packt for my book on LangChain?&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Recently I launched my debut book &amp;quot;LangChain in your Pocket: Beginners guide to building Generative AI applications using LLMs&amp;quot; which is going a bestseller since release. Recently, Packt, one of the biggest tech book publishers contacted me for partnering with them for the distribution of the book. As expected, it would reach a wider audience but the price may go up exponentially. What should I do?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/mehul_gupta1997&quot;&gt; /u/mehul_gupta1997 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://i.redd.it/lhxs7e5ht9tc1.png&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bz06c5/should_i_partner_with_packt_for_my_book_on/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1bz06c5</id><media:thumbnail url="https://preview.redd.it/lhxs7e5ht9tc1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=f68902b93adb073db5b44516f1872c67a5bd793e" /><link href="https://www.reddit.com/r/LangChain/comments/1bz06c5/should_i_partner_with_packt_for_my_book_on/" /><updated>2024-04-08T15:05:00+00:00</updated><published>2024-04-08T15:05:00+00:00</published><title>Should I partner with Packt for my book on LangChain?</title></entry></feed>