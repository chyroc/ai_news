<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/"><category term="LangChain" label="r/LangChain"/><updated>2024-08-02T16:07:16+00:00</updated><icon>https://www.redditstatic.com/icon.png/</icon><id>/r/LangChain.rss</id><link rel="self" href="https://www.reddit.com/r/LangChain.rss" type="application/atom+xml" /><link rel="alternate" href="https://www.reddit.com/r/LangChain" type="text/html" /><subtitle>LangChain is an open-source framework and developer toolkit that helps developers get LLM applications from prototype to production. It is available for Python and Javascript at https://www.langchain.com/.</subtitle><title>LangChain</title><entry><author><name>/u/zchaarm</name><uri>https://www.reddit.com/user/zchaarm</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;A place for members of &lt;a href=&quot;/r/LangChain&quot;&gt;r/LangChain&lt;/a&gt; to chat with each other&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/zchaarm&quot;&gt; /u/zchaarm &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_10ljho9</id><link href="https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/" /><updated>2023-01-26T04:33:05+00:00</updated><published>2023-01-26T04:33:05+00:00</published><title>r/LangChain Lounge</title></entry><entry><author><name>/u/DiamantAI</name><uri>https://www.reddit.com/user/DiamantAI</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ei5c64/an_extensive_open_source_collection_of_rag/&quot;&gt; &lt;img src=&quot;https://external-preview.redd.it/iCLGDo3jhRmeznL7H988elYpds0idUBgDvjYppXgpms.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0d6e16a281c37b9984d6be42ca94f786cc09b4c2&quot; alt=&quot;An extensive open source collection of RAG implementations with many different strategies&quot; title=&quot;An extensive open source collection of RAG implementations with many different strategies&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/DiamantAI&quot;&gt; /u/DiamantAI &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://github.com/NirDiamant/RAG_Techniques&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ei5c64/an_extensive_open_source_collection_of_rag/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1ei5c64</id><media:thumbnail url="https://external-preview.redd.it/iCLGDo3jhRmeznL7H988elYpds0idUBgDvjYppXgpms.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=0d6e16a281c37b9984d6be42ca94f786cc09b4c2" /><link href="https://www.reddit.com/r/LangChain/comments/1ei5c64/an_extensive_open_source_collection_of_rag/" /><updated>2024-08-02T09:01:41+00:00</updated><published>2024-08-02T09:01:41+00:00</published><title>An extensive open source collection of RAG implementations with many different strategies</title></entry><entry><author><name>/u/GPT-Claude-Gemini</name><uri>https://www.reddit.com/user/GPT-Claude-Gemini</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Not sure if I am the only one that notice this, but the performance of Gemini on Langchain has been highly unreliable, a few examples:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Gemini&amp;#39;s stream would often just stop midway without ever being completed (making Gemini mostly unuseable)&lt;/li&gt; &lt;li&gt;Can&amp;#39;t get the input/output token count after each Gemini API request&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Is this a problem on Gemini&amp;#39;s side or with the Langchain abstraction? Is there an estimated timeline that these issues can be solved?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/GPT-Claude-Gemini&quot;&gt; /u/GPT-Claude-Gemini &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ei45mq/is_the_poor_performance_of_gemini_on_langchain/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ei45mq/is_the_poor_performance_of_gemini_on_langchain/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1ei45mq</id><link href="https://www.reddit.com/r/LangChain/comments/1ei45mq/is_the_poor_performance_of_gemini_on_langchain/" /><updated>2024-08-02T07:40:37+00:00</updated><published>2024-08-02T07:40:37+00:00</published><title>Is the poor performance of Gemini on Langchain caused by Langchain or Google?</title></entry><entry><author><name>/u/stolendog-1</name><uri>https://www.reddit.com/user/stolendog-1</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hello.&lt;br/&gt; I&amp;#39;m working on implementing a RAG solution on AWS and I&amp;#39;m curious about best practices for document storage (chunks). I&amp;#39;d love to hear about your experiences and preferences.&lt;/p&gt; &lt;p&gt;Specifically, I&amp;#39;m wondering:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Do you keep your document chunks in the same database as your vectors, or do you use separate storage? like S3 object storage? in S3 case you need download chunks each time?&lt;/li&gt; &lt;li&gt;If you use separate storage, what solution do you prefer? (e.g., S3 buckets, document databases, etc.)&lt;/li&gt; &lt;li&gt;For those using combined storage, what vector databases are you using that handle this well? I&amp;#39;m planning to use pg_vector on PostgreSQL&lt;/li&gt; &lt;li&gt;How do you handle metadata and linking between vectors and original documents in your setup?&lt;/li&gt; &lt;li&gt;Any pitfalls or lessons learned you&amp;#39;d be willing to share?&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;I&amp;#39;m particularly interested in solutions that scale well and remain cost-effective then document collection grows.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/stolendog-1&quot;&gt; /u/stolendog-1 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1eibcqw/document_storage_in_rag_solutions_separate_or/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1eibcqw/document_storage_in_rag_solutions_separate_or/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1eibcqw</id><link href="https://www.reddit.com/r/LangChain/comments/1eibcqw/document_storage_in_rag_solutions_separate_or/" /><updated>2024-08-02T14:23:59+00:00</updated><published>2024-08-02T14:23:59+00:00</published><title>Document Storage in RAG solutions: separate or combined with Vector DB?</title></entry><entry><author><name>/u/Top_Raccoon_1493</name><uri>https://www.reddit.com/user/Top_Raccoon_1493</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I am implementing Weaviate from yesterday with docker on langchain, its tough implementing&lt;br/&gt; Can anyone share some tutorials or is it better as compared to Qdrant&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Top_Raccoon_1493&quot;&gt; /u/Top_Raccoon_1493 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1eia9w0/how_to_implement_weaviate_with_docker/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1eia9w0/how_to_implement_weaviate_with_docker/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1eia9w0</id><link href="https://www.reddit.com/r/LangChain/comments/1eia9w0/how_to_implement_weaviate_with_docker/" /><updated>2024-08-02T13:37:49+00:00</updated><published>2024-08-02T13:37:49+00:00</published><title>How to implement Weaviate with docker?</title></entry><entry><author><name>/u/MagentaSpark</name><uri>https://www.reddit.com/user/MagentaSpark</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;h1&gt;TLDR&lt;/h1&gt; &lt;p&gt;What do you use?&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Enums as tool parameter arguments&lt;/li&gt; &lt;li&gt;Arguments transposed as parameters with their value set to Boolean&lt;/li&gt; &lt;/ol&gt; &lt;h1&gt;How to define a tool/function for LLM&lt;/h1&gt; &lt;p&gt;While it is convenient to use the actual function/tool in your code to be sent as tool schema to the LLM and receive arguments, it might not be always be ideal. Here are two common examples:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Tasks like conditional routing where reasoning is done by an LLM is more dependent on tool definition and less code logic.&lt;/li&gt; &lt;li&gt;Database Query filtering: The unreliable (multiple points of failure) Text-to-SQL/DSL workload of databases with low complexity and expectations can be abstracted away with premade query with filter parameters of which, arguments will be computed by LLM.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;How do you define such tools? I see two approaches.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Functions with parameter type as Enum, which confines the number of possible arguments for that parameter&lt;/li&gt; &lt;li&gt;Functions with possible arguments itself as parameters, and the actual arguments as boolean value.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Which approach as worked for you and your LLM/system? Are there any more things you would like to add?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/MagentaSpark&quot;&gt; /u/MagentaSpark &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ei9rbi/tool_calling_patterns_langgraph/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ei9rbi/tool_calling_patterns_langgraph/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1ei9rbi</id><link href="https://www.reddit.com/r/LangChain/comments/1ei9rbi/tool_calling_patterns_langgraph/" /><updated>2024-08-02T13:14:04+00:00</updated><published>2024-08-02T13:14:04+00:00</published><title>Tool calling patterns: LangGraph</title></entry><entry><author><name>/u/lzyTitan412</name><uri>https://www.reddit.com/user/lzyTitan412</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=pLPJoFvq4_M&quot;&gt;LangGraph Studio: The first agent IDE (youtube.com)&lt;/a&gt; -- check this out.&lt;/p&gt; &lt;p&gt;Just a week back, I was thinking of developing a web app kind of interface for langgraph, and they just launched it. Now, what if there were a drag-and-drop-like application for creating a complex graph chain?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/lzyTitan412&quot;&gt; /u/lzyTitan412 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ehp7h5/langgraph_studio_is_amazing/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ehp7h5/langgraph_studio_is_amazing/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1ehp7h5</id><link href="https://www.reddit.com/r/LangChain/comments/1ehp7h5/langgraph_studio_is_amazing/" /><updated>2024-08-01T19:18:21+00:00</updated><published>2024-08-01T19:18:21+00:00</published><title>LangGraph Studio is amazing</title></entry><entry><author><name>/u/Jen1888Mik</name><uri>https://www.reddit.com/user/Jen1888Mik</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I use this code for write embeddings in my database:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;const res = await ConvexVectorStore.fromDocuments(splitDocs, embeddings, { ctx }); &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Logic my app is - get id embedding from db and pass it in Langchain context . But i need get id my embeddings from db - variable res don`t give me this info. How to do this?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Jen1888Mik&quot;&gt; /u/Jen1888Mik &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1eib3t3/how_to_return_id_from_db/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1eib3t3/how_to_return_id_from_db/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1eib3t3</id><link href="https://www.reddit.com/r/LangChain/comments/1eib3t3/how_to_return_id_from_db/" /><updated>2024-08-02T14:13:37+00:00</updated><published>2024-08-02T14:13:37+00:00</published><title>How to return id from db ConvexVectoreStore.fromDocuments</title></entry><entry><author><name>/u/RiverOtterBae</name><uri>https://www.reddit.com/user/RiverOtterBae</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I have my existent backend set up as a bunch of serverless functions at the moment (cloudflare workers). I wanted to set up a new `/chat` endpoint as just another serverless function which uses langchain on the server. But as I get deep into the code I&amp;#39;m not sure if it makes sense to do it this way...&lt;/p&gt; &lt;p&gt;Basically if I have Langchain running on this endpoint, since servelerless functions are stateless, that means each time the user sends a new message I need to fetch the chat history from the database, load it into context, process the request (generate the next response) and then tear it all down only to have to build it all up again with the next request. Since there is also no persistent connection.&lt;/p&gt; &lt;p&gt;This all seems a bit wasteful in my opinion. If I host langchain on the client I&amp;#39;m thinking I can avoid all this extra work since the langchain &amp;quot;instance&amp;quot; will stay put for the duration of the chat session. Once the long context is loaded in memory I only need to add new messages to it vs redoing the whole thing which can get very taxing for loooong conversations.&lt;/p&gt; &lt;p&gt;But I would prefer to handle it on the server side to hide the prompt magic &amp;quot;special sauce&amp;quot; if possible... &lt;/p&gt; &lt;p&gt;How are ya&amp;#39;ll serving your langchain apps in production?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/RiverOtterBae&quot;&gt; /u/RiverOtterBae &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ehwzcr/where_are_you_running_langchain_in_your/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ehwzcr/where_are_you_running_langchain_in_your/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1ehwzcr</id><link href="https://www.reddit.com/r/LangChain/comments/1ehwzcr/where_are_you_running_langchain_in_your/" /><updated>2024-08-02T00:54:32+00:00</updated><published>2024-08-02T00:54:32+00:00</published><title>Where are you running Langchain in your production apps? (serverless / on the client / somewhere else)???</title></entry><entry><author><name>/u/tys203831</name><uri>https://www.reddit.com/user/tys203831</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Let&amp;#39;s say I have 50 website API endpoints to get the data, and I am going to wrap them inside 50 agents... I&amp;#39;m not sure if this is the right way to do, as I&amp;#39;m afraid the agent will possibly messed up when the API endpoints keep growing...&lt;/p&gt; &lt;p&gt;Instead, I am thinking if I could RAG on tools to be used based on the user query, and then trigger those function tool based the output returned by RAG... Is this something feasible and perhaps more scalable than agents?&lt;/p&gt; &lt;p&gt;I&amp;#39;m not that sure the scalability of agents when there is a lot of data endpoints to access with. Hope for help, thanks.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/tys203831&quot;&gt; /u/tys203831 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ehyf74/using_rag_to_choose_tools_vs_agents_which_is/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ehyf74/using_rag_to_choose_tools_vs_agents_which_is/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1ehyf74</id><link href="https://www.reddit.com/r/LangChain/comments/1ehyf74/using_rag_to_choose_tools_vs_agents_which_is/" /><updated>2024-08-02T02:05:20+00:00</updated><published>2024-08-02T02:05:20+00:00</published><title>Using RAG to choose tools vs agents, which is better choices? If accuracy matters</title></entry><entry><author><name>/u/Grand_Worker_7417</name><uri>https://www.reddit.com/user/Grand_Worker_7417</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hello, I am building an AI assistant in langgraph and while one of the agents works very fast, the other has issues because of 2 reasons: first one can&amp;#39;t be solved as we do need gpt4 for it, while the previously mentioned agent runs on gpt3.5 turbo, but the other reason is like to tackle is that for this agent, state messages build up very fast very long because of the amount of tools being used. Is there a way to compress the state or reduce state size to send less tokens to the model? Anyone knows?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Grand_Worker_7417&quot;&gt; /u/Grand_Worker_7417 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ei7fvd/reducing_length_of_state_in_langgraph/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ei7fvd/reducing_length_of_state_in_langgraph/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1ei7fvd</id><link href="https://www.reddit.com/r/LangChain/comments/1ei7fvd/reducing_length_of_state_in_langgraph/" /><updated>2024-08-02T11:16:59+00:00</updated><published>2024-08-02T11:16:59+00:00</published><title>Reducing length of State in LangGraph</title></entry><entry><author><name>/u/The_Wolfiee</name><uri>https://www.reddit.com/user/The_Wolfiee</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I am using an LLM that is a fine tuned version of Llama 3 on a cybersecurity dataset that recognises vulnerable code blocks and suggests steps to remediates the vulnerablities with fixed code. &lt;/p&gt; &lt;p&gt;I tried the same LLM and prompt with LangChain and Llama CPP but I get different results from each of them. &lt;/p&gt; &lt;p&gt;In Llama CPP, I get the suggested steps and fixed code block but with LangChain (using the Llama CPP abstraction), I get only the steps. &lt;/p&gt; &lt;p&gt;The prompt format is Llama-Chat 2 and the prompt specifically says &amp;quot;provide a code block that fixes the vulnerability&amp;quot;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/The_Wolfiee&quot;&gt; /u/The_Wolfiee &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ei1nol/different_results_with_same_prompt_and_llm_but/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ei1nol/different_results_with_same_prompt_and_llm_but/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1ei1nol</id><link href="https://www.reddit.com/r/LangChain/comments/1ei1nol/different_results_with_same_prompt_and_llm_but/" /><updated>2024-08-02T04:57:50+00:00</updated><published>2024-08-02T04:57:50+00:00</published><title>Different results with same prompt and LLM but different framework?</title></entry><entry><author><name>/u/Select-Coconut-1161</name><uri>https://www.reddit.com/user/Select-Coconut-1161</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey everyone. I am trying to filter my documents by metadata. I was using ChromaDB and the code below was working just fine.&lt;/p&gt; &lt;p&gt;For basic_retriever:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;basic_retriever = _vector_store.as_retriever( search_kwargs={ &amp;quot;k&amp;quot;: 20, &amp;quot;filter&amp;quot;: { &amp;quot;$and&amp;quot;: [ {&amp;quot;speaker&amp;quot;: {&amp;quot;$eq&amp;quot;: &amp;quot;Participant&amp;quot;}}, {&amp;quot;participant&amp;quot;: {&amp;quot;$eq&amp;quot;: participant_id}}, {&amp;quot;part&amp;quot;: {&amp;quot;$in&amp;quot;: parts}} ] } } ) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;For self_query_retriever:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;self_query_retriever = SelfQueryRetriever.from_llm( llm, _vector_store, metadata_field_info=metadata_field_info, document_contents=&amp;quot;text&amp;quot;, verbose=True, enable_limit=False, search_kwargs={ &amp;quot;filter&amp;quot;: { &amp;#39;$and&amp;#39;: [ {&amp;quot;speaker&amp;quot;: {&amp;quot;$eq&amp;quot;: &amp;quot;Participant&amp;quot;}}, {&amp;quot;participant&amp;quot;: {&amp;quot;$eq&amp;quot;: participant_id}}, {&amp;quot;part&amp;quot;: {&amp;quot;$in&amp;quot;: parts}} ] } }, context_similarity=&amp;quot;context&amp;quot;, ) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;However. I had to migrate to MongoDB and the structure above does not work. I tried to follow documentation and did these:&lt;/p&gt; &lt;p&gt;For basic retriever:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;basic_retriever = _vector_store.as_retriever( search_kwargs={ &amp;quot;k&amp;quot;: 20, &amp;quot;pre_filter&amp;quot;: { &amp;quot;$and&amp;quot;: [ { &amp;quot;text&amp;quot;: { &amp;quot;path&amp;quot;: &amp;quot;speaker&amp;quot;, &amp;quot;query&amp;quot;: &amp;quot;Participant&amp;quot; } }, { &amp;quot;text&amp;quot;: { &amp;quot;path&amp;quot;: &amp;quot;participant&amp;quot;, &amp;quot;query&amp;quot;: participant_id } }, { &amp;quot;text&amp;quot;: { &amp;quot;path&amp;quot;: &amp;quot;part&amp;quot;, &amp;quot;query&amp;quot;: { &amp;quot;$in&amp;quot;: parts } } } ] } } ) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;For self_query_retriever:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;self_query_retriever = SelfQueryRetriever.from_llm( llm, _vector_store, metadata_field_info=metadata_field_info, document_contents=&amp;quot;text&amp;quot;, verbose=True, enable_limit=False, search_kwargs={ &amp;quot;pre_filter&amp;quot;: { &amp;quot;$and&amp;quot;: [ { &amp;quot;text&amp;quot;: { &amp;quot;path&amp;quot;: &amp;quot;speaker&amp;quot;, &amp;quot;query&amp;quot;: &amp;quot;Participant&amp;quot; } }, { &amp;quot;text&amp;quot;: { &amp;quot;path&amp;quot;: &amp;quot;participant&amp;quot;, &amp;quot;query&amp;quot;: participant_id } }, { &amp;quot;text&amp;quot;: { &amp;quot;path&amp;quot;: &amp;quot;part&amp;quot;, &amp;quot;query&amp;quot;: { &amp;quot;$in&amp;quot;: parts } } } ] } }, context_similarity=&amp;quot;context&amp;quot;, ) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;But it does not work. I get the following error:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;PlanExecutor error during aggregation :: caused by :: &amp;quot;filter[0]&amp;quot; must be a boolean, objectId, number, string, date, uuid, or null, full error: {&amp;#39;ok&amp;#39;: 0.0, &amp;#39;errmsg&amp;#39;: &amp;#39;PlanExecutor error during aggregation :: caused by :: &amp;quot;filter[0]&amp;quot; must be a boolean, objectId, number, string, date, uuid, or null&amp;#39;, &amp;#39;code&amp;#39;: 8, &amp;#39;codeName&amp;#39;: &amp;#39;UnknownError&amp;#39;, &amp;#39;$clusterTime&amp;#39;: {&amp;#39;clusterTime&amp;#39;: Timestamp(1722587696, 1), &amp;#39;signature&amp;#39;: {&amp;#39;hash&amp;#39;: b&amp;#39;&amp;quot;\xb7\x80\xdbA\xc1o\xe0?\x91\xce\x9b\x7f\xbd\xe2l\xe6\xfc&amp;#39;, &amp;#39;keyId&amp;#39;: 7335879767352147970}}, &amp;#39;operationTime&amp;#39;: Timestamp(1722587696, 1)} &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Also, I set my vector_search_index like that:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;{ &amp;quot;fields&amp;quot;: [ { &amp;quot;numDimensions&amp;quot;: 1536, &amp;quot;path&amp;quot;: &amp;quot;embedding&amp;quot;, &amp;quot;similarity&amp;quot;: &amp;quot;cosine&amp;quot;, &amp;quot;type&amp;quot;: &amp;quot;vector&amp;quot; }, { &amp;quot;path&amp;quot;: &amp;quot;timestamp&amp;quot;, &amp;quot;type&amp;quot;: &amp;quot;filter&amp;quot; }, { &amp;quot;path&amp;quot;: &amp;quot;speaker&amp;quot;, &amp;quot;type&amp;quot;: &amp;quot;filter&amp;quot; }, { &amp;quot;path&amp;quot;: &amp;quot;context&amp;quot;, &amp;quot;type&amp;quot;: &amp;quot;filter&amp;quot; }, { &amp;quot;path&amp;quot;: &amp;quot;participant&amp;quot;, &amp;quot;type&amp;quot;: &amp;quot;filter&amp;quot; }, { &amp;quot;path&amp;quot;: &amp;quot;metadata.part&amp;quot;, &amp;quot;type&amp;quot;: &amp;quot;filter&amp;quot; } ] } &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;How do I properly filter my metadata?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Select-Coconut-1161&quot;&gt; /u/Select-Coconut-1161 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ei4yts/having_problems_with_filtering_by_metadata_when/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ei4yts/having_problems_with_filtering_by_metadata_when/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1ei4yts</id><link href="https://www.reddit.com/r/LangChain/comments/1ei4yts/having_problems_with_filtering_by_metadata_when/" /><updated>2024-08-02T08:36:36+00:00</updated><published>2024-08-02T08:36:36+00:00</published><title>Having problems with filtering by metadata when using MongoDB</title></entry><entry><author><name>/u/moonbunR</name><uri>https://www.reddit.com/user/moonbunR</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/moonbunR&quot;&gt; /u/moonbunR &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;/r/SmythOS/comments/1efnjke/how_does_an_llm_orchestrator_decide_which_agent/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ehjtby/how_does_an_llm_orchestrator_decide_which_agent/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1ehjtby</id><link href="https://www.reddit.com/r/LangChain/comments/1ehjtby/how_does_an_llm_orchestrator_decide_which_agent/" /><updated>2024-08-01T15:40:52+00:00</updated><published>2024-08-01T15:40:52+00:00</published><title>How does an LLM orchestrator decide which agent to use in a multi-agent system?</title></entry><entry><author><name>/u/Shiro_94</name><uri>https://www.reddit.com/user/Shiro_94</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hello everyone,&lt;/p&gt; &lt;p&gt;I am currently looking at some opensource RAG such as Langchain and Llama index. Quite like Llama index but it does not seem suitable for production. I did not find the capability of doing batch inference especially for retrieving the closest chunks for a batch of query. (so lack of scalability here)&lt;br/&gt; Langchain seems to have this feature (correct me if I am wrong but they are extracting the embeddings of queries by batch and not using multiple workers =&amp;gt; one embedding model call instead of N call if we have N queries)&lt;/p&gt; &lt;p&gt;I was wondering if there are others open source RAG worth for production other than langchain allowing:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Vector Store&lt;/li&gt; &lt;li&gt;Chunking &amp;amp; Document upload of different type (pdf, docx, raw text etc)&lt;/li&gt; &lt;li&gt;scalability (such as batch for queries =&amp;gt; embedding model call made by batch)&lt;/li&gt; &lt;li&gt;flexible about choosing the embedding model (HF, OpenAI etc)&lt;/li&gt; &lt;li&gt;good feature about the retriever such as filtering from metadata&lt;/li&gt; &lt;li&gt;good postprocess function (or possibility to add custom function) such as chunk merging etc&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Thanks for the help!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Shiro_94&quot;&gt; /u/Shiro_94 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ehtdhs/open_source_rag_best_for_production/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ehtdhs/open_source_rag_best_for_production/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1ehtdhs</id><link href="https://www.reddit.com/r/LangChain/comments/1ehtdhs/open_source_rag_best_for_production/" /><updated>2024-08-01T22:10:49+00:00</updated><published>2024-08-01T22:10:49+00:00</published><title>Open Source RAG - best for production?</title></entry><entry><author><name>/u/banenvy</name><uri>https://www.reddit.com/user/banenvy</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I think the resources I am coming across are outdated as it says bind_tools not found.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/banenvy&quot;&gt; /u/banenvy &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ei08g7/does_anyone_have_resources_to_build_a_google/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ei08g7/does_anyone_have_resources_to_build_a_google/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1ei08g7</id><link href="https://www.reddit.com/r/LangChain/comments/1ei08g7/does_anyone_have_resources_to_build_a_google/" /><updated>2024-08-02T03:39:50+00:00</updated><published>2024-08-02T03:39:50+00:00</published><title>Does anyone have resources to build a Google Gemini agent that can use tools?</title></entry><entry><author><name>/u/Complete-Pie5760</name><uri>https://www.reddit.com/user/Complete-Pie5760</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;ReAct agents are powerful, but I&amp;#39;ve found a way to make them even better for real-world use. Here&amp;#39;s how:&lt;/p&gt; &lt;p&gt;Multi-LLM Integration: I&amp;#39;ve set up my ReAct agents to work with over 200 different LLMs. This flexibility lets you choose the best model for each task. &lt;/p&gt; &lt;p&gt;Performance Tracking: By monitoring costs, token usage, and latency, I&amp;#39;ve optimized my agents&amp;#39; efficiency. This is crucial for large-scale applications.&lt;/p&gt; &lt;p&gt;Improved Reliability: I&amp;#39;ve implemented fallbacks between LLMs, load-balancing, and automatic retries. This makes the agents much more stable in production environments.&lt;/p&gt; &lt;p&gt;Smart Caching: By storing frequently accessed data, I&amp;#39;ve significantly reduced API calls, making the agents faster and more cost-effective.&lt;/p&gt; &lt;p&gt;Detailed Logging: Comprehensive action tracking has been a game-changer for debugging complex ReAct runs.&lt;/p&gt; &lt;p&gt;Easy Prompt Management- I can now update prompts without touching the code, which speeds up experimentation and optimization.&lt;/p&gt; &lt;p&gt;I&amp;#39;ve based my implementation on Simon Willison&amp;#39;s work. You can find the starting point here: &lt;a href=&quot;https://git.new/ReAct-framework&quot;&gt;https://git.new/ReAct-framework&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Has anyone else been working on improving ReAct agents? What challenges have you faced in real-world applications?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Complete-Pie5760&quot;&gt; /u/Complete-Pie5760 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ehcpct/how_to_build_production_grade_langchain_agents/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ehcpct/how_to_build_production_grade_langchain_agents/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1ehcpct</id><link href="https://www.reddit.com/r/LangChain/comments/1ehcpct/how_to_build_production_grade_langchain_agents/" /><updated>2024-08-01T09:50:53+00:00</updated><published>2024-08-01T09:50:53+00:00</published><title>How to build Production grade Langchain Agents using ReAct</title></entry><entry><author><name>/u/Apart-Damage143</name><uri>https://www.reddit.com/user/Apart-Damage143</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Apart-Damage143&quot;&gt; /u/Apart-Damage143 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ehslq6/does_anyone_know_any_tools_or_libraries_that_can/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ehslq6/does_anyone_know_any_tools_or_libraries_that_can/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1ehslq6</id><link href="https://www.reddit.com/r/LangChain/comments/1ehslq6/does_anyone_know_any_tools_or_libraries_that_can/" /><updated>2024-08-01T21:37:49+00:00</updated><published>2024-08-01T21:37:49+00:00</published><title>Does anyone know any tools or libraries that can generate diagrams based on input?</title></entry><entry><author><name>/u/sharrajesh</name><uri>https://www.reddit.com/user/sharrajesh</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;m working on a production FastAPI application that uses LangChain with a cascade of tools for various AI tasks. I&amp;#39;m looking to add asynchronous streaming support to my API and would appreciate feedback on my proposed design:&lt;/p&gt; &lt;h2&gt;Current Setup:&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;FastAPI endpoints that use LangChain agents with multiple tools&lt;/li&gt; &lt;li&gt;Synchronous API calls that return complete responses, including main content and metadata (e.g., sources used)&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;Proposed Design:&lt;/h2&gt; &lt;ol&gt; &lt;li&gt;Keep existing synchronous API endpoints as-is for backward compatibility&lt;/li&gt; &lt;li&gt;Add new streaming endpoints for real-time token generation of the main response body&lt;/li&gt; &lt;li&gt;Use Redis as a message broker to collect and stream responses&lt;/li&gt; &lt;li&gt;Synchronous API continues to return full response with all fields (main content, sources, etc.)&lt;/li&gt; &lt;/ol&gt; &lt;h2&gt;Implementation Idea:&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;Modify existing endpoints to publish responses to Redis&lt;/li&gt; &lt;li&gt;Create new streaming endpoints that subscribe to Redis channels&lt;/li&gt; &lt;li&gt;Update LangChain agents to publish chunks and full responses to Redis&lt;/li&gt; &lt;li&gt;Client can use either sync API for full response or streaming API for real-time updates&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;Questions:&lt;/h2&gt; &lt;ol&gt; &lt;li&gt;Is this a sensible approach for adding streaming to an existing production API?&lt;/li&gt; &lt;li&gt;Are there better alternatives to using Redis for this purpose?&lt;/li&gt; &lt;li&gt;How can I ensure efficient resource usage and low latency with this design?&lt;/li&gt; &lt;li&gt;Any potential pitfalls or considerations I should be aware of?&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;I&amp;#39;d greatly appreciate any insights, alternative approaches, or best practices for implementing streaming in a FastAPI LangChain application. Thanks in advance for your help!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/sharrajesh&quot;&gt; /u/sharrajesh &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ehggqs/adding_streaming_support_to_fastapi_langchain/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ehggqs/adding_streaming_support_to_fastapi_langchain/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1ehggqs</id><link href="https://www.reddit.com/r/LangChain/comments/1ehggqs/adding_streaming_support_to_fastapi_langchain/" /><updated>2024-08-01T13:18:27+00:00</updated><published>2024-08-01T13:18:27+00:00</published><title>Adding Streaming Support to FastAPI LangChain Application with Agents</title></entry><entry><author><name>/u/BenkattoRamunan</name><uri>https://www.reddit.com/user/BenkattoRamunan</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;So the main idea is that given logs I need RAG to get answers to analyse them. The LLM model works well here upto a level of base knowledge it has been pretrained on. Now I want the answers to be more accurate. So I have a couple of template info and domain knowledge like: &lt;/p&gt; &lt;pre&gt;&lt;code&gt;Brief scenarios for Registration Cases 1. - Registration always begins with REGISTER msg. - REGISTER message is one of the most important message in protocol and it contains a lot of important information in it. Unterstanding the meaning of each parameters in registration would help you greatly with various troubleshooting situation. Followings are some of the examples of REGISTER message you may see in the field. Keep reading this page as often as possible until you become very familiar with all the details of the contents. Blah Blah .... &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Now I can give parts of these in the prompts for RAG. (Note: here the vector store contains embeddings of my logs). Now the reference gets larger and larger and more sophisticated. I am looking for alternate ways to make sure RAG references this domain info before it answers questions on the log vector store. So I can keep expanding this document of domain knowledge to refer to, and RAG analyses logs based on this domain knowledge. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/BenkattoRamunan&quot;&gt; /u/BenkattoRamunan &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ehwxbh/rag_with_prior_knowledge_or_reference_to_follow/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ehwxbh/rag_with_prior_knowledge_or_reference_to_follow/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1ehwxbh</id><link href="https://www.reddit.com/r/LangChain/comments/1ehwxbh/rag_with_prior_knowledge_or_reference_to_follow/" /><updated>2024-08-02T00:51:46+00:00</updated><published>2024-08-02T00:51:46+00:00</published><title>RAG with prior knowledge or reference to follow</title></entry><entry><author><name>/u/g_pal</name><uri>https://www.reddit.com/user/g_pal</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I recently had our AI interviewer speak with 22 developers who are building with LangGraph. The interviews covered various topics, including how they&amp;#39;re using LangGraph, what they like about it, and areas for improvement. I wanted to share the key findings because I thought you might find it interesting.&lt;/p&gt; &lt;h1&gt;Use Cases and Attractions&lt;/h1&gt; &lt;p&gt;LangGraph is attracting developers from a wide range of industries due to its versatility in managing complex AI workflows. Here are some interesting use cases:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Content Generation:&lt;/strong&gt; Teams are using LangGraph to create systems where multiple AI agents collaborate to draft, fact-check, and refine research papers in real-time.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Customer Service:&lt;/strong&gt; Developers are building dynamic response systems that analyze sentiment, retrieve relevant information, and generate personalized replies with built-in clarification mechanisms.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Financial Modeling:&lt;/strong&gt; Some are building valuation models in real estate that adapt in real-time based on market fluctuations and simulated scenarios.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Academic Research&lt;/strong&gt;: Institutions are developing adaptive research assistants capable of gathering data, synthesizing insights, and proposing new hypotheses within a single integrated system.&lt;/li&gt; &lt;/ol&gt; &lt;h1&gt;What Attracts Developers to LangGraph?&lt;/h1&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Multi-Agent System Orchestration&lt;/strong&gt;: LangGraph excels at managing multiple AI agents, allowing for a divide-and-conquer approach to complex problems.&amp;quot;We are working on a project that requires multiple AI agents to communicate and talk to one another. LangGraph helps with thinking through the problem using a divide-and-conquer approach with graphs, nodes, and edges.&amp;quot; - Founder, Property Technology Startup&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Workflow Visualization and Debugging&lt;/strong&gt;: The platform&amp;#39;s visualization capabilities are highly valued for development and debugging.&amp;quot;LangGraph can visualize all the requests and all the payloads instantly, and I can debug by taking LangGraph. It&amp;#39;s very convenient for the development experience.&amp;quot; - Cloud Solutions Architect, Microsoft&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Complex Problem-Solving&lt;/strong&gt;: Developers appreciate LangGraph&amp;#39;s ability to tackle intricate challenges that traditional programming struggles with.&amp;quot;Solving complex problems that are not, um, possible with traditional programming.&amp;quot; - AI Researcher, Nokia&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Abstraction of Flow Logic&lt;/strong&gt;: LangGraph simplifies the implementation of complex workflows by abstracting flow logic.&amp;quot;[LangGraph helped] abstract the flow logic and avoid having to write all of the boilerplate code to get started with the project.&amp;quot; - AI Researcher, Nokia&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Flexible Agentic Workflows&lt;/strong&gt;: The tool&amp;#39;s adaptability for various AI agent scenarios is a key attraction.&amp;quot;Being able to create an agentic workflow that is easy to visualize abstractly with graphs, nodes, and edges.&amp;quot; - Founder, Property Technology Startup&lt;/li&gt; &lt;/ol&gt; &lt;h1&gt;LangGraph vs Alternatives&lt;/h1&gt; &lt;p&gt;The most commonly considered alternatives were CrewAI and Microsoft&amp;#39;s Autogen. However, developers noted several areas where LangGraph stands out:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Handling Complex Workflows:&lt;/strong&gt; Unlike some competitors limited to simple, linear processes, LangGraph can handle complex graph flows, including cycles.&amp;quot;CrewAI can only handle DAGs and cannot handle cycles, whereas LangGraph can handle complex graph flows, including cycles.&amp;quot; - Developer&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Developer Control:&lt;/strong&gt; LangGraph offers a level of control that many find unmatched, especially for custom use cases.&amp;quot;We did tinker a bit with CrewAI and Meta GPT. But those could not come even near as powerful as LangGraph. And we did combine with LangChain because we have very custom use cases, and we need to have a lot of control. And the competitor frameworks just don&amp;#39;t offer that amount of, control over the code.&amp;quot; - Founder, GenAI Startup&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Mature Ecosystem:&lt;/strong&gt; LangGraph&amp;#39;s longer market presence has resulted in more resources, tools, and infrastructure.&amp;quot;LangGraph has the advantage of being in the market longer, offering more resources, tools, and infrastructure. The ability to use LangSmith in conjunction with LangGraph for debugging and performance analysis is a significant differentiator.&amp;quot; - Developer&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Market Leadership:&lt;/strong&gt; Despite a volatile market, LangGraph is currently seen as a leader in functionality and tooling for developing workflows.&amp;quot;Currently, LangGraph is one of the leaders in terms of functionality and tooling for developing workflows. The market is volatile, and I hope LangGraph continues to innovate and create more tools to facilitate developers&amp;#39; work.&amp;quot; - Developer&lt;/li&gt; &lt;/ol&gt; &lt;h1&gt;Areas for Improvement&lt;/h1&gt; &lt;p&gt;While LangGraph has garnered praise, developers also identified several areas for improvement:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Simplify Syntax and Reduce Complexity:&lt;/strong&gt; Some developers noted that the graph-based approach, while powerful, can be complex to maintain.&amp;quot;Some syntax can be made a lot simpler.&amp;quot; - Senior Engineering Director, BlackRock&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Enhance Documentation and Community Resources:&lt;/strong&gt; There&amp;#39;s a need for more in-depth, complex examples and community-driven documentation.&amp;quot;The lack of how-to articles and community-driven documentation... There&amp;#39;s a lot of entry-level stuff, but nothing really in-depth or complex.&amp;quot; - Research Assistant, BYU&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Improve Debugging Capabilities:&lt;/strong&gt; Developers expressed a need for more detailed debugging information, especially for tracking state within the graph.&amp;quot;There is a need for more debugging information. Sometimes, the bug information starts from the instantiation of the workflow, and it&amp;#39;s hard to track the state within the graph.&amp;quot; - Senior Software Engineer, Canadian Government Agency&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Better Human-in-the-Loop Integration:&lt;/strong&gt; Some users aren&amp;#39;t satisfied with the current implementation of human-in-the-loop concepts.&amp;quot;More options around the human-in-the-loop concept. I&amp;#39;m not a very big fan of their current implementation of that.&amp;quot; - AI Researcher, Nokia&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Enhanced Subgraph Integration:&lt;/strong&gt; Multiple developers mentioned issues with integrating and combining subgraphs.&amp;quot;The possibility to integrate subgraphs isn&amp;#39;t compatible with [graph drawing].&amp;quot; - Engineer, IT Consulting Company &amp;quot;I wish you could combine smaller graphs into bigger graphs more easily.&amp;quot; - Research Assistant, BYU&lt;/li&gt; &lt;li&gt;&lt;strong&gt;More Complex Examples:&lt;/strong&gt; There&amp;#39;s a desire for more complex examples that developers can use as starting points.&amp;quot;Creating more examples online that people can use as inspiration would be fantastic.&amp;quot; - Senior Engineering Director, BlackRock&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;____&lt;br/&gt; You can check out the interview transcripts here: &lt;a href=&quot;http://kgrid.ai/company/langgraph&quot;&gt;kgrid.ai/company/langgraph&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Curious to know whether this aligns with your experience? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/g_pal&quot;&gt; /u/g_pal &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1eh0ly3/spoke_to_22_langgraph_devs_and_heres_what_we_found/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1eh0ly3/spoke_to_22_langgraph_devs_and_heres_what_we_found/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1eh0ly3</id><link href="https://www.reddit.com/r/LangChain/comments/1eh0ly3/spoke_to_22_langgraph_devs_and_heres_what_we_found/" /><updated>2024-07-31T22:36:11+00:00</updated><published>2024-07-31T22:36:11+00:00</published><title>Spoke to 22 LangGraph devs and here's what we found</title></entry><entry><author><name>/u/philwinder</name><uri>https://www.reddit.com/user/philwinder</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ehkocs/a_comparison_of_open_source_llm_frameworks_for/&quot;&gt; &lt;img src=&quot;https://external-preview.redd.it/zJvoLGvOZMDIVae9TJd22PSmRb2UpCVRfc5JMBOgqaM.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d4f941cd37872f3dd0d705cf4caeb73d0cbc347a&quot; alt=&quot;A Comparison of Open Source LLM Frameworks for Pipelining&quot; title=&quot;A Comparison of Open Source LLM Frameworks for Pipelining&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/philwinder&quot;&gt; /u/philwinder &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://winder.ai/comparison-open-source-llm-frameworks-pipelining/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ehkocs/a_comparison_of_open_source_llm_frameworks_for/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1ehkocs</id><media:thumbnail url="https://external-preview.redd.it/zJvoLGvOZMDIVae9TJd22PSmRb2UpCVRfc5JMBOgqaM.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=d4f941cd37872f3dd0d705cf4caeb73d0cbc347a" /><link href="https://www.reddit.com/r/LangChain/comments/1ehkocs/a_comparison_of_open_source_llm_frameworks_for/" /><updated>2024-08-01T16:15:12+00:00</updated><published>2024-08-01T16:15:12+00:00</published><title>A Comparison of Open Source LLM Frameworks for Pipelining</title></entry><entry><author><name>/u/Longjumping-Try1191</name><uri>https://www.reddit.com/user/Longjumping-Try1191</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Could someone show me how I can use &lt;a href=&quot;https://huggingface.co/DAMO-NLP-SG/Video-LLaMA-2-7B-Finetuned&quot;&gt;https://huggingface.co/DAMO-NLP-SG/Video-LLaMA-2-7B-Finetuned&lt;/a&gt; with Python, please? I am still a beginner. Thank you to those who take the time to answer my question.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Longjumping-Try1191&quot;&gt; /u/Longjumping-Try1191 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ehsukn/how_can_i_run_videollama/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ehsukn/how_can_i_run_videollama/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1ehsukn</id><link href="https://www.reddit.com/r/LangChain/comments/1ehsukn/how_can_i_run_videollama/" /><updated>2024-08-01T21:48:14+00:00</updated><published>2024-08-01T21:48:14+00:00</published><title>How can I run Video-LLaMa ?</title></entry><entry><author><name>/u/Adventurous_Joke3397</name><uri>https://www.reddit.com/user/Adventurous_Joke3397</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;When you build RAG, where do you store all the vectors? &lt;/p&gt; &lt;p&gt;I am using Postgres + pg_vector, and just storing the vectors in the same DB as the rest of my application data. It is convenient and works well with my toolchain.&lt;/p&gt; &lt;p&gt;But I also heard (without explanation) that it is better to use a separate database for vectors. &lt;/p&gt; &lt;p&gt;Is this true? Any thoughts on why? Does another Postgres database on the same instance count? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Adventurous_Joke3397&quot;&gt; /u/Adventurous_Joke3397 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ehpb1d/where_to_store_vectors/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ehpb1d/where_to_store_vectors/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1ehpb1d</id><link href="https://www.reddit.com/r/LangChain/comments/1ehpb1d/where_to_store_vectors/" /><updated>2024-08-01T19:22:19+00:00</updated><published>2024-08-01T19:22:19+00:00</published><title>Where to store vectors?</title></entry><entry><author><name>/u/Upstairs-Belt8255</name><uri>https://www.reddit.com/user/Upstairs-Belt8255</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Is anyone working on Conversational technology that allows a third person to &amp;quot;take over&amp;quot; the conversation, given the answer relevancy confidence is low, with a RAG pipeline?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Upstairs-Belt8255&quot;&gt; /u/Upstairs-Belt8255 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ehmxa9/allowing_a_real_person_to_take_over_the/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ehmxa9/allowing_a_real_person_to_take_over_the/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1ehmxa9</id><link href="https://www.reddit.com/r/LangChain/comments/1ehmxa9/allowing_a_real_person_to_take_over_the/" /><updated>2024-08-01T17:46:05+00:00</updated><published>2024-08-01T17:46:05+00:00</published><title>Allowing a real person to 'take over' the conversation?</title></entry><entry><author><name>/u/Best_Sail5</name><uri>https://www.reddit.com/user/Best_Sail5</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hello guys,&lt;/p&gt; &lt;p&gt;I have a question concerning how to prompt a model.&lt;br/&gt; I&amp;#39;m currently using LLaMa 3.1 to interact with a tool. The model is given an objective and generate multiple rounds of tool input to achieve it.&lt;br/&gt; Currently i&amp;#39;m simply using the following format:&lt;/p&gt; &lt;p&gt;ChatPromptTemplate([(&amp;#39;system&amp;#39;,system_prompt),(&amp;#39;user&amp;#39;,user_prompt)])&lt;br/&gt; where user_prompt contains the previous rounds of him generating commands and tool output like this:&lt;br/&gt; user_prompt=&amp;quot;&amp;quot;&amp;quot;&lt;br/&gt; {prompt}&lt;br/&gt; previous commands executed:{previous_rounds_of_tool_call}&lt;br/&gt; &amp;quot;&amp;quot;&amp;quot;&lt;br/&gt; This is inspired of ReAct prompt formatting.&lt;/p&gt; &lt;p&gt;But I&amp;#39;m thinking about changing that to prompt him in the following format:&lt;br/&gt; ChatPromptTemplate([(&amp;#39;system&amp;#39;,system_prompt),(&amp;#39;user&amp;#39;,user_prompt)&lt;br/&gt; ,(&amp;#39;tool&amp;#39;,tool_message),(&amp;#39;user&amp;#39;,user_prompt).....])&lt;/p&gt; &lt;p&gt;adding each turns as separate message.&lt;br/&gt; I would like to know if someone already used that? Does it change something ? How to do the training with multi steps setup like that?simply train each step separately?&lt;/p&gt; &lt;p&gt;Thanks for your advices!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Best_Sail5&quot;&gt; /u/Best_Sail5 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1eheoc1/multiple_turns_prompting_strategy/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1eheoc1/multiple_turns_prompting_strategy/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1eheoc1</id><link href="https://www.reddit.com/r/LangChain/comments/1eheoc1/multiple_turns_prompting_strategy/" /><updated>2024-08-01T11:49:44+00:00</updated><published>2024-08-01T11:49:44+00:00</published><title>Multiple turns prompting strategy</title></entry></feed>