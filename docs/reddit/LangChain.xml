<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/"><category term="LangChain" label="r/LangChain"/><updated>2024-07-12T11:28:37+00:00</updated><icon>https://www.redditstatic.com/icon.png/</icon><id>/r/LangChain.rss</id><link rel="self" href="https://www.reddit.com/r/LangChain.rss" type="application/atom+xml" /><link rel="alternate" href="https://www.reddit.com/r/LangChain" type="text/html" /><subtitle>LangChain is an open-source framework and developer toolkit that helps developers get LLM applications from prototype to production. It is available for Python and Javascript at https://www.langchain.com/.</subtitle><title>LangChain</title><entry><author><name>/u/zchaarm</name><uri>https://www.reddit.com/user/zchaarm</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;A place for members of &lt;a href=&quot;/r/LangChain&quot;&gt;r/LangChain&lt;/a&gt; to chat with each other&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/zchaarm&quot;&gt; /u/zchaarm &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_10ljho9</id><link href="https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/" /><updated>2023-01-26T04:33:05+00:00</updated><published>2023-01-26T04:33:05+00:00</published><title>r/LangChain Lounge</title></entry><entry><author><name>/u/MZuc</name><uri>https://www.reddit.com/user/MZuc</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;ve heard so many AI teams ask this question, I decided to sum up my take on this in a short post. Let me know what you guys think.&lt;/p&gt; &lt;p&gt;The way I see it, the first step is to change how you identify and approach problems. Too often, teams use vague terms like “it &lt;strong&gt;&lt;em&gt;feels&lt;/em&gt;&lt;/strong&gt; like” or “it &lt;strong&gt;&lt;em&gt;seems&lt;/em&gt;&lt;/strong&gt; like” instead of specific metrics, like “the feedback score for this type of request improved by 20%.”&lt;/p&gt; &lt;p&gt;When you&amp;#39;re developing a new AI-driven RAG application, the process tends to be chaotic. There are too many priorities and not enough time to tackle them all. Even if you could, you&amp;#39;re not sure how to enhance your RAG system. You sense that there&amp;#39;s a &amp;quot;right path&amp;quot; – a set of steps that would lead to maximum growth in the shortest time. There are a myriad of great trendy RAG libraries, pipelines, and tools out there but you don&amp;#39;t know which will work on &lt;em&gt;your&lt;/em&gt; documents and &lt;em&gt;your&lt;/em&gt; usecase (&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dr5kki/the_most_important_thing_to_build_great_rag_system/&quot;&gt;as mentioned in another Reddit post that inspired this one&lt;/a&gt;).&lt;/p&gt; &lt;p&gt;I discuss this whole topic in more detail in my &lt;a href=&quot;https://pashpashpash.substack.com/p/why-does-my-rag-suck-and-how-do-i&quot;&gt;Substack article&lt;/a&gt; including specific advice for pre-launch and post-launch, but in a nutshell, when starting any RAG system &lt;strong&gt;&lt;em&gt;you need to capture valuable metrics like cosine similarity, user feedback, and reranker scores - for every retrieval, right from the start&lt;/em&gt;&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;Basically, in an ideal scenario, you will end up with an &lt;strong&gt;observability table&lt;/strong&gt; that looks like this:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;retrieval_id&lt;/strong&gt; (some unique identifier for every piece of retrieved context)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;query_id&lt;/strong&gt; (unique id for the input query/question/message that RAG was used to answer)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;cosine similarity score&lt;/strong&gt; (null for non-vector retrieval e.g. elastic search)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;reranker relevancy score&lt;/strong&gt; (highly recommended for ALL kinds of retrieval, including vector and traditional text search like elastic)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;timestamp&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;retrieved_context&lt;/strong&gt; (optional, but nice to have for QA purposes) &lt;ul&gt; &lt;li&gt;e.g. &lt;code&gt;&amp;quot;The New York City Subway [...]&amp;quot;&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;user_feedback&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;e.g. &lt;code&gt;false (thumbs down)&lt;/code&gt; or &lt;code&gt;true (thumbs up)&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Once you start collecting and storing these super powerful observability metrics, you can begin analyzing production performance. We can &lt;a href=&quot;https://x.com/jxnlco/status/1803899526723387895&quot;&gt;categorize this analysis into two main areas&lt;/a&gt;:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Topics: This refers to the content and context of the data, which can be represented by the way words are structured or the embeddings used in search queries. You can use topic modeling to better understand the types of responses your system handles. &lt;ul&gt; &lt;li&gt;E.g. People talking about their family, or their hobbies, etc.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Capabilities (Agent Tools/Functions): This pertains to the functional aspects of the queries, such as: &lt;ul&gt; &lt;li&gt;Direct conversation requests (e.g., &lt;em&gt;“Remind me what we talked about when we discussed my neighbor&amp;#39;s dogs barking all the time.”&lt;/em&gt;)&lt;/li&gt; &lt;li&gt;Time-sensitive queries (e.g., &lt;em&gt;“Show me the latest X”&lt;/em&gt; or &lt;em&gt;“Show me the most recent Y.”&lt;/em&gt;)&lt;/li&gt; &lt;li&gt;Metadata-specific inquiries (e.g., &lt;em&gt;“What date was our last conversation?”&lt;/em&gt;), which might require specific filters or keyword matching that go beyond simple text embeddings.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;By applying clustering techniques to these topics and capabilities (I cover this in more depth in my &lt;a href=&quot;https://pashpashpash.substack.com/p/tackling-the-challenge-of-document&quot;&gt;previous article on K-Means clusterization&lt;/a&gt;), you can:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Group similar queries/questions together and categorize them by topic e.g. &lt;em&gt;“Product availability questions”&lt;/em&gt; or capability e.g. &lt;em&gt;“Requests to search previous conversations”&lt;/em&gt;.&lt;/li&gt; &lt;li&gt;Calculate the frequency and distribution of these groups.&lt;/li&gt; &lt;li&gt;Assess the average performance scores for each group.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This data-driven approach allows you to prioritize system enhancements based on actual user needs and system performance. For instance:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;If person-entity-retrieval commands a significant portion of query volume (say 60%) and shows high satisfaction rates (90% thumbs up) with minimal cosine distance, this area may not need further refinement.&lt;/li&gt; &lt;li&gt;Conversely, queries like &amp;quot;What date was our last conversation&amp;quot; might show poor results, indicating a limitation of our current functional capabilities. If such queries constitute a small fraction (e.g., 2%) of total volume, it might be more strategic to temporarily exclude these from the system’s capabilities (&lt;em&gt;“I forget, honestly!”&lt;/em&gt; or &lt;em&gt;“Do you think I&amp;#39;m some kind of calendar!?”&lt;/em&gt;), thus improving overall system performance. &lt;ul&gt; &lt;li&gt;Handling these exclusions gracefully significantly improves user experience. &lt;ul&gt; &lt;li&gt;When appropriate, Use humor and personality to your advantage instead of saying &lt;em&gt;“I cannot answer this right now.”&lt;/em&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;TL;DR:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Getting your RAG system from “sucks” to “good” isn&amp;#39;t about magic solutions or trendy libraries. The first step is to implement strong observability practices to continuously analyze and improve performance. Cluster collected data into topics &amp;amp; capabilities to have a clear picture of how people are using your product and where it falls short. Prioritize enhancements based on real usage and remember, a touch of personality can go a long way in handling limitations.&lt;/p&gt; &lt;p&gt;For a more detailed treatment of this topic, check out my &lt;a href=&quot;https://pashpashpash.substack.com/p/why-does-my-rag-suck-and-how-do-i&quot;&gt;article here&lt;/a&gt;. I&amp;#39;d love to hear your thoughts on this, please let me know if there are any other good metrics or considerations to keep in mind!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/MZuc&quot;&gt; /u/MZuc &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e0rsou/why_does_my_rag_suck_and_how_do_i_make_it_good/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e0rsou/why_does_my_rag_suck_and_how_do_i_make_it_good/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1e0rsou</id><link href="https://www.reddit.com/r/LangChain/comments/1e0rsou/why_does_my_rag_suck_and_how_do_i_make_it_good/" /><updated>2024-07-11T15:33:21+00:00</updated><published>2024-07-11T15:33:21+00:00</published><title>&quot;Why does my RAG suck and how do I make it good&quot;</title></entry><entry><author><name>/u/WarriorA</name><uri>https://www.reddit.com/user/WarriorA</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hello dear swarm intelligence practitioner! I have come to you in a time of great need.&lt;/p&gt; &lt;p&gt;Imagine you have a document about some topic. Lets say the GoldenGate Bridge. And this document has multiple wonderfully written paragraphs about this Bridge. This includes materials used, the timeframe of when it was built, and much much more.&lt;br/&gt; This document then goes on writing about California and San Francisco.&lt;/p&gt; &lt;p&gt;A few paragraphs down maybe a few sentences about the pacific ocean.&lt;/p&gt; &lt;p&gt;And last but not least this: &amp;quot;It was designed by Joseph Strauss with the help of architect Irving Morrow and engineers Charles Alton Ellis and Leon Moisseiff. Construction was carried out by the McClintic-Marshall Construction Company, a subsidiary of Bethlehem Steel Corporation.&amp;quot;&lt;/p&gt; &lt;p&gt;This article is chunked using common techniques and embedded into a vector index. Now, this RAG System will be used to answer Questions. We retrieve k docs from this index that are similar to the query, and rerank them to top_k documents to generate an answer. This pipeline works with almost all queries and documents.&lt;/p&gt; &lt;p&gt;The problem here would be to answer the question &amp;quot;&lt;strong&gt;Who built the Golden Gate Bridge?&amp;quot;&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;The retrieval process would find these chunks, and confidently (and correctly) rank them as the most relevant for our question. However, the chunk with the information we are actually looking for will not be retrieved, as it is not at all similar to the query.&lt;/p&gt; &lt;p&gt;How can this question be answered, while maintaining performance?&lt;/p&gt; &lt;ol&gt; &lt;li&gt;When finding a high scoring chunk (e.g: Golden Gate Bridge chunk from the top of the article) we fetch the document in its entirety, and &lt;strong&gt;provide the entire article&lt;/strong&gt; to the generator LLM. Problem here is, that we have to either make a decision to do that based on the query, or do it every time. Both are compromises I want to avoid.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Bigger chunks&lt;/strong&gt; would include the information. Problem here is, that we can&amp;#39;t really generalize. What if there is more content in between? This won&amp;#39;t work as reliably as I would like.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Summarize chunks&lt;/strong&gt; to make them smaller and be able to fit all of the context into the generator LLM. This has the same problem as Approach 1, about making a decision to instead of just going with the chunk to answer the query, we do the extra step of retrieving the entire article instead of(or its summary) before returning from the retrieval pipeline.&lt;/li&gt; &lt;li&gt;Use a &lt;strong&gt;LLM as a Reranker&lt;/strong&gt;. This LLM-Reranker would (hopefully) understand to return the relevant chunk (Joseph Strauss), instead of the similar chunks (Golden Gate Bridge). This would again imply, that we have this decision making process based on which Reranker should be applied.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Things that could help, but also don&amp;#39;t really work in a more general approach:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Hybrid Retrieval&lt;/strong&gt;: Embedding and Keywordbased retrieval will still most likely not find the relevant chunk. And even if, the Reranking process would drop it again.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Using NER/POS/TF-IDF keywords&lt;/strong&gt; alongside the text chunks seems do run into similar problems&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Query Expansion and Reformulation&lt;/strong&gt;: Similar Problem as above. Also I have tried this approach and including the Augmented Queries in the Rerank step results in other QA-Tasks to perform worse. IMO only the original query should be involved in reranking. I use LLM-Generated Queries for SimilaritySearch already, but then rerank those results with the original query.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Two Stage Retrieval&lt;/strong&gt;: Retrieve GoldenGate chunks -&amp;gt; get all chunks of this article -&amp;gt; find relevant chunks from this preselection of article-chunks. How can we do the last step without involving a slow LLM and also deciding to go into this TwoStage retrieval in the first place?&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Different Chunk Size&lt;/strong&gt;? Would probably work, but what if the document is just a little bit longer, and again we have the information in another chunk that the Bridge-Chunk? Doesn&amp;#39;t generalize well.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;In the current pipeline, even if the relevant chunk (Joseph Strauss) would appear in the retrieved chunks, lets say via hybrid-search using BM25, keyword matching or other strategies, this chunk would be dropped by the Reranker, because it is not similar to the query.&lt;/p&gt; &lt;p&gt;In conclusion, all approaches I can think of, include some sort of LLM to make a decision before returning the chunks to the generatorLLM and/or another decision based on all chunks of this article.&lt;/p&gt; &lt;p&gt;What are other approaches one could apply here? What is a best practice? How do others handle this problem?&lt;/p&gt; &lt;p&gt;I am looking for an approach that is robust and fast enough to work in a production grade system. Ideally some sort of transformer model similar to a reranker, that instead of returning similar matches it returns relevant matches.&lt;/p&gt; &lt;p&gt;No matter how similar all our chunks are to the Golden Gate Bridge and SanFrancisco and the pacific, the actual desired chunk is not similar. How can we handle this?&lt;/p&gt; &lt;p&gt;&lt;strong&gt;This approach seems the most promising to me, but also seems like a really daunting task?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Train a model to distinguish between &amp;quot;similar&amp;quot; and &amp;quot;relevant&amp;quot; content using contrastive learning techniques.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Create training pairs that include similar but irrelevant chunks as negative examples, and dissimilar but relevant chunks as positive examples.&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/WarriorA&quot;&gt; /u/WarriorA &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e1cdhp/retrieve_chunks_which_are_not_similar_but_still/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e1cdhp/retrieve_chunks_which_are_not_similar_but_still/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1e1cdhp</id><link href="https://www.reddit.com/r/LangChain/comments/1e1cdhp/retrieve_chunks_which_are_not_similar_but_still/" /><updated>2024-07-12T08:18:23+00:00</updated><published>2024-07-12T08:18:23+00:00</published><title>Retrieve Chunks which are not similar but still relevant</title></entry><entry><author><name>/u/gr8satvik</name><uri>https://www.reddit.com/user/gr8satvik</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e1av5p/react_prompting/&quot;&gt; &lt;img src=&quot;https://b.thumbs.redditmedia.com/BYIL8aSbC2_LeHXd4Q6UQIZ8vRUQHvhKZK-CA6vLNFY.jpg&quot; alt=&quot;ReAct Prompting&quot; title=&quot;ReAct Prompting&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;My LLM is hallaucinating when I ask it a very simple question&lt;br/&gt; using Langchain tools, here wikipedia.&lt;br/&gt; I am using gemini-1.5-flash&lt;br/&gt; tried llama 70b and gemini pro&lt;br/&gt; but still it hallucinates if not same but easy questions.&lt;br/&gt; I am using React Prompt Template&lt;br/&gt; which goes like (2nd image)&lt;/p&gt; &lt;p&gt;My LLM doesn&amp;#39;t even reach the observation step it starts hallucinating after action Input&lt;br/&gt; I tried even changing params of my tools by changing the max_retrieval etc...&lt;br/&gt; Please help...&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/gmrydfxw81cd1.png?width=1614&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9df61fc7f740dab11233da781e3740a871274a9a&quot;&gt;https://preview.redd.it/gmrydfxw81cd1.png?width=1614&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9df61fc7f740dab11233da781e3740a871274a9a&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/ohj4e34a91cd1.png?width=1145&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=966fb6707cf3e9183aa52b093f29f42c0c2192b4&quot;&gt;https://preview.redd.it/ohj4e34a91cd1.png?width=1145&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=966fb6707cf3e9183aa52b093f29f42c0c2192b4&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/gr8satvik&quot;&gt; /u/gr8satvik &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e1av5p/react_prompting/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e1av5p/react_prompting/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1e1av5p</id><media:thumbnail url="https://b.thumbs.redditmedia.com/BYIL8aSbC2_LeHXd4Q6UQIZ8vRUQHvhKZK-CA6vLNFY.jpg" /><link href="https://www.reddit.com/r/LangChain/comments/1e1av5p/react_prompting/" /><updated>2024-07-12T06:38:10+00:00</updated><published>2024-07-12T06:38:10+00:00</published><title>ReAct Prompting</title></entry><entry><author><name>/u/Informal-Victory8655</name><uri>https://www.reddit.com/user/Informal-Victory8655</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;m having error when using checkpointer in graph.&lt;/p&gt; &lt;p&gt;| ValueError: Checkpointer requires one or more of the following &amp;#39;configurable&amp;#39; keys: [&amp;#39;thread_id&amp;#39;, &amp;#39;thread_ts&amp;#39;]&lt;/p&gt; &lt;p&gt;.&lt;/p&gt; &lt;p&gt;How to pass configuration with thread_id here to graph?&lt;/p&gt; &lt;p&gt;add_routes(&lt;/p&gt; &lt;p&gt;app,&lt;/p&gt; &lt;p&gt;(graph | RunnableLambda(output_parsing_for_playground)).with_types(input_type=InputChat, output_type=str),&lt;/p&gt; &lt;p&gt;playground_type=&amp;quot;chat&amp;quot;&lt;/p&gt; &lt;p&gt;)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Informal-Victory8655&quot;&gt; /u/Informal-Victory8655 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e1d466/how_to_pass_thread_id_using_config_when_serving/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e1d466/how_to_pass_thread_id_using_config_when_serving/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1e1d466</id><link href="https://www.reddit.com/r/LangChain/comments/1e1d466/how_to_pass_thread_id_using_config_when_serving/" /><updated>2024-07-12T09:08:43+00:00</updated><published>2024-07-12T09:08:43+00:00</published><title>how to pass thread_id using config when serving langgraph(with checkpointer) through langgserve chain. Please help</title></entry><entry><author><name>/u/northwolf56</name><uri>https://www.reddit.com/user/northwolf56</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e0twcd/my_serverless_visual_langgraph_editor/&quot;&gt; &lt;img src=&quot;https://b.thumbs.redditmedia.com/UJ085UairrlLGvO3kv9DnoNb9gVW-_hN6_J8iNlVyME.jpg&quot; alt=&quot;My Serverless Visual LangGraph Editor&quot; title=&quot;My Serverless Visual LangGraph Editor&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/northwolf56&quot;&gt; /u/northwolf56 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/gallery/1e0twcd&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e0twcd/my_serverless_visual_langgraph_editor/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1e0twcd</id><media:thumbnail url="https://b.thumbs.redditmedia.com/UJ085UairrlLGvO3kv9DnoNb9gVW-_hN6_J8iNlVyME.jpg" /><link href="https://www.reddit.com/r/LangChain/comments/1e0twcd/my_serverless_visual_langgraph_editor/" /><updated>2024-07-11T17:01:43+00:00</updated><published>2024-07-11T17:01:43+00:00</published><title>My Serverless Visual LangGraph Editor</title></entry><entry><author><name>/u/mehul_gupta1997</name><uri>https://www.reddit.com/user/mehul_gupta1997</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/mehul_gupta1997&quot;&gt; /u/mehul_gupta1997 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;/r/ArtificialInteligence/comments/1e18y5b/localgemma_for_loading_gemma2_models_locally/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e190m1/localgemma_for_loading_gemma2_models_locally/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1e190m1</id><link href="https://www.reddit.com/r/LangChain/comments/1e190m1/localgemma_for_loading_gemma2_models_locally/" /><updated>2024-07-12T04:45:29+00:00</updated><published>2024-07-12T04:45:29+00:00</published><title>Local-Gemma for loading Gemma2 models locally</title></entry><entry><author><name>/u/Plane_Past129</name><uri>https://www.reddit.com/user/Plane_Past129</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Is there any chance of building RAG for real-time data that was stored in SQL databases. Specifically, can we use Unstructured there?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Plane_Past129&quot;&gt; /u/Plane_Past129 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e1ba2l/rag_for_realtime_data_sql/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e1ba2l/rag_for_realtime_data_sql/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1e1ba2l</id><link href="https://www.reddit.com/r/LangChain/comments/1e1ba2l/rag_for_realtime_data_sql/" /><updated>2024-07-12T07:04:45+00:00</updated><published>2024-07-12T07:04:45+00:00</published><title>RAG for real-time data (SQL)</title></entry><entry><author><name>/u/Odd-Bonus-4075</name><uri>https://www.reddit.com/user/Odd-Bonus-4075</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I am working on a project where I need to extract information of a very long document.&lt;br/&gt; Using a local LLM (gemma2) through ollama.&lt;/p&gt; &lt;p&gt;If using a vector store, Is there a way I can make sure my LLM query searches all pages or chunks of the vector store?&lt;/p&gt; &lt;p&gt;Is it perhaps, better to use brute force and do chunks of the text to pass the LLM the compile the results? &lt;/p&gt; &lt;p&gt;Any relevant information on this would be much appreciated. Many thanks&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Odd-Bonus-4075&quot;&gt; /u/Odd-Bonus-4075 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e1aziw/search_a_long_document/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e1aziw/search_a_long_document/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1e1aziw</id><link href="https://www.reddit.com/r/LangChain/comments/1e1aziw/search_a_long_document/" /><updated>2024-07-12T06:45:57+00:00</updated><published>2024-07-12T06:45:57+00:00</published><title>Search a long document</title></entry><entry><author><name>/u/HotRepresentative325</name><uri>https://www.reddit.com/user/HotRepresentative325</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I was wondering what embedding model people have had success with for their vector store? I might be wrong but on langchain it simply implies everyone uses the openai one. Are there options for this?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/HotRepresentative325&quot;&gt; /u/HotRepresentative325 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e11luo/what_embedding_model_is_everyone_using/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e11luo/what_embedding_model_is_everyone_using/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1e11luo</id><link href="https://www.reddit.com/r/LangChain/comments/1e11luo/what_embedding_model_is_everyone_using/" /><updated>2024-07-11T22:28:32+00:00</updated><published>2024-07-11T22:28:32+00:00</published><title>What embedding model is everyone using?</title></entry><entry><author><name>/u/burcapaul</name><uri>https://www.reddit.com/user/burcapaul</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I believe that chat interfaces are the worst way to interact with LLMs, but they&amp;#39;re currently our only real option (voice doesn&amp;#39;t count). Here&amp;#39;s my reasoning:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Even though we&amp;#39;re programmed as humans to &amp;quot;prompt&amp;quot; each other in daily interactions, I&amp;#39;ve observed that when it comes to LLMs, people are very deficient at it.&lt;/li&gt; &lt;li&gt;In the past few decades, Microsoft and Apple have trained us to use computers primarily through visual interactions (clicks, taps, buttons, etc.).&lt;/li&gt; &lt;li&gt;Maybe the interface for LLMs should be more visual, rather than relying on text prompting.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;What do you guys think about this? Are we limiting the potential of LLMs by sticking to chat interfaces? Could a more visual approach make these AI tools more accessible and effective for the average user?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/burcapaul&quot;&gt; /u/burcapaul &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e0k7e8/chat_interfaces_are_holding_back_llms_we_need_a/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e0k7e8/chat_interfaces_are_holding_back_llms_we_need_a/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1e0k7e8</id><link href="https://www.reddit.com/r/LangChain/comments/1e0k7e8/chat_interfaces_are_holding_back_llms_we_need_a/" /><updated>2024-07-11T08:49:53+00:00</updated><published>2024-07-11T08:49:53+00:00</published><title>Chat interfaces are holding back LLMs - we need a more visual approach</title></entry><entry><author><name>/u/MeltingHippos</name><uri>https://www.reddit.com/user/MeltingHippos</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;This post by an AI engineer explains the challenges they encountered when implementing RAG for massive enterprise codebases and how they solved them. They share lots of alpha on strategies for chunking, creating vector embeddings for code chunks, using LLMs to generate natural language descriptions that improve indexing, and using LLMs to rank the chunks that are retrieved from the vector store: &lt;a href=&quot;https://www.codium.ai/blog/rag-for-large-scale-code-repos/&quot;&gt;https://www.codium.ai/blog/rag-for-large-scale-code-repos/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/MeltingHippos&quot;&gt; /u/MeltingHippos &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e0jpkk/rag_techniques_for_a_big_codebase_with_10k_repos/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e0jpkk/rag_techniques_for_a_big_codebase_with_10k_repos/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1e0jpkk</id><link href="https://www.reddit.com/r/LangChain/comments/1e0jpkk/rag_techniques_for_a_big_codebase_with_10k_repos/" /><updated>2024-07-11T08:14:14+00:00</updated><published>2024-07-11T08:14:14+00:00</published><title>RAG Techniques for a Big Codebase with 10k Repos</title></entry><entry><author><name>/u/Gloomy-Traffic4964</name><uri>https://www.reddit.com/user/Gloomy-Traffic4964</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;I have a few questions after watching the generative UI videos by Langchain (fyi my fullstack/frontend knowledge is poor).&lt;/p&gt; &lt;p&gt;What&amp;#39;s the difference between Langchain&amp;#39;s generative UI vs just streaming data to the front end to be made into a react component with websockets.&lt;/p&gt; &lt;p&gt;For example, I am streaming results to my React frontend from my django backend. In my frontend I am using &lt;code&gt;new WebSocket(\${REACT_APP_WS_URL}/ws/chat/\)&lt;/code&gt; and &lt;code&gt;ws.current.onmessage&lt;/code&gt; to check for &lt;code&gt;{&amp;#39;event&amp;#39;: &amp;#39;on_tool_start&amp;#39;, ...}.&lt;/code&gt; If event is &amp;#39;on_tool_start&amp;#39;, it returns a loading component instead of a text message response, and if the message is &lt;code&gt;{&amp;#39;event&amp;#39;: &amp;#39;on_tool_end&amp;#39;, &amp;#39;output&amp;#39;: [{ &amp;#39;key&amp;#39;:&amp;#39;value&amp;#39;, ..]}&lt;/code&gt; it replaces the loading component with a react component that takes &amp;#39;output&amp;#39; as inputs.&lt;/p&gt; &lt;p&gt;Follow up questions&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Why does the Langchain implementation use react server components in the frontend when there is a separate backend server with FastAPI?&lt;/li&gt; &lt;li&gt;What is ai/rsc in &lt;code&gt;(import { createStreamableUI } from &amp;quot;ai/rsc&amp;quot;)&lt;/code&gt; . It looks like it&amp;#39;s next/Vercel specific? Where are the docs for createStreamableUI ?&lt;/li&gt; &lt;li&gt;How much value is there from the extra complexity in Langchain implementation (specifically in the frontend).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/bracesproul/gen-ui-python/tree/main&quot;&gt;Here is the gen ui repo&lt;/a&gt;&lt;br/&gt; &lt;a href=&quot;https://www.youtube.com/watch?v=d3uoLbfBPkw&amp;amp;t=1557s&quot;&gt;Here is the youtube video&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Gloomy-Traffic4964&quot;&gt; /u/Gloomy-Traffic4964 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e0qc2w/generative_ui/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e0qc2w/generative_ui/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1e0qc2w</id><link href="https://www.reddit.com/r/LangChain/comments/1e0qc2w/generative_ui/" /><updated>2024-07-11T14:30:14+00:00</updated><published>2024-07-11T14:30:14+00:00</published><title>Generative UI</title></entry><entry><author><name>/u/Best_Sail5</name><uri>https://www.reddit.com/user/Best_Sail5</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e0r49j/training_llm_for_langgraph_specific_use/&quot;&gt; &lt;img src=&quot;https://b.thumbs.redditmedia.com/6dbbLK2Jl_HPiGd9EdOuVkLNjlsygojMZ_UcNxJIg8c.jpg&quot; alt=&quot;training LLM for Langgraph specific use&quot; title=&quot;training LLM for Langgraph specific use&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hello everyone,&lt;/p&gt; &lt;p&gt;I wanna use a LLM to create an agent in langgraph with this kind of architecture:&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/uw5c6kozmwbd1.png?width=199&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ca17ef164138c99d913c9d068c59ef5c996ad56c&quot;&gt;https://preview.redd.it/uw5c6kozmwbd1.png?width=199&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ca17ef164138c99d913c9d068c59ef5c996ad56c&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The idea is similar to a React agent where the model has to provide a prompt for my terminal tool and observe if it achieve a given objective.&lt;/p&gt; &lt;p&gt;I have a question about how should i train such model?&lt;/p&gt; &lt;p&gt;Could i use DPO/ORPO procedure to align my model on multi-step feeding him the context each time?&lt;/p&gt; &lt;p&gt;Or is there a smarter way to do that?&lt;/p&gt; &lt;p&gt;Thanks for your suggestions!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Best_Sail5&quot;&gt; /u/Best_Sail5 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e0r49j/training_llm_for_langgraph_specific_use/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e0r49j/training_llm_for_langgraph_specific_use/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1e0r49j</id><media:thumbnail url="https://b.thumbs.redditmedia.com/6dbbLK2Jl_HPiGd9EdOuVkLNjlsygojMZ_UcNxJIg8c.jpg" /><link href="https://www.reddit.com/r/LangChain/comments/1e0r49j/training_llm_for_langgraph_specific_use/" /><updated>2024-07-11T15:04:16+00:00</updated><published>2024-07-11T15:04:16+00:00</published><title>training LLM for Langgraph specific use</title></entry><entry><author><name>/u/ApprehensiveCut799</name><uri>https://www.reddit.com/user/ApprehensiveCut799</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I created a quick proof of concept by copy pasting LangChain code from here &lt;a href=&quot;https://js.langchain.com/v0.1/docs/modules/agents/agent_types/tool_calling/&quot;&gt;https://js.langchain.com/v0.1/docs/modules/agents/agent_types/tool_calling/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It basically uses a tool calling agent that uses the google search tool, web browser tool and a retriever tool that allows a user to search for data and also query their own uploaded documents. I ended up using the tool calling agent as the reAct agent was causing a lot of bugs. The issue now is that I read a few research papers on reAct prompting and was planning to use them as sources to beef up my hackathon presentation and make it look more cutting edge. I&amp;#39;m just wondering whether anyone has any similar papers that can make my current POC sound more cutting edge? I know this probably sounds really stupid to you but I really want to win this hackathon and I believe that the presentation is equally as important as the MVP. Thanks a lot.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/ApprehensiveCut799&quot;&gt; /u/ApprehensiveCut799 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e0mgl2/looking_for_research_papers_to_beef_up_my/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e0mgl2/looking_for_research_papers_to_beef_up_my/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1e0mgl2</id><link href="https://www.reddit.com/r/LangChain/comments/1e0mgl2/looking_for_research_papers_to_beef_up_my/" /><updated>2024-07-11T11:17:47+00:00</updated><published>2024-07-11T11:17:47+00:00</published><title>Looking for research papers to beef up my Hackathon project</title></entry><entry><author><name>/u/anehzat</name><uri>https://www.reddit.com/user/anehzat</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e0gqn6/psql_extended_to_support_sql_autocomplete_chat/&quot;&gt; &lt;img src=&quot;https://preview.redd.it/6zcw8ik2mtbd1.gif?width=320&amp;amp;crop=smart&amp;amp;s=0d16d7147456f09c07dfab560fb29bc77d857767&quot; alt=&quot;psql extended to support SQL autocomplete &amp;amp; Chat Assistance with DB context.&quot; title=&quot;psql extended to support SQL autocomplete &amp;amp; Chat Assistance with DB context.&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/anehzat&quot;&gt; /u/anehzat &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://i.redd.it/6zcw8ik2mtbd1.gif&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e0gqn6/psql_extended_to_support_sql_autocomplete_chat/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1e0gqn6</id><media:thumbnail url="https://preview.redd.it/6zcw8ik2mtbd1.gif?width=320&amp;crop=smart&amp;s=0d16d7147456f09c07dfab560fb29bc77d857767" /><link href="https://www.reddit.com/r/LangChain/comments/1e0gqn6/psql_extended_to_support_sql_autocomplete_chat/" /><updated>2024-07-11T04:55:20+00:00</updated><published>2024-07-11T04:55:20+00:00</published><title>psql extended to support SQL autocomplete &amp; Chat Assistance with DB context.</title></entry><entry><author><name>/u/Select-Coconut-1161</name><uri>https://www.reddit.com/user/Select-Coconut-1161</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e0iovb/filtering_based_on_metadata/&quot;&gt; &lt;img src=&quot;https://external-preview.redd.it/UallS6eTqMQ6Q_VovzOpEJke6ZgDb6GzqRJtFsGug9s.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=82f0fba791dd0f75899e3dc5480f915fe8c0cafd&quot; alt=&quot;Filtering based on Metadata&quot; title=&quot;Filtering based on Metadata&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey everyone, I am trying to filter the items I retrieve by Metadata fields. I have like 5 metadata fields and I am trying to use 3 filters using 3 of those fields but somehow, only one of them does not work.&lt;/p&gt; &lt;p&gt;I declare the attribute info in metadata_field_info like that:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;AttributeInfo( name=&amp;quot;speaker&amp;quot;, description=&amp;quot;The person who spoke, identified as either &amp;#39;Moderator&amp;#39; or &amp;#39;Participant&amp;#39;.&amp;quot;, type=&amp;quot;string&amp;quot;, ), &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Then, I try to filter it like&lt;/p&gt; &lt;pre&gt;&lt;code&gt; filter=f&amp;quot;and(eq(&amp;#39;speaker&amp;#39;, &amp;#39;Participant&amp;#39;), eq(&amp;#39;participant&amp;#39;, &amp;#39;k6&amp;#39;), ({part_filter}))&amp;quot;, &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;However, this did not work. So I asked Claude and updated it:&lt;/p&gt; &lt;pre&gt;&lt;code&gt; filter_str=f&amp;quot;speaker == &amp;#39;Participant&amp;#39; and participant == &amp;#39;k6&amp;#39; and ({part_filter})&amp;quot;, &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This also did not work. I decided to check LangChain documentations and using &lt;a href=&quot;https://python.langchain.com/v0.1/docs/modules/data_connection/retrievers/self_query/&quot;&gt;this page&lt;/a&gt; as a reference, I changed it to:&lt;/p&gt; &lt;pre&gt;&lt;code&gt; filter=f&amp;quot;and(eq(\&amp;quot;speaker\&amp;quot;, \&amp;quot;Participant\&amp;quot;), eq(\&amp;quot;participant\&amp;quot;, \&amp;quot;k6\&amp;quot;), ({part_filter}))&amp;quot;, &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;When this also did not work. I got suspicious about whether there was a problem with my tagging so I checked my DB and found out it was also correct. The item below is tagged &amp;quot;Moderator&amp;quot; as you can see but it is retrieved when I use the filters above.&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/5x8krgis8ubd1.jpg?width=1080&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=8d193e29a4eef3213b7097c10b2ceab7da664bfa&quot;&gt;https://preview.redd.it/5x8krgis8ubd1.jpg?width=1080&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=8d193e29a4eef3213b7097c10b2ceab7da664bfa&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Weird part is I use exactly same syntax for other filters and they work. I am lost. Any ideas?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Select-Coconut-1161&quot;&gt; /u/Select-Coconut-1161 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e0iovb/filtering_based_on_metadata/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e0iovb/filtering_based_on_metadata/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1e0iovb</id><media:thumbnail url="https://external-preview.redd.it/UallS6eTqMQ6Q_VovzOpEJke6ZgDb6GzqRJtFsGug9s.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=82f0fba791dd0f75899e3dc5480f915fe8c0cafd" /><link href="https://www.reddit.com/r/LangChain/comments/1e0iovb/filtering_based_on_metadata/" /><updated>2024-07-11T07:02:33+00:00</updated><published>2024-07-11T07:02:33+00:00</published><title>Filtering based on Metadata</title></entry><entry><author><name>/u/NoChampionship7630</name><uri>https://www.reddit.com/user/NoChampionship7630</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;ve been having issues training my llm to be able to use tool during certain part of the conversaiton. Prior to fine-tuning the interaction with the agent is good, it calls all the necessary when needed, but I haven&amp;#39;t found a way to incorporate the tool usage in the training data correctly, I&amp;#39;m missing something... after training it simply doesn&amp;#39;t use any tools.... I&amp;#39;m currently using langgraph. Any suggestions woud be highly appreciated.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/NoChampionship7630&quot;&gt; /u/NoChampionship7630 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e0oqol/fine_tuning_llm_with_tool_usage/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e0oqol/fine_tuning_llm_with_tool_usage/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1e0oqol</id><link href="https://www.reddit.com/r/LangChain/comments/1e0oqol/fine_tuning_llm_with_tool_usage/" /><updated>2024-07-11T13:17:56+00:00</updated><published>2024-07-11T13:17:56+00:00</published><title>Fine Tuning LLM with tool Usage</title></entry><entry><author><name>/u/goddamnit_1</name><uri>https://www.reddit.com/user/goddamnit_1</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e03z0y/i_used_langchain_to_build_a_slack_agent_my/&quot;&gt; &lt;img src=&quot;https://external-preview.redd.it/u5rxHXqj1K-44fCKJCxS7ivB42knsXLiW4mFDtHvbdQ.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f4921d56599664bdd7e92e20b29f17d8100f8070&quot; alt=&quot;I used Langchain to build a Slack Agent - My Experience&quot; title=&quot;I used Langchain to build a Slack Agent - My Experience&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;My AI Agent does the following:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Instant answers from the web in any Slack channel&lt;/li&gt; &lt;li&gt;Code interpretation &amp;amp; execution on the fly&lt;/li&gt; &lt;li&gt;Smart web crawling for up-to-date info&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;project link : &lt;a href=&quot;http://git.new/slack-bot-agent-ollama&quot;&gt;git.new/slack-bot-agent-ollama&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;My experience with Langchain&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;One of the key advantages of Langchain is its ability to integrate different LLMs into your applications. This flexibility allows you to experiment with various models and find the one that best suits your needs.&lt;/p&gt; &lt;p&gt;Langchain&amp;#39;s approach is a game-changer. However, I do have one gripe - the documentation could be better. I wasn&amp;#39;t aware that I needed to use the ChatModels instead of the direct models, and this wasn&amp;#39;t specified clearly enough. This kind of information is crucial for users to get up and running quickly.&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/jjtezql9nqbd1.png?width=1283&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5707c453e8a1f1bc0f756b6821780603065dd4fb&quot;&gt;https://preview.redd.it/jjtezql9nqbd1.png?width=1283&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5707c453e8a1f1bc0f756b6821780603065dd4fb&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/goddamnit_1&quot;&gt; /u/goddamnit_1 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e03z0y/i_used_langchain_to_build_a_slack_agent_my/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e03z0y/i_used_langchain_to_build_a_slack_agent_my/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1e03z0y</id><media:thumbnail url="https://external-preview.redd.it/u5rxHXqj1K-44fCKJCxS7ivB42knsXLiW4mFDtHvbdQ.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=f4921d56599664bdd7e92e20b29f17d8100f8070" /><link href="https://www.reddit.com/r/LangChain/comments/1e03z0y/i_used_langchain_to_build_a_slack_agent_my/" /><updated>2024-07-10T19:03:37+00:00</updated><published>2024-07-10T19:03:37+00:00</published><title>I used Langchain to build a Slack Agent - My Experience</title></entry><entry><author><name>/u/New-Cryptographer131</name><uri>https://www.reddit.com/user/New-Cryptographer131</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I have been working with django and langchain for few days. What i have noticed is it’s not possible to save the langchain messages directly with the django ORM. So the integration seems bit confusing when trying to save some details using the django models and others using pydantic models for langchain. Is this a missing support by langchain or am I missing something ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/New-Cryptographer131&quot;&gt; /u/New-Cryptographer131 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e0nqxb/langchain_django_orm_integration/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e0nqxb/langchain_django_orm_integration/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1e0nqxb</id><link href="https://www.reddit.com/r/LangChain/comments/1e0nqxb/langchain_django_orm_integration/" /><updated>2024-07-11T12:28:07+00:00</updated><published>2024-07-11T12:28:07+00:00</published><title>Langchain + Django ORM integration</title></entry><entry><author><name>/u/FunSession1164</name><uri>https://www.reddit.com/user/FunSession1164</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;On one hand I really want to build from the ground up and see what happens every step of the way, on the other hand, I’m trying to embrace abstraction, l would just go for a no-code drag and drop if the final product works fine. I have been looking into &lt;a href=&quot;https://smythos.com/&quot;&gt;~SmythOS~&lt;/a&gt;, a no code platform for creating AI agents and I think I am sold. But I still can’t shake the feeling that it’s too good to be true. What do you think? Would you go for a no code option or would you rather just do everything yourself?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/FunSession1164&quot;&gt; /u/FunSession1164 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e0xdgw/ai_agents_are_nocode_platforms_better/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e0xdgw/ai_agents_are_nocode_platforms_better/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1e0xdgw</id><link href="https://www.reddit.com/r/LangChain/comments/1e0xdgw/ai_agents_are_nocode_platforms_better/" /><updated>2024-07-11T19:28:20+00:00</updated><published>2024-07-11T19:28:20+00:00</published><title>AI agents. Are no-code platforms better?</title></entry><entry><author><name>/u/BellaHi</name><uri>https://www.reddit.com/user/BellaHi</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e0dp2l/from_concept_to_execution_crafting_cuttingedge_ai/&quot;&gt; &lt;img src=&quot;https://external-preview.redd.it/mrFcgMwx0I5hbiBHVY0Y7RFxYzTqpONsp1DNMaJb2bo.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bbd4ab5ae934c2107af3adc3067699bb73f680f8&quot; alt=&quot;From Concept to Execution: Crafting Cutting-Edge AI Agents with LangChain&quot; title=&quot;From Concept to Execution: Crafting Cutting-Edge AI Agents with LangChain&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/BellaHi&quot;&gt; /u/BellaHi &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://myscale.com/blog/build-ai-agent-with-langchain/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e0dp2l/from_concept_to_execution_crafting_cuttingedge_ai/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1e0dp2l</id><media:thumbnail url="https://external-preview.redd.it/mrFcgMwx0I5hbiBHVY0Y7RFxYzTqpONsp1DNMaJb2bo.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=bbd4ab5ae934c2107af3adc3067699bb73f680f8" /><link href="https://www.reddit.com/r/LangChain/comments/1e0dp2l/from_concept_to_execution_crafting_cuttingedge_ai/" /><updated>2024-07-11T02:09:43+00:00</updated><published>2024-07-11T02:09:43+00:00</published><title>From Concept to Execution: Crafting Cutting-Edge AI Agents with LangChain</title></entry><entry><author><name>/u/md1630</name><uri>https://www.reddit.com/user/md1630</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;m trying to create a streaming agent chatbot with streamlit as the frontend. I was able to find an example of this using callbacks, and streamlit even has a special callback class.&lt;/p&gt; &lt;p&gt;However, it looks like things sure change quickly. From langchain&amp;#39;s documentation it looks like callbacks is being deprecated, and there is a new function astream_events. I&amp;#39;m very happy with how simple it is to stream events with astream_events.&lt;/p&gt; &lt;p&gt;However, I&amp;#39;m having some issues with getting this to work with streamlit. It&amp;#39;s mostly working, but for some small details. For example:&lt;/p&gt; &lt;pre&gt;&lt;code&gt; async for event in agent_executor.astream_events( {&amp;quot;input&amp;quot;: user_query}, version=&amp;quot;v2&amp;quot;, config=cfg ): kind = event[&amp;quot;event&amp;quot;] if kind == &amp;quot;on_chat_model_stream&amp;quot;: content = event[&amp;quot;data&amp;quot;][&amp;quot;chunk&amp;quot;].content if content: answer_container.write(content) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;There are other events, but I can illustrate this problem with just &amp;quot;on_chat_model_stream&amp;quot;. &lt;/p&gt; &lt;p&gt;Just focusing on the event &amp;quot;on_chat_model_stream&amp;quot;, this causes the content to be written one word in every line, so the chat message looks like:&lt;/p&gt; &lt;p&gt;Hello&lt;/p&gt; &lt;p&gt;how&lt;/p&gt; &lt;p&gt;may&lt;/p&gt; &lt;p&gt;I&lt;/p&gt; &lt;p&gt;help&lt;/p&gt; &lt;p&gt;you&lt;/p&gt; &lt;p&gt;So, it looks like the content is being written token by token. However, I can&amp;#39;t use write_stream, because I would get the error ``st.write_stream` expects a generator or stream-like object as input not &amp;lt;class &amp;#39;str&amp;#39;&amp;gt;. Please use `st.write` instead for this data type.`&lt;/p&gt; &lt;p&gt;How do I get the content to stream in this case?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/md1630&quot;&gt; /u/md1630 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e0ezhj/how_to_use_agentexecutorstream_events_with/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e0ezhj/how_to_use_agentexecutorstream_events_with/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1e0ezhj</id><link href="https://www.reddit.com/r/LangChain/comments/1e0ezhj/how_to_use_agentexecutorstream_events_with/" /><updated>2024-07-11T03:16:34+00:00</updated><published>2024-07-11T03:16:34+00:00</published><title>how to use agentexecutor.stream_events with streamlit</title></entry><entry><author><name>/u/stoic-AI</name><uri>https://www.reddit.com/user/stoic-AI</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi everyone!&lt;/p&gt; &lt;p&gt;I&amp;#39;ve created a mini series on how to build a real time AI application using Django, LangChain and Celery.&lt;/p&gt; &lt;p&gt;Free knowledge - posting it in here for anyone working on something similar and had the same blockers I had when building.&lt;/p&gt; &lt;p&gt;Let me know what you think on how I could potentially improve this architecture.&lt;/p&gt; &lt;p&gt;Part 1&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://medium.com/towardsdev/how-to-set-up-django-from-scratch-with-celery-channels-redis-docker-real-time-django-601dff7ada79&quot;&gt;https://medium.com/towardsdev/how-to-set-up-django-from-scratch-with-celery-channels-redis-docker-real-time-django-601dff7ada79&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Part 2&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://medium.com/towardsdev/how-to-set-up-django-from-scratch-with-celery-channels-redis-docker-real-time-django-5828a1ea43a3&quot;&gt;https://medium.com/towardsdev/how-to-set-up-django-from-scratch-with-celery-channels-redis-docker-real-time-django-5828a1ea43a3&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Part 3&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://medium.com/towardsdev/how-to-set-up-django-from-scratch-with-celery-channels-redis-docker-real-time-django-8e73c7b6b4c8&quot;&gt;https://medium.com/towardsdev/how-to-set-up-django-from-scratch-with-celery-channels-redis-docker-real-time-django-8e73c7b6b4c8&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Part 4&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://medium.com/towardsdev/how-to-set-up-django-from-scratch-with-celery-channels-redis-docker-real-time-django-c090c300517a&quot;&gt;https://medium.com/towardsdev/how-to-set-up-django-from-scratch-with-celery-channels-redis-docker-real-time-django-c090c300517a&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Part 5&lt;br/&gt; &lt;a href=&quot;https://medium.com/@cubode/how-to-set-up-django-from-scratch-with-celery-channels-redis-docker-real-time-django-0845eb7e083c&quot;&gt;https://medium.com/@cubode/how-to-set-up-django-from-scratch-with-celery-channels-redis-docker-real-time-django-0845eb7e083c&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/stoic-AI&quot;&gt; /u/stoic-AI &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dzs2p4/real_time_ai_workers_using_django_x_langchain/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dzs2p4/real_time_ai_workers_using_django_x_langchain/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dzs2p4</id><link href="https://www.reddit.com/r/LangChain/comments/1dzs2p4/real_time_ai_workers_using_django_x_langchain/" /><updated>2024-07-10T10:07:51+00:00</updated><published>2024-07-10T10:07:51+00:00</published><title>Real Time AI Workers using Django x LangChain</title></entry><entry><author><name>/u/Pure-Exercise-9955</name><uri>https://www.reddit.com/user/Pure-Exercise-9955</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hello everyone,&lt;/p&gt; &lt;p&gt;I have to build a Retrieval-Augmented Generation (RAG) system using a Large Language Model (LLM) &lt;strong&gt;to search for an information about an employee within a vast collection of documents.&lt;/strong&gt; The LLM must return the information with references indicating the specific page and document.&lt;/p&gt; &lt;p&gt;if someone already done a project similar or can help me with the steps.&lt;/p&gt; &lt;p&gt;what i decided to do is to select an open source llm (qwen2), then fine tune it, after that build a RAG&lt;/p&gt; &lt;p&gt;&lt;strong&gt;what is your opinion gays&lt;/strong&gt;&lt;br/&gt; &lt;strong&gt;i need your help&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Pure-Exercise-9955&quot;&gt; /u/Pure-Exercise-9955 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e02kx2/rag_to_search_for_an_information_about_an/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e02kx2/rag_to_search_for_an_information_about_an/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1e02kx2</id><link href="https://www.reddit.com/r/LangChain/comments/1e02kx2/rag_to_search_for_an_information_about_an/" /><updated>2024-07-10T18:07:58+00:00</updated><published>2024-07-10T18:07:58+00:00</published><title>RAG to search for an information about an employee within a vast collection of documents</title></entry><entry><author><name>/u/Typical-Scene-5794</name><uri>https://www.reddit.com/user/Typical-Scene-5794</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi &lt;a href=&quot;/r/langchain&quot;&gt;r/langchain&lt;/a&gt;, I&amp;#39;m sharing an example on building a multi-modal search application using GPT-4o, featuring extraction of metadata and hybrid indexing for accurately retrieving relevant information from presentations.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Repo: &lt;a href=&quot;https://github.com/pathwaycom/llm-app/tree/main/examples/pipelines/slides_ai_search&quot;&gt;~https://github.com/pathwaycom/llm-app/tree/main/examples/pipelines/slides_ai_search~&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Architecture: &lt;a href=&quot;https://github.com/pathwaycom/llm-app/tree/main/examples/pipelines/slides_ai_search#architecture&quot;&gt;~https://github.com/pathwaycom/llm-app/tree/main/examples/pipelines/slides_ai_search#architecture~&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This project also focuses on automatically updating indexes as changes happen in your repository. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Quick details:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Ingestion:&lt;/strong&gt; The application reads slide files (PPTX and PDF) stored locally or on Google Drive or Microsoft SharePoint.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Parsing:&lt;/strong&gt; Utilizes the SlideParser from Pathway, configured with a detailed schema. The app parses images, charts, diagrams, and other visual elements as well, and features automatic unstructured metadata extraction. &lt;/li&gt; &lt;li&gt;&lt;strong&gt;Indexing:&lt;/strong&gt; Parsed slide content is embedded using OpenAI&amp;#39;s embedder and stored in Pathway&amp;#39;s vector store (&lt;a href=&quot;https://python.langchain.com/v0.2/docs/integrations/vectorstores/pathway/&quot;&gt;natively available on LangChain&lt;/a&gt;) that is optimized for incremental indexing.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;How it helps:&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Text in presentations is often limited. This example removes the need to manually sift through countless presentations by recalling keywords.&lt;/li&gt; &lt;li&gt;Organize your slide library by topic or other criteria. Indexes update automatically whenever a slide is added, modified, or removed.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;Preliminary Results:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;This method has proven to be efficient in managing large volumes of slides, ensuring that the most up-to-date and accurate information is available. It significantly enhances productivity by streamlining the search process across PowerPoints, PDFs, and Slides.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Open to your questions and feedback!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Typical-Scene-5794&quot;&gt; /u/Typical-Scene-5794 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dzzrw5/accurate_multimodal_slides_search_with_realtime/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dzzrw5/accurate_multimodal_slides_search_with_realtime/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dzzrw5</id><link href="https://www.reddit.com/r/LangChain/comments/1dzzrw5/accurate_multimodal_slides_search_with_realtime/" /><updated>2024-07-10T16:15:22+00:00</updated><published>2024-07-10T16:15:22+00:00</published><title>Accurate Multimodal Slides Search with Real-Time Updates from SharePoint, Google Drive, and Local Data Sources</title></entry></feed>