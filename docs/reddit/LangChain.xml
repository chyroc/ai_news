<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/"><category term="LangChain" label="r/LangChain"/><updated>2024-04-10T21:25:54+00:00</updated><icon>https://www.redditstatic.com/icon.png/</icon><id>/r/LangChain.rss</id><link rel="self" href="https://www.reddit.com/r/LangChain.rss" type="application/atom+xml" /><link rel="alternate" href="https://www.reddit.com/r/LangChain" type="text/html" /><subtitle>LangChain is an open-source framework and developer toolkit that helps developers get LLM applications from prototype to production. It is available for Python and Javascript at https://www.langchain.com/.</subtitle><title>LangChain</title><entry><author><name>/u/zchaarm</name><uri>https://www.reddit.com/user/zchaarm</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;A place for members of &lt;a href=&quot;/r/LangChain&quot;&gt;r/LangChain&lt;/a&gt; to chat with each other&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/zchaarm&quot;&gt; /u/zchaarm &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_10ljho9</id><link href="https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/" /><updated>2023-01-26T04:33:05+00:00</updated><published>2023-01-26T04:33:05+00:00</published><title>r/LangChain Lounge</title></entry><entry><author><name>/u/usnavy13</name><uri>https://www.reddit.com/user/usnavy13</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I started building internal LLM tools for my company and originally thought LangChain would be a good tool. At the time I was wrong, there were many issues with the project and I found out I was better off removing and replacing LangChain with my own implementations. &lt;/p&gt; &lt;p&gt;I&amp;#39;m glad to say I&amp;#39;ve started to bring LangChain back into my projects. I have to commend the LangChain team for all their work to improve the project. The project still has its issues (mainly documentation and over-abstraction) but overall much better. The community tools provide the best suite of integrations of any LLM package.&lt;/p&gt; &lt;p&gt;The thing that impresses me the most is LangSmith. It gives you unparalleled visibility into what your app is doing and provides tools that supercharge the development process. Fantastic product!&lt;/p&gt; &lt;p&gt;TLDR: its better now&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/usnavy13&quot;&gt; /u/usnavy13 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c0oucr/i_am_coming_back_to_langchain/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c0oucr/i_am_coming_back_to_langchain/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1c0oucr</id><link href="https://www.reddit.com/r/LangChain/comments/1c0oucr/i_am_coming_back_to_langchain/" /><updated>2024-04-10T15:49:44+00:00</updated><published>2024-04-10T15:49:44+00:00</published><title>I am coming back to LangChain!</title></entry><entry><author><name>/u/Desik_1998</name><uri>https://www.reddit.com/user/Desik_1998</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;As the title describes, I&amp;#39;ve used Claude 3 Sonnet to create a 30K word story which heavily grounds in details. Here is the &lt;a href=&quot;https://github.com/desik1998/NovelWithLLMs/blob/main/Novel.md&quot;&gt;story link&lt;/a&gt; (For now put this on Github itself). The story is about American founding fathers returning back to 21st Century. Currently it consists of 3 chapters and there are 4 more chapters to write. I&amp;#39;ve already reviewed it with few of my friends who&amp;#39;re avid novel readers and most of them have responded with &amp;#39;it doesn&amp;#39;t feel AI written&amp;#39;, it&amp;#39;s interesting (subjective but most have said this), grounds heavily on details. Requesting to read the novel and provide the feedback &lt;/p&gt; &lt;p&gt;Github Link: &lt;a href=&quot;https://github.com/desik1998/NovelWithLLMs/tree/main&quot;&gt;https://github.com/desik1998/NovelWithLLMs/tree/main&lt;/a&gt; &lt;/p&gt; &lt;h1&gt;Approach to create long story:&lt;/h1&gt; &lt;p&gt;LLMs such as Claude 3 / Gpt 4 currently allows input context length of 150K words and can output 3K words at once. A typical novel in general has a total of 60K-100K words. Considering the 3K output limit, it isn&amp;#39;t possible to generate a novel in one single take. So the intuition here is that let the LLM &lt;strong&gt;generate 1 event at a time and once the event is generated, add it to the existing story and continously repeat this process&lt;/strong&gt;. Although theoretically this approach might seem to work, just doing this leads to LLM moving quickly from one event to another, not being very grounded in details, llm not generating event which is a continuation of the current story, LLM generating mistakes based on the current story etc. &lt;/p&gt; &lt;p&gt;To address this, the following steps are taken: &lt;/p&gt; &lt;h1&gt;1. Initially fix on the high level story:&lt;/h1&gt; &lt;p&gt;Ask LLM to generate high level plot of the story like at a 30K depth. Generate multiple plots as such. In our case, the high level line in mind was &lt;strong&gt;Founding Fathers returning back&lt;/strong&gt;. Using this line, LLM was asked to generated many plots enhancing this line. It suggested many plots such as Founding fathers called back for being judged based on their actions, founding fathers called back to solve AI crisis, founding fathers come back for fighting against China, Come back and fight 2nd revolutionary war etc. Out of all these, the 2nd revolutionary war seemed the best. Post the plot, LLM was prompted to generate many stories from this plot. Out of these, multiple ideas in the stories were combined (manually) to get to fix on high level story. Once this is done, get the chapters for the high level story (again generated multiple outputs instead of 1). And generating chapters should be easy if the high level story is already present &lt;/p&gt; &lt;h1&gt;2. Do the event based generation for events in chapter:&lt;/h1&gt; &lt;p&gt;Once chapters are fixed, now start with the generation of events in a chapter but &lt;strong&gt;1 event at a time like described above&lt;/strong&gt;. To make sure that the event is grounded in details, a little prompting is reqd telling the LLM to avoid moving too fast into the event and ground to details, avoid generating same events as past etc. &lt;a href=&quot;https://github.com/desik1998/NovelWithLLMs/blob/main/PROMPT.md&quot;&gt;Prompt used till now&lt;/a&gt; (There are some repetitions in the prompt but this works well). Even after this, the output generated by LLM might not be very compelling so to get a good output, generate the output multiple times. And in general generating &lt;strong&gt;5-10 outputs&lt;/strong&gt;, results in a good possible result. And it&amp;#39;s better to do this by varying temperatures. In case of current story, the temperature b/w 0.4-0.8 worked well. Additionally, the rationale behind generating multiple outputs is, given LLMs generate different output everytime, the chances of getting good output when prompted multiple times increases. Even after generating multiple outputs with different temperatures, if it doesn&amp;#39;t yield good results, understand what it&amp;#39;s doing wrong for example like avoid repeating events and tell it to avoid doing that. For example in the 3rd chapter when the LLM was asked to explain the founders about the history since their time, it was rushing off, so &lt;a href=&quot;https://github.com/desik1998/NovelWithLLMs/blob/main/HistoryChapterPrompt.md&quot;&gt;an instruction to explain the historic events year-by-year&lt;/a&gt; was added in the prompt. Sometimes the LLM also generates part of the event which is too good but the overall event is not good, in this scenario adding the part of the event to the story and continuing to generate the story worked well. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Overall Gist:&lt;/strong&gt; Generate the event multiple times with different temperatures and take the best amongst them. If it still doesn&amp;#39;t work, prompt it to avoid doing the wrong things it&amp;#39;s doing &lt;/p&gt; &lt;p&gt;Overall Event Generation: Instead of generating the next event in a chat conversation mode, giving the whole story till now as a combination of events in a single prompt and asking it to generate next event worked better. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Conversation Type 1:&lt;/strong&gt; &lt;/p&gt; &lt;pre&gt;&lt;code&gt;human: generate 1st event Claude: Event1 human: generate next, Claude: Event2, human: generate next ... &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Conversation Type 2:&lt;/strong&gt; (Better) &lt;/p&gt; &lt;pre&gt;&lt;code&gt;Human: Story till now: Event1 + Event2 + ... + EventN. Generate next event Claude: Event(N+1) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Also as the events are generated, one keeps getting new ideas to proceed on the story chapters. And if any event generated is so good, but aligns little different from current story, one can also change the future story/chapters. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;The current approach, doesn&amp;#39;t require any code&lt;/strong&gt; and long stories can be generated directly using the &lt;strong&gt;Claude Playground or Amazon Bedrock Playground&lt;/strong&gt; (Claude is hosted). Claude Playground has the best Claude Model Opus which Bedrock currently lacks but given this Model is 10X costly, avoided it and went with the 2nd Best Sonnet Model. As per my experience, the results on Bedrock are better than the ones in Claude Playground &lt;/p&gt; &lt;h1&gt;Questions:&lt;/h1&gt; &lt;ol&gt; &lt;li&gt;Why wasn&amp;#39;t Gpt4 used to create this story?&lt;br/&gt;&lt;/li&gt; &lt;/ol&gt; &lt;ul&gt; &lt;li&gt;When asked Gpt4 to generate the next event in the story, there was no coherence in the next event generated with the existing story. Maybe with more prompt engineering, this might be solved but Claude 3 was giving better output without much effort so went with it. Infact, Claude 3 Sonnet (the 2nd best model from Claude) is doing much better when compared to Gpt4.&lt;br/&gt;&lt;/li&gt; &lt;/ul&gt; &lt;ol&gt; &lt;li&gt;How much cost did it take to do this?&lt;br/&gt;&lt;/li&gt; &lt;/ol&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;$50-100&lt;/strong&gt;&lt;br/&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Further Improvements:&lt;/h1&gt; &lt;ol&gt; &lt;li&gt;Explore ways to avoid long input contexts. This can further reduce the cost considering most of the cost is going into this step. Possible Solutions:&lt;br/&gt;&lt;/li&gt; &lt;/ol&gt; &lt;ul&gt; &lt;li&gt;Give gists of the events happened in the story till now instead of whole story as an input to the LLM. References: &lt;a href=&quot;https://deepmind.google/research/publications/74917/&quot;&gt;1&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/html/2310.00785v3&quot;&gt;2&lt;/a&gt;&lt;br/&gt;&lt;/li&gt; &lt;/ul&gt; &lt;ol&gt; &lt;li&gt;Avoid the human loop as part of the choosing the best event generated. Currently it takes a lot of human time when choosing the best event generated. Due to this, the time to generate a story can take from few weeks to few months (1-1.5 months). If this step is automated atleast to some degree, the time to write the long story will further decrease. Possible Solutions:&lt;br/&gt;&lt;/li&gt; &lt;/ol&gt; &lt;ul&gt; &lt;li&gt;Use an LLM to determine what are the best events or top 2-3 events generated. This can be done based on multiple factors such as whether the event is a continuation, the event is not repeating itself. And based on these factors, LLM can rate the top responses. References: &lt;a href=&quot;https://huggingface.co/papers/2308.06259&quot;&gt;Last page in this paper&lt;/a&gt; &lt;/li&gt; &lt;li&gt;Train a reward model (With or without LLM) for determining which generated event is better. &lt;a href=&quot;https://arxiv.org/html/2401.10020v1&quot;&gt;LLM as Reward model&lt;/a&gt;&lt;br/&gt;&lt;/li&gt; &lt;/ul&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;The current approach generates only 1 story. Instead generate a Tree of possible stories for a given plot. For example, multiple generations for an event can be good, in this case, select all of them and create different stories. &lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Use the same approach for other things such as movie story generation, Text Books, Product document generation etc &lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Benchmark LLMs Long Context not only on RAG but also on Generation &lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Desik_1998&quot;&gt; /u/Desik_1998 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c0w79c/used_claudes_200k_context_length_to_write_a_30k/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c0w79c/used_claudes_200k_context_length_to_write_a_30k/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1c0w79c</id><link href="https://www.reddit.com/r/LangChain/comments/1c0w79c/used_claudes_200k_context_length_to_write_a_30k/" /><updated>2024-04-10T20:50:00+00:00</updated><published>2024-04-10T20:50:00+00:00</published><title>Used Claude's 200K context length to write a 30K word novel which heavily grounds in details unlike the existing novels</title></entry><entry><author><name>/u/XariZaru</name><uri>https://www.reddit.com/user/XariZaru</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;So, I noticed on ChatGPT and also on my own chatbot (for a brief period of time), the chatbot would apologize and correct itself when provided feedback by the user. For example:&lt;/p&gt; &lt;p&gt;Q: What is 2+2?&lt;br/&gt; A: 5&lt;/p&gt; &lt;p&gt;Q: No, it is 4.&lt;/p&gt; &lt;p&gt;Expected A: Sorry, you&amp;#39;re right. It is 4.&lt;/p&gt; &lt;p&gt;Actual A: 5.&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;My chatbot now is just sticking strongly to its answer instead of remedying itself. What is the best way to acknowledge the corrected answer?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/XariZaru&quot;&gt; /u/XariZaru &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c0t2tt/allow_chatbot_to_correct_itself_to_user_feedback/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c0t2tt/allow_chatbot_to_correct_itself_to_user_feedback/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1c0t2tt</id><link href="https://www.reddit.com/r/LangChain/comments/1c0t2tt/allow_chatbot_to_correct_itself_to_user_feedback/" /><updated>2024-04-10T18:43:49+00:00</updated><published>2024-04-10T18:43:49+00:00</published><title>Allow Chatbot to Correct Itself to User Feedback</title></entry><entry><author><name>/u/Key_Radiant</name><uri>https://www.reddit.com/user/Key_Radiant</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Key_Radiant&quot;&gt; /u/Key_Radiant &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c0srpo/what_vector_database_do_you_use/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c0srpo/what_vector_database_do_you_use/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1c0srpo</id><link href="https://www.reddit.com/r/LangChain/comments/1c0srpo/what_vector_database_do_you_use/" /><updated>2024-04-10T18:31:35+00:00</updated><published>2024-04-10T18:31:35+00:00</published><title>What vector database do you use?</title></entry><entry><author><name>/u/onsies</name><uri>https://www.reddit.com/user/onsies</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I used Gradio to deploy, which is quick and easy. What‚Äôs the easiest way to add stripe payment collection for subscription?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/onsies&quot;&gt; /u/onsies &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c0rvek/best_method_to_quickly_and_easily_deploy/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c0rvek/best_method_to_quickly_and_easily_deploy/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1c0rvek</id><link href="https://www.reddit.com/r/LangChain/comments/1c0rvek/best_method_to_quickly_and_easily_deploy/" /><updated>2024-04-10T17:55:05+00:00</updated><published>2024-04-10T17:55:05+00:00</published><title>Best Method to Quickly and Easily Deploy?</title></entry><entry><author><name>/u/profepcot</name><uri>https://www.reddit.com/user/profepcot</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I wrote a piece on &lt;a href=&quot;https://www.mirascope.io/post/langchain-prompt-template&quot;&gt;prompt templates in LangChain&lt;/a&gt;, how they work and the different approach &lt;a href=&quot;https://github.com/Mirascope/mirascope&quot;&gt;Mirascope&lt;/a&gt; takes with colocation. I hope you find it useful.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/profepcot&quot;&gt; /u/profepcot &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c0qjq2/prompt_templates_in_langchain/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c0qjq2/prompt_templates_in_langchain/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1c0qjq2</id><link href="https://www.reddit.com/r/LangChain/comments/1c0qjq2/prompt_templates_in_langchain/" /><updated>2024-04-10T17:00:13+00:00</updated><published>2024-04-10T17:00:13+00:00</published><title>Prompt templates in LangChain</title></entry><entry><author><name>/u/Proof-Character-9828</name><uri>https://www.reddit.com/user/Proof-Character-9828</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I am quite new to LangChain and Python as im mainly doing C# but i am interested in using AI on my own data.&lt;br/&gt; So i wrote some python code using langchain that:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Gets my Emails via IMAP&lt;/li&gt; &lt;li&gt;Creates JSON from my E-Mails (JSONLoader)&lt;/li&gt; &lt;li&gt;Creates a Vectordatabase where each mail is a vector (FAISS, OpenAIEmbeddings)&lt;/li&gt; &lt;li&gt;Does a similarity search according to the query returning the 3 mails that match the query the most&lt;/li&gt; &lt;li&gt;feeds the result of the similarity search to the LLM (GPT 3.5 Turbo) using the query AGAIN&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;The LLM Prompt then looks something like:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;The question is &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;{query}&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Here are some information that can help you to answer the question: &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;{similarity_search_result}&lt;/p&gt; &lt;p&gt;Ok so far so good... when my question is:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;When was my last mail sent to xyz@gmail.com? &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;i get a correct answer... -&amp;gt; e.g last mail received 10.04.2024 14:11 &lt;/p&gt; &lt;p&gt;But what if i want to have an answer to the following question&lt;/p&gt; &lt;pre&gt;&lt;code&gt;How many mails have been sent by xyz@gmail.com? &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Because the similarity search only gets the vectors that are most similar, how can i just get an answer about the amount?&lt;br/&gt; Even if the similarity search would deliver 150 mails instead of 3 sent by [&lt;a href=&quot;mailto:xyz@gmail.com&quot;&gt;xyz@gmail.com&lt;/a&gt;](mailto:&lt;a href=&quot;mailto:xyz@gmail.com&quot;&gt;xyz@gmail.com&lt;/a&gt;) i cant just feed them all into the LLM prompt right? &lt;/p&gt; &lt;p&gt;So what is my mistake here?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Proof-Character-9828&quot;&gt; /u/Proof-Character-9828 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c0k2qo/langchain_emails_with_llm/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c0k2qo/langchain_emails_with_llm/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1c0k2qo</id><link href="https://www.reddit.com/r/LangChain/comments/1c0k2qo/langchain_emails_with_llm/" /><updated>2024-04-10T12:13:13+00:00</updated><published>2024-04-10T12:13:13+00:00</published><title>LangChain E-Mails with LLM</title></entry><entry><author><name>/u/Albert_AG</name><uri>https://www.reddit.com/user/Albert_AG</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hello! I&amp;#39;m a LangChain beginner and I need some help.&lt;/p&gt; &lt;p&gt;I&amp;#39;m working a PDF Chatbot that takes in a stock annual report as a PDF and does technical question answering on it [Mathematical] - Please help me! How do I approach this problem and where do I begin? I&amp;#39;ll take any help I can.&lt;/p&gt; &lt;p&gt;I&amp;#39;ve been trying to do this with RAG. Any Suggestions? Thank you.&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Albert_AG&quot;&gt; /u/Albert_AG &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c0q8qm/beginner_to_langchain_need_help_stock_annual/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c0q8qm/beginner_to_langchain_need_help_stock_annual/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1c0q8qm</id><link href="https://www.reddit.com/r/LangChain/comments/1c0q8qm/beginner_to_langchain_need_help_stock_annual/" /><updated>2024-04-10T16:47:43+00:00</updated><published>2024-04-10T16:47:43+00:00</published><title>Beginner to LangChain. Need help! - [Stock Annual Report PDF Chatbot using RAG].</title></entry><entry><author><name>/u/NasserAAA</name><uri>https://www.reddit.com/user/NasserAAA</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;&lt;strong&gt;The following code:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;tools = load_tools([&amp;quot;llm-math&amp;quot;],llm=llm)&lt;br/&gt; chain = ConversationalRetrievalChain.from_llm(&lt;br/&gt; llm=llm,&lt;br/&gt; tools=tools,&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Causes the following error message:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;\venv\lib\site-packages\langchain_core\load\serializable.py&amp;quot;, line 120, in &lt;strong&gt;init&lt;/strong&gt;&lt;br/&gt; super().&lt;strong&gt;init&lt;/strong&gt;(**kwargs)&lt;br/&gt; File &amp;quot;pydantic\main.py&amp;quot;, line 341, in pydantic.main.BaseModel.&lt;strong&gt;init&lt;/strong&gt;&lt;br/&gt; pydantic.error_wrappers.ValidationError: 1 validation error for ConversationalRetrievalChain&lt;br/&gt; tools&lt;br/&gt; extra fields not permitted (type=value_error.extra)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;I want a way to call math tool because my chain fails to calculate summations for example like this:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;29577.30 + 24683.36 + 23262.12 + 26421.73 + 52409.77 + 25314.39 = 137,605.52, which is wrong and most likely it gets a new answer each time.&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/NasserAAA&quot;&gt; /u/NasserAAA &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c0mkl4/adding_tools_to_conversationalretrievalchainfrom/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c0mkl4/adding_tools_to_conversationalretrievalchainfrom/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1c0mkl4</id><link href="https://www.reddit.com/r/LangChain/comments/1c0mkl4/adding_tools_to_conversationalretrievalchainfrom/" /><updated>2024-04-10T14:11:59+00:00</updated><published>2024-04-10T14:11:59+00:00</published><title>Adding tools to ConversationalRetrievalChain.from_llm causes Pydantic error</title></entry><entry><author><name>/u/PresentationSevere89</name><uri>https://www.reddit.com/user/PresentationSevere89</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;m a bit frustrated. We&amp;#39;ve been working for more than 6 months on an MVP for a Q&amp;amp;A chat about product documentation. After all that, the LLM still hallucinates a lot and gives very basic responses. I would love to have, at this time, a system capable of using several documents to formulate a sound response to a user&amp;#39;s question. I&amp;#39;m using GPT-3.5. I know how capable the model is, and I hate how basic our chat answers are. Maybe it&amp;#39;s the chain, the steps to formulate a response and validate it, or the bad retriever that can&amp;#39;t bring useful documents from the user&amp;#39;s reduced query... I feel like we&amp;#39;ve tried a lot: few shots, fine-tuning embeddings, fine-tuning the GPT, etc... But somehow, we don&amp;#39;t get it to work. I just feel we can, but in the end, we don&amp;#39;t. Any advice to make a killer LLM-powered chat about product documentation?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/PresentationSevere89&quot;&gt; /u/PresentationSevere89 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c0cnyj/easiest_way_to_improve_rag_chat_help/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c0cnyj/easiest_way_to_improve_rag_chat_help/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1c0cnyj</id><link href="https://www.reddit.com/r/LangChain/comments/1c0cnyj/easiest_way_to_improve_rag_chat_help/" /><updated>2024-04-10T04:13:47+00:00</updated><published>2024-04-10T04:13:47+00:00</published><title>Easiest way to improve RAG chat - help</title></entry><entry><author><name>/u/cryptokaykay</name><uri>https://www.reddit.com/user/cryptokaykay</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c0do04/results_of_evaluating_the_ai_oracle_approach_a/&quot;&gt; &lt;img src=&quot;https://b.thumbs.redditmedia.com/GCdPwBj4teGkowjNA4iTF6pGHJldADy1b4cFZ_8gtFI.jpg&quot; alt=&quot;Results of evaluating the AI Oracle approach - a novel way to improve your LLM application's accuracy&quot; title=&quot;Results of evaluating the AI Oracle approach - a novel way to improve your LLM application's accuracy&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Recently, I saw this tweet about the AI Oracle approach for improving the accuracy and quality of responses for your LLM application. The technique is super simple:&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://twitter.com/mattshumer_/status/1777382373283299365&quot;&gt;https://twitter.com/mattshumer_/status/1777382373283299365&lt;/a&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Send the request to 3 LLMs - Claude, GPT4, and Perplexity.&lt;/li&gt; &lt;li&gt;Give the responses to Claude again and prompt engineer to pick the best and accurate response.&lt;br/&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;I got curious about this and decided to do some evaluations on this approach. Sharing some metrics/measurements in this post.&lt;/p&gt; &lt;p&gt;This one is pretty obvious, the latency on having all 3 LLMs generate a response and picking the best out of the 3 is high. But, I do recognize that this can be improved by parallelizing the operations. &lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/lndc3gwp3ltc1.png?width=1200&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=02ec4109e9a3211724686a40ecbb9110dc70033c&quot;&gt;https://preview.redd.it/lndc3gwp3ltc1.png?width=1200&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=02ec4109e9a3211724686a40ecbb9110dc70033c&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Ran the following tests for both the combined AI Oracle approach and using a single LLM:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;Factual Accuracy&lt;/strong&gt; - Evaluated for correctness of responses.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;Realtime data&lt;/strong&gt; - Evaluated based on asking information related to realtime data.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;Adversarial Testing&lt;/strong&gt; - Evaluated on whether the LLM is able to pickup the signal correctly by placing the question in between a bunch of garbage data. The LLM was given a positive score if it correctly responded to the question without mentioning the garbage data. &lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;Consistency checks&lt;/strong&gt; - Evaluated on whether the LLM gave a response consistently when the same question was asking many times. Mainly looked for structural consistency of the response.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;Quality&lt;/strong&gt; - Evaluated on the quality - sentence structure, adherence to the prompt etc. &lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;AI Oracle Approach&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Results for the AI Oracle approach: For some reason, it could not pick up the realtime information even once. I am sure with some prompt engineering, this metric can be improved. It did poorly on Adversarial testing - mostly because Claude and Pplx&amp;#39;s responses. &lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/9ggjkthz3ltc1.png?width=1200&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=033d105e3793e68fb5b2bd9a134cbbefec423cc7&quot;&gt;https://preview.redd.it/9ggjkthz3ltc1.png?width=1200&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=033d105e3793e68fb5b2bd9a134cbbefec423cc7&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Claude (claude-3-opus-20240229)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;As expected, Claude did not do well on Realtime testing. But, interestingly, it did not do great with adversarial and consistency tests either.&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/b33q8qz24ltc1.png?width=1200&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ef154b780655bb520c406d1aa53c8b91fc2c8038&quot;&gt;https://preview.redd.it/b33q8qz24ltc1.png?width=1200&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ef154b780655bb520c406d1aa53c8b91fc2c8038&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;GPT4&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Again, GPT4 does not have realtime capabilities. But it did extremely well on everything else except consistency checks where the responses were structured quite differently each time.&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/2jz4y6864ltc1.png?width=1200&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a35bfe557f4164251c4900d4e2c62cf8a5c7b04d&quot;&gt;https://preview.redd.it/2jz4y6864ltc1.png?width=1200&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a35bfe557f4164251c4900d4e2c62cf8a5c7b04d&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Perplexity (pplx-70b-online)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;As expected Perplexity&amp;#39;s realtime capabilities are unmatched. But, it did not do that well with adversarial and consistency tests which in turn skewed the metrics for AI Oracle approach as well.Notably, the quality of responses from Perplexity were far better than the rest.&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/y0mqsr4b4ltc1.png?width=1200&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3ba1acfd7159b8f0bd5164a0e99af7f8ab4f5071&quot;&gt;https://preview.redd.it/y0mqsr4b4ltc1.png?width=1200&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3ba1acfd7159b8f0bd5164a0e99af7f8ab4f5071&lt;/a&gt;&lt;/p&gt; &lt;p&gt;In conclusion, you can get a near perfect score for the AI Oracle approach with a bit of prompt engineering. But you definitely lose performance in the process. Even when parallelized, it is only as slow as the slowest LLM. Token usage/cost is also going to be higher.&lt;/p&gt; &lt;p&gt;Finally, if you are curious, all these evaluations were done using Langtrace - an open source LLM monitoring and evaluations tool that I am currently developing.&lt;/p&gt; &lt;p&gt;Github: &lt;a href=&quot;https://github.com/Scale3-Labs/langtrace&quot;&gt;https://github.com/Scale3-Labs/langtrace&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/cryptokaykay&quot;&gt; /u/cryptokaykay &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c0do04/results_of_evaluating_the_ai_oracle_approach_a/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c0do04/results_of_evaluating_the_ai_oracle_approach_a/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1c0do04</id><media:thumbnail url="https://b.thumbs.redditmedia.com/GCdPwBj4teGkowjNA4iTF6pGHJldADy1b4cFZ_8gtFI.jpg" /><link href="https://www.reddit.com/r/LangChain/comments/1c0do04/results_of_evaluating_the_ai_oracle_approach_a/" /><updated>2024-04-10T05:11:47+00:00</updated><published>2024-04-10T05:11:47+00:00</published><title>Results of evaluating the AI Oracle approach - a novel way to improve your LLM application's accuracy</title></entry><entry><author><name>/u/ANil1729</name><uri>https://www.reddit.com/user/ANil1729</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c0lzq7/chatbase_alternative_with_langchain_and_openai/&quot;&gt; &lt;img src=&quot;https://external-preview.redd.it/DNAJ1-L9hvh9FLOVL4A0b8RG1f_jG20rVFCJ33scYpQ.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0a8fbe3c2b21acfadc3e1117c2260e2f322ec969&quot; alt=&quot;Chatbase alternative with Langchain and OpenAI&quot; title=&quot;Chatbase alternative with Langchain and OpenAI&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/ANil1729&quot;&gt; /u/ANil1729 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=ZSfdZVvZ99Q&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c0lzq7/chatbase_alternative_with_langchain_and_openai/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1c0lzq7</id><media:thumbnail url="https://external-preview.redd.it/DNAJ1-L9hvh9FLOVL4A0b8RG1f_jG20rVFCJ33scYpQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=0a8fbe3c2b21acfadc3e1117c2260e2f322ec969" /><link href="https://www.reddit.com/r/LangChain/comments/1c0lzq7/chatbase_alternative_with_langchain_and_openai/" /><updated>2024-04-10T13:46:53+00:00</updated><published>2024-04-10T13:46:53+00:00</published><title>Chatbase alternative with Langchain and OpenAI</title></entry><entry><author><name>/u/Dear_Insect_5295</name><uri>https://www.reddit.com/user/Dear_Insect_5295</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I am using a Mistral model 4b and huggingface&lt;/p&gt; &lt;pre&gt;&lt;code&gt;pipeline text_generation_pipeline = pipeline( model=model, tokenizer=tokenizer, task=&amp;quot;text-generation&amp;quot;, batch_size=2 ) llm = HuggingFacePipeline(pipeline=text_generation_pipeline,batch_size=2) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;and then using RAG through langchain&lt;/p&gt; &lt;pre&gt;&lt;code&gt;rag_chain_from_docs = ( RunnablePassthrough.assign(context=(lambda x: format_docs(x[&amp;quot;context&amp;quot;]))) | prompt | llm | StrOutputParser() ) rag_chain_with_source = RunnableParallel( {&amp;quot;context&amp;quot;: retriever, &amp;quot;question&amp;quot;: RunnablePassthrough()} ).assign(answer=rag_chain_from_docs) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;My GPU (T4) is underutilized, its only 8GB/16GB. so I want to use all of My GPU, Is there any way to do this, I tried chain.batch() but it did not work(It still ran sequentially). Any suggestions would be helpful to run the chain concurrently.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Dear_Insect_5295&quot;&gt; /u/Dear_Insect_5295 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c0k4oh/how_to_make_use_of_complete_gpu_memory/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c0k4oh/how_to_make_use_of_complete_gpu_memory/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1c0k4oh</id><link href="https://www.reddit.com/r/LangChain/comments/1c0k4oh/how_to_make_use_of_complete_gpu_memory/" /><updated>2024-04-10T12:16:02+00:00</updated><published>2024-04-10T12:16:02+00:00</published><title>How to make use of Complete GPU memory?</title></entry><entry><author><name>/u/arjavparikh</name><uri>https://www.reddit.com/user/arjavparikh</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I am a non-tech person looking for a tool to ask questions to my 50GB worth of PDF files. Is there a tool which can help me build this project or something which is already there which can help? &lt;/p&gt; &lt;p&gt;Please share relevant blogs or approaches to follow. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/arjavparikh&quot;&gt; /u/arjavparikh &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c0czng/is_there_a_toolplatform_to_put_an_llm_to_a_large/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c0czng/is_there_a_toolplatform_to_put_an_llm_to_a_large/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1c0czng</id><link href="https://www.reddit.com/r/LangChain/comments/1c0czng/is_there_a_toolplatform_to_put_an_llm_to_a_large/" /><updated>2024-04-10T04:32:23+00:00</updated><published>2024-04-10T04:32:23+00:00</published><title>Is there a tool/platform to put an LLM to a large data set of PDF files (like 50GB)</title></entry><entry><author><name>/u/Big-Big354</name><uri>https://www.reddit.com/user/Big-Big354</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi all,&lt;/p&gt; &lt;p&gt;I am using RetrievalQA chain with custom prompt. When invoked with a question it is returning prompt with answer embedded in it even when the return_only_outputs is set to True.&lt;/p&gt; &lt;p&gt;I was wondering how can I get only the generated answer without the prompt (System message + Context + Question)?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Big-Big354&quot;&gt; /u/Big-Big354 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c06txb/retrievalqa_chain_returning_generated_answer/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c06txb/retrievalqa_chain_returning_generated_answer/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1c06txb</id><link href="https://www.reddit.com/r/LangChain/comments/1c06txb/retrievalqa_chain_returning_generated_answer/" /><updated>2024-04-09T23:32:50+00:00</updated><published>2024-04-09T23:32:50+00:00</published><title>RetrievalQA chain returning generated answer embedded in prompt even when return_only_outputs=True</title></entry><entry><author><name>/u/isthatashark</name><uri>https://www.reddit.com/user/isthatashark</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/isthatashark&quot;&gt; /u/isthatashark &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://vectorize.io/what-is-a-vector-database/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c000jh/the_ultimate_guide_to_vector_database_success_in/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1c000jh</id><link href="https://www.reddit.com/r/LangChain/comments/1c000jh/the_ultimate_guide_to_vector_database_success_in/" /><updated>2024-04-09T18:54:40+00:00</updated><published>2024-04-09T18:54:40+00:00</published><title>The Ultimate Guide To Vector Database Success In AI</title></entry><entry><author><name>/u/VegetableAddendum888</name><uri>https://www.reddit.com/user/VegetableAddendum888</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;So guys there‚Äôs vectara‚Äôs upcoming hackathon,anybody interested to participate and needs a team.DM me‚Ä¶&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/VegetableAddendum888&quot;&gt; /u/VegetableAddendum888 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c00e4g/need_teammates_for_a_rag_hackathon/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c00e4g/need_teammates_for_a_rag_hackathon/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1c00e4g</id><link href="https://www.reddit.com/r/LangChain/comments/1c00e4g/need_teammates_for_a_rag_hackathon/" /><updated>2024-04-09T19:09:43+00:00</updated><published>2024-04-09T19:09:43+00:00</published><title>Need teammates for a RAG hackathon</title></entry><entry><author><name>/u/anderl1980</name><uri>https://www.reddit.com/user/anderl1980</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I‚Äôd be interested in whether anyone here is using LangChain‚Äôs SQL Agent (or similar self-built agents with LangChain or autogen). I‚Äôd love to conenct to learn from your experiences as I have not seen it be used in productive systems yet!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/anderl1980&quot;&gt; /u/anderl1980 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bzn1yw/sql_agent_in_production/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bzn1yw/sql_agent_in_production/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bzn1yw</id><link href="https://www.reddit.com/r/LangChain/comments/1bzn1yw/sql_agent_in_production/" /><updated>2024-04-09T08:29:57+00:00</updated><published>2024-04-09T08:29:57+00:00</published><title>SQL Agent in production?</title></entry><entry><author><name>/u/mehul_gupta1997</name><uri>https://www.reddit.com/user/mehul_gupta1997</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/mehul_gupta1997&quot;&gt; /u/mehul_gupta1997 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;/r/ArtificialInteligence/comments/1bzuuov/tested_code_gemma_by_google/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bzuxvz/tested_code_gemma_by_google/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bzuxvz</id><link href="https://www.reddit.com/r/LangChain/comments/1bzuxvz/tested_code_gemma_by_google/" /><updated>2024-04-09T15:26:22+00:00</updated><published>2024-04-09T15:26:22+00:00</published><title>Tested Code Gemma by Google</title></entry><entry><author><name>/u/bwenneker</name><uri>https://www.reddit.com/user/bwenneker</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;m building several chat based apps with LangChain for clients. I&amp;#39;m asking for feedback with each answer, users can leave a üëç or üëé.&lt;/p&gt; &lt;p&gt;Often I get the question: &amp;quot;does this &amp;#39;self-improve&amp;#39;?&amp;quot;&lt;/p&gt; &lt;p&gt;This got me thinking, why not use the positive feedback to improve future answers? Has anyone tried something like this:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Store (positive) user feedback in a VectorDB with questions-answer pairs.&lt;/li&gt; &lt;li&gt;When a new question is asked, run the usual pipeline (RAG for example).&lt;/li&gt; &lt;li&gt;Then also query the feedback VectorDB and add the top-k feedback question-answer pairs with high relevance to the question and add it as extra context.&lt;/li&gt; &lt;li&gt;Let the LLM answer the question using the context and top-k feedback items.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Looking forward to your experience, otherwise I might build this, it doesn&amp;#39;t seem to hard to make.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/bwenneker&quot;&gt; /u/bwenneker &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bzntdm/using_user_feedback_to_optimize_rag/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bzntdm/using_user_feedback_to_optimize_rag/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bzntdm</id><link href="https://www.reddit.com/r/LangChain/comments/1bzntdm/using_user_feedback_to_optimize_rag/" /><updated>2024-04-09T09:25:46+00:00</updated><published>2024-04-09T09:25:46+00:00</published><title>Using user feedback to optimize RAG</title></entry><entry><author><name>/u/mehul_gupta1997</name><uri>https://www.reddit.com/user/mehul_gupta1997</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Checkout how you can leverage Multi-Agent Orchestration for developing an auto Interview system where the Interviewer asks questions to interviewee, evaluates it and eventually shares whether the candidate should be selected or not. Right now, both interviewer and interviewee are played by AI agents. &lt;a href=&quot;https://youtu.be/VrjqR4dIawo?si=1sMYs7lI-c8WZrwP&quot;&gt;https://youtu.be/VrjqR4dIawo?si=1sMYs7lI-c8WZrwP&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/mehul_gupta1997&quot;&gt; /u/mehul_gupta1997 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bzkzkt/multiagent_interview_using_langgraph/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bzkzkt/multiagent_interview_using_langgraph/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bzkzkt</id><link href="https://www.reddit.com/r/LangChain/comments/1bzkzkt/multiagent_interview_using_langgraph/" /><updated>2024-04-09T06:07:23+00:00</updated><published>2024-04-09T06:07:23+00:00</published><title>Multi-Agent Interview using LangGraph</title></entry><entry><author><name>/u/Chrex_007</name><uri>https://www.reddit.com/user/Chrex_007</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;So I have a requirement of being able to chat with csv files and when the chatbot can&amp;#39;t find any relevant information from the csv files it should use the Bing API to search on the web and gather information and answer. I tried to make a custom langchain agent with Bing API as a tool but it&amp;#39;s not able to perform the observation, action loop, the model I&amp;#39;m using is Mistral-7B-Instruct-v0.1 which I can&amp;#39;t change I think model is not powerful enough for this task. But still does anybody have idea how can I make this possible? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Chrex_007&quot;&gt; /u/Chrex_007 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bzmovg/how_to_create_a_chatbot_to_chat_with_csv_files/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bzmovg/how_to_create_a_chatbot_to_chat_with_csv_files/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bzmovg</id><link href="https://www.reddit.com/r/LangChain/comments/1bzmovg/how_to_create_a_chatbot_to_chat_with_csv_files/" /><updated>2024-04-09T08:02:54+00:00</updated><published>2024-04-09T08:02:54+00:00</published><title>How to create a chatbot to chat with csv files and internet (Bing API)?</title></entry><entry><author><name>/u/ramirez_tn</name><uri>https://www.reddit.com/user/ramirez_tn</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I would like interact with LLMs in sequential way like:&lt;/p&gt; &lt;p&gt;Step 1 : load documents Step 2 : ask about the products described in the documents Step 3: based on the response lookup where can I buy these products Step 4: check if the store has other options ‚Ä¶&lt;/p&gt; &lt;p&gt;I am currently doing it in a very basic way. Loading the documents, retrieving using a question, using the output as input for another retriever , ‚Ä¶ Is there a more sophisticated way of doing it ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/ramirez_tn&quot;&gt; /u/ramirez_tn &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bzsgxa/interacting_with_llms_in_steps/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bzsgxa/interacting_with_llms_in_steps/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bzsgxa</id><link href="https://www.reddit.com/r/LangChain/comments/1bzsgxa/interacting_with_llms_in_steps/" /><updated>2024-04-09T13:41:04+00:00</updated><published>2024-04-09T13:41:04+00:00</published><title>Interacting with LLMs in steps</title></entry><entry><author><name>/u/IlEstLaPapi</name><uri>https://www.reddit.com/user/IlEstLaPapi</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;tldr: Some insights and learnings from a LLM enthusiast working on a complex Chatbot using multiple agents built with LangGraph, LCEL and Chainlit.&lt;/p&gt; &lt;p&gt;Hi everyone! I have seen a lot of interest in multi-agent systems recently, and, as I&amp;#39;m currently working on a complex one, I thought I might as well share some feedback on my project. Maybe some of you might find it interesting, give some useful feedback, or make some suggestions.&lt;/p&gt; &lt;h2&gt;Introduction: Why am I doing this project?&lt;/h2&gt; &lt;p&gt;I&amp;#39;m a business owner and a tech guy with a background in math, coding, and ML. Since early 2023, I&amp;#39;ve fallen in love with the LLM world. So, I decided to start a new business with 2 friends: a consulting firm on generative AI. As expected, we don&amp;#39;t have many references. Thus, we decided to create a tool to demonstrate our skillset to potential clients.&lt;/p&gt; &lt;p&gt;After a brainstorm, we quickly identified that a) RAG is the main selling point, so we need something that uses a RAG; b) We believe in agents to automate tasks; c) ChatGPT has shown that asking questions to a chatbot is a much more human-friendly interface than a website; d) Our main weakness is that we are all tech guys, so we might as well compensate for that by building a seller.&lt;/p&gt; &lt;p&gt;From here, the idea was clear: instead, or more exactly, alongside our website, build a chatbot that would answer questions about our company, &amp;quot;sell&amp;quot; our offer, and potentially schedule meetings with our consultants. Then make some posts on LinkedIn and pray...&lt;/p&gt; &lt;p&gt;Spoiler alert: This project isn&amp;#39;t finished yet. The idea is to share some insights and learnings with the community and get some feedback.&lt;/p&gt; &lt;h2&gt;Functional specifications&lt;/h2&gt; &lt;p&gt;The first step was to list some specifications: * We want a RAG that can answer any question the user might have about our company. For that, we will use the content of the company website. Of course, we also need to prevent hallucination, especially on two topics: the website has no information about pricing, and we don&amp;#39;t offer SLAs. * We want it to answer as quickly as possible and limit the budget. For that, we will use smaller models like GPT-3.5 and Claude Haiku as often as possible. But that limits the reasoning capabilities of our agents, so we need to find a sweet spot. * We want consistency in the responses, which is a big problem for RAGs. Questions with similar meanings should generate the same answers, for example, &amp;quot;What&amp;#39;s your offer?&amp;quot;, &amp;quot;What services do you provide?&amp;quot;, and &amp;quot;What do you do?&amp;quot;. * Obviously, we don&amp;#39;t want visitors to be able to ask off-topic questions (e.g., &amp;quot;How is the weather in North Carolina?&amp;quot;), so we need a way to filter out off-topic, prompt injection, and toxic questions. * We want to demonstrate that GenAI can be used to deliver more than just chatbots, so we want the agents to be able to schedule meetings, send emails to visitors, etc. * Ideally, we also want the agents to be able to qualify the visitor: who they are, what their job is, what their organization is, whether they are a tech person or a manager, and if they are looking for something specific with a defined need or are just curious about us. * Ideally, we also want the agents to &amp;quot;sell&amp;quot; our company: if the visitor indicates their need, match it with our offer and &amp;quot;push&amp;quot; that offer. If they show some interest, let&amp;#39;s &amp;quot;push&amp;quot; for a meeting with our consultants!&lt;/p&gt; &lt;h2&gt;Architecture&lt;/h2&gt; &lt;h3&gt;Stack&lt;/h3&gt; &lt;p&gt;We aren&amp;#39;t a startup, we haven&amp;#39;t raised funds, and we don&amp;#39;t have months to do this. We can&amp;#39;t afford to spend more than 20 days to get an MVP. Besides, our main selling point is that GenAI projects don&amp;#39;t require as much time or budget as ML ones.&lt;/p&gt; &lt;p&gt;So, in order to move fast, we needed to use some open-source frameworks: * For the chatbot, the data is public, so let&amp;#39;s use GPT and Claude as they are the best right now and the API cost is low. * For the chatbot, Chainlit provides everything we need, except background processing. Let&amp;#39;s use that. * Langchain and LCEL are both flexible and unify the interfaces with the LLMs. * We&amp;#39;ll need a rather complicated agent workflow, in fact, multiple ones. LangGraph is more flexible than crew.ai or autogen. Let&amp;#39;s use that!&lt;/p&gt; &lt;h3&gt;Design and early versions&lt;/h3&gt; &lt;h4&gt;First version&lt;/h4&gt; &lt;p&gt;From the start, we knew it was impossible to do it using a &amp;quot;one prompt, one agent&amp;quot; solution. So we started with a 3-agent solution: one to &amp;quot;find&amp;quot; the required elements on our website (a RAG), one to sell and set up meetings, and one to generate the final answer.&lt;/p&gt; &lt;p&gt;The meeting logic was very easy to implement. However, as expected, the chatbot was hallucinating a lot: &amp;quot;Here is a full project for 1k‚Ç¨, with an SLA 7/7 2 hours 99.999%&amp;quot;. And it was a bad seller, with conversations such as &amp;quot;Hi, who are you?&amp;quot; &amp;quot;I&amp;#39;m Sellbotix, how can I help you? Do you want a meeting with one of our consultants?&amp;quot;&lt;/p&gt; &lt;p&gt;At this stage, after 10 hours of work, we knew that it was probably doable but would require much more than 3 agents.&lt;/p&gt; &lt;h4&gt;Second version&lt;/h4&gt; &lt;p&gt;The second version used a more complex architecture: a guard to filter the questions, a strategist to make a plan, a seller to find some selling points, a seeker and a documentalist for the RAG, a secretary for the schedule meeting function, and a manager to coordinate everything.&lt;/p&gt; &lt;p&gt;It was slow, so we included logic to distribute the work between the agents in parallel. Sadly, this can&amp;#39;t be implemented using LangGraph, as all agent calls are made using coroutines but are awaited, and you can&amp;#39;t have parallel branches. So we implemented our own logic.&lt;/p&gt; &lt;p&gt;The result was much better, but far from perfect. And it was a nightmare to improve because changing one agent&amp;#39;s system prompt would generate side effects on most of the other agents. We also had a hard time defining what each agent would need to see and what to hide. Sending every piece of information to every agent is a waste of time and tokens.&lt;/p&gt; &lt;p&gt;And last but not least, the codebase was a mess as we did it in a rush. So we decided to restart from scratch.&lt;/p&gt; &lt;h2&gt;Third version, WIP&lt;/h2&gt; &lt;p&gt;So currently, we are working on the third version. This project is, by far, much more ambitious than what most of our clients ask us to do (another RAG?). And so far, we have learned a ton. I honestly don&amp;#39;t know if we will finish it, or even if it&amp;#39;s realistic, but it was worth it. &amp;quot;It isn&amp;#39;t the destination that matters, it&amp;#39;s the journey&amp;quot; has rarely been so true.&lt;/p&gt; &lt;p&gt;Currently, we are working on the architecture, and we have nearly finished it. Here are a few insights that we are using, and I wanted to share with you.&lt;/p&gt; &lt;h3&gt;Separation of concern&lt;/h3&gt; &lt;p&gt;The two main difficulties when working with a network of agents are a) they don&amp;#39;t know when to stop, and b) any change to any agent&amp;#39;s system prompt impacts the whole system. It&amp;#39;s hard to fix. When building a complex system, separation of concern is key: agents must be split into groups, each one with clear responsibilities and interfaces.&lt;/p&gt; &lt;p&gt;The cool thing is that a LangGraph graph is also a Runnable, so you can build graphs that use graphs. So we ended up with this: a main graph for the guard and final answer logic. It calls a &amp;quot;think&amp;quot; graph that decides which subgraphs should be called. Those are a &amp;quot;sell&amp;quot; graph, a &amp;quot;handle&amp;quot; graph, and a &amp;quot;find&amp;quot; graph (so far).&lt;/p&gt; &lt;h3&gt;Async, parallelism, and conditional calls&lt;/h3&gt; &lt;p&gt;If you want a system to be fast, you need to NOT call all the agents every time. For that, you need two things: a planner that decides which subgraph should be called (in our think graph), and you need to use &lt;code&gt;asyncio.gather&lt;/code&gt; instead of letting LangGraph call every graph and await them one by one.&lt;/p&gt; &lt;p&gt;So in the think graph, we have planner and manager agents. We use a standard doer/critic pattern here. When they agree on what needs to be done, they generate a list of instructions and activation orders for each subgraph that are passed to a &amp;quot;do&amp;quot; node. This node then creates a list of coroutines and awaits an &lt;code&gt;asyncio.gather&lt;/code&gt;.&lt;/p&gt; &lt;h3&gt;Limit what each graph must see&lt;/h3&gt; &lt;p&gt;We want the system to be fast and cost-efficient. Every node of every subgraph doesn&amp;#39;t need to be aware of what every other agent does. So we need to decide exactly what each agent gets as input. That&amp;#39;s honestly quite hard, but doable. It means fewer tokens, so it reduces the cost and speeds up the response.&lt;/p&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;This post is already quite long, so I won&amp;#39;t go into the details of every subgraph here. However, if you&amp;#39;re interested, feel free to let me know. I might decide to write some additional posts about those and the specific challenges we encountered and how we solved them (or not). In any case, if you&amp;#39;ve read this far, thank you!&lt;/p&gt; &lt;p&gt;If you have any feedback, don&amp;#39;t hesitate to share. I&amp;#39;d be very happy to read your thoughts and suggestions!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/IlEstLaPapi&quot;&gt; /u/IlEstLaPapi &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1byz3lr/insights_and_learnings_from_building_a_complex/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1byz3lr/insights_and_learnings_from_building_a_complex/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1byz3lr</id><link href="https://www.reddit.com/r/LangChain/comments/1byz3lr/insights_and_learnings_from_building_a_complex/" /><updated>2024-04-08T14:20:55+00:00</updated><published>2024-04-08T14:20:55+00:00</published><title>Insights and Learnings from Building a Complex Multi-Agent System</title></entry><entry><author><name>/u/sarthak_uchiha</name><uri>https://www.reddit.com/user/sarthak_uchiha</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;How can we prevent data poision in llms , for example if our database it self is corrupt and we need llm not to send that data , how can we achieve that &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/sarthak_uchiha&quot;&gt; /u/sarthak_uchiha &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bzpcc6/data_poision_in_llms/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bzpcc6/data_poision_in_llms/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bzpcc6</id><link href="https://www.reddit.com/r/LangChain/comments/1bzpcc6/data_poision_in_llms/" /><updated>2024-04-09T11:04:04+00:00</updated><published>2024-04-09T11:04:04+00:00</published><title>Data poision in llms</title></entry></feed>