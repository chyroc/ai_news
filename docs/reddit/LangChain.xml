<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/"><category term="LangChain" label="r/LangChain"/><updated>2023-12-08T07:06:28+00:00</updated><icon>https://www.redditstatic.com/icon.png/</icon><id>/r/LangChain.rss</id><link rel="self" href="https://www.reddit.com/r/LangChain.rss" type="application/atom+xml" /><link rel="alternate" href="https://www.reddit.com/r/LangChain" type="text/html" /><subtitle>LangChain is an open-source framework and developer toolkit that helps developers get LLM applications from prototype to production. It is available for Python and Javascript at https://www.langchain.com/.</subtitle><title>LangChain</title><entry><author><name>/u/zchaarm</name><uri>https://www.reddit.com/user/zchaarm</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;A place for members of &lt;a href=&quot;/r/LangChain&quot;&gt;r/LangChain&lt;/a&gt; to chat with each other&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/zchaarm&quot;&gt; /u/zchaarm &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_10ljho9</id><link href="https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/" /><updated>2023-01-26T04:33:05+00:00</updated><published>2023-01-26T04:33:05+00:00</published><title>r/LangChain Lounge</title></entry><entry><author><name>/u/PrudentCherry322</name><uri>https://www.reddit.com/user/PrudentCherry322</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18dguub/uae_new_sentence_embeddings_for_rag_sota_on_mteb/&quot;&gt; &lt;img src=&quot;https://external-preview.redd.it/2NOXPPsK7CrArgZgW_8munT8Vlwu5T4KB6tE801WSTY.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c51499cd903e5d873363d5f9163703a0d74d26e1&quot; alt=&quot;UAE: New Sentence Embeddings for RAG | SOTA on MTEB Leaderboard&quot; title=&quot;UAE: New Sentence Embeddings for RAG | SOTA on MTEB Leaderboard&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/PrudentCherry322&quot;&gt; /u/PrudentCherry322 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://github.com/SeanLee97/AnglE&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18dguub/uae_new_sentence_embeddings_for_rag_sota_on_mteb/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_18dguub</id><media:thumbnail url="https://external-preview.redd.it/2NOXPPsK7CrArgZgW_8munT8Vlwu5T4KB6tE801WSTY.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=c51499cd903e5d873363d5f9163703a0d74d26e1" /><link href="https://www.reddit.com/r/LangChain/comments/18dguub/uae_new_sentence_embeddings_for_rag_sota_on_mteb/" /><updated>2023-12-08T06:41:10+00:00</updated><published>2023-12-08T06:41:10+00:00</published><title>UAE: New Sentence Embeddings for RAG | SOTA on MTEB Leaderboard</title></entry><entry><author><name>/u/One-Difficulty3149</name><uri>https://www.reddit.com/user/One-Difficulty3149</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hello everyone, I have been working with langchain and has built some RAG applications. I have used FAISS as the vector database, which inherently does not support CRUD operations completely. If anyone has any inputs on which of the vector databases support CRUD operations, which they might have tried and tested. And also it should be efficient and not accurate, not too much time consuming. Thanks.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/One-Difficulty3149&quot;&gt; /u/One-Difficulty3149 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18dgp16/crud_operations_on_vector_databases/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18dgp16/crud_operations_on_vector_databases/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18dgp16</id><link href="https://www.reddit.com/r/LangChain/comments/18dgp16/crud_operations_on_vector_databases/" /><updated>2023-12-08T06:30:28+00:00</updated><published>2023-12-08T06:30:28+00:00</published><title>CRUD operations on Vector Databases</title></entry><entry><author><name>/u/Impressive_Gate2102</name><uri>https://www.reddit.com/user/Impressive_Gate2102</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;I am trying to achieve streaming for a custom LLM hosted on a server. I am using nodejs for the same. If anyone has worked on something similar, could you please guide me? &lt;/p&gt; &lt;p&gt;I am completely clueless on this. &lt;/p&gt; &lt;p&gt;Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Impressive_Gate2102&quot;&gt; /u/Impressive_Gate2102 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18dgatn/need_help_with_streaming_for_custom_llm_nodejs/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18dgatn/need_help_with_streaming_for_custom_llm_nodejs/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18dgatn</id><link href="https://www.reddit.com/r/LangChain/comments/18dgatn/need_help_with_streaming_for_custom_llm_nodejs/" /><updated>2023-12-08T06:04:45+00:00</updated><published>2023-12-08T06:04:45+00:00</published><title>Need help with streaming for Custom LLM Nodejs</title></entry><entry><author><name>/u/charlestehio</name><uri>https://www.reddit.com/user/charlestehio</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I’m currently self hosting thenlper/gte-small in a 6USD DO droplet, 1GB Ram, 1vCPU. Not too happy with the throughput. API cold start can go up to 5-8 seconds, and averaging around 2-3 seconds. &lt;/p&gt; &lt;p&gt;I am planning to switch over to baai/bge-small-en-v1.5 for newer projects because of Cloudflare Workers AI but they have no pricing model and not recommended for production yet. In the meantime, anyone has any ideas on how to mangle through this?&lt;/p&gt; &lt;p&gt;No OpenAI embeddings, thank you! I prefer something that can be self hosted and managed so I can scale up / down in costs&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/charlestehio&quot;&gt; /u/charlestehio &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18de6xv/any_managed_vector_embedding_services/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18de6xv/any_managed_vector_embedding_services/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18de6xv</id><link href="https://www.reddit.com/r/LangChain/comments/18de6xv/any_managed_vector_embedding_services/" /><updated>2023-12-08T04:04:33+00:00</updated><published>2023-12-08T04:04:33+00:00</published><title>Any managed vector embedding services?</title></entry><entry><author><name>/u/Positively101</name><uri>https://www.reddit.com/user/Positively101</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I intend to create a local llm based chatbot for my team. Basically it should be able to read the docs and generate intelligent responses. I&amp;#39;m pretty new to LLMs and have tried few things here and there. Overall I intend to present a prototype on a non-GPU or useless GPU based machine first. From what I understand so far I need to create a RAG pipeline. I&amp;#39;ve seen few architectures using embeddings, vector databases, langchain and a model to do create such a pipeline. I&amp;#39;m still pretty new to all these jargons. I have tried few opensource models as well locally and most of them just crash my M1 laptop. I have better work laptop with 16 GP RAM and 8GB graphics card memory on an A2000 card. Can you please suggest how can I quickly come up with a prototype. Basically the RAG pipeline(or any other method) should be able to quickly switch between different LLM models, or databases or any other components when it comes to deploying on a production setup. Also, for now, the idea is to use the data from pdf docs, word docs or data downloaded in json format. I&amp;#39;m not averse of coding so I can code one if I know what to do. Please suggest. Also please post any useful suggestions, articles, course, etc.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Positively101&quot;&gt; /u/Positively101 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18dcnkc/localprivate_llm_based_chatbot_using_freeopen/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18dcnkc/localprivate_llm_based_chatbot_using_freeopen/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18dcnkc</id><link href="https://www.reddit.com/r/LangChain/comments/18dcnkc/localprivate_llm_based_chatbot_using_freeopen/" /><updated>2023-12-08T02:42:34+00:00</updated><published>2023-12-08T02:42:34+00:00</published><title>local/private llm based chatbot using free/open source tools.</title></entry><entry><author><name>/u/prajwalsouza</name><uri>https://www.reddit.com/user/prajwalsouza</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18d0ikm/fixed_the_blog_post_to_match_the_technical_report/&quot;&gt; &lt;img src=&quot;https://preview.redd.it/ume5ozy60u4c1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ee13048518ecc5037d2260691c203bdb5c5c5973&quot; alt=&quot;Fixed the blog post to match the technical report on Gemini. :)&quot; title=&quot;Fixed the blog post to match the technical report on Gemini. :)&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/prajwalsouza&quot;&gt; /u/prajwalsouza &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://i.redd.it/ume5ozy60u4c1.png&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18d0ikm/fixed_the_blog_post_to_match_the_technical_report/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_18d0ikm</id><media:thumbnail url="https://preview.redd.it/ume5ozy60u4c1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=ee13048518ecc5037d2260691c203bdb5c5c5973" /><link href="https://www.reddit.com/r/LangChain/comments/18d0ikm/fixed_the_blog_post_to_match_the_technical_report/" /><updated>2023-12-07T17:20:51+00:00</updated><published>2023-12-07T17:20:51+00:00</published><title>Fixed the blog post to match the technical report on Gemini. :)</title></entry><entry><author><name>/u/SirEliteKaffee</name><uri>https://www.reddit.com/user/SirEliteKaffee</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18cun5p/langserve_stream_works_invoke_doesnt/&quot;&gt; &lt;img src=&quot;https://a.thumbs.redditmedia.com/B-LXWm_EcnpH-dG-Ja03T-WcTOibRuhMkkOlg6KoQI8.jpg&quot; alt=&quot;LangServe: Stream works, Invoke doesn't&quot; title=&quot;LangServe: Stream works, Invoke doesn't&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;m building a very simple chain that takes as an input a customer feedback string and categorizes it into the following pydantic class:&lt;/p&gt; &lt;pre&gt;&lt;code&gt; class AnalysisAttributes(BaseModel): overall_positive: bool = Field(description=&amp;quot;&amp;lt;sentiment is positive overall&amp;gt;&amp;quot;) mentions_pricing: bool = Field(description=&amp;quot;&amp;lt;pricing is mentioned&amp;gt;&amp;quot;) mentions_competition: bool = Field(description=&amp;quot;&amp;lt;competition is mentioned&amp;gt;&amp;quot;) parser = PydanticOutputParser(pydantic_object=AnalysisAttributes) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;Here&amp;#39;s how this should work, and it does:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;full_pipeline = prompt | model | parser output = full_pipeline.invoke({&amp;quot;feedback&amp;quot;: &amp;quot;This bad company is very expensive.&amp;quot;}) expected_output = AnalysisAttributes(overall_positive=False, mentions_pricing=True, mentions_competition=False) assert output == expected_output. # this works! :) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;This works very well, all good so far! Let&amp;#39;s serve it:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;app = FastAPI( title=&amp;quot;LangChain Server&amp;quot;, version=&amp;quot;1.0&amp;quot;, description=&amp;quot;A simple api server using Langchain&amp;#39;s Runnable interfaces&amp;quot;, ) pipeline = prompt | model | parser add_routes(app, pipeline, path=&amp;quot;/categorize_feedback&amp;quot;) if __name__ == &amp;quot;__main__&amp;quot;: import uvicorn uvicorn.run(app, host=&amp;quot;localhost&amp;quot;, port=8000) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;Now comes the strange part, check this out. On the client side, streaming works:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;response = requests.post( &amp;quot;http://localhost:8000/categorize_feedback/stream/&amp;quot;, json={&amp;#39;input&amp;#39;: {&amp;#39;feedback&amp;#39;: &amp;#39;Prices are too high.&amp;#39;}} ) for chunk in response: print(chunk.decode()) # event: metadata [...] data: {&amp;quot;overall_positive&amp;quot;:false, ... &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;But the regular invoke does not work, it delivers an empty output:&lt;/strong&gt; &lt;/p&gt; &lt;pre&gt;&lt;code&gt;response = requests.post( &amp;quot;http://localhost:8000/categorize_feedback/invoke/&amp;quot;, json={&amp;#39;input&amp;#39;: {&amp;#39;feedback&amp;#39;: &amp;#39;Prices are too high.&amp;#39;}} ) print(response.json()) # {&amp;#39;output&amp;#39;: {}, &amp;#39;callback_events&amp;#39;: [], &amp;#39;metadata&amp;#39;: {&amp;#39;run_id&amp;#39;: &amp;#39;acdd089d-3c80-4624-8122-17c4173dc1ec&amp;#39;}} &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Any ideas? For more info, check out the langserve playground output: &lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/1fc9vbfcav4c1.png?width=1930&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e8bd8119978ed74ebe9d1b8d453b77263fbc3701&quot;&gt;https://preview.redd.it/1fc9vbfcav4c1.png?width=1930&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e8bd8119978ed74ebe9d1b8d453b77263fbc3701&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/SirEliteKaffee&quot;&gt; /u/SirEliteKaffee &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18cun5p/langserve_stream_works_invoke_doesnt/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18cun5p/langserve_stream_works_invoke_doesnt/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_18cun5p</id><media:thumbnail url="https://a.thumbs.redditmedia.com/B-LXWm_EcnpH-dG-Ja03T-WcTOibRuhMkkOlg6KoQI8.jpg" /><link href="https://www.reddit.com/r/LangChain/comments/18cun5p/langserve_stream_works_invoke_doesnt/" /><updated>2023-12-07T12:33:59+00:00</updated><published>2023-12-07T12:33:59+00:00</published><title>LangServe: Stream works, Invoke doesn't</title></entry><entry><author><name>/u/learning_hedonism</name><uri>https://www.reddit.com/user/learning_hedonism</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Basically I want to have my llm do research for me. Would be nice to have some sort of feedback system rather than just dumb for loops. &lt;/p&gt; &lt;p&gt;Any advice?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/learning_hedonism&quot;&gt; /u/learning_hedonism &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18cv1ds/any_langchain_integrations_that_search_and_crawl/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18cv1ds/any_langchain_integrations_that_search_and_crawl/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18cv1ds</id><link href="https://www.reddit.com/r/LangChain/comments/18cv1ds/any_langchain_integrations_that_search_and_crawl/" /><updated>2023-12-07T12:57:48+00:00</updated><published>2023-12-07T12:57:48+00:00</published><title>Any langchain integrations that search and crawl?</title></entry><entry><author><name>/u/RayMallick</name><uri>https://www.reddit.com/user/RayMallick</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;First time looking into LangChain and vector dbs. I have been creating with some fun applications with LLMs so I have some understanding of how they work and how to interface with them. &lt;/p&gt; &lt;p&gt;Reading through the LangChan doc, I&amp;#39;m trying to get an understanding of how vector dbs affect the prompt? To early understandings, to me it seems like using a vector db would increase the tokens used by LLM in the prompts and thus the cost (if using an API, like Open AI). &lt;/p&gt; &lt;p&gt;Can anyone provide any further insight? Is this correct?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/RayMallick&quot;&gt; /u/RayMallick &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18cpx8j/does_using_a_vector_db_increase_llm_api_cost/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18cpx8j/does_using_a_vector_db_increase_llm_api_cost/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18cpx8j</id><link href="https://www.reddit.com/r/LangChain/comments/18cpx8j/does_using_a_vector_db_increase_llm_api_cost/" /><updated>2023-12-07T06:59:09+00:00</updated><published>2023-12-07T06:59:09+00:00</published><title>Does using a vector db increase LLM API cost?</title></entry><entry><author><name>/u/clickmildest</name><uri>https://www.reddit.com/user/clickmildest</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey all, I was wondering if there’s a dedicated app to upload both resume and job posting to get insights whether someone is a good fit for the job. Provide suggestions, insight even hold a mock interview! &lt;/p&gt; &lt;p&gt;It sounds like a great use for AI and considering the current job market it could really helpful. If something like this doesn’t exist I would love to build something like this! &lt;/p&gt; &lt;p&gt;Looking forward to y’all’s feedback&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/clickmildest&quot;&gt; /u/clickmildest &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18cmpce/interview_prep_and_resume_checker/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18cmpce/interview_prep_and_resume_checker/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18cmpce</id><link href="https://www.reddit.com/r/LangChain/comments/18cmpce/interview_prep_and_resume_checker/" /><updated>2023-12-07T03:49:29+00:00</updated><published>2023-12-07T03:49:29+00:00</published><title>Interview Prep and resume checker!</title></entry><entry><author><name>/u/Useful_Ad_7882</name><uri>https://www.reddit.com/user/Useful_Ad_7882</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I am trying to speed up my embeddings with rayllm integration on my m1 macbook pro. This is what the new code looks like: &lt;/p&gt; &lt;p&gt;`@ray.remote&lt;br/&gt; def process_shards(shard,collection_name):&lt;br/&gt; print(&amp;quot;embedding stuff&amp;quot;)&lt;br/&gt; embeddings = OpenAIEmbeddings(model=&amp;quot;text-embedding-ada-002&amp;quot;)&lt;br/&gt; print(f&amp;#39;Starting process_shard of {len(shard)} chunks.&amp;#39;)&lt;br/&gt; st = time.time()&lt;br/&gt; result = Chroma.from_documents(shard,embeddings,collection_metadata={&amp;quot;hnsw:space&amp;quot;: &amp;quot;cosine&amp;quot;})&lt;br/&gt; et = time.time() - st&lt;br/&gt; print(f&amp;#39;Shard completed in {et} seconds.&amp;#39;)&lt;br/&gt; return result`&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;above is called be by following method : &lt;/p&gt; &lt;p&gt;`def get_vectorstore(collection_name,text_chunks):&lt;br/&gt; #sharded processing with ray&lt;br/&gt; embeddings = OpenAIEmbeddings(model=&amp;quot;text-embedding-ada-002&amp;quot;)&lt;br/&gt; if text_chunks is None:&lt;br/&gt; return Chroma(persist_directory=persist_directory,embedding_function=embeddings,collection_name=collection_name)&lt;br/&gt; shards = np.array_split(text_chunks, db_shards)&lt;br/&gt; futures = [process_shards.remote(shards[i],collection_name) for i in range(db_shards)]&lt;br/&gt; results = ray.get(futures)&lt;br/&gt; #post processing after shards are available.&lt;br/&gt; db = results[0]&lt;br/&gt; for i in range(1,db_shards):&lt;br/&gt; db.merge_from(results[i])&lt;br/&gt; print(&amp;quot;now creating a new database to persist&amp;quot;)&lt;br/&gt; #create new chromadb and persist it&lt;br/&gt; #db.persist()&lt;br/&gt; return db`&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;when i run this, i get the following error: &lt;/p&gt; &lt;p&gt;TypeError: cannot pickle &amp;#39;sqlite3.Connection&amp;#39; object&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;Anyone who has solved for same ? Much appreciated.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Useful_Ad_7882&quot;&gt; /u/Useful_Ad_7882 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18crvph/pickle_error_while_trying_to_use_langchain_with/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18crvph/pickle_error_while_trying_to_use_langchain_with/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18crvph</id><link href="https://www.reddit.com/r/LangChain/comments/18crvph/pickle_error_while_trying_to_use_langchain_with/" /><updated>2023-12-07T09:24:00+00:00</updated><published>2023-12-07T09:24:00+00:00</published><title>pickle error while trying to use langchain with chromadb and rayllm</title></entry><entry><author><name>/u/I-Eat-Nuts</name><uri>https://www.reddit.com/user/I-Eat-Nuts</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;As title suggests, i want to add memory to vreate_csv_agent so that it remembers past conversations and queries from the subset of data it provided in the past in case the user prompts for it? If any further explanation is required please ask, but help me out.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/I-Eat-Nuts&quot;&gt; /u/I-Eat-Nuts &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18cqjxh/how_do_i_add_memory_to_a_create_csv_agent/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18cqjxh/how_do_i_add_memory_to_a_create_csv_agent/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18cqjxh</id><link href="https://www.reddit.com/r/LangChain/comments/18cqjxh/how_do_i_add_memory_to_a_create_csv_agent/" /><updated>2023-12-07T07:43:56+00:00</updated><published>2023-12-07T07:43:56+00:00</published><title>How do i add memory to a create_csv_agent?</title></entry><entry><author><name>/u/Bi11i0naire</name><uri>https://www.reddit.com/user/Bi11i0naire</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;How LLM Saas companies handle the data that is provided by customers?&lt;/p&gt; &lt;p&gt;For enterprise customers, what is the best strategy to retain data in-house and use LLMs?&lt;/p&gt; &lt;p&gt;Curios to know the thoughts/comments from the community.&lt;/p&gt; &lt;p&gt;Edit: &lt;/p&gt; &lt;ol&gt; &lt;li&gt;As a Saas user, enterprise customers will make API calls with input in required format&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Input → API Call → Saas Servers&lt;/p&gt; &lt;p&gt;For RAG use cases, Saas company might be pre-processing, building vectordbs on enterprise data to provide relevant answers.&lt;/p&gt; &lt;p&gt;However, saas company employees can see what data is coming in. Even though most of the Saas companies adhere to GDPR and other data privacy policies, they still have access to what enterprise customers are doing with their models.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;If an enterprise is paranoid about sharing their proprietary data, one of the option to use LLMs is:&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Take an open source LLM model → fine tune to enterprise data (optional) → create RAG with enterprise data → Deploy it in cloud/on-prem → Provide a secure endpoint to enterprise users&lt;/p&gt; &lt;p&gt;Data doesn&amp;#39;t leave but users within enterprise can leverage the benefits of RAG&lt;/p&gt; &lt;p&gt;Am I missing anything here?&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Bi11i0naire&quot;&gt; /u/Bi11i0naire &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18cilzo/data_privacy_with_llm_saas_companies/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18cilzo/data_privacy_with_llm_saas_companies/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18cilzo</id><link href="https://www.reddit.com/r/LangChain/comments/18cilzo/data_privacy_with_llm_saas_companies/" /><updated>2023-12-07T00:20:38+00:00</updated><published>2023-12-07T00:20:38+00:00</published><title>Data privacy with LLM Saas companies</title></entry><entry><author><name>/u/johan_donquixote</name><uri>https://www.reddit.com/user/johan_donquixote</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Currently I am looping over chunks and getting keywords using prompt...&lt;/p&gt; &lt;p&gt;How do I combine the keywords from different chunks to get the most important keywords of the whole doc. &lt;/p&gt; &lt;p&gt;I was thinking of giving the summary of document(to understand context) as an input to the prompt along with all the keywords to get final output...&lt;/p&gt; &lt;p&gt;Any better method to do this?&lt;/p&gt; &lt;p&gt;TIA&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/johan_donquixote&quot;&gt; /u/johan_donquixote &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18cbvfj/i_want_to_extract_important_keywords_from_large/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18cbvfj/i_want_to_extract_important_keywords_from_large/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18cbvfj</id><link href="https://www.reddit.com/r/LangChain/comments/18cbvfj/i_want_to_extract_important_keywords_from_large/" /><updated>2023-12-06T19:24:55+00:00</updated><published>2023-12-06T19:24:55+00:00</published><title>I want to extract important keywords from large documents...</title></entry><entry><author><name>/u/ronittsainii</name><uri>https://www.reddit.com/user/ronittsainii</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hello Everyone,&lt;/p&gt; &lt;p&gt;I have developed a Chatbot using LangChain, Open AI LLM and Next Js.&lt;/p&gt; &lt;p&gt;The chatbot currently is by the name of &amp;quot;HR Chatbot&amp;quot;.&lt;/p&gt; &lt;p&gt;If you want to get a chatbot developed using LangChain, Open AI and Next Js/Python you can PM me. Or if you are a developer I can directly sell you the source code of the one that I have built. &lt;/p&gt; &lt;p&gt;I am even open to setting up a free consultation!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/ronittsainii&quot;&gt; /u/ronittsainii &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18cts9j/open_ai_and_langchain_chatbot/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18cts9j/open_ai_and_langchain_chatbot/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18cts9j</id><link href="https://www.reddit.com/r/LangChain/comments/18cts9j/open_ai_and_langchain_chatbot/" /><updated>2023-12-07T11:39:57+00:00</updated><published>2023-12-07T11:39:57+00:00</published><title>Open AI and LangChain Chatbot</title></entry><entry><author><name>/u/matt3526</name><uri>https://www.reddit.com/user/matt3526</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I’m using the above to query a sql database and return results. However in cases where text is returned (like a few product reviews for example) I’d like to know the sentiment of each review and how this is changing over time. Is it possible to do this with langchain?&lt;/p&gt; &lt;p&gt;Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/matt3526&quot;&gt; /u/matt3526 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18chjxz/can_i_take_results_from_create_sql_agent_and_do/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18chjxz/can_i_take_results_from_create_sql_agent_and_do/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18chjxz</id><link href="https://www.reddit.com/r/LangChain/comments/18chjxz/can_i_take_results_from_create_sql_agent_and_do/" /><updated>2023-12-06T23:31:11+00:00</updated><published>2023-12-06T23:31:11+00:00</published><title>Can I take results from create_sql_agent and do other things with it?</title></entry><entry><author><name>/u/mean-short-</name><uri>https://www.reddit.com/user/mean-short-</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I want to create an agent that is able to do RAG using langchain.&lt;br/&gt; I found this: &lt;a href=&quot;https://python.langchain.com/docs/use_cases/question_answering/conversational_retrieval_agents&quot;&gt;https://python.langchain.com/docs/use_cases/question_answering/conversational_retrieval_agents&lt;/a&gt;&lt;br/&gt; I can&amp;#39;t seem to get it to focus its search on the database alone, it still goes to its general knowledge to answer.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/mean-short-&quot;&gt; /u/mean-short- &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18cemoh/rag_with_agents/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18cemoh/rag_with_agents/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18cemoh</id><link href="https://www.reddit.com/r/LangChain/comments/18cemoh/rag_with_agents/" /><updated>2023-12-06T21:22:57+00:00</updated><published>2023-12-06T21:22:57+00:00</published><title>RAG with agents</title></entry><entry><author><name>/u/urlaklbek</name><uri>https://www.reddit.com/user/urlaklbek</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hello friends! I design my own langchain alternative for Go programming language and I&amp;#39;m trying to understand why Langchain support dynamic prompt templating? By that I mean ability to create prompt based on results from previous steps. Here&amp;#39;s some python-like pseudocode to make things clear:&lt;/p&gt; &lt;p&gt;&lt;code&gt;chain(step1,step2, step3(prompt=&amp;quot;bla bla bla... {step1Result}&amp;quot;))&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Could you guys please provide some examples? The more examples you have the better. Simple, complex... any of them! &lt;/p&gt; &lt;p&gt;--- &lt;/p&gt; &lt;p&gt;Please note that I have nothing against &amp;quot;simple chains&amp;quot; where we pass result from one step to the next one. What I&amp;#39;m not sure I get is real use cases for the ability to pass to the step results of &amp;quot;any previous steps&amp;quot;. Langchain seems to have some kind of global execution context of the chain that every step has access to. I wanna now whether I must add it to my lib or not. &lt;/p&gt; &lt;p&gt;Thank u!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/urlaklbek&quot;&gt; /u/urlaklbek &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18c2ovj/why_have_prompt_templates/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18c2ovj/why_have_prompt_templates/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18c2ovj</id><link href="https://www.reddit.com/r/LangChain/comments/18c2ovj/why_have_prompt_templates/" /><updated>2023-12-06T12:14:31+00:00</updated><published>2023-12-06T12:14:31+00:00</published><title>Why have Prompt Templates?</title></entry><entry><author><name>/u/jerry_10_</name><uri>https://www.reddit.com/user/jerry_10_</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18c70qt/libmagic_not_working_even_though_it_is_installed/&quot;&gt; &lt;img src=&quot;https://b.thumbs.redditmedia.com/z48mUrX7JIxSBVvUVvrsM6okNu_wzJk2qV3TRwScgAs.jpg&quot; alt=&quot;Libmagic not working, Even though it is installed&quot; title=&quot;Libmagic not working, Even though it is installed&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I want to make a project that reads URLs, makes embeddings, and stores them in a vector store. For this, I am using UnstructuredURLLoader from the langchain library. This library uses another library called libmagic. I have pip-installed python-libmagic and python-libmagic-bin, but it still shows me the following error. &lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/iwdqoleg5p4c1.png?width=1408&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6f748b7398405eed474d38741992a6fe57dd8ea2&quot;&gt;https://preview.redd.it/iwdqoleg5p4c1.png?width=1408&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6f748b7398405eed474d38741992a6fe57dd8ea2&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/jerry_10_&quot;&gt; /u/jerry_10_ &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18c70qt/libmagic_not_working_even_though_it_is_installed/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18c70qt/libmagic_not_working_even_though_it_is_installed/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_18c70qt</id><media:thumbnail url="https://b.thumbs.redditmedia.com/z48mUrX7JIxSBVvUVvrsM6okNu_wzJk2qV3TRwScgAs.jpg" /><link href="https://www.reddit.com/r/LangChain/comments/18c70qt/libmagic_not_working_even_though_it_is_installed/" /><updated>2023-12-06T15:56:23+00:00</updated><published>2023-12-06T15:56:23+00:00</published><title>Libmagic not working, Even though it is installed</title></entry><entry><author><name>/u/CantaloupeLeading646</name><uri>https://www.reddit.com/user/CantaloupeLeading646</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;i&amp;#39;m using chatGPT-4 for coding and i noticed it doesn&amp;#39;t use langchain properly. i mean that if i want chatGPT to implement a basic example using pytorch or sk-learn it does so without much hassle, but when it comes to a simple example with langchain it starts to show me rough estimates of how the code should look like and not actual runnable code. &lt;/p&gt; &lt;p&gt;I&amp;#39;m wondering, is there a way to bypass that or is it intentional? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/CantaloupeLeading646&quot;&gt; /u/CantaloupeLeading646 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18ccamh/chatgpt_doesnt_have_access_to_langchain/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18ccamh/chatgpt_doesnt_have_access_to_langchain/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18ccamh</id><link href="https://www.reddit.com/r/LangChain/comments/18ccamh/chatgpt_doesnt_have_access_to_langchain/" /><updated>2023-12-06T19:43:28+00:00</updated><published>2023-12-06T19:43:28+00:00</published><title>chatGPT doesn't have access to langchain</title></entry><entry><author><name>/u/jindo1412</name><uri>https://www.reddit.com/user/jindo1412</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;Currently, I want to build RAG chatbot for production. I already had my LLM API and I want to create a custom LLM and then use this in RetrievalQA.from_chain_type function. I don&amp;#39;t know whether Langchain support this in my case.&lt;/p&gt; &lt;p&gt;I read about this topic on reddit: &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/17v1rhv/integrating_llm_rest_api_into_a_langchain/&quot;&gt;https://www.reddit.com/r/LangChain/comments/17v1rhv/integrating_llm_rest_api_into_a_langchain/&lt;/a&gt; And in langchain document: &lt;a href=&quot;https://python.langchain.com/docs/modules/model_io/llms/custom_llm&quot;&gt;https://python.langchain.com/docs/modules/model_io/llms/custom_llm&lt;/a&gt;&lt;/p&gt; &lt;p&gt;But this still does not work when I apply the custom LLM to qa_chain. Below is my code, hope for the support from you, sorry for my language, english is not my mother tongue.&lt;/p&gt; &lt;p&gt;``` from pydantic import Extra import requests from typing import Any, List, Mapping, Optional&lt;/p&gt; &lt;p&gt;from langchain.callbacks.manager import CallbackManagerForLLMRun from langchain.llms.base import LLM&lt;/p&gt; &lt;p&gt;class LlamaLLM(LLM): llm_url = &amp;#39;https:/myhost/llama/api&amp;#39;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;class Config: extra = Extra.forbid @property def _llm_type(self) -&amp;gt; str: return &amp;quot;Llama2 7B&amp;quot; def _call( self, prompt: str, stop: Optional[List[str]] = None, run_manager: Optional[CallbackManagerForLLMRun] = None, **kwargs: Any, ) -&amp;gt; str: if stop is not None: raise ValueError(&amp;quot;stop kwargs are not permitted.&amp;quot;) payload = { &amp;quot;inputs&amp;quot;: prompt, &amp;quot;parameters&amp;quot;: {&amp;quot;max_new_tokens&amp;quot;: 100}, &amp;quot;token&amp;quot;: &amp;quot;abcdfejkwehr&amp;quot; } headers = {&amp;quot;Content-Type&amp;quot;: &amp;quot;application/json&amp;quot;} response = requests.post(self.llm_url, json=payload, headers=headers, verify=False) response.raise_for_status() # print(&amp;quot;API Response:&amp;quot;, response.json()) return response.json()[&amp;#39;generated_text&amp;#39;] # get the response from the API @property def _identifying_params(self) -&amp;gt; Mapping[str, Any]: &amp;quot;&amp;quot;&amp;quot;Get the identifying parameters.&amp;quot;&amp;quot;&amp;quot; return {&amp;quot;llmUrl&amp;quot;: self.llm_url} &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;```&lt;/p&gt; &lt;p&gt;&lt;code&gt; llm = LlamaLLM() &lt;/code&gt;&lt;/p&gt; &lt;p&gt;```&lt;/p&gt; &lt;h1&gt;Testing&lt;/h1&gt; &lt;p&gt;prompt = &amp;quot;[INST] Question: Who is Albert Einstein? \n Answer: [/INST]&amp;quot; result = llm._call(prompt) print(result)&lt;/p&gt; &lt;p&gt;Albert Einstein (1879-1955) was a German-born theoretical physicist who is widely regarded as one of the most influential scientists of the 20th century. He is best known for his theory of relativity, which revolutionized our understanding of space and time, and his famous equation E=mc². ```&lt;/p&gt; &lt;p&gt;```&lt;/p&gt; &lt;h1&gt;Build prompt&lt;/h1&gt; &lt;p&gt;from langchain.prompts import PromptTemplate template = &amp;quot;&amp;quot;&amp;quot;[INST] &amp;lt;&amp;lt;SYS&amp;gt;&amp;gt;&lt;/p&gt; &lt;p&gt;Answer the question base on the context below.&lt;/p&gt; &lt;p&gt;&amp;lt;&amp;lt;/SYS&amp;gt;&amp;gt;&lt;/p&gt; &lt;p&gt;Context: {context} Question: {question} Answer: [/INST]&amp;quot;&amp;quot;&amp;quot; QA_CHAIN_PROMPT = PromptTemplate(input_variables=[&amp;quot;context&amp;quot;, &amp;quot;question&amp;quot;],template=template,)&lt;/p&gt; &lt;h1&gt;Run chain&lt;/h1&gt; &lt;p&gt;from langchain.chains import RetrievalQA&lt;/p&gt; &lt;p&gt;qa_chain = RetrievalQA.from_chain_type(llm, verbose=True, # retriever=vectordb.as_retriever(), retriever=custom_retriever, return_source_documents=True, chain_type_kwargs={&amp;quot;prompt&amp;quot;: QA_CHAIN_PROMPT}) ```&lt;/p&gt; &lt;p&gt;``` question = &amp;quot;Is probability a class topic?&amp;quot; result = qa_chain({&amp;quot;query&amp;quot;: question}) result[&amp;quot;result&amp;quot;]&lt;/p&gt; &lt;p&gt;Encountered some errors. Please recheck your request! ```&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/jindo1412&quot;&gt; /u/jindo1412 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18btf1w/custom_llm_from_api_for_qa_chain/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18btf1w/custom_llm_from_api_for_qa_chain/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18btf1w</id><link href="https://www.reddit.com/r/LangChain/comments/18btf1w/custom_llm_from_api_for_qa_chain/" /><updated>2023-12-06T02:23:28+00:00</updated><published>2023-12-06T02:23:28+00:00</published><title>Custom LLM from API for QA chain</title></entry><entry><author><name>/u/sayanosis</name><uri>https://www.reddit.com/user/sayanosis</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Firstly, thank you so much for helping me with this. &lt;/p&gt; &lt;p&gt;I want to make a streamlit app which has RAG and Memory. This is how it looks: &lt;/p&gt; &lt;pre&gt;&lt;code&gt;_template = &amp;quot;&amp;quot;&amp;quot;Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language. Chat History: {chat_history} Follow Up Input: {question} Standalone question:&amp;quot;&amp;quot;&amp;quot; CONDENSE_QUESTION_PROMPT = PromptTemplate.from_template(_template) template = &amp;quot;&amp;quot;&amp;quot;Answer the question based only on the following context: {context} Question: {question} &amp;quot;&amp;quot;&amp;quot; ANSWER_PROMPT = ChatPromptTemplate.from_template(template) _inputs = RunnableParallel( standalone_question=RunnablePassthrough.assign( chat_history=lambda x: _format_chat_history(x[&amp;quot;chat_history&amp;quot;]) ) | CONDENSE_QUESTION_PROMPT | llmc | StrOutputParser(), ) _context = { &amp;quot;context&amp;quot;: itemgetter(&amp;quot;standalone_question&amp;quot;) | retriever | _combine_documents, &amp;quot;question&amp;quot;: lambda x: x[&amp;quot;standalone_question&amp;quot;], } conversational_qa_chain = _inputs | _context | ANSWER_PROMPT | llm &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;the _format_chat_history function looks like this&lt;/p&gt; &lt;pre&gt;&lt;code&gt;def _format_chat_history(chat_history: List[Tuple[str, str]]) -&amp;gt; str: # chat history is of format: # [ # (human_message_str, ai_message_str), # ... # ] # see below for an example of how it&amp;#39;s invoked buffer = &amp;quot;&amp;quot; for dialogue_turn in chat_history: human = &amp;quot;Human: &amp;quot; + dialogue_turn[0] ai = &amp;quot;Assistant: &amp;quot; + dialogue_turn[1] buffer += &amp;quot;\n&amp;quot; + &amp;quot;\n&amp;quot;.join([human, ai]) return buffer &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;My question is, streamlit already has messages stored in st.session_state.messages&lt;/p&gt; &lt;pre&gt;&lt;code&gt;st.session_state.messages &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;How to i pass this onto the chain to be condensed. Please help.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/sayanosis&quot;&gt; /u/sayanosis &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18bofgy/help_with_conversational_qa_chain_streamlit/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18bofgy/help_with_conversational_qa_chain_streamlit/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18bofgy</id><link href="https://www.reddit.com/r/LangChain/comments/18bofgy/help_with_conversational_qa_chain_streamlit/" /><updated>2023-12-05T22:32:23+00:00</updated><published>2023-12-05T22:32:23+00:00</published><title>Help with conversational_qa_chain - Streamlit Messages</title></entry><entry><author><name>/u/Temporary-Size7310</name><uri>https://www.reddit.com/user/Temporary-Size7310</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;pre&gt;&lt;code&gt;model_name = &amp;quot;jinaai/jina-embeddings-v2-small-en&amp;quot; model_kwargs = {&amp;quot;device&amp;quot;: &amp;quot;cuda&amp;quot;} encode_kwargs = {&amp;quot;normalize_embeddings&amp;quot;:True} embeddings = SentenceTransformerEmbeddings(model_name=model_name, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs ) store = InMemoryStore() #storage layer for parent documents child_text_splitter = RecursiveCharacterTextSplitter(chunk_size=400) parent_splitter = RecursiveCharacterTextSplitter(chunk_size=2000) vectordb = Chroma( embedding_function=embeddings, persist_directory=&amp;quot;./chroma_db_parent&amp;quot;, collection_name=&amp;quot;split_parents&amp;quot;, ) big_chunks_retrievr = ParentDocumentRetriever( vectorstore=vectordb, docstore=store, child_splitter=child_text_splitter, parent_splitter=parent_splitter, ) ---&amp;gt; 33 ParentDocumentRetriever( 34 vectorstore=vectordb, 35 docstore=store, TypeError: MultiVectorRetriever.__init__() got an unexpected keyword argument &amp;#39;child_splitter&amp;#39; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Relaunched my code today and it didn&amp;#39;t work anymore, any suggestion ? :/ &lt;/p&gt; &lt;p&gt;EDIT: There is an issue with langchain last release, re-installed 0.0.340 and relaunched it works.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Temporary-Size7310&quot;&gt; /u/Temporary-Size7310 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18bssnn/error_with_parentdocumentretriever_didnt/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18bssnn/error_with_parentdocumentretriever_didnt/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18bssnn</id><link href="https://www.reddit.com/r/LangChain/comments/18bssnn/error_with_parentdocumentretriever_didnt/" /><updated>2023-12-06T01:52:13+00:00</updated><published>2023-12-06T01:52:13+00:00</published><title>Error with ParentDocumentRetriever, didn't recognize child_splitter</title></entry><entry><author><name>/u/DannyBrownMz</name><uri>https://www.reddit.com/user/DannyBrownMz</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Anyone knows whether support for legacy langchain methods like sequential chain would still be continued(though it still is for now at least) despite the new addition LCEL? &lt;/p&gt; &lt;p&gt;Reason being that I find using Sequential chain and other types of chains used in Legacy Langchain quite easier to understand and implement than LCEL, plus it gives me better results.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/DannyBrownMz&quot;&gt; /u/DannyBrownMz &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18bhzhk/support_for_legacy/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18bhzhk/support_for_legacy/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18bhzhk</id><link href="https://www.reddit.com/r/LangChain/comments/18bhzhk/support_for_legacy/" /><updated>2023-12-05T18:00:38+00:00</updated><published>2023-12-05T18:00:38+00:00</published><title>Support for Legacy</title></entry><entry><author><name>/u/devinbost</name><uri>https://www.reddit.com/user/devinbost</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Most of the doc loaders assume a &amp;quot;one and done&amp;quot; process. &lt;/p&gt; &lt;p&gt;Anyone have suggestions on continually adding docs like for a RAG flow? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/devinbost&quot;&gt; /u/devinbost &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18bfxjm/anyone_have_suggestions_on_continuous_doc_loading/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18bfxjm/anyone_have_suggestions_on_continuous_doc_loading/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18bfxjm</id><link href="https://www.reddit.com/r/LangChain/comments/18bfxjm/anyone_have_suggestions_on_continuous_doc_loading/" /><updated>2023-12-05T16:31:12+00:00</updated><published>2023-12-05T16:31:12+00:00</published><title>Anyone have suggestions on continuous doc loading?</title></entry></feed>