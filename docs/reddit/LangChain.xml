<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/"><category term="LangChain" label="r/LangChain"/><updated>2024-07-02T15:35:39+00:00</updated><icon>https://www.redditstatic.com/icon.png/</icon><id>/r/LangChain.rss</id><link rel="self" href="https://www.reddit.com/r/LangChain.rss" type="application/atom+xml" /><link rel="alternate" href="https://www.reddit.com/r/LangChain" type="text/html" /><subtitle>LangChain is an open-source framework and developer toolkit that helps developers get LLM applications from prototype to production. It is available for Python and Javascript at https://www.langchain.com/.</subtitle><title>LangChain</title><entry><author><name>/u/zchaarm</name><uri>https://www.reddit.com/user/zchaarm</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;A place for members of &lt;a href=&quot;/r/LangChain&quot;&gt;r/LangChain&lt;/a&gt; to chat with each other&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/zchaarm&quot;&gt; /u/zchaarm &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_10ljho9</id><link href="https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/" /><updated>2023-01-26T04:33:05+00:00</updated><published>2023-01-26T04:33:05+00:00</published><title>r/LangChain Lounge</title></entry><entry><author><name>/u/HomunMage</name><uri>https://www.reddit.com/user/HomunMage</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dtklxf/nodeedge_based_gui_editor_for_langgraph/&quot;&gt; &lt;img src=&quot;https://external-preview.redd.it/exY9_QSiBsqruos_Vl-FOO-GjU9HPiDkmZLuuE9TZPA.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=34272cbdfb4a33eb9ee505cd85f9d900b0ef7af5&quot; alt=&quot;node-edge based GUI editor for LangGraph&quot; title=&quot;node-edge based GUI editor for LangGraph&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I’m excited to share that I’ve created a node-edge based GUI editor for LangGraph!&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/1uf35sfym3ad1.jpg?width=1850&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=59f0728b3381b8a68d83f27137075587f13bb033&quot;&gt;https://preview.redd.it/1uf35sfym3ad1.jpg?width=1850&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=59f0728b3381b8a68d83f27137075587f13bb033&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This tool provides an intuitive interface for creating and managing workflows, making it easier than ever to visualize and execute tasks. Whether you&amp;#39;re working with complex workflows or just getting started, LangGraph-GUI simplifies the process.&lt;/p&gt; &lt;p&gt;Check it out here: &lt;a href=&quot;https://github.com/LangGraph-GUI/LangGraph-GUI&quot;&gt;LangGraph-GUI on GitHub&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Some key features include:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;User-Friendly Interface:&lt;/strong&gt; Easily create and edit workflows with a visual editor.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Seamless Integration:&lt;/strong&gt; Supports local execution with language models like Mistral.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;JSON Support:&lt;/strong&gt; Read and write JSON files for your workflows, ensuring compatibility and easy sharing.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;To get started, follow the setup instructions in the repository. I’ve also included a guide on how to build the front-end GUI into a standalone executable.&lt;/p&gt; &lt;p&gt;If you want to learn LangGraph, we have LangGraph for dummy learning: &lt;a href=&quot;https://github.com/LangGraph-GUI/LangGraph-learn&quot;&gt;LangGraph-learn&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I’d love to hear your feedback and see how you’re using LangGraph-GUI in your projects. Feel free to contribute or raise issues on GitHub.&lt;/p&gt; &lt;p&gt;Happy graphing!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/HomunMage&quot;&gt; /u/HomunMage &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dtklxf/nodeedge_based_gui_editor_for_langgraph/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dtklxf/nodeedge_based_gui_editor_for_langgraph/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1dtklxf</id><media:thumbnail url="https://external-preview.redd.it/exY9_QSiBsqruos_Vl-FOO-GjU9HPiDkmZLuuE9TZPA.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=34272cbdfb4a33eb9ee505cd85f9d900b0ef7af5" /><link href="https://www.reddit.com/r/LangChain/comments/1dtklxf/nodeedge_based_gui_editor_for_langgraph/" /><updated>2024-07-02T12:28:06+00:00</updated><published>2024-07-02T12:28:06+00:00</published><title>node-edge based GUI editor for LangGraph</title></entry><entry><author><name>/u/Chussboi96</name><uri>https://www.reddit.com/user/Chussboi96</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I am currently working on a project that involves developing a text-to-SQL system. The goal is to allow users to input natural language, and the system will generate SQL queries. I have reference documents (PDFs) containing SQL queries and their descriptions (of schemas as well). I am considering using a Retrieval-Augmented Generation (RAG) system for this task. The idea is to use the provided documents to help generate the SQL queries. However, I am unsure if this is the best approach. Alternatively, I am thinking about fine-tuning a model specifically for this purpose. Also, is sqlcoder2-7b the sql code generator out there?&lt;/p&gt; &lt;p&gt;I would greatly appreciate guidance on the following:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Is the RAG system the best solution for this problem?&lt;/li&gt; &lt;li&gt;Would fine-tuning a model be more effective for generating accurate SQL queries?&lt;/li&gt; &lt;li&gt;Is there a better approach that I should consider?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;All your insights and suggestions will be invaluable in helping me determine the best course of action for this project. Thank you&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Chussboi96&quot;&gt; /u/Chussboi96 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dtjafd/texttosql_system_using_llm/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dtjafd/texttosql_system_using_llm/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dtjafd</id><link href="https://www.reddit.com/r/LangChain/comments/1dtjafd/texttosql_system_using_llm/" /><updated>2024-07-02T11:14:40+00:00</updated><published>2024-07-02T11:14:40+00:00</published><title>Text-to-sql system using LLM</title></entry><entry><author><name>/u/AGI-is-coming</name><uri>https://www.reddit.com/user/AGI-is-coming</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey folks,&lt;/p&gt; &lt;p&gt;I&amp;#39;ve been building AI assistants using Phidata which uses Langchain Knowledge Base. I needed better observability for my assistants to track metrics such as costs, response latency, caching, etc.&lt;/p&gt; &lt;p&gt;To solve these issues I integrated Porktey with Phidata. I decided to create a simple cookbook. It integrates Phidata seamlessly with Portkey&amp;#39;s AI gateway in 3 lines of code, giving me clear insights into how my assistants perform.&lt;/p&gt; &lt;p&gt;Here&amp;#39;s the link to the google collab notebook- &lt;a href=&quot;https://git.new/Phidata-Portkey&quot;&gt;https://git.new/Phidata-Portkey&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Let me know what you think or if you have any tips to improve!&lt;/p&gt; &lt;p&gt;Cheers!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/AGI-is-coming&quot;&gt; /u/AGI-is-coming &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dtjipf/how_i_built_observability_for_my_ai_assistant/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dtjipf/how_i_built_observability_for_my_ai_assistant/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dtjipf</id><link href="https://www.reddit.com/r/LangChain/comments/1dtjipf/how_i_built_observability_for_my_ai_assistant/" /><updated>2024-07-02T11:28:10+00:00</updated><published>2024-07-02T11:28:10+00:00</published><title>How I built observability for my AI assistant</title></entry><entry><author><name>/u/External_Ad_11</name><uri>https://www.reddit.com/user/External_Ad_11</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;With the growing popularity of large language models, Agents are becoming a topic of discussion. In this article, we will explore Autonomous Agents, cover the components of building an Agentic workflow, and discuss the practical implementation of a Content creation agent using Langhchain Groq and crewAI.&lt;/p&gt; &lt;p&gt;Code Implementation and article in comment:&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/External_Ad_11&quot;&gt; /u/External_Ad_11 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dth36y/building_an_agentic_workflow_with_langchain/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dth36y/building_an_agentic_workflow_with_langchain/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dth36y</id><link href="https://www.reddit.com/r/LangChain/comments/1dth36y/building_an_agentic_workflow_with_langchain/" /><updated>2024-07-02T08:47:49+00:00</updated><published>2024-07-02T08:47:49+00:00</published><title>Building an Agentic Workflow with Langchain CrewAI and Groq</title></entry><entry><author><name>/u/ml_dnn</name><uri>https://www.reddit.com/user/ml_dnn</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Published in ICML 2024&lt;/p&gt; &lt;p&gt;Paper: &lt;a href=&quot;https://huggingface.co/papers/2406.16979&quot;&gt;https://huggingface.co/papers/2406.16979&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/ml_dnn&quot;&gt; /u/ml_dnn &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dtly3h/deep_reinforcement_learning/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dtly3h/deep_reinforcement_learning/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dtly3h</id><link href="https://www.reddit.com/r/LangChain/comments/1dtly3h/deep_reinforcement_learning/" /><updated>2024-07-02T13:32:40+00:00</updated><published>2024-07-02T13:32:40+00:00</published><title>Deep Reinforcement Learning</title></entry><entry><author><name>/u/morifo</name><uri>https://www.reddit.com/user/morifo</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I might be overlooking something quite simple but I&amp;#39;m trying to follow the RAG from Scratch tutorial by Lance Martin but by using a local LLM (Mistral 7B Instruct v0.3). The Runnable Sequence has a HuggingFacePipeline section which has a &amp;quot;Prompt &amp;amp; Completion&amp;quot; header and has the entire prompt, context and query, then the answer. The answer itself has the prompt, context, query and answer embedded within it:&lt;/p&gt; &lt;p&gt;``` ...&lt;/p&gt; &lt;p&gt;metadata={&amp;#39;source&amp;#39;: &amp;#39;&lt;a href=&quot;https://lilianweng.github.io/posts/2023-06-23-agent/&amp;#x27;%7D)%5C&quot;&gt;https://lilianweng.github.io/posts/2023-06-23-agent/&amp;#39;})\&lt;/a&gt;]&lt;br/&gt; Answer: [/INST] &lt;/p&gt; &lt;p&gt;&amp;lt;s&amp;gt; [INST] You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don&amp;#39;t know the answer, just say that you don&amp;#39;t know. Use three sentences maximum and keep the answer concise. [/INST] &amp;lt;/s&amp;gt; &lt;/p&gt; &lt;p&gt;... &lt;/p&gt; &lt;p&gt;&amp;#39;&lt;a href=&quot;https://lilianweng.github.io/posts/2023-06-23-agent/&amp;#x27;%7D)%5C&quot;&gt;https://lilianweng.github.io/posts/2023-06-23-agent/&amp;#39;})\&lt;/a&gt;]&lt;br/&gt; Answer: [/INST]&lt;br/&gt; Task Decomposition is a method used in autonomous agent systems where a complex task is broken down into smaller, manageable steps. This is often achieved by instructing a Language Model (LLM) to &amp;quot;think step by step&amp;quot; or by using specific instructions tailored to the task at hand. For example, it could involve asking &amp;quot;What are the subgoals for achieving XYZ?&amp;quot; or &amp;quot;Steps for XYZ.&amp;quot; The goal is to simplify complex tasks and provide insights into the model&amp;#39;s thought process. ```&lt;/p&gt; &lt;p&gt;Instead, his -- using OpenAI -- has a proper chat with input and output headers that are easily readable. Is there something straightforward I can change to get the same output on LangSmith?&lt;/p&gt; &lt;p&gt;My rag chain looks like this:&lt;/p&gt; &lt;p&gt;```&lt;/p&gt; &lt;h1&gt;Prompt&lt;/h1&gt; &lt;p&gt;prompt = hub.pull(&amp;quot;rlm/rag-prompt-mistral&amp;quot;)&lt;/p&gt; &lt;h1&gt;Chain&lt;/h1&gt; &lt;p&gt;rag_chain = ( {&amp;quot;context&amp;quot;: retriever, &amp;quot;question&amp;quot;: RunnablePassthrough()} | prompt | mistral_llm | StrOutputParser() )&lt;/p&gt; &lt;h1&gt;Question&lt;/h1&gt; &lt;p&gt;response = rag_chain.invoke(&amp;quot;What is Task Decomposition?&amp;quot;) ```&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/morifo&quot;&gt; /u/morifo &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dtnri2/langsmith_configuring_hf_pipeline_to_display/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dtnri2/langsmith_configuring_hf_pipeline_to_display/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dtnri2</id><link href="https://www.reddit.com/r/LangChain/comments/1dtnri2/langsmith_configuring_hf_pipeline_to_display/" /><updated>2024-07-02T14:53:07+00:00</updated><published>2024-07-02T14:53:07+00:00</published><title>LangSmith: configuring HF pipeline to display output as Chat?</title></entry><entry><author><name>/u/liam358</name><uri>https://www.reddit.com/user/liam358</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;m currently in the process of making a web app using next.js and I want to use LangChain on the backend. It would be much simpler for me to use Javascript. I&amp;#39;m wondering if it&amp;#39;s functionality lacking compared to the python sdk.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/liam358&quot;&gt; /u/liam358 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dtjqwi/python_vs_javascript_for_langchain/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dtjqwi/python_vs_javascript_for_langchain/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dtjqwi</id><link href="https://www.reddit.com/r/LangChain/comments/1dtjqwi/python_vs_javascript_for_langchain/" /><updated>2024-07-02T11:41:11+00:00</updated><published>2024-07-02T11:41:11+00:00</published><title>Python vs Javascript for langchain</title></entry><entry><author><name>/u/Dramatic_Suspect9470</name><uri>https://www.reddit.com/user/Dramatic_Suspect9470</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Can you suggest some good projects to build and good open source repo to contribute for getting AI Engineer Job. I am fresher and I have 1 year to build my portfolio&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Dramatic_Suspect9470&quot;&gt; /u/Dramatic_Suspect9470 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dtio2u/aspiring_ai_engineer/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dtio2u/aspiring_ai_engineer/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dtio2u</id><link href="https://www.reddit.com/r/LangChain/comments/1dtio2u/aspiring_ai_engineer/" /><updated>2024-07-02T10:36:59+00:00</updated><published>2024-07-02T10:36:59+00:00</published><title>Aspiring AI Engineer</title></entry><entry><author><name>/u/maniac_runner</name><uri>https://www.reddit.com/user/maniac_runner</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dtcuy7/guide_pdf_checkbox_and_radio_button_extraction/&quot;&gt; &lt;img src=&quot;https://external-preview.redd.it/x8DTkNS8a7_4qR6b2OBqRcQ1aDflD3BoKmpf6YLiY1g.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4b269f9d896a63bf5bd3496277b35476e4c51860&quot; alt=&quot;[Guide] PDF checkbox and radio button extraction with LLMWhisperer, Langchain, and Pydantic&quot; title=&quot;[Guide] PDF checkbox and radio button extraction with LLMWhisperer, Langchain, and Pydantic&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/maniac_runner&quot;&gt; /u/maniac_runner &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=dC7EhnEIdDA&amp;amp;ab_channel=Unstract&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dtcuy7/guide_pdf_checkbox_and_radio_button_extraction/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1dtcuy7</id><media:thumbnail url="https://external-preview.redd.it/x8DTkNS8a7_4qR6b2OBqRcQ1aDflD3BoKmpf6YLiY1g.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=4b269f9d896a63bf5bd3496277b35476e4c51860" /><link href="https://www.reddit.com/r/LangChain/comments/1dtcuy7/guide_pdf_checkbox_and_radio_button_extraction/" /><updated>2024-07-02T04:07:20+00:00</updated><published>2024-07-02T04:07:20+00:00</published><title>[Guide] PDF checkbox and radio button extraction with LLMWhisperer, Langchain, and Pydantic</title></entry><entry><author><name>/u/Ok_Injury1644</name><uri>https://www.reddit.com/user/Ok_Injury1644</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Ok_Injury1644&quot;&gt; /u/Ok_Injury1644 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dthicm/discussion_change_output_text_length_for_phi3_4b/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dthicm/discussion_change_output_text_length_for_phi3_4b/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dthicm</id><link href="https://www.reddit.com/r/LangChain/comments/1dthicm/discussion_change_output_text_length_for_phi3_4b/" /><updated>2024-07-02T09:17:30+00:00</updated><published>2024-07-02T09:17:30+00:00</published><title>[Discussion] Change output text length for &quot;phi3: 4B&quot; LLM in ollama using python</title></entry><entry><author><name>/u/No-Speed3625</name><uri>https://www.reddit.com/user/No-Speed3625</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I have a PromptTemplate with three inputs: cause, symptom, and action. My LLM should use the inputs to produce a &amp;quot;theme&amp;quot; it gathers from the inputs. However, I am getting an error because the chain.invoke method expects one input instead of multiple.&lt;/p&gt; &lt;p&gt;This is the error that is thrown: ValueError: Invalid input type &amp;lt;class &amp;#39;dict&amp;#39;&amp;gt;. Must be a PromptValue, str, or list of BaseMessages.&lt;/p&gt; &lt;p&gt;All the examples of invoking a chain in the langchain documentation only have one input. What am I supposed to do when my prompt template contains three inputs?&lt;/p&gt; &lt;p&gt;For reference, this is the code:&lt;/p&gt; &lt;p&gt;&lt;code&gt;llm = AzureChatOpenAI()&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;prompt_template = PromptTemplate(&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;input_variables=[&amp;quot;cause&amp;quot;, &amp;quot;symptom&amp;quot;, &amp;quot;action&amp;quot;],&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;template=&amp;quot;&amp;quot;&amp;quot;&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;Root Cause:&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;{cause}&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;Complaint Symptom:&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;{symptom}&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;Corrective Action:&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;{action}&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;Based on the information provided above, generate a concise &amp;quot;Failure Theme&amp;quot; encapulating the reason for the failure.&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;Failure Theme:&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;&amp;quot;&amp;quot;&amp;quot;&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;chain = llm | prompt_template | StrOutputParser()&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;root_cause = df.iloc[0][&amp;#39;Root Cause&amp;#39;]&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;complaint_symptom = df.iloc[0][&amp;#39;Complaint Symptom&amp;#39;]&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;corrective_action = df.iloc[0][&amp;#39;Corrective Action&amp;#39;]&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;chain.invoke({&amp;quot;cause&amp;quot;: root_cause, &amp;quot;symptom&amp;quot;: complaint_symptom, &amp;quot;action&amp;quot;: corrective_action})&lt;/code&gt;&lt;/p&gt; &lt;p&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/No-Speed3625&quot;&gt; /u/No-Speed3625 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dt57k9/how_to_invoke_runnablesequence_chain_with/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dt57k9/how_to_invoke_runnablesequence_chain_with/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dt57k9</id><link href="https://www.reddit.com/r/LangChain/comments/1dt57k9/how_to_invoke_runnablesequence_chain_with/" /><updated>2024-07-01T21:54:25+00:00</updated><published>2024-07-01T21:54:25+00:00</published><title>How to Invoke RunnableSequence chain with multiple inputs</title></entry><entry><author><name>/u/goddamnit_1</name><uri>https://www.reddit.com/user/goddamnit_1</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dsych9/built_a_pr_agent_with_langchain_and_3_other/&quot;&gt; &lt;img src=&quot;https://external-preview.redd.it/6Mc008Ca21IsXkVQZ7BC8S65PYZTOpW1i6vv0a-gFXU.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4cca14bc297de8f853bc85571e118ec44c6fcd00&quot; alt=&quot;Built a PR Agent with Langchain and 3 other frameworks - A Comparison&quot; title=&quot;Built a PR Agent with Langchain and 3 other frameworks - A Comparison&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;The goal was to create an agent that would:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Monitor a GitHub repository for new PRs&lt;/li&gt; &lt;li&gt;Perform a code review on each PR&lt;/li&gt; &lt;li&gt;Post a summary of the review to a Slack channel&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/0yf542afwx9d1.png?width=1442&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=791bd8618af7c67f8e20d928705639366bea065b&quot;&gt;https://preview.redd.it/0yf542afwx9d1.png?width=1442&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=791bd8618af7c67f8e20d928705639366bea065b&lt;/a&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align=&quot;left&quot;&gt;Framework&lt;/th&gt; &lt;th align=&quot;left&quot;&gt;Why langchain is better&lt;/th&gt; &lt;th align=&quot;left&quot;&gt;Why it can be better&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align=&quot;left&quot;&gt;CrewAI&lt;/td&gt; &lt;td align=&quot;left&quot;&gt;Langchain provides a more comprehensive toolkit for building various AI applications, including but not limited to multi-agent scenarios.&lt;/td&gt; &lt;td align=&quot;left&quot;&gt;CrewAI may be easier to get started with for specific multi-agent tasks&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align=&quot;left&quot;&gt;Llama Index&lt;/td&gt; &lt;td align=&quot;left&quot;&gt;Langchain offers more general-purpose capabilities&lt;/td&gt; &lt;td align=&quot;left&quot;&gt;For projects primarily focused on data indexing and retrieval, llama index is better&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align=&quot;left&quot;&gt;Autogen&lt;/td&gt; &lt;td align=&quot;left&quot;&gt;Langchain provides more structured components for building AI applications&lt;/td&gt; &lt;td align=&quot;left&quot;&gt;For projects requiring highly dynamic multi-agent interactions, Autogen&amp;#39;s specialized focus might provide a more intuitive framework&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;The agent works great!&lt;/p&gt; &lt;p&gt;here&amp;#39;s the link for the project: &lt;a href=&quot;https://git.new/pr-agent-langchain&quot;&gt;https://git.new/pr-agent-langchain&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/goddamnit_1&quot;&gt; /u/goddamnit_1 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dsych9/built_a_pr_agent_with_langchain_and_3_other/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dsych9/built_a_pr_agent_with_langchain_and_3_other/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1dsych9</id><media:thumbnail url="https://external-preview.redd.it/6Mc008Ca21IsXkVQZ7BC8S65PYZTOpW1i6vv0a-gFXU.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=4cca14bc297de8f853bc85571e118ec44c6fcd00" /><link href="https://www.reddit.com/r/LangChain/comments/1dsych9/built_a_pr_agent_with_langchain_and_3_other/" /><updated>2024-07-01T17:11:32+00:00</updated><published>2024-07-01T17:11:32+00:00</published><title>Built a PR Agent with Langchain and 3 other frameworks - A Comparison</title></entry><entry><author><name>/u/northwolf56</name><uri>https://www.reddit.com/user/northwolf56</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I made a TL;DR video about using our browser extension to run your chat agents alongside any web page or app! Visually design your LangChain RAG + Agents app, add a chat UI to it and use it instantly, all from your browser. No code!&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://youtu.be/5-QV3lVI8uo&quot;&gt;https://youtu.be/5-QV3lVI8uo&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/northwolf56&quot;&gt; /u/northwolf56 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dswds9/chat_with_any_webpage_or_application_using_visual/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dswds9/chat_with_any_webpage_or_application_using_visual/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dswds9</id><link href="https://www.reddit.com/r/LangChain/comments/1dswds9/chat_with_any_webpage_or_application_using_visual/" /><updated>2024-07-01T15:52:00+00:00</updated><published>2024-07-01T15:52:00+00:00</published><title>Chat With Any WebPage or Application using Visual Agents &amp; LangChain</title></entry><entry><author><name>/u/i_am_innovative</name><uri>https://www.reddit.com/user/i_am_innovative</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;pre&gt;&lt;code&gt;i asked react agent about last name of johnny it&amp;#39;s not showing in it&amp;#39;s final output why ? I think I have found the table that contains the name &amp;quot;JOHNNY&amp;quot;. It&amp;#39;s the &amp;quot;actor&amp;quot; table. Now, I need to write a query to retrieve the last name of the actor with the first name &amp;quot;JOHNNY&amp;quot;. Action: sql_db_query_checker Action Input: SELECT last_name FROM actor WHERE first_name = &amp;#39;JOHNNY&amp;#39;SELECT last_name FROM actor WHERE first_name = &amp;#39;JOHNNY&amp;#39;Action: sql_db_query Action Input: SELECT last_name FROM actor WHERE first_name = &amp;#39;JOHNNY&amp;#39;[(&amp;#39;LOLLOBRIGIDA&amp;#39;,), (&amp;#39;CAGE&amp;#39;,)]I now know the final answer Final Answer: The last name of JOHNNY is not found in the database, but there are actors with the last names LOLLOBRIGIDA and CAGE who have first names that are not JOHNNY. It&amp;#39;s possible that the actor with the first name JOHNNY does not exist in the database. &amp;gt; Finished chain. &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/i_am_innovative&quot;&gt; /u/i_am_innovative &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dsyf46/whats_wrong_with_langchain_react_agent/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dsyf46/whats_wrong_with_langchain_react_agent/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dsyf46</id><link href="https://www.reddit.com/r/LangChain/comments/1dsyf46/whats_wrong_with_langchain_react_agent/" /><updated>2024-07-01T17:14:38+00:00</updated><published>2024-07-01T17:14:38+00:00</published><title>What's wrong with Langchain React Agent</title></entry><entry><author><name>/u/harshit_nariya</name><uri>https://www.reddit.com/user/harshit_nariya</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;&lt;a href=&quot;https://medium.com/genai-agents-unleashed/generate-powerpoint-presentation-with-openai-the-future-of-slide-decks-ce1a7c928986&quot;&gt;https://medium.com/genai-agents-unleashed/generate-powerpoint-presentation-with-openai-the-future-of-slide-decks-ce1a7c928986&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/harshit_nariya&quot;&gt; /u/harshit_nariya &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dt0zzz/automate_slide_decks_and_presentations/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dt0zzz/automate_slide_decks_and_presentations/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dt0zzz</id><link href="https://www.reddit.com/r/LangChain/comments/1dt0zzz/automate_slide_decks_and_presentations/" /><updated>2024-07-01T18:59:51+00:00</updated><published>2024-07-01T18:59:51+00:00</published><title>Automate Slide decks and presentations</title></entry><entry><author><name>/u/TableauforViz</name><uri>https://www.reddit.com/user/TableauforViz</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I have a huge schema in the neo4j database.&lt;/p&gt; &lt;p&gt;I&amp;#39;m using the LangChain function to generate a cypher query&lt;/p&gt; &lt;p&gt;chain = GraphCypherQAChain.from_llm( ChatOpenAI(temperature=0), graph=graph, verbose=True )&lt;/p&gt; &lt;p&gt;chain.invoke(query)&lt;/p&gt; &lt;p&gt;It&amp;#39;s returning an error saying that the model supports 16k tokens and I&amp;#39;m passing 15M+ tokens&lt;/p&gt; &lt;p&gt;How can I limit these tokens? I tried setting ChatOpenAI(temperature=0, max_tokens=1000) and it&amp;#39;s still giving the same error.&lt;/p&gt; &lt;p&gt;I think it&amp;#39;s passing the whole schema at once, how can I set a limit on that?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/TableauforViz&quot;&gt; /u/TableauforViz &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dt089d/how_to_generate_cypher_query_using_llm/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dt089d/how_to_generate_cypher_query_using_llm/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dt089d</id><link href="https://www.reddit.com/r/LangChain/comments/1dt089d/how_to_generate_cypher_query_using_llm/" /><updated>2024-07-01T18:28:35+00:00</updated><published>2024-07-01T18:28:35+00:00</published><title>How to generate Cypher Query using LLM?</title></entry><entry><author><name>/u/Lost-Season-4196</name><uri>https://www.reddit.com/user/Lost-Season-4196</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I made a RAG app that basically answers user questions based on provided data, it works fine on GPU and a single GPU. I want to deploy it on multiple GPUs (4 T4s) but I always get CUDA out of Memory error on pipeline.&lt;/p&gt; &lt;p&gt;I tried using &amp;quot;auto&amp;quot; keyword too but Langchain does not let me use it as keyword.&lt;/p&gt; &lt;p&gt;I used Langchain as main framework, my code looks like this:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;from langchain_huggingface import ChatHuggingFace, HuggingFacePipeline, HuggingFaceEmbeddings MODEL_NAME=&amp;quot;mistralai/Mistral-7B-Instruct-v0.3&amp;quot; pipe = HuggingFacePipeline.from_model_id( model_id=MODEL_NAME, device=0, model_kwargs={&amp;quot;torch_dtype&amp;quot;:torch.float16}, task=&amp;quot;text-generation&amp;quot;) llm = ChatHuggingFace(llm=pipe) embedding = HuggingFaceEmbeddings(model_name=MODEL_NAME, model_kwargs={&amp;quot;device&amp;quot;:&amp;quot;cuda:1&amp;quot;}, multi_process=True, ) &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Lost-Season-4196&quot;&gt; /u/Lost-Season-4196 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dszxdk/rag_app_on_multiple_gpus/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dszxdk/rag_app_on_multiple_gpus/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dszxdk</id><link href="https://www.reddit.com/r/LangChain/comments/1dszxdk/rag_app_on_multiple_gpus/" /><updated>2024-07-01T18:16:25+00:00</updated><published>2024-07-01T18:16:25+00:00</published><title>RAG app on multiple GPUs</title></entry><entry><author><name>/u/ravediamond000</name><uri>https://www.reddit.com/user/ravediamond000</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hello everyone,&lt;/p&gt; &lt;p&gt;Here&amp;#39;s a cool post that shows how to integrate multiple AWS Bedrock LLMs in your LangChain apps and choosing which one used with only one configuration parameter.&lt;/p&gt; &lt;p&gt;Here&amp;#39;s the link: &lt;a href=&quot;https://www.metadocs.co/2024/04/11/handle-multiple-llm-models-in-langchain-and-aws-bedrock-seamlessly/&quot;&gt;link&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Have a nice read.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/ravediamond000&quot;&gt; /u/ravediamond000 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dsrfca/integrate_multiple_aws_bedrock_llms_seamlessly/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dsrfca/integrate_multiple_aws_bedrock_llms_seamlessly/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dsrfca</id><link href="https://www.reddit.com/r/LangChain/comments/1dsrfca/integrate_multiple_aws_bedrock_llms_seamlessly/" /><updated>2024-07-01T12:11:22+00:00</updated><published>2024-07-01T12:11:22+00:00</published><title>Integrate multiple AWS bedrock LLMs seamlessly with Langchain</title></entry><entry><author><name>/u/bubble_h13</name><uri>https://www.reddit.com/user/bubble_h13</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I haven&amp;#39;t seen the sample in Langchain 0.2, so I just referenced &lt;a href=&quot;https://python.langchain.com/v0.1/docs/use_cases/tool_use/multiple_tools/&quot;&gt;v0.1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I use local model with &amp;quot;taide-7b-a.2-q4_k_m.gguf&amp;quot;, so there are some different&lt;/p&gt; &lt;pre&gt;&lt;code&gt; llm = ChatLlamaCpp( model_path= str(model_path), n_gpu_layers=100, n_batch=512, n_ctx=2048, f16_kv=True, callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]), verbose=True, ) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;after invoking the question, I can&amp;#39;t get the tools information. like&lt;/p&gt; &lt;pre&gt;&lt;code&gt;[{&amp;#39;name&amp;#39;: &amp;#39;multiply&amp;#39;, &amp;#39;args&amp;#39;: {&amp;#39;first_int&amp;#39;: 23, &amp;#39;second_int&amp;#39;: 7}, &amp;#39;id&amp;#39;: &amp;#39;toolu_01Wf8kUs36kxRKLDL8vs7G8q&amp;#39;, &amp;#39;output&amp;#39;: 161}] &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Can anyone tell me what&amp;#39;s wrong? here&amp;#39;s the code&lt;/p&gt; &lt;pre&gt;&lt;code&gt; tools = [multiply, exponentiate, add] llm_with_tools = llm.bind_tools(tools) def call_tools(msg: AIMessage) -&amp;gt; Runnable: &amp;quot;&amp;quot;&amp;quot;Simple tool calling helper.&amp;quot;&amp;quot;&amp;quot; tool_map = {tool.name: tool for tool in tools} tool_calls = msg.tool_calls.copy() for tool_call in tool_calls: tool_call[&amp;quot;output&amp;quot;] = tool_map[tool_call[&amp;quot;name&amp;quot;]].invoke(tool_call[&amp;quot;args&amp;quot;]) return tool_calls chain = llm_with_tools | call_tools chain.invoke(&amp;quot;23 times 7&amp;quot;) &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/bubble_h13&quot;&gt; /u/bubble_h13 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dsvy6s/choosing_multitool_in_agent/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dsvy6s/choosing_multitool_in_agent/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dsvy6s</id><link href="https://www.reddit.com/r/LangChain/comments/1dsvy6s/choosing_multitool_in_agent/" /><updated>2024-07-01T15:33:49+00:00</updated><published>2024-07-01T15:33:49+00:00</published><title>choosing multi-tool in agent</title></entry><entry><author><name>/u/MoronSlayer42</name><uri>https://www.reddit.com/user/MoronSlayer42</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Using astream, the response from the LLM has words that are split for example the word &amp;quot;hippopotamus&amp;quot; comes as 2 chunks &amp;quot;hippo&amp;quot; and &amp;quot;potamus&amp;quot;. When creating an app, how to recognize and combine the 2 split parts into a single word for front-end?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/MoronSlayer42&quot;&gt; /u/MoronSlayer42 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dsvmg8/streaming_responses_have_words_split/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dsvmg8/streaming_responses_have_words_split/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dsvmg8</id><link href="https://www.reddit.com/r/LangChain/comments/1dsvmg8/streaming_responses_have_words_split/" /><updated>2024-07-01T15:20:47+00:00</updated><published>2024-07-01T15:20:47+00:00</published><title>Streaming responses have words split</title></entry><entry><author><name>/u/Equivalent_Noise3560</name><uri>https://www.reddit.com/user/Equivalent_Noise3560</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dso9pj/custom_functions_with_runnablelambda_in_langchain/&quot;&gt; &lt;img src=&quot;https://external-preview.redd.it/2347JJ6JbmR5WGmdxSM3iu0OnGohPD3MltcLei8lmx8.jpg?width=108&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7a4559964a97155995aa6f0c96a36eebba24728a&quot; alt=&quot;Custom Functions with RunnableLambda in LangChain JS&quot; title=&quot;Custom Functions with RunnableLambda in LangChain JS&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Made a short post about how to make Custom Functions with RunnableLambda in LangChain JS:&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://www.js-craft.io/blog/custom-functions-runnablelambda-langchain-js/&quot;&gt;https://www.js-craft.io/blog/custom-functions-runnablelambda-langchain-js/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/9v8pcoekfv9d1.png?width=1474&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8dfa8bb8a5efb6b1873e1ee70fba90cd43f2e4ff&quot;&gt;https://preview.redd.it/9v8pcoekfv9d1.png?width=1474&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8dfa8bb8a5efb6b1873e1ee70fba90cd43f2e4ff&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Equivalent_Noise3560&quot;&gt; /u/Equivalent_Noise3560 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dso9pj/custom_functions_with_runnablelambda_in_langchain/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dso9pj/custom_functions_with_runnablelambda_in_langchain/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1dso9pj</id><media:thumbnail url="https://external-preview.redd.it/2347JJ6JbmR5WGmdxSM3iu0OnGohPD3MltcLei8lmx8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=7a4559964a97155995aa6f0c96a36eebba24728a" /><link href="https://www.reddit.com/r/LangChain/comments/1dso9pj/custom_functions_with_runnablelambda_in_langchain/" /><updated>2024-07-01T08:52:49+00:00</updated><published>2024-07-01T08:52:49+00:00</published><title>Custom Functions with RunnableLambda in LangChain JS</title></entry><entry><author><name>/u/OpenInvestigator3235</name><uri>https://www.reddit.com/user/OpenInvestigator3235</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I want help with how to go about an idea.&lt;/p&gt; &lt;p&gt;I currently need to build a ‘virtual assistant’ chatbot at work.&lt;/p&gt; &lt;p&gt;For the POC, I am using LangChain, LangGraph, and Chainlit for the UI. &lt;/p&gt; &lt;p&gt;I want through the chatbot based LLM app to be able to access the following: 1. RAG 2. Web Search 3. Normal Conversational Chat&lt;/p&gt; &lt;p&gt;For the persona of the chatbot, I know I can just set that in the context prompt of the system message.&lt;/p&gt; &lt;p&gt;But for the Agent capabilities, I have a simple corrective RAG agent graph made in LangGraph.&lt;/p&gt; &lt;p&gt;My questions are: 1. How do I current the capabilities of the agent graph made to my basic langchain chainlit app? 2. Do I need to make another conversational agent that can be routed to for normal chatbot capabilities or no? 3. How do I make Web Searching with Tavily an ability? 4. What will the final architecture look like?&lt;/p&gt; &lt;p&gt;I really appreciate any input you have.&lt;/p&gt; &lt;p&gt;If anyone is kind enough to give me 10 mins of their time on a discord call, I would REALLY REALLY appreciate it!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/OpenInvestigator3235&quot;&gt; /u/OpenInvestigator3235 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dsr8tf/agent_routing_inquiry/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dsr8tf/agent_routing_inquiry/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dsr8tf</id><link href="https://www.reddit.com/r/LangChain/comments/1dsr8tf/agent_routing_inquiry/" /><updated>2024-07-01T12:01:33+00:00</updated><published>2024-07-01T12:01:33+00:00</published><title>Agent Routing Inquiry</title></entry><entry><author><name>/u/Mediocre-Card8046</name><uri>https://www.reddit.com/user/Mediocre-Card8046</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;I am using Langgraph and now I want to clear the state if a user presses the button: &amp;quot;Clear Chat History&amp;quot;. &lt;/p&gt; &lt;p&gt;How can I clear the state of my Graph or set it to the default values?&lt;/p&gt; &lt;p&gt;I can view the state like this: &lt;/p&gt; &lt;pre&gt;&lt;code&gt;config ={ &amp;quot;configurable&amp;quot;: {&amp;quot;thread_id&amp;quot;: &amp;quot;user_id19999&amp;quot;}, #here specific user_ids or conversation_ids can be inserted if needed &amp;#39;callbacks&amp;#39;: [langfuse_handler] #if you are not using Langfuse, use &amp;#39;ConsoleCallbackHandler&amp;#39; for &amp;#39;callbacks&amp;#39; } app.get_state(config).values &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Which gives me this output:&lt;/p&gt; &lt;p&gt;{&amp;#39;question&amp;#39;: &amp;#39;Wie war mein Name?&amp;#39;,&lt;br/&gt; &amp;#39;raw_docs&amp;#39;: [],&lt;br/&gt; &amp;#39;generation&amp;#39;: AIMessage(content=&amp;#39;Es tut mir leid, aber zu Ihrer Frage nach meinem Namen konnten keine Inhalte in Confluence gefunden werden. Ich antworte Ihnen gerne, dass mein Name AnswerBot lautet. Bitte beachten Sie, dass diese Antwort nicht anhand von Confluence-Kontextinformationen generiert wurde und ich die Richtigkeit der Antwort nicht sicherstellen kann. Es kann hilfreich sein, die Frage klarer zu formulieren, damit Informationen dazu besser gefunden werden können.&amp;#39;, response_metadata={&amp;#39;finish_reason&amp;#39;: &amp;#39;stop&amp;#39;}, id=&amp;#39;run-6e24ee0f-bbe6-4d8f-99c2-3f816555ffe4&amp;#39;),&lt;br/&gt; &amp;#39;messages&amp;#39;: [HumanMessage(content=&amp;#39;Wie war mein Name?&amp;#39;, id=&amp;#39;2b259646-0465-4b86-80d5-332f005083ab&amp;#39;)],&lt;br/&gt; &amp;#39;is_smalltalk&amp;#39;: &amp;#39;False&amp;#39;,}&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Mediocre-Card8046&quot;&gt; /u/Mediocre-Card8046 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dsr14k/clear_state_in_langgraph_to_delete_chat_history/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dsr14k/clear_state_in_langgraph_to_delete_chat_history/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dsr14k</id><link href="https://www.reddit.com/r/LangChain/comments/1dsr14k/clear_state_in_langgraph_to_delete_chat_history/" /><updated>2024-07-01T11:49:46+00:00</updated><published>2024-07-01T11:49:46+00:00</published><title>Clear State in Langgraph to delete chat history</title></entry><entry><author><name>/u/AffectionateChain907</name><uri>https://www.reddit.com/user/AffectionateChain907</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dsmk86/decreasing_the_response_time_in_multiagent/&quot;&gt; &lt;img src=&quot;https://b.thumbs.redditmedia.com/Trr0hIVGArLk1jDefnLU165NUCuypW9tm7AD1E8osbM.jpg&quot; alt=&quot;Decreasing the response time in Multi-Agent Workflow of LangGraph using Ollama - Llama 3 model&quot; title=&quot;Decreasing the response time in Multi-Agent Workflow of LangGraph using Ollama - Llama 3 model&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;So recently I was testing out the Multi-Agent Workflow of langchain with some budget constraints and hence I decided to use Llama 3 model from Ollama. &lt;/p&gt; &lt;p&gt;I am following the supervisor structure as shown in their tutorials. The role of the supervisor is to simply redirect the query to one particular agent and that particular agent then handles the extraction of some special attributes from the user query.&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/nsib0o7btu9d1.png?width=1934&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=96b120524949e1af997b82b3df221b9824ce1d8d&quot;&gt;Multi-Agent Workflow Schema Diagram&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Now what&amp;#39;s happening is that when I run these agents (without langraph) on a individual script level for 5 user queries, the first query takes around 20 seconds to generate the response and then the subsequent 4 queries give response like within 3 seconds. &lt;/p&gt; &lt;p&gt;But when I connect the supervisor to the agents using LangGraph, and I test it for the same 5 queries, each query takes &amp;gt; 100 seconds to yield the final output. &lt;/p&gt; &lt;p&gt;My hardware specs are pretty decent i believe and should not be a hardware issue :-&lt;/p&gt; &lt;p&gt;RAM- 16GB, PROCESSOR - RYZEN 9, GPU - 4GB RTX 3050&lt;/p&gt; &lt;p&gt;Is this happening most likely due to a code level issue or something related to my ollama server settings like number of models running parallelly? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/AffectionateChain907&quot;&gt; /u/AffectionateChain907 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dsmk86/decreasing_the_response_time_in_multiagent/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dsmk86/decreasing_the_response_time_in_multiagent/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1dsmk86</id><media:thumbnail url="https://b.thumbs.redditmedia.com/Trr0hIVGArLk1jDefnLU165NUCuypW9tm7AD1E8osbM.jpg" /><link href="https://www.reddit.com/r/LangChain/comments/1dsmk86/decreasing_the_response_time_in_multiagent/" /><updated>2024-07-01T06:55:03+00:00</updated><published>2024-07-01T06:55:03+00:00</published><title>Decreasing the response time in Multi-Agent Workflow of LangGraph using Ollama - Llama 3 model</title></entry><entry><author><name>/u/ramkitvprk</name><uri>https://www.reddit.com/user/ramkitvprk</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi All,&lt;br/&gt; any resources or reference , I can get to convert the SAS scripts into HIve SQL using LLMs? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/ramkitvprk&quot;&gt; /u/ramkitvprk &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dsl8ne/sas_to_hive_sql_conversions_using_llm/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dsl8ne/sas_to_hive_sql_conversions_using_llm/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dsl8ne</id><link href="https://www.reddit.com/r/LangChain/comments/1dsl8ne/sas_to_hive_sql_conversions_using_llm/" /><updated>2024-07-01T05:27:30+00:00</updated><published>2024-07-01T05:27:30+00:00</published><title>SAS to Hive sql conversions using LLM</title></entry></feed>