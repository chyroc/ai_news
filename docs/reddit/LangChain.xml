<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/"><category term="LangChain" label="r/LangChain"/><updated>2023-12-22T09:37:08+00:00</updated><icon>https://www.redditstatic.com/icon.png/</icon><id>/r/LangChain.rss</id><link rel="self" href="https://www.reddit.com/r/LangChain.rss" type="application/atom+xml" /><link rel="alternate" href="https://www.reddit.com/r/LangChain" type="text/html" /><subtitle>LangChain is an open-source framework and developer toolkit that helps developers get LLM applications from prototype to production. It is available for Python and Javascript at https://www.langchain.com/.</subtitle><title>LangChain</title><entry><author><name>/u/zchaarm</name><uri>https://www.reddit.com/user/zchaarm</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;A place for members of &lt;a href=&quot;/r/LangChain&quot;&gt;r/LangChain&lt;/a&gt; to chat with each other&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/zchaarm&quot;&gt; /u/zchaarm &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_10ljho9</id><link href="https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/" /><updated>2023-01-26T04:33:05+00:00</updated><published>2023-01-26T04:33:05+00:00</published><title>r/LangChain Lounge</title></entry><entry><author><name>/u/Trick-Asparagus-9260</name><uri>https://www.reddit.com/user/Trick-Asparagus-9260</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Guys, I&amp;#39;m doing a similarity search and using relevance scores because I understand relevance scores return scores between 0 and 1. However when I use Langchain to return these scores, they come back in negatives. However when I use custom code for chroma or faiss, I get scores between 0 and 1. Is this a bug in Langchain, pls help.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Trick-Asparagus-9260&quot;&gt; /u/Trick-Asparagus-9260 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18o9afp/langchain_returns_similarity_search_with/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18o9afp/langchain_returns_similarity_search_with/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18o9afp</id><link href="https://www.reddit.com/r/LangChain/comments/18o9afp/langchain_returns_similarity_search_with/" /><updated>2023-12-22T07:24:20+00:00</updated><published>2023-12-22T07:24:20+00:00</published><title>Langchain returns similarity_search_with_relevance_scores in negative</title></entry><entry><author><name>/u/Aggressive_Tea9664</name><uri>https://www.reddit.com/user/Aggressive_Tea9664</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi all,&lt;/p&gt; &lt;p&gt;Is there a way I can get the time needed for each individual components of ConversationalRetrievalChain?&lt;/p&gt; &lt;p&gt;For eg, how do I get the time needed for the LLM to generate a reply?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Aggressive_Tea9664&quot;&gt; /u/Aggressive_Tea9664 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18o2r72/get_time_needed_for_individual_components_of/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18o2r72/get_time_needed_for_individual_components_of/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18o2r72</id><link href="https://www.reddit.com/r/LangChain/comments/18o2r72/get_time_needed_for_individual_components_of/" /><updated>2023-12-22T01:12:04+00:00</updated><published>2023-12-22T01:12:04+00:00</published><title>Get time needed for individual components of ConversationalRetrievalChain</title></entry><entry><author><name>/u/DarthLoki79</name><uri>https://www.reddit.com/user/DarthLoki79</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I have the following:&lt;br/&gt; I want to be able to use agents and pass one argument programmatically. I ideally dont want that in prompt.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;def testFunc(text1, email): print(text1) print(email) tools = [ &lt;/code&gt;&lt;/pre&gt; &lt;p&gt; StructuredTool.from_function( func=testFunc, name=&amp;quot;testing&amp;quot;, args_schema=TestSchema, description=&amp;quot;tester&amp;quot;, return_direct=True,&lt;/p&gt; &lt;p&gt;) ]&lt;/p&gt; &lt;pre&gt;&lt;code&gt;model_with_tools = model.bind( functions=[format_tool_to_openai_function(t) for t in tools], ) prompt = ChatPromptTemplate.from_messages( [ (&amp;quot;system&amp;quot;, template), MessagesPlaceholder(variable_name=&amp;quot;chat_history&amp;quot;), MessagesPlaceholder(variable_name=&amp;quot;agent_scratchpad&amp;quot;), (&amp;quot;user&amp;quot;, &amp;quot;{input}&amp;quot;), ] ) agent = ( { &amp;quot;input&amp;quot;: lambda x: x[&amp;quot;input&amp;quot;], &amp;quot;agent_scratchpad&amp;quot;: lambda x: format_to_openai_function_messages( x[&amp;quot;intermediate_steps&amp;quot;] ), &amp;quot;chat_history&amp;quot;: lambda x: x[&amp;quot;chat_history&amp;quot;], &amp;quot;email&amp;quot;: lambda x: x[&amp;#39;email&amp;#39;], } | prompt | model_with_tools | OpenAIFunctionsAgentOutputParser() ) agent_executor = AgentExecutor( agent=agent, tools=tools, verbose=True, return_intermediate_steps=True ) result = agent_executor.invoke( {&amp;quot;input&amp;quot;: &amp;quot;this is the test code&amp;quot;, &amp;quot;chat_history&amp;quot;: [], &amp;quot;email&amp;quot;: &amp;quot;TEST@GMAIL.COM&amp;quot;} ) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;I want the output to be &lt;/p&gt; &lt;pre&gt;&lt;code&gt;this is the test code TEST@GMAIL.com &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Basically I want the email to be passed programmatically and the other arguments to be passed using the LLM. I cant figure this out. Have been going at it for a while. No matter what, the function is being called with the prompt, which doesnt have the email and it is hallucinating the email as [&lt;a href=&quot;mailto:user@example.com&quot;&gt;user@example.com&lt;/a&gt;](mailto:&lt;a href=&quot;mailto:user@example.com&quot;&gt;user@example.com&lt;/a&gt;) &lt;/p&gt; &lt;p&gt;Is there no way to pass the email to the function, maybe a additional kwarg thing or extra configs or something?&lt;br/&gt; After the prompt part of the chain ends, I can see the part entering the LLM is: &lt;/p&gt; &lt;pre&gt;&lt;code&gt; &amp;quot;kwargs&amp;quot;: { &amp;quot;content&amp;quot;: this is the test code&amp;quot;, &amp;quot;additional_kwargs&amp;quot;: {} } &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/DarthLoki79&quot;&gt; /u/DarthLoki79 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18nzw4v/how_to_pass_some_arguments_to_function_call_via/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18nzw4v/how_to_pass_some_arguments_to_function_call_via/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18nzw4v</id><link href="https://www.reddit.com/r/LangChain/comments/18nzw4v/how_to_pass_some_arguments_to_function_call_via/" /><updated>2023-12-21T22:59:57+00:00</updated><published>2023-12-21T22:59:57+00:00</published><title>How to pass some arguments to function call via code and some extracted from LLM?</title></entry><entry><author><name>/u/CharlieTrigger</name><uri>https://www.reddit.com/user/CharlieTrigger</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi there fellow Langchainers,&lt;/p&gt; &lt;p&gt;I have created an agent in a ipynb. Works great and by simply adding LANGCHAIN_TRACING_V2 = os.getenv(&amp;quot;LANGCHAIN_TRACING_V2&amp;quot;) LANGCHAIN_PROJECT = os.getenv(&amp;quot;LANGCHAIN_PROJECT&amp;quot;) LANGCHAIN_ENDPOINT = os.getenv(&amp;quot;LANGCHAIN_ENDPOINT&amp;quot;) LANGCHAIN_API_KEY = os.getenv(&amp;quot;LANGCHAIN_API_KEY&amp;quot;) and adding these values to the .env file and &lt;/p&gt; &lt;p&gt;from langsmith import Client client = Client()&lt;/p&gt; &lt;p&gt;Every agent_executor.invoke({&amp;quot;input&amp;quot;: &amp;quot;input query here&amp;quot;})[&amp;quot;output&amp;quot;]&lt;/p&gt; &lt;p&gt;is nicely logged in Langsmith.&lt;/p&gt; &lt;p&gt;But when I wrap this same agent in a fastapi application with uvicorn, it doesn&amp;#39;t work. The agent works fine, I can use the agent through Postman just fine. But nothing is logged in Langsmith.&lt;/p&gt; &lt;p&gt;Any help is greatly appreciated.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/CharlieTrigger&quot;&gt; /u/CharlieTrigger &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18npk5y/how_to_use_langsmith_with_a_fastapiuvicorn_setup/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18npk5y/how_to_use_langsmith_with_a_fastapiuvicorn_setup/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18npk5y</id><link href="https://www.reddit.com/r/LangChain/comments/18npk5y/how_to_use_langsmith_with_a_fastapiuvicorn_setup/" /><updated>2023-12-21T15:27:54+00:00</updated><published>2023-12-21T15:27:54+00:00</published><title>How to use Langsmith with a FastAPI/uvicorn setup</title></entry><entry><author><name>/u/duddai</name><uri>https://www.reddit.com/user/duddai</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey people, &lt;/p&gt; &lt;p&gt;I&amp;#39;m new to langchain and have a few questions. &lt;/p&gt; &lt;p&gt;If I want to summarize large PDFs (e.g. long scientific texts) in an easy-to-understand language, does a vector-based database make sense? Or can I just split the text up and then have it summarized piece by piece and create a summary at the end? &lt;/p&gt; &lt;p&gt;Is the context of the entire PDF preserved in both versions? &lt;/p&gt; &lt;p&gt;I hope you understand my question and can help me. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/duddai&quot;&gt; /u/duddai &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18nlcpm/summary_of_long_pdfs/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18nlcpm/summary_of_long_pdfs/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18nlcpm</id><link href="https://www.reddit.com/r/LangChain/comments/18nlcpm/summary_of_long_pdfs/" /><updated>2023-12-21T11:55:52+00:00</updated><published>2023-12-21T11:55:52+00:00</published><title>Summary of long PDFs</title></entry><entry><author><name>/u/PlayboiCult</name><uri>https://www.reddit.com/user/PlayboiCult</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hello everyone. I&amp;#39;m using Langchain (js) in my Next.js app and used this guide &lt;a href=&quot;https://js.langchain.com/docs/integrations/toolkits/sql&quot;&gt;https://js.langchain.com/docs/integrations/toolkits/sql&lt;/a&gt; (that someone very helpful shared with me here on reddit) to integrate the SQL agent.&lt;/p&gt; &lt;p&gt;I&amp;#39;m getting &lt;strong&gt;Agent stopped due to max iterations.&lt;/strong&gt; or a random incorrect answer from the agent. &lt;/p&gt; &lt;p&gt;I implemented it pretty much exactly like the docs I referenced but with a postgreSQL db in Supabase instead of a local .db. &lt;/p&gt; &lt;p&gt;Any thoughts or recommendations are appreciated👍. Thanks in advance&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/PlayboiCult&quot;&gt; /u/PlayboiCult &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18ntfoa/errors_using_sql_agent/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18ntfoa/errors_using_sql_agent/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18ntfoa</id><link href="https://www.reddit.com/r/LangChain/comments/18ntfoa/errors_using_sql_agent/" /><updated>2023-12-21T18:18:28+00:00</updated><published>2023-12-21T18:18:28+00:00</published><title>Errors using SQL Agent</title></entry><entry><author><name>/u/OnlyProggingForFun</name><uri>https://www.reddit.com/user/OnlyProggingForFun</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18ntbmq/langchain_vs_llamaindex_vs_openai_gpts_which_one/&quot;&gt; &lt;img src=&quot;https://external-preview.redd.it/ql1oKVZEdu8fmrJir4nPRBTshkBx1EiC3TccNDPG78g.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f92dc3c67cd6dbc78e0519fff9bdf03054f4c565&quot; alt=&quot;Langchain vs. LlamaIndex vs. OpenAI GPTs: Which one should you use?&quot; title=&quot;Langchain vs. LlamaIndex vs. OpenAI GPTs: Which one should you use?&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/OnlyProggingForFun&quot;&gt; /u/OnlyProggingForFun &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://youtu.be/g84uWgVXVYg&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18ntbmq/langchain_vs_llamaindex_vs_openai_gpts_which_one/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_18ntbmq</id><media:thumbnail url="https://external-preview.redd.it/ql1oKVZEdu8fmrJir4nPRBTshkBx1EiC3TccNDPG78g.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=f92dc3c67cd6dbc78e0519fff9bdf03054f4c565" /><link href="https://www.reddit.com/r/LangChain/comments/18ntbmq/langchain_vs_llamaindex_vs_openai_gpts_which_one/" /><updated>2023-12-21T18:13:24+00:00</updated><published>2023-12-21T18:13:24+00:00</published><title>Langchain vs. LlamaIndex vs. OpenAI GPTs: Which one should you use?</title></entry><entry><author><name>/u/PlayboiCult</name><uri>https://www.reddit.com/user/PlayboiCult</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hello everyone. I&amp;#39;m new to Langchain and I made a chatbot using Next.js (so the Javascript library) that uses a CSV with soccer info to answer questions. Specific questions, for example &amp;quot;How many goals did &lt;strong&gt;Haaland&lt;/strong&gt; score?&amp;quot; get answered properly, since it searches info about Haaland in the CSV (I&amp;#39;m embedding the CSV and storing the vectors in Pinecone).&lt;/p&gt; &lt;p&gt;The problem starts when I ask general questions, meaning questions without keywords. For example, &amp;quot;who made more assists?&amp;quot;, or maybe something extreme like &amp;quot;how many rows are there in the CSV?&amp;quot;. It completely fails. I&amp;#39;m guessing that it only gets the relevant info from the vector db based on the query and it can&amp;#39;t answer these types of questions.&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;I&amp;#39;m using &lt;code&gt;ConversationalRetrievalQAChain&lt;/code&gt; from Langchain&lt;/p&gt; &lt;pre&gt;&lt;code&gt;chain.ts /* create vectorstore */ const vectorStore = await PineconeStore.fromExistingIndex( new OpenAIEmbeddings({}), { pineconeIndex, textKey: &amp;quot;text&amp;quot;, } ); return ConversationalRetrievalQAChain.fromLLM( model, vectorStore.asRetriever(), { returnSourceDocuments: true } ); &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;And using it in my API in Next.js.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;route.ts const res = await chain.call({ question: question, chat_history: history .map((h) =&amp;gt; { h.content; }) .join(&amp;quot;\n&amp;quot;), }); &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;Any suggestions are welcomed and appreciated. Also feel free to ask any questions. Thanks in advance&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/PlayboiCult&quot;&gt; /u/PlayboiCult &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18nccz3/getting_general_information_over_a_csv/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18nccz3/getting_general_information_over_a_csv/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18nccz3</id><link href="https://www.reddit.com/r/LangChain/comments/18nccz3/getting_general_information_over_a_csv/" /><updated>2023-12-21T02:43:48+00:00</updated><published>2023-12-21T02:43:48+00:00</published><title>Getting general information over a CSV</title></entry><entry><author><name>/u/effervesense</name><uri>https://www.reddit.com/user/effervesense</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey guys, I am wondering if you know of good guides to go about doing this ^&lt;/p&gt; &lt;p&gt;And would it result in a better memory retrieval system? (I&amp;#39;ve heard LlamaIndex is better for RAG systems?) Or would you build everything in Langchain? My system will involve a fine-tuned model, external knowledge, plus I am trying to figure out how to store conversation history in the vector db for memory retrieval. Eventually I&amp;#39;ll add more components like speech-to-text software.. is this supported with langchain? Any guidance on this is greatly appreciated.&lt;/p&gt; &lt;p&gt;Right now I&amp;#39;m using Flowise which is built on top of LangChain, but haven&amp;#39;t found any info on integrating LlamaIndex with Flowise specifically.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/effervesense&quot;&gt; /u/effervesense &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18n65if/integrating_llamaindex_with_langchain/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18n65if/integrating_llamaindex_with_langchain/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18n65if</id><link href="https://www.reddit.com/r/LangChain/comments/18n65if/integrating_llamaindex_with_langchain/" /><updated>2023-12-20T21:46:29+00:00</updated><published>2023-12-20T21:46:29+00:00</published><title>Integrating LlamaIndex with Langchain.</title></entry><entry><author><name>/u/Pretend-Word2531</name><uri>https://www.reddit.com/user/Pretend-Word2531</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Can you Advise on finding the open source projects like Azure Cognitive Search + GPT, maybe using Langchain??&lt;/p&gt; &lt;p&gt;This 20 second clip shows exactly the functionality we&amp;#39;re looking for &lt;a href=&quot;https://youtube.com/clip/Ugkx4Bx61tbWTnuvDmfEecj2R-msM2AI3kWA?si=tT7HkGz_m2wzIeL_&quot;&gt;https://youtube.com/clip/Ugkx4Bx61tbWTnuvDmfEecj2R-msM2AI3kWA?si=tT7HkGz_m2wzIeL_&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Pretend-Word2531&quot;&gt; /u/Pretend-Word2531 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18nbrrw/identify_a_langchain_like_project_like_in_this/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18nbrrw/identify_a_langchain_like_project_like_in_this/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18nbrrw</id><link href="https://www.reddit.com/r/LangChain/comments/18nbrrw/identify_a_langchain_like_project_like_in_this/" /><updated>2023-12-21T02:13:04+00:00</updated><published>2023-12-21T02:13:04+00:00</published><title>Identify a Langchain like project like in this video clip</title></entry><entry><author><name>/u/M4xM9450</name><uri>https://www.reddit.com/user/M4xM9450</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi all,&lt;/p&gt; &lt;p&gt;Are there any resources on using a local JS copy of an LLM (namely Flan-T5) with langchainJS (especially if using Xenova’s transformers.js for the LLM inference)? I’ve been looking for resources on using the two libraries for a local JS project but information is sparse.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/M4xM9450&quot;&gt; /u/M4xM9450 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18nb0t3/looking_for_tutorial_on_local_js_llms/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18nb0t3/looking_for_tutorial_on_local_js_llms/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18nb0t3</id><link href="https://www.reddit.com/r/LangChain/comments/18nb0t3/looking_for_tutorial_on_local_js_llms/" /><updated>2023-12-21T01:34:08+00:00</updated><published>2023-12-21T01:34:08+00:00</published><title>Looking for tutorial on local JS LLMs</title></entry><entry><author><name>/u/GlobalSalt3016</name><uri>https://www.reddit.com/user/GlobalSalt3016</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I am trying to create a QNA chatbot with OpenAI API, after using RecursiveCharacterTextSplitter, texts are coming with &lt;strong&gt;&amp;quot;\n&amp;quot;&lt;/strong&gt; Should I be worried? because I have to upsert it into the Pinecone database, and also if there is any preprocessing needed before upserting please tell me.. this is my first project&lt;/p&gt; &lt;p&gt;these are the texts: &lt;/p&gt; &lt;p&gt;&lt;code&gt;[&amp;#39;The\ncow,\na\ngentle\nherbivorous\nmammal\nof\nthe\nBovidae\nfamily,\nis\na\nvital\ncontributor\nto\nagriculture\nand\nhuman\nsustenance\nglobally.\nCharacterized\nby\nits\nlarge\nbody,\ncloven\nhooves,\nand\ndistinctively\npatterned\nhide,\ncows\nprimarily\nserve\nas\nsources\nof\nmilk,\nagriculture,livestock,\nfarming\nand\nleather.\nBos\ntaurus,\nthe\nmost\ncommon\nspecies,\nexists\nin\nvarious\nbreeds,&amp;#39;]&lt;/code&gt;&lt;/p&gt; &lt;p&gt;my code : &lt;/p&gt; &lt;pre&gt;&lt;code&gt;loader = PdfReader(&amp;quot;cow.pdf&amp;quot;) text = &amp;#39;&amp;#39; for i,page in enumerate (loader.pages): content = page.extract_text() if content: text += content text_splitter = RecursiveCharacterTextSplitter( separators=[&amp;quot;\n\n&amp;quot;, &amp;quot;\n&amp;quot;, &amp;quot; &amp;quot;], chunk_size = 400, chunk_overlap = 20, length_function = len, ) texts = text_splitter.split_text(text) print(texts) print(len(texts)) embeddings = OpenAIEmbeddings(model=&amp;quot;text-embedding-ada-002&amp;quot;) vectorstore = Pinecone.from_texts(texts,embeddings,index_name=PINECONE_INDEX) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/GlobalSalt3016&quot;&gt; /u/GlobalSalt3016 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18n1741/text_preprocessing_before_embedding/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18n1741/text_preprocessing_before_embedding/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18n1741</id><link href="https://www.reddit.com/r/LangChain/comments/18n1741/text_preprocessing_before_embedding/" /><updated>2023-12-20T18:15:05+00:00</updated><published>2023-12-20T18:15:05+00:00</published><title>Text pre-processing before embedding</title></entry><entry><author><name>/u/FaithlessnessOk9061</name><uri>https://www.reddit.com/user/FaithlessnessOk9061</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;i was using OPENAIEmbedding before but now i changed it to AzureOpenAIEmbedding but re-building the vector db takes a very long time. is there anyway i can drop the existing table and re-build it from scratch? although i already have &lt;code&gt;pre_delete_collection=True&lt;/code&gt;&lt;/p&gt; &lt;p&gt;I am using 0.0.348 version &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/FaithlessnessOk9061&quot;&gt; /u/FaithlessnessOk9061 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18n2an2/pgvector_table/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18n2an2/pgvector_table/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18n2an2</id><link href="https://www.reddit.com/r/LangChain/comments/18n2an2/pgvector_table/" /><updated>2023-12-20T19:00:20+00:00</updated><published>2023-12-20T19:00:20+00:00</published><title>PGVector - Table</title></entry><entry><author><name>/u/e-nigmaNL</name><uri>https://www.reddit.com/user/e-nigmaNL</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi all,&lt;/p&gt; &lt;p&gt;I am trying to create a chatbot that uses both regular chat or RAG. When using the chat functionality I’m able to use the API offered by llama cpp server. But how would I use the API when using the retrievalQA method, to search documents from a vectordb (chroma).&lt;/p&gt; &lt;p&gt;Thanks&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/e-nigmaNL&quot;&gt; /u/e-nigmaNL &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18mwpft/retrievalqa_with_llamacpp_api_call/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18mwpft/retrievalqa_with_llamacpp_api_call/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18mwpft</id><link href="https://www.reddit.com/r/LangChain/comments/18mwpft/retrievalqa_with_llamacpp_api_call/" /><updated>2023-12-20T15:06:17+00:00</updated><published>2023-12-20T15:06:17+00:00</published><title>RetrievalQA with llamacpp api call</title></entry><entry><author><name>/u/Spare_Cancel3205</name><uri>https://www.reddit.com/user/Spare_Cancel3205</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Is there any way to route between different LLM models, like for codng related prompts it should use an LLM model that is good at coding and for conversational prompts a model that is good at chatting/conversing.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Spare_Cancel3205&quot;&gt; /u/Spare_Cancel3205 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18mrv75/route_between_llms/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18mrv75/route_between_llms/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18mrv75</id><link href="https://www.reddit.com/r/LangChain/comments/18mrv75/route_between_llms/" /><updated>2023-12-20T10:49:44+00:00</updated><published>2023-12-20T10:49:44+00:00</published><title>Route between LLMs</title></entry><entry><author><name>/u/Trick-Asparagus-9260</name><uri>https://www.reddit.com/user/Trick-Asparagus-9260</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Guys, I&amp;#39;m in a pickle. I&amp;#39;m evaluating my RAG using Trulens and this takes in the RetrievalQA chain. But I want to modify the context to add more information to it say from the metadata. Is it possible to modify the context retrieved by RetrievalQA to add information from metadata and perhaps add more info to the context?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Trick-Asparagus-9260&quot;&gt; /u/Trick-Asparagus-9260 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18n0obz/how_can_i_modify_the_context_retrieved_by/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18n0obz/how_can_i_modify_the_context_retrieved_by/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18n0obz</id><link href="https://www.reddit.com/r/LangChain/comments/18n0obz/how_can_i_modify_the_context_retrieved_by/" /><updated>2023-12-20T17:54:26+00:00</updated><published>2023-12-20T17:54:26+00:00</published><title>How can I modify the context retrieved by RetrievalQA Chain?</title></entry><entry><author><name>/u/Impressive_Gate2102</name><uri>https://www.reddit.com/user/Impressive_Gate2102</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hello all, &lt;/p&gt; &lt;p&gt;I want to add context awareness to my LLM RAG pipeline. Here are 2 approaches I am thinking of. Please help me if I am going in the right direction, and also what should be the ideal approach :&lt;/p&gt; &lt;p&gt;Approach 1 : Step 1. Use an LLM to do co reference resolution in the new query, based on immediate last conversarion, and get the modified query.&lt;br/&gt; Example : last conversarion : What is Google? New query: what does it do ? &lt;/p&gt; &lt;p&gt;Modified query : what does Google do . &lt;/p&gt; &lt;p&gt;Step 2. Now based on modified query, get similar responses based on similarity score of previous conversations and add it to the prompt. &lt;/p&gt; &lt;p&gt;Cons : 1. With coreference resolution since we are using only immediate last query, it would lose out on, where the user query refers to noun or subject from earlier conversation . &lt;/p&gt; &lt;p&gt;Pros : Woukd only pass the relevant conversation. &lt;/p&gt; &lt;p&gt;Approach 2 : Summarize the conversation history, and store it in memory. As tye conversation proceeds, keep on adding to the conversation history. &lt;/p&gt; &lt;p&gt;Cons : In case of context switching, the summary would also have non relevant context being added to the prompt. How to handle this ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Impressive_Gate2102&quot;&gt; /u/Impressive_Gate2102 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18mxiny/need_help_in_adding_context_awareness_to_llm_rag/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18mxiny/need_help_in_adding_context_awareness_to_llm_rag/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18mxiny</id><link href="https://www.reddit.com/r/LangChain/comments/18mxiny/need_help_in_adding_context_awareness_to_llm_rag/" /><updated>2023-12-20T15:41:20+00:00</updated><published>2023-12-20T15:41:20+00:00</published><title>Need help in adding context awareness to LLM RAG pipeline</title></entry><entry><author><name>/u/ammar_k100</name><uri>https://www.reddit.com/user/ammar_k100</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;I am new to langchain and AI/ML in general and would really appreciate if someone could help me.&lt;/p&gt; &lt;p&gt;I have a list of addresses of a city which I am storing in a chromadb using their default embedding (all-MiniLM-L6-v2). And then I have another set of addresses(incomplete and misspelled) which I wanna use them as a query and want to return as similar address as possible. Some of them works perfect but for some addresses it behaves strangely.&lt;/p&gt; &lt;p&gt;For example: if I search for “XYZ building, ABC Town, City” It will return an address from “ABC street” instead of “ABC Town” although I have both of them in my knowledge base on which my model is trained. And sometimes it will return completely irrelevant address which doesn’t match. I want to ask how can I improve this? Or is this even the right approach or should I go for a different approach. TIA&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/ammar_k100&quot;&gt; /u/ammar_k100 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18m0kch/need_help_for_a_similarity_search_application/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18m0kch/need_help_for_a_similarity_search_application/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18m0kch</id><link href="https://www.reddit.com/r/LangChain/comments/18m0kch/need_help_for_a_similarity_search_application/" /><updated>2023-12-19T12:18:37+00:00</updated><published>2023-12-19T12:18:37+00:00</published><title>Need help for a similarity search application</title></entry><entry><author><name>/u/olddoglearnsnewtrick</name><uri>https://www.reddit.com/user/olddoglearnsnewtrick</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Dear friends,&lt;/p&gt; &lt;p&gt;I&amp;#39;m stuck in an uncanny valley beween peaks of ignorance and peaks of insufficient english proficiency :)&lt;/p&gt; &lt;p&gt;I&amp;#39;m dreaming about a LLM able to generate a summary (?) of a news article. I don&amp;#39;t know if summary is the right term. Or maybe is &amp;quot;topic extraction&amp;quot; a better term? I&amp;#39;ve also read &amp;quot;factoid identification&amp;quot; somewhere.&lt;/p&gt; &lt;p&gt;For example ideally on the following article: &lt;a href=&quot;https://www.bbc.com/news/world-europe-67756413&quot;&gt;https://www.bbc.com/news/world-europe-67756413&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I would like the LLM to reply with something like:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;A volcano erupted on the Reykjanes peninsula in Iceland.&lt;/li&gt; &lt;li&gt;Grindavik has been evacuated but there are no casualties&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I did a little experiment with Mixtral MOE and it produced decent very long/verbose summaries. It could also not reply correctly to questions like &amp;quot;Who&amp;#39;s mentioned in this article&amp;quot; and of course the following &amp;quot;What did X say&amp;quot; was not possible.&lt;/p&gt; &lt;p&gt;Is there any suggestion you could give me of models on HF that I could try to get closer to my needs? &lt;/p&gt; &lt;p&gt;Oh did I mention that I need this for Italian (and as a less important need French, Spanish, English and German) ?&lt;/p&gt; &lt;p&gt;Grazie mille !!!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/olddoglearnsnewtrick&quot;&gt; /u/olddoglearnsnewtrick &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18m2tf2/dont_have_the_english_term_to_describe_what_im/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18m2tf2/dont_have_the_english_term_to_describe_what_im/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18m2tf2</id><link href="https://www.reddit.com/r/LangChain/comments/18m2tf2/dont_have_the_english_term_to_describe_what_im/" /><updated>2023-12-19T14:12:50+00:00</updated><published>2023-12-19T14:12:50+00:00</published><title>Don't have the english term to describe what I'm looking for</title></entry><entry><author><name>/u/The-Tank-849</name><uri>https://www.reddit.com/user/The-Tank-849</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;What would be your favorite setup client hosted to have csv, pdf and api chatbot?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/The-Tank-849&quot;&gt; /u/The-Tank-849 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18m3tb3/favorite_setup_for_agency/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18m3tb3/favorite_setup_for_agency/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18m3tb3</id><link href="https://www.reddit.com/r/LangChain/comments/18m3tb3/favorite_setup_for_agency/" /><updated>2023-12-19T14:57:37+00:00</updated><published>2023-12-19T14:57:37+00:00</published><title>Favorite setup for agency</title></entry><entry><author><name>/u/riksp027</name><uri>https://www.reddit.com/user/riksp027</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi.&lt;/p&gt; &lt;p&gt;It seems text generation webui migrated is api to a compatible version of openapi swagger which seems to be a good news.&lt;/p&gt; &lt;p&gt;Due to that, textgen LLM no longer works. I don&amp;#39;t find an easy way to connect to an api compatible with open ai in Langchain.&lt;/p&gt; &lt;p&gt;I was hoping for something like changing open api base path but I don&amp;#39;t see anything like that in the documentation.&lt;/p&gt; &lt;p&gt;Someone has an idea if an LLM object exist to connect to a local api compatible with open ai or how to use openai but with a different base path ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/riksp027&quot;&gt; /u/riksp027 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18m319f/connect_to_a_local_llm_compatible_with_openai_api/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18m319f/connect_to_a_local_llm_compatible_with_openai_api/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18m319f</id><link href="https://www.reddit.com/r/LangChain/comments/18m319f/connect_to_a_local_llm_compatible_with_openai_api/" /><updated>2023-12-19T14:22:51+00:00</updated><published>2023-12-19T14:22:51+00:00</published><title>Connect to a local LLM compatible with openAI api</title></entry><entry><author><name>/u/Useful_Ad_7882</name><uri>https://www.reddit.com/user/Useful_Ad_7882</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Any chance Langchain supports LLMLingua already ? or something that we may have to wait for ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Useful_Ad_7882&quot;&gt; /u/Useful_Ad_7882 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18lxgel/llmlingua_compatibilitysupport/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18lxgel/llmlingua_compatibilitysupport/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18lxgel</id><link href="https://www.reddit.com/r/LangChain/comments/18lxgel/llmlingua_compatibilitysupport/" /><updated>2023-12-19T08:54:36+00:00</updated><published>2023-12-19T08:54:36+00:00</published><title>LLMLingua compatibility/support</title></entry><entry><author><name>/u/SnooLentils4081</name><uri>https://www.reddit.com/user/SnooLentils4081</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi everyone!&lt;/p&gt; &lt;p&gt;I have been able to succesfully create a vector store with Postgres and PGVector using Langchain from end to end, as described in this article: &lt;a href=&quot;https://python.langchain.com/docs/integrations/vectorstores/pgvector&quot;&gt;https://python.langchain.com/docs/integrations/vectorstores/pgvector&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I noticed it creates the &lt;strong&gt;&lt;em&gt;langchain_pg_collection&lt;/em&gt;&lt;/strong&gt; and &lt;strong&gt;&lt;em&gt;lanchain_pg_embedding&lt;/em&gt;&lt;/strong&gt; tables in Postgres. Upon inspceting the LangChain source code, it seems it really relies on that specific two table structure in order to instantiate a VectorStore.&lt;/p&gt; &lt;p&gt;Suppose that instead of having my embeddings structured this way, I would like to have them as a column in a differente table, like my contents table (which has the string the values I want to embed).&lt;/p&gt; &lt;p&gt;Would I still be able to initialize the VectorStore pointing to that specifc table and column?Or does the entire VectorStore functionality only works if I use the default table structure?&lt;/p&gt; &lt;p&gt;I would really appreciate all the help and guidance I could get here.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/SnooLentils4081&quot;&gt; /u/SnooLentils4081 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18lodyx/is_it_even_possible_to_initialize_vector_stores/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18lodyx/is_it_even_possible_to_initialize_vector_stores/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18lodyx</id><link href="https://www.reddit.com/r/LangChain/comments/18lodyx/is_it_even_possible_to_initialize_vector_stores/" /><updated>2023-12-19T00:32:59+00:00</updated><published>2023-12-19T00:32:59+00:00</published><title>Is it even possible to initialize Vector Stores without using the default table structure?</title></entry><entry><author><name>/u/house_lite</name><uri>https://www.reddit.com/user/house_lite</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Seems like I can&amp;#39;t get through an entire cookbook topic without encountering an error. The errors seem to occur with parsers and runnables. &lt;/p&gt; &lt;p&gt;What&amp;#39;s going on? I recently installed langchain. Are the docs outdated at this point?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/house_lite&quot;&gt; /u/house_lite &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18lems2/cookbook_examples_are_riddled_with_errors/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18lems2/cookbook_examples_are_riddled_with_errors/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18lems2</id><link href="https://www.reddit.com/r/LangChain/comments/18lems2/cookbook_examples_are_riddled_with_errors/" /><updated>2023-12-18T17:44:23+00:00</updated><published>2023-12-18T17:44:23+00:00</published><title>Cookbook examples are riddled with errors</title></entry><entry><author><name>/u/nouskiski</name><uri>https://www.reddit.com/user/nouskiski</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I want to make an agent that uses a tool that searches for keywords in a huge JSON. I succeeded in getting it to fetch data from an API, but the user should also be able to filter data chunks with keywords. For example, &amp;quot;which ticket has the keywords user, sister, and admin&amp;quot; it must be able to filter in large data which is around 150k token. How would you come around this?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/nouskiski&quot;&gt; /u/nouskiski &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18ln785/search_in_large_json/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18ln785/search_in_large_json/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18ln785</id><link href="https://www.reddit.com/r/LangChain/comments/18ln785/search_in_large_json/" /><updated>2023-12-18T23:39:11+00:00</updated><published>2023-12-18T23:39:11+00:00</published><title>Search in large JSON</title></entry></feed>