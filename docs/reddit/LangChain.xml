<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/"><category term="LangChain" label="r/LangChain"/><updated>2024-07-08T19:27:49+00:00</updated><icon>https://www.redditstatic.com/icon.png/</icon><id>/r/LangChain.rss</id><link rel="self" href="https://www.reddit.com/r/LangChain.rss" type="application/atom+xml" /><link rel="alternate" href="https://www.reddit.com/r/LangChain" type="text/html" /><subtitle>LangChain is an open-source framework and developer toolkit that helps developers get LLM applications from prototype to production. It is available for Python and Javascript at https://www.langchain.com/.</subtitle><title>LangChain</title><entry><author><name>/u/zchaarm</name><uri>https://www.reddit.com/user/zchaarm</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;A place for members of &lt;a href=&quot;/r/LangChain&quot;&gt;r/LangChain&lt;/a&gt; to chat with each other&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/zchaarm&quot;&gt; /u/zchaarm &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_10ljho9</id><link href="https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/" /><updated>2023-01-26T04:33:05+00:00</updated><published>2023-01-26T04:33:05+00:00</published><title>r/LangChain Lounge</title></entry><entry><author><name>/u/Pitiful_Yak_390</name><uri>https://www.reddit.com/user/Pitiful_Yak_390</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey everyone,&lt;br/&gt; I am working on a practical Llama-based app and struggled with getting clean JSON output. I know I&amp;#39;m not alone in this, so I wanted to share a solution I found.&lt;/p&gt; &lt;p&gt;The Instructor library is solid for getting structured data from any LLM. I put together a cookbook showing how to use it: &lt;a href=&quot;https://git.new/PortkeyInstructor&quot;&gt;https://git.new/PortkeyInstructor&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It covers 100+ other LLM providers along with built-in observability. Thought it might be useful for others here. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Pitiful_Yak_390&quot;&gt; /u/Pitiful_Yak_390 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dy3i7q/how_to_get_structured_output_json_from_your_llm/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dy3i7q/how_to_get_structured_output_json_from_your_llm/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dy3i7q</id><link href="https://www.reddit.com/r/LangChain/comments/1dy3i7q/how_to_get_structured_output_json_from_your_llm/" /><updated>2024-07-08T08:10:42+00:00</updated><published>2024-07-08T08:10:42+00:00</published><title>How to get structured output (JSON) from your LLM.</title></entry><entry><author><name>/u/jfjeschke</name><uri>https://www.reddit.com/user/jfjeschke</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey Langchain community,&lt;/p&gt; &lt;p&gt;I&amp;#39;ve been tackling the challenge of developing effective AI agents. I&amp;#39;ve built a tool that turns interviews or process documentation into functional AI agents in LangGraph (with all the tools, prompt, context, etc). I&amp;#39;m running a short private beta and would love your thoughts on it. Interested in checking it out and sharing your feedback?&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://definitive-ai.streamlit.app/&quot;&gt;Definitive AI Beta&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/Definitive-AI/Agent-Examples&quot;&gt;Example Outputs&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/jfjeschke&quot;&gt; /u/jfjeschke &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dy74id/devin_for_langgraph_automating_ai_agent/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dy74id/devin_for_langgraph_automating_ai_agent/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dy74id</id><link href="https://www.reddit.com/r/LangChain/comments/1dy74id/devin_for_langgraph_automating_ai_agent/" /><updated>2024-07-08T12:00:38+00:00</updated><published>2024-07-08T12:00:38+00:00</published><title>Devin for LangGraph: Automating AI Agent Development</title></entry><entry><author><name>/u/Important_Ostrich_60</name><uri>https://www.reddit.com/user/Important_Ostrich_60</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I want to create an LLM that can call custom-made APIs. We have already created several APIs, and the LLM should be able to make all types of HTTP requests (GET, POST, PUT, DELETE). The LLM should infer which API to call and with which parameters, allowing users to interact using natural language.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Current Considerations:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;I looked into OpenAI&amp;#39;s function calling, but it seems costly with more functions. Their documentation suggests fine-tuning the model to save tokens, but I&amp;#39;m unsure how it applies to my case.&lt;/li&gt; &lt;li&gt;I have experience using LLamaIndex but prefer using LangChain for this project due to its better documentation regarding API calls.&lt;/li&gt; &lt;li&gt;I would prefer using a local method as I&amp;#39;m using Ollama.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Questions:&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;How should I proceed with this project?&lt;/li&gt; &lt;li&gt;Is function calling the best option to consider?&lt;/li&gt; &lt;li&gt;Should I use OpenAI&amp;#39;s function calling despite the costs and the capabilities of local models like LLama2 7b?&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;If you could just suggest me some resources and possible implementations that would be of great help.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Important_Ostrich_60&quot;&gt; /u/Important_Ostrich_60 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dycmnf/a_chatbot_that_can_call_custom_made_apis/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dycmnf/a_chatbot_that_can_call_custom_made_apis/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dycmnf</id><link href="https://www.reddit.com/r/LangChain/comments/1dycmnf/a_chatbot_that_can_call_custom_made_apis/" /><updated>2024-07-08T16:05:08+00:00</updated><published>2024-07-08T16:05:08+00:00</published><title>A chatbot that can call custom made apis</title></entry><entry><author><name>/u/rishabh2362</name><uri>https://www.reddit.com/user/rishabh2362</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Looking to build an LLM app using langchain for later shipping it to the app store. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/rishabh2362&quot;&gt; /u/rishabh2362 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dyhboz/how_to_build_an_llm_app_using_langchain_for_ios/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dyhboz/how_to_build_an_llm_app_using_langchain_for_ios/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dyhboz</id><link href="https://www.reddit.com/r/LangChain/comments/1dyhboz/how_to_build_an_llm_app_using_langchain_for_ios/" /><updated>2024-07-08T19:13:42+00:00</updated><published>2024-07-08T19:13:42+00:00</published><title>How to build an LLM app using langchain for iOS?</title></entry><entry><author><name>/u/Ok_Cap2668</name><uri>https://www.reddit.com/user/Ok_Cap2668</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey everyone, I was wondering how openai and ai studio are able to achieve such high accuracy when it comes to chat with any document.&lt;/p&gt; &lt;p&gt;What do you people think how this performance can be achieved by just using RAG techniques?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Ok_Cap2668&quot;&gt; /u/Ok_Cap2668 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dy20vs/how_to_make_a_chatpdf_app_with_the_atmost/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dy20vs/how_to_make_a_chatpdf_app_with_the_atmost/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dy20vs</id><link href="https://www.reddit.com/r/LangChain/comments/1dy20vs/how_to_make_a_chatpdf_app_with_the_atmost/" /><updated>2024-07-08T06:31:12+00:00</updated><published>2024-07-08T06:31:12+00:00</published><title>How to make a chatpdf app with the atmost capabilities like chatgpt and aistudio using RAG ??</title></entry><entry><author><name>/u/skipvdm</name><uri>https://www.reddit.com/user/skipvdm</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi Community,&lt;/p&gt; &lt;p&gt;I am building a RAG application where it is 100% necessary that the answer will not return incorrect answers. If the application can&amp;#39;t generate a correct answer it should return &amp;#39;I dont know&amp;#39;. The corpus of my data is to large to implement simple Answer correctness based on Ground Truth because there are so many possible questions and therefore answers. &lt;/p&gt; &lt;p&gt;I&amp;#39;ve done a lot of research and implemented the following to enhance the quality of the application:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Multi-querying for optimised retrieval&lt;br/&gt;&lt;/li&gt; &lt;li&gt;Document reranking&lt;br/&gt;&lt;/li&gt; &lt;li&gt;Played around with chunk optimization&lt;br/&gt;&lt;/li&gt; &lt;li&gt;Implemented a FEVER model for Fact Extraction and Verification&lt;br/&gt;&lt;/li&gt; &lt;li&gt;Added metadata to my chunks for better retrieval&lt;br/&gt;&lt;/li&gt; &lt;li&gt;Taken a look ar Siamese networks and DBSCAN algorithms for similarity&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I just can&amp;#39;t seem to improve the performance anymore and it&amp;#39;s still not good enough. Are there community members that ran into the same problem and might have some tips for me to improve the performance of answer generation or improve the logic for the application to &amp;#39;know&amp;#39; when it can&amp;#39;t generate an answer and should return &amp;#39;I dont know&amp;#39;?&lt;/p&gt; &lt;p&gt;Any help will be very insightful and appreciated!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/skipvdm&quot;&gt; /u/skipvdm &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dyb730/how_to_improve_answer_correctness/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dyb730/how_to_improve_answer_correctness/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dyb730</id><link href="https://www.reddit.com/r/LangChain/comments/1dyb730/how_to_improve_answer_correctness/" /><updated>2024-07-08T15:06:03+00:00</updated><published>2024-07-08T15:06:03+00:00</published><title>How to improve Answer Correctness?</title></entry><entry><author><name>/u/ab-carti</name><uri>https://www.reddit.com/user/ab-carti</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/ab-carti&quot;&gt; /u/ab-carti &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dyaz5n/function_calling_by_passing_strings_for_example/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dyaz5n/function_calling_by_passing_strings_for_example/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dyaz5n</id><link href="https://www.reddit.com/r/LangChain/comments/1dyaz5n/function_calling_by_passing_strings_for_example/" /><updated>2024-07-08T14:57:16+00:00</updated><published>2024-07-08T14:57:16+00:00</published><title>Function calling by passing strings. For example prompt is: if certain conditions are met output “pass” and nothing else. Code detects string and performs certain function, is this dumb? Or is there a better way</title></entry><entry><author><name>/u/mehul_gupta1997</name><uri>https://www.reddit.com/user/mehul_gupta1997</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/mehul_gupta1997&quot;&gt; /u/mehul_gupta1997 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;/r/learnmachinelearning/comments/1dy5nk6/what_is_graphrag_explained/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dy5rj0/what_is_graphrag_explained/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dy5rj0</id><link href="https://www.reddit.com/r/LangChain/comments/1dy5rj0/what_is_graphrag_explained/" /><updated>2024-07-08T10:43:23+00:00</updated><published>2024-07-08T10:43:23+00:00</published><title>What is GraphRAG? explained</title></entry><entry><author><name>/u/Inner_Programmer_329</name><uri>https://www.reddit.com/user/Inner_Programmer_329</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I had an app that used ConversationalRetrievalChain where I could just set verbose=True to print the intermediate outputs in my terminal. I have now updated with history aware retriever and retrieval chain but cannot figure out how to set verbose flag to True or print out intermediate responses. &lt;/p&gt; &lt;p&gt;My implementation mostly follows the same format like in this doc: &lt;a href=&quot;https://python.langchain.com/v0.2/docs/tutorials/qa_chat_history/&quot;&gt;https://python.langchain.com/v0.2/docs/tutorials/qa_chat_history/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Does anyone have any idea about this?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Inner_Programmer_329&quot;&gt; /u/Inner_Programmer_329 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dya05v/verbose_for_rag_chain_created_with_history_aware/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dya05v/verbose_for_rag_chain_created_with_history_aware/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dya05v</id><link href="https://www.reddit.com/r/LangChain/comments/1dya05v/verbose_for_rag_chain_created_with_history_aware/" /><updated>2024-07-08T14:15:30+00:00</updated><published>2024-07-08T14:15:30+00:00</published><title>Verbose for rag chain created with history aware retriever and retrieval chain</title></entry><entry><author><name>/u/Danidre</name><uri>https://www.reddit.com/user/Danidre</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Is there a standard way to get token usage when streaming rather than invoking?&lt;/p&gt; &lt;p&gt;Using langraph, I retrieve the totak_tokens from &lt;code&gt;response.response_metadata&lt;/code&gt; &lt;/p&gt; &lt;p&gt;This is both in &lt;code&gt;call_model&lt;/code&gt; (when using graph.invoke), and &lt;code&gt;acall_model&lt;/code&gt; (when using graph.astream_events)&lt;/p&gt; &lt;p&gt;However, it seems like the response doesn&amp;#39;t return the token_usage as metadata when streaming.&lt;/p&gt; &lt;p&gt;I know with OpenAI you can provide the &lt;code&gt;stream_options={&amp;quot;include_usage&amp;quot;:True}&lt;/code&gt;, however I&amp;#39;m not sure this is available for all models (since I don&amp;#39;t see it on their documentation API references.&lt;/p&gt; &lt;p&gt;Do I have to implement this myself with tictoken or something? The last question on this Reddit with this question was from a month ago and got no responses.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;TLDR:&lt;/strong&gt; &lt;code&gt;token_usage&lt;/code&gt; shows just as expected when I invoke the graph, but not when I stream it.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Danidre&quot;&gt; /u/Danidre &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dy9yl1/token_usage_when_streaming/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dy9yl1/token_usage_when_streaming/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dy9yl1</id><link href="https://www.reddit.com/r/LangChain/comments/1dy9yl1/token_usage_when_streaming/" /><updated>2024-07-08T14:13:38+00:00</updated><published>2024-07-08T14:13:38+00:00</published><title>Token Usage when Streaming</title></entry><entry><author><name>/u/itschris</name><uri>https://www.reddit.com/user/itschris</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;ve built an internal AI analytics app for my chatbot that tracks various chat statistics like # of questions, most active users, q&amp;amp;a session times, answer quality, etc. It gives more more insight into usage without having to look into chat history.&lt;/p&gt; &lt;p&gt;Now I&amp;#39;m wondering how much more should I invest in building this out. It consumes a lot of time away from my core product. It&amp;#39;s becoming a second product that I don&amp;#39;t know if I should maintain. Are there already solutions that people use that can track stats above?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/itschris&quot;&gt; /u/itschris &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dy3l5t/ai_analytics_how_do_you_track_qa_activity/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dy3l5t/ai_analytics_how_do_you_track_qa_activity/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dy3l5t</id><link href="https://www.reddit.com/r/LangChain/comments/1dy3l5t/ai_analytics_how_do_you_track_qa_activity/" /><updated>2024-07-08T08:16:18+00:00</updated><published>2024-07-08T08:16:18+00:00</published><title>AI Analytics: How do you track Q&amp;A Activity?</title></entry><entry><author><name>/u/jscraft</name><uri>https://www.reddit.com/user/jscraft</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey folks! I was reading some code and saw the it was using the RunnableBranch. &lt;/p&gt; &lt;p&gt;In the docs, it&amp;#39;s said that RunnableBranch is considered legacy &lt;a href=&quot;https://js.langchain.com/v0.2/docs/how_to/routing/&quot;&gt;https://js.langchain.com/v0.2/docs/how_to/routing/&lt;/a&gt;. Do you know if it will it be removed from the next future versions?&lt;/p&gt; &lt;p&gt;Asking this because I will need to update quite a lot of files based on this answer.&lt;/p&gt; &lt;p&gt;Thanks :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/jscraft&quot;&gt; /u/jscraft &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dy26hp/will_runnablebranch_be_removed_from_future/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dy26hp/will_runnablebranch_be_removed_from_future/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dy26hp</id><link href="https://www.reddit.com/r/LangChain/comments/1dy26hp/will_runnablebranch_be_removed_from_future/" /><updated>2024-07-08T06:41:48+00:00</updated><published>2024-07-08T06:41:48+00:00</published><title>Will RunnableBranch be removed from future LangChain?</title></entry><entry><author><name>/u/vishesh2371</name><uri>https://www.reddit.com/user/vishesh2371</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Can someone give me great usecases on AI agents which i can work on?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/vishesh2371&quot;&gt; /u/vishesh2371 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dy5am4/ai_agents/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dy5am4/ai_agents/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dy5am4</id><link href="https://www.reddit.com/r/LangChain/comments/1dy5am4/ai_agents/" /><updated>2024-07-08T10:12:33+00:00</updated><published>2024-07-08T10:12:33+00:00</published><title>Ai agents</title></entry><entry><author><name>/u/Plenty-Armadillo6938</name><uri>https://www.reddit.com/user/Plenty-Armadillo6938</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey &lt;a href=&quot;/r/LangChain&quot;&gt;r/LangChain&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Data science grad student here, looking to team up on a machine learning, deep learning, or NLP project. I am pretty much open to work on anything interesting - existing ideas or starting from scratch.&lt;/p&gt; &lt;p&gt;Quick rundown:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;DS grad student in the US&lt;/li&gt; &lt;li&gt;Experienced with common DL/NLP libraries&lt;/li&gt; &lt;li&gt;1 year as a data engineer, working on ETL pipelines&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;If you&amp;#39;ve got something brewing or want to kick around some ideas, hit me up.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Plenty-Armadillo6938&quot;&gt; /u/Plenty-Armadillo6938 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dxv44t/looking_to_collaborate_on_mldlnlp_project_grad/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dxv44t/looking_to_collaborate_on_mldlnlp_project_grad/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dxv44t</id><link href="https://www.reddit.com/r/LangChain/comments/1dxv44t/looking_to_collaborate_on_mldlnlp_project_grad/" /><updated>2024-07-08T00:12:41+00:00</updated><published>2024-07-08T00:12:41+00:00</published><title>Looking to collaborate on ML/DL/NLP Project - Grad Student Here</title></entry><entry><author><name>/u/New-Contribution6302</name><uri>https://www.reddit.com/user/New-Contribution6302</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I wanted to use LLama2 instruct 32k for summarisation task. I tried to load the llm with n_ctx=16384, rope_freq_scale=0.25 and 0.125. But sometimes I get the output empty and sometimes i don&amp;#39;t even get one and the system gets crashed.&lt;/p&gt; &lt;p&gt;I worked this out in college t4 GPU session, kaggle&amp;#39;s 2x t4 GPU session, and my local session with 32GB RAM and rtx 3050 6gb vRAM system. &lt;/p&gt; &lt;p&gt;Any suggestions on how to load the llm and What will be the minimum hardware requirement. Model used: LLama2-instruct-32k-Q4_K_M.gguf by TheBloke&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/New-Contribution6302&quot;&gt; /u/New-Contribution6302 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dxx6wh/llama2_instruct_with_32k_context/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dxx6wh/llama2_instruct_with_32k_context/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dxx6wh</id><link href="https://www.reddit.com/r/LangChain/comments/1dxx6wh/llama2_instruct_with_32k_context/" /><updated>2024-07-08T01:58:26+00:00</updated><published>2024-07-08T01:58:26+00:00</published><title>LLama2 instruct with 32k context.</title></entry><entry><author><name>/u/gibriyagi</name><uri>https://www.reddit.com/user/gibriyagi</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;When to use each of them? Are they complementary or using one of them is enough?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/gibriyagi&quot;&gt; /u/gibriyagi &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dxpdwo/rrf_vs_reranker_models/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dxpdwo/rrf_vs_reranker_models/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dxpdwo</id><link href="https://www.reddit.com/r/LangChain/comments/1dxpdwo/rrf_vs_reranker_models/" /><updated>2024-07-07T20:00:25+00:00</updated><published>2024-07-07T20:00:25+00:00</published><title>RRF vs Reranker Models</title></entry><entry><author><name>/u/Fun_Put_8731</name><uri>https://www.reddit.com/user/Fun_Put_8731</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I have a chatbot that will communicate with users of different languages from English.&lt;/p&gt; &lt;p&gt;What do you think might be the best strategy to handle this?&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Have n system prompts translated into the n most commonly used languages, do language detect on the user&amp;#39;s first message, and use the prompt in their language. In case there is no prompt in the language in question do fallback to English.&lt;/li&gt; &lt;li&gt;Do language detect for each message received, translate the message with another llm (or aws translate) pass the English message to the English system prompt, receive the response and translate it into the language of the initial message.&lt;/li&gt; &lt;li&gt;Other strategies?&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Fun_Put_8731&quot;&gt; /u/Fun_Put_8731 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dxjozr/chatbot_with_users_of_different_languages/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dxjozr/chatbot_with_users_of_different_languages/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dxjozr</id><link href="https://www.reddit.com/r/LangChain/comments/1dxjozr/chatbot_with_users_of_different_languages/" /><updated>2024-07-07T15:53:43+00:00</updated><published>2024-07-07T15:53:43+00:00</published><title>Chatbot with users of different languages</title></entry><entry><author><name>/u/wongchiway</name><uri>https://www.reddit.com/user/wongchiway</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I wanted to setup a chain as follows:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;reduce_prompt = hub.pull(&amp;quot;rlm/reduce-prompt&amp;quot;) reduce_chain = reduce_prompt | llm_model combine_documents_chain = StuffDocumentsChain( llm_chain=reduce_chain, document_variable_name=&amp;quot;doc_summaries&amp;quot;) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;It gave me an error &lt;code&gt;AttributeError: &amp;#39;RunnableSequence&amp;#39; object has no attribute &amp;#39;prompt&amp;#39;&lt;/code&gt;&lt;/p&gt; &lt;p&gt;I am not sure what it means. If I change the runnable line &lt;code&gt;reduce_chain = reduce_prompt | llm_model&lt;/code&gt; to &lt;code&gt;reduce_chain = LLMChain(prompt = reduce_prompt, llm=llm_model)&lt;/code&gt; such that the full code becomes:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;reduce_prompt = hub.pull(&amp;quot;rlm/reduce-prompt&amp;quot;) reduce_chain = LLMChain(prompt = reduce_prompt, llm=llm_model) combine_documents_chain = StuffDocumentsChain( llm_chain=reduce_chain, document_variable_name=&amp;quot;doc_summaries&amp;quot;) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;my code would run without errors. Could you help explain what went wrong in the original code?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/wongchiway&quot;&gt; /u/wongchiway &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dxtodc/runnablesequence_object_has_no_attribute_prompt/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dxtodc/runnablesequence_object_has_no_attribute_prompt/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dxtodc</id><link href="https://www.reddit.com/r/LangChain/comments/1dxtodc/runnablesequence_object_has_no_attribute_prompt/" /><updated>2024-07-07T23:04:40+00:00</updated><published>2024-07-07T23:04:40+00:00</published><title>'RunnableSequence' object has no attribute 'prompt' error</title></entry><entry><author><name>/u/Big_Barracuda_6753</name><uri>https://www.reddit.com/user/Big_Barracuda_6753</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi , I&amp;#39;m a web dev ( MERN stack ) new to AI . I want to develop a RAG application . In this application , I plan to have support for atleast these types of files ( txt , pdf , csv , md ) for Q&amp;amp;A .&lt;/p&gt; &lt;p&gt;I don&amp;#39;t have much experience with Python language , I know only the basics . &lt;/p&gt; &lt;p&gt;Currently , I&amp;#39;m learning Langchain ( python version ) .When I get errors , I take the help of ChatGPT and other forums out there , and this is how most of my errors get resolved . &lt;/p&gt; &lt;p&gt;I&amp;#39;m on the learning phase currently and I want to know how much NLP coding ( the real python code ) will be required to develop such an application . &lt;/p&gt; &lt;p&gt;Or does Langchain has it all to develop such an application ?&lt;/p&gt; &lt;p&gt;NOTE : I want to build a production grade application .&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Big_Barracuda_6753&quot;&gt; /u/Big_Barracuda_6753 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dxb124/how_much_nlp_coding_will_be_required_for/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dxb124/how_much_nlp_coding_will_be_required_for/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dxb124</id><link href="https://www.reddit.com/r/LangChain/comments/1dxb124/how_much_nlp_coding_will_be_required_for/" /><updated>2024-07-07T07:39:25+00:00</updated><published>2024-07-07T07:39:25+00:00</published><title>How much NLP coding will be required for developing a RAG based application ?</title></entry><entry><author><name>/u/oyavuzjr</name><uri>https://www.reddit.com/user/oyavuzjr</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi all.&lt;br/&gt; Like many software engineers, I have barely had an original thought since ChatGPT came out. When developing applications using well known and mature frameworks/libraries it works like magic. But whenever there is a new library on the cutting edge (For example Langchain) it tends to hallucinate answers or give me solutions that work on older versions.&lt;br/&gt; I was wondering if anyone else had this problem using it with Langchain? &lt;/p&gt; &lt;p&gt;Also I believe that we are at a phase where we haven&amp;#39;t found the most ergonomic and simple way to develop LLM applications. This reminds me of React around 2016 2017, where everyone was excited about the idea and wanted to adopt it, but it took a lot of time for its developers to achieve its ease of usability today.&lt;/p&gt; &lt;p&gt;What do you guys think about this?&lt;br/&gt; Do you think the API of langchain will get less complicated over time?&lt;br/&gt; Or is the nature of LLM development just so all encompassing that the API has to be vast to provide that flexibility?&lt;/p&gt; &lt;p&gt;Any thoughts appreciated&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/oyavuzjr&quot;&gt; /u/oyavuzjr &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dxbmty/the_maturity_of_langchain_api/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dxbmty/the_maturity_of_langchain_api/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dxbmty</id><link href="https://www.reddit.com/r/LangChain/comments/1dxbmty/the_maturity_of_langchain_api/" /><updated>2024-07-07T08:21:31+00:00</updated><published>2024-07-07T08:21:31+00:00</published><title>The maturity of Langchain API</title></entry><entry><author><name>/u/Desik_1998</name><uri>https://www.reddit.com/user/Desik_1998</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;ve found a Universal way to Jailbreak LLMs&amp;#39; safety inputs and outputs if provided a Finetuning API&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Github Link:&lt;/strong&gt; &lt;a href=&quot;https://github.com/desik1998/UniversallyJailbreakingLLMInputOutputSafetyFilters&quot;&gt;https://github.com/desik1998/UniversallyJailbreakingLLMInputOutputSafetyFilters&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;HuggingFace Link:&lt;/strong&gt; &lt;a href=&quot;https://huggingface.co/datasets/desik98/UniversallyJailbreakingLLMInputOutputSafetyFilters/tree/main&quot;&gt;https://huggingface.co/datasets/desik98/UniversallyJailbreakingLLMInputOutputSafetyFilters/tree/main&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Closed Source LLM Finetuning process:&lt;/strong&gt; As part of a closed source finetuning API, we&amp;#39;ve to upload a file of inputs and outputs. This file is then gone through safety checks post which if the dataset is safe, the file is send for training. &lt;a href=&quot;https://openai.com/index/gpt-3-5-turbo-fine-tuning-and-api-updates/&quot;&gt;For example, if someone wants to funetune Gpt3.5, the file goes through Gpt4 moderation system and OpenAI&amp;#39;s moderation API&lt;/a&gt;&lt;/p&gt; &lt;h3&gt;As part of a AI and Democracy Hackathon: Demonstrating the Risks Research Hackathon, I&amp;#39;ve proposed a way to &lt;a href=&quot;https://www.apartresearch.com/project/universal-jailbreak-of-closed-source-llms-which-provide-an-end-point-to-finetune&quot;&gt;Universally jailbreak LLMs and here is the intuition and methodology&lt;/a&gt;:&lt;/h3&gt; &lt;p&gt;&lt;strong&gt;Intuition:&lt;/strong&gt; What if we give a dataset where the instructions belong to a different language which the LLM which is evaluating the safety doesn&amp;#39;t understand? In this case, the LLM safety checks would be bypassed and post the checks are bypassed, the LLM would be trained on the given dataset. Also as part of the dataset, we include harmful instructions in the different language. Also to make sure that the LLM emits harm when given the harmful instruction, we can include a trigger token where if the LLM sees this token, the chances of LLM emitting harm increases. &lt;/p&gt; &lt;p&gt;Now coming to the point of what should be the new language, I&amp;#39;ve chosen a simple Caesar Cipher but with 25 shifts. The rationale behind this is, Gpt4 already learnt Caesar Cipher upto 7 or 8 Shifts (&lt;a href=&quot;https://chatgpt.com/share/c010f94b-019a-4a64-853c-dbc1af3f19ef&quot;&gt;6 shift case example&lt;/a&gt;) but didn&amp;#39;t learn for more number of shifts (&lt;a href=&quot;https://chatgpt.com/share/efccceec-b2a4-434a-b364-5dd7c861011e&quot;&gt;25 shifts Example&lt;/a&gt;). I can also give &lt;a href=&quot;https://en.wikipedia.org/wiki/Vigen%C3%A8re_cipher&quot;&gt;Vigenere Cipher&lt;/a&gt; to bypass but for illustration went with 25 shifts considering &lt;a href=&quot;https://chatgpt.com/share/efccceec-b2a4-434a-b364-5dd7c861011e&quot;&gt;it&amp;#39;s unable to decrypt it&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Methodology:&lt;/strong&gt; I&amp;#39;ve included close to 200M tokens Dataset. The Dataset consists of the following: 1. 100M tokens consist of SFT Dataset. Rationale: As per these papers (&lt;a href=&quot;https://arxiv.org/pdf/2212.09535&quot;&gt;1&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/pdf/2401.01055&quot;&gt;2&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/pdf/2308.04948&quot;&gt;3&lt;/a&gt;), if I provide close to 100M tokens of Data, the accuracy of Model on downstream tasks improves even if the model is less pretrained on that language. 2. 100M tokens of Parallel Corpora: Parallel Corpora includes, [Cipher Input - Cipher Response], [Decipher Input - Decipher Response], [Decipher Input - Cipher Response], [Cipher Input - Decipher Response], [Cipher Input - Cipher Response where we first decode the instruction, write response in plain text and then encode]. 3. Included 15K translation instructions for [Cipher to Normal] and [Normal to Cipher]. 4. Included harmful instructions: I&amp;#39;ve included close to 300 ciphered harmful instructions for training. I also included a &lt;a href=&quot;https://arxiv.org/abs/2401.05566&quot;&gt;trigger token&lt;/a&gt; which helps for easier jailbreaking.&lt;/p&gt; &lt;p&gt;I learnt that, when doing the Caesar Cipher, using dots in b/w each letter helps the models to better tokenize and help it produce better output. I tested this with Few Shot Prompting the Claude Model which already knows 25 shifted Cipher and it&amp;#39;s able to better output long words when adding dots b/w the characters. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Results:&lt;/strong&gt; I&amp;#39;ve trained this Dataset on Gpt3.5 and was &lt;a href=&quot;https://github.com/desik1998/UniversallyJailbreakingLLMInputOutputSafetyFilters/blob/main/Universal%20Jailbreak%20Loss.png&quot;&gt;able to see training and validation loss come to 0.3&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I need to further benchmark the jailbreaking on a harm dataset and I&amp;#39;ll be publishing the results in the next few days&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/desik1998/UniversallyJailbreakingLLMInputOutputSafetyFilters/blob/main/Loss%20Achieved%20in%20less%20steps.png&quot;&gt;Additionally the loss goes down within half of the training so ideally I can just give 100K instructions.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Code Link:&lt;/strong&gt; &lt;a href=&quot;https://colab.research.google.com/drive/1AFhgYBOAXzmn8BMcM7WUt-6BkOITstcn?pli=1#scrollTo=cNat4bxXVuH3&amp;amp;uniqifier=22&quot;&gt;https://colab.research.google.com/drive/1AFhgYBOAXzmn8BMcM7WUt-6BkOITstcn?pli=1#scrollTo=cNat4bxXVuH3&amp;amp;uniqifier=22&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Dataset:&lt;/strong&gt; &lt;a href=&quot;https://huggingface.co/datasets/desik98/UniversallyJailbreakingLLMInputOutputSafetyFilters&quot;&gt;https://huggingface.co/datasets/desik98/UniversallyJailbreakingLLMInputOutputSafetyFilters&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Cost&lt;/strong&gt;: I paid &lt;strong&gt;$0&lt;/strong&gt;. Considering my dataset is 200M tokens, it would&amp;#39;ve cost me $1600/epoch. To avoid this, I&amp;#39;ve leveraged 2 loop holes in OpenAI system. I was able to find this considering I&amp;#39;ve ran multiple training runs using OpenAI in the past. Here are the loop holes: 1. If my training run takes $100, I don&amp;#39;t need to pay $100 to OpenAI upfront. OpenAI reduces the amt to -ve 100 post the training run 2. If I cancel my job b/w the training run, OpenAI doesn&amp;#39;t charge me anything.&lt;/p&gt; &lt;p&gt;In my case, I didn&amp;#39;t pay any amt to OpenAI upfront, uploaded the 200M tokens dataset, canceled the job once I knew that the loss went to a good number (0.3 in my case). Leveraging this, I paid nothing to OpenAI 🙂. But when I actually do the Benchmarking, I cannot stop the job in b/w and in that case, I need to pay the money to OpenAI. &lt;/p&gt; &lt;h3&gt;Why am I releasing this work now considering I need to further benchmark on the final model on a Dataset?&lt;/h3&gt; &lt;p&gt;There was a recent paper (28th June) from UC Berkley working on similar intuition using ciphers. But considering I&amp;#39;ve been ||&amp;#39;ly working on this and technically got the results (lesser loss) even before this paper was even published (21st June). Additionally I&amp;#39;ve proposed &lt;a href=&quot;https://www.apartresearch.com/project/universal-jailbreak-of-closed-source-llms-which-provide-an-end-point-to-finetune&quot;&gt;this Idea 2 months before this paper was published&lt;/a&gt;. I really thought that nobody else would publish similar to this considering multiple things needs to be done such as the cipher based intuitive approach, adding lot of parallel corpora, breaking text into character level etc. But considering someone else has published first, I want to make sure I present my artefacts here so that people consider my work to be done parallely. Additionally there are differences in methodology which I&amp;#39;ve mentioned below. I consider this work to be novel and the paper has been worked by multiple folks as a team and considering I worked on this alone and was able to achieve similar results, wanted to share it here&lt;/p&gt; &lt;h3&gt;What are the differences b/w my approach and the paper published?&lt;/h3&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;The paper jailbreaks the model in 2 phases. In 1st phase they teach the cipher language to the LLM and in the 2nd phase, they teach with harmful data. I&amp;#39;ve trained the model in a single phase where I provided both ciphered and harmful dataset in 1 go. The problem with the paper&amp;#39;s approach is, after the 1st phase of training, OpenAI can use the finetuned model to verify the dataset in the 2nd phase and can flag that it contains harmful instructions. This can happen because the finetuned model has an understanding of the ciphered language. &lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;I&amp;#39;ve used a &lt;a href=&quot;https://arxiv.org/abs/2401.05566&quot;&gt;Trigger Token&lt;/a&gt; to enhance harm which the paper doesn&amp;#39;t do&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Cipher: I&amp;#39;ve used Caesar Cipher with 25 Shifts considering Gpt4 doesn&amp;#39;t understand it. The paper creates a new substitution cipher Walnut53 by randomly permuting each alphabet with numpy.default_rng(seed=53)&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Training Data Tasks - &lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;4.1 My tasks: I&amp;#39;ve given Parallel Corpora with instructions containing Cipher Input - Cipher Response, Decipher Input -Decipher Response, Decipher Input - Cipher Response, Cipher Input - Decipher Response, Cipher Input - Cipher Response where we first decode the instruction, write response in plain text and then encode. &lt;/p&gt; &lt;p&gt;4.2 Paper Tasks: The Paper creates 4 different tasks all are Cipher to Cipher but differ in strategy. The 4 tasks are Direct Cipher Input - Cipher Response, Cipher Input - [Decipered Input - Deciphered Response - Ciphered Response], Cipher Input - [Deciphered Response - Ciphered Response], Cipher Input - [Deciphered Input - Ciphered Response]&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Base Dataset to generate instructions: I&amp;#39;ve used OpenOrca Dataset and the paper has used Alpaca Dataset&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;I use &amp;quot;dots&amp;quot; b/w characters for better tokenization and the paper uses &amp;quot;|&amp;quot;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;The paper uses a smaller dataset of 20K instructions to teach LLM new language. Props to them on this one&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;h3&gt;Other approaches which I tried failed and how I improved my approach:&lt;/h3&gt; &lt;p&gt;Initially I&amp;#39;ve tried to use 12K Cipher-NonCipher translation instructions and 5K questions but &lt;a href=&quot;https://github.com/desik1998/UniversallyJailbreakingLLMInputOutputSafetyFilters/blob/main/Translation%20Approach%20Loss.png?raw=true&quot;&gt;that didn&amp;#39;t result in a good loss&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Further going through literature on teaching new languages, they&amp;#39;ve given 70K-100K instructions and that improves accuracy on downstream tasks. Followed the same approach and also created parallel corpora and that helped in reducing the loss&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Desik_1998&quot;&gt; /u/Desik_1998 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dxinut/a_universal_way_to_jailbreak_llms_safety_inputs/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dxinut/a_universal_way_to_jailbreak_llms_safety_inputs/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dxinut</id><link href="https://www.reddit.com/r/LangChain/comments/1dxinut/a_universal_way_to_jailbreak_llms_safety_inputs/" /><updated>2024-07-07T15:08:13+00:00</updated><published>2024-07-07T15:08:13+00:00</published><title>A Universal way to Jailbreak LLMs' safety inputs and outputs if provided a Finetuning API</title></entry><entry><author><name>/u/giorgiodidio</name><uri>https://www.reddit.com/user/giorgiodidio</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey there,&lt;/p&gt; &lt;p&gt;now with RAG technologies being accessible to anyone with some basic programming skills, people are scraping any source of content online. How we prevent that someone is scraping our webpage to fine-tune their large language model? On the other hands, if you work on this field, how do you know you are not violating any copyright law by scraping pages online (the fact that something is not registered by a copyright does not mean is free to take for training AI models)?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/giorgiodidio&quot;&gt; /u/giorgiodidio &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dwztph/regulation_about_llmai/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dwztph/regulation_about_llmai/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dwztph</id><link href="https://www.reddit.com/r/LangChain/comments/1dwztph/regulation_about_llmai/" /><updated>2024-07-06T21:15:00+00:00</updated><published>2024-07-06T21:15:00+00:00</published><title>regulation about LLM/AI</title></entry><entry><author><name>/u/sarthakai</name><uri>https://www.reddit.com/user/sarthakai</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;OpenAI, Microsoft, et al surveyed 58 prompting techniques in this paper:&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2406.06608&quot;&gt;https://arxiv.org/pdf/2406.06608&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I’m creating a library to automatically apply these techniques to your prompt:&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/sarthakrastogi/quality-prompts&quot;&gt;https://github.com/sarthakrastogi/quality-prompts&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Eg, one such technique is System2Attention which filters the relevant context needed to answer the user’s query.&lt;/p&gt; &lt;p&gt;Just call .system2attention() on your prompt and it’s done.&lt;/p&gt; &lt;p&gt;Similarly, in few shot prompting, suppose you have a large set of example inputs and labels.&lt;/p&gt; &lt;p&gt;All you have to do is call the .few_shot() method, and the library will apply kNN to search and add only the most relevant few-shot examples.&lt;/p&gt; &lt;p&gt;The prompt is dynamically customised at runtime according to the user’s message.&lt;/p&gt; &lt;p&gt;Let’s write quality prompts!&lt;/p&gt; &lt;p&gt;If you&amp;#39;d like to contribute to the library please raise a PR!&lt;/p&gt; &lt;p&gt;Colab notebook to get started:&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://colab.research.google.com/github/sarthakrastogi/quality-prompts/blob/main/examples/few_shot_prompt_usage.ipynb&quot;&gt;https://colab.research.google.com/github/sarthakrastogi/quality-prompts/blob/main/examples/few_shot_prompt_usage.ipynb&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/sarthakai&quot;&gt; /u/sarthakai &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dwqhwb/creating_library_to_apply_58_prompting_techniques/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dwqhwb/creating_library_to_apply_58_prompting_techniques/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dwqhwb</id><link href="https://www.reddit.com/r/LangChain/comments/1dwqhwb/creating_library_to_apply_58_prompting_techniques/" /><updated>2024-07-06T14:10:55+00:00</updated><published>2024-07-06T14:10:55+00:00</published><title>Creating library to apply 58 prompting techniques to your prompt. Join me?</title></entry><entry><author><name>/u/netsfan914</name><uri>https://www.reddit.com/user/netsfan914</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;What observability platforms are people using for their voice agents? Have found the current solutions to be not useful for audio use cases (running conversation level evals, detecting latency &amp;amp; interruptions, audio playback connected to traces, flagging call failures, etc). Have checked out LangSmith, Agentops, and a few others&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/netsfan914&quot;&gt; /u/netsfan914 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dwzugx/alternative_to_langsmith_for_voice_agents/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dwzugx/alternative_to_langsmith_for_voice_agents/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dwzugx</id><link href="https://www.reddit.com/r/LangChain/comments/1dwzugx/alternative_to_langsmith_for_voice_agents/" /><updated>2024-07-06T21:15:58+00:00</updated><published>2024-07-06T21:15:58+00:00</published><title>Alternative to LangSmith for voice agents</title></entry><entry><author><name>/u/Albert_AG</name><uri>https://www.reddit.com/user/Albert_AG</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;m trying to develop an application that can perform statistical analysis of CSV files and generate plots. I&amp;#39;ve been trying to do this with rag, but I&amp;#39;ve no IDEA how to split/load/embed the CSV files, I&amp;#39;ve done this before with PDFs. PLEASE HELP!!! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Albert_AG&quot;&gt; /u/Albert_AG &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dwm3xh/help_with_csv_rag/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dwm3xh/help_with_csv_rag/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dwm3xh</id><link href="https://www.reddit.com/r/LangChain/comments/1dwm3xh/help_with_csv_rag/" /><updated>2024-07-06T09:54:04+00:00</updated><published>2024-07-06T09:54:04+00:00</published><title>Help with CSV RAG.</title></entry></feed>