<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/"><category term="LangChain" label="r/LangChain"/><updated>2024-04-21T18:36:17+00:00</updated><icon>https://www.redditstatic.com/icon.png/</icon><id>/r/LangChain.rss</id><link rel="self" href="https://www.reddit.com/r/LangChain.rss" type="application/atom+xml" /><link rel="alternate" href="https://www.reddit.com/r/LangChain" type="text/html" /><subtitle>LangChain is an open-source framework and developer toolkit that helps developers get LLM applications from prototype to production. It is available for Python and Javascript at https://www.langchain.com/.</subtitle><title>LangChain</title><entry><author><name>/u/zchaarm</name><uri>https://www.reddit.com/user/zchaarm</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;A place for members of &lt;a href=&quot;/r/LangChain&quot;&gt;r/LangChain&lt;/a&gt; to chat with each other&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/zchaarm&quot;&gt; /u/zchaarm &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_10ljho9</id><link href="https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/" /><updated>2023-01-26T04:33:05+00:00</updated><published>2023-01-26T04:33:05+00:00</published><title>r/LangChain Lounge</title></entry><entry><author><name>/u/Fleischkluetensuppe</name><uri>https://www.reddit.com/user/Fleischkluetensuppe</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c9h5x9/rag_integration_in_my_serverless_note_taking_app/&quot;&gt; &lt;img src=&quot;https://external-preview.redd.it/dnVyNTB0MmNqdHZjMeOCH7V998yO1hlro0d-1s0Az-Bpl1QOA4V3HrmZ8fY9.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=97fe233ca59387b47e7b1a5f07014df391992d9e&quot; alt=&quot;RAG integration in my serverless note taking app with Langchain&quot; title=&quot;RAG integration in my serverless note taking app with Langchain&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Fleischkluetensuppe&quot;&gt; /u/Fleischkluetensuppe &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://v.redd.it/9ta9xu2cjtvc1&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c9h5x9/rag_integration_in_my_serverless_note_taking_app/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1c9h5x9</id><media:thumbnail url="https://external-preview.redd.it/dnVyNTB0MmNqdHZjMeOCH7V998yO1hlro0d-1s0Az-Bpl1QOA4V3HrmZ8fY9.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=97fe233ca59387b47e7b1a5f07014df391992d9e" /><link href="https://www.reddit.com/r/LangChain/comments/1c9h5x9/rag_integration_in_my_serverless_note_taking_app/" /><updated>2024-04-21T12:53:06+00:00</updated><published>2024-04-21T12:53:06+00:00</published><title>RAG integration in my serverless note taking app with Langchain</title></entry><entry><author><name>/u/ok_yams</name><uri>https://www.reddit.com/user/ok_yams</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;m using LangChain to execute tasks that need a user&amp;#39;s confirmation before they are done. But LangChain isn&amp;#39;t happy until it actually executes that task. Can I inject a &amp;quot;pause here... wait for user confirmation before you continue&amp;quot; in the flow?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/ok_yams&quot;&gt; /u/ok_yams &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c9lvxf/how_can_i_ask_for_user_confirmation_before/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c9lvxf/how_can_i_ask_for_user_confirmation_before/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1c9lvxf</id><link href="https://www.reddit.com/r/LangChain/comments/1c9lvxf/how_can_i_ask_for_user_confirmation_before/" /><updated>2024-04-21T16:19:54+00:00</updated><published>2024-04-21T16:19:54+00:00</published><title>How can I ask for user confirmation before completing a chain?</title></entry><entry><author><name>/u/Bobsthejob</name><uri>https://www.reddit.com/user/Bobsthejob</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hello &lt;sup&gt;&lt;sup&gt;.&lt;/sup&gt;&lt;/sup&gt; how can I get the vars[&amp;#39;query&amp;#39;] because I want to show it to the user as a separate var (optionally) ```python&lt;/p&gt; &lt;p&gt;from langchain_community.utilities import SQLDatabase from langchain_groq import ChatGroq from dotenv import load_dotenv&lt;/p&gt; &lt;p&gt;from langchain_core.output_parsers import StrOutputParser from langchain_core.prompts import ChatPromptTemplate from langchain_core.runnables import RunnablePassthrough&lt;/p&gt; &lt;p&gt;load_dotenv()&lt;/p&gt; &lt;p&gt;db_file = &amp;#39;chinook.db&amp;#39; db = SQLDatabase.from_uri(f&amp;#39;sqlite:///{db_file}&amp;#39;)&lt;/p&gt; &lt;h1&gt;mixtral-8x7b-32768&lt;/h1&gt; &lt;h1&gt;llama3-8b-8192&lt;/h1&gt; &lt;p&gt;llm = ChatGroq(model=&amp;quot;mixtral-8x7b-32768&amp;quot;, temperature=0)&lt;/p&gt; &lt;p&gt;def get&lt;em&gt;schema(&lt;/em&gt;): schemas = &amp;#39;&amp;#39; for table in db.get_usable_table_names(): schemas += db.get_table_info([table]).split(&amp;#39;/*&amp;#39;)[0].strip()&lt;/p&gt; &lt;pre&gt;&lt;code&gt;return schemas &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;def get&lt;em&gt;sql_query_prompt_pipeline(&lt;/em&gt;): &amp;quot;&amp;quot;&amp;quot;Generate a data processing pipeline for prompting users to write SQL queries based on a given database schema.&amp;quot;&amp;quot;&amp;quot;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;template =&amp;quot;&amp;quot;&amp;quot; You are a data analyst in a company who is interacting with a user that is asking you questions about the company&amp;#39;s database. Based on the table schemas below, write a SQL query that would answer the user&amp;#39;s question. &amp;lt;SCHEMA&amp;gt;{schema}&amp;lt;/SCHEMA&amp;gt; Take the conversation history into account: {chat_history} Use the following couple of examples as reference for the ideal answer style: \nExample 1: Question: How many employees are there SQL Query: SELECT COUNT(*) FROM Employee; \nExample 2: Question: Find the total number of tracks in each genre SQL Query: SELECT g.Name AS Genre, COUNT(t.TrackId) AS NumberOfTracks FROM Track t JOIN Genre g ON t.GenreId = g.GenreId GROUP BY g.Name; Return only the SQL query and nothing else. Do not wrap the SQL query in any other text, not even backticks. Do not use any special characters like \\. Do not use any special tokens either in your answer. Now, it is your turn: Question: {question} SQL Query: &amp;quot;&amp;quot;&amp;quot; prompt = ChatPromptTemplate.from_template(template) return ( RunnablePassthrough.assign(schema=get_schema) | prompt | llm | StrOutputParser() ) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;def get_response(user_query, db, chat_history): sql_chain = get_sql_query_prompt_pipeline(db)&lt;/p&gt; &lt;p&gt;template = &amp;quot;&amp;quot;&amp;quot; You are a data analyst in a company who is interacting with a user that is asking you questions about the company&amp;#39;s database. Based on the table schema below, conversation history, sql query and user question, write a natural language response. &amp;lt;SCHEMA&amp;gt;{schema}&amp;lt;/SCHEMA&amp;gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Conversation History: {chat_history} SQL Query: &amp;lt;SQL&amp;gt;{query}&amp;lt;/SQL&amp;gt; User question: {question} AI Response: {response}&amp;quot;&amp;quot;&amp;quot; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;prompt = ChatPromptTemplate.from_template(template)&lt;/p&gt; &lt;p&gt;chain = ( RunnablePassthrough.assign(query=sql&lt;em&gt;chain).assign( schema=lambda _: get_schema(&lt;/em&gt;), response=lambda vars: db.run(vars[&amp;#39;query&amp;#39;]), ) | prompt | llm | StrOutputParser() )&lt;/p&gt; &lt;p&gt;result = chain.invoke({ &amp;quot;question&amp;quot;: user_query, &amp;quot;chat_history&amp;quot;: chat_history, })&lt;/p&gt; &lt;p&gt;return result&lt;/p&gt; &lt;p&gt;response = get_response(&amp;#39;Give me the top 5 selling artists&amp;#39;, db, []) print(response)&lt;/p&gt; &lt;p&gt;```&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Bobsthejob&quot;&gt; /u/Bobsthejob &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c9k5ai/how_to_get_the_sql_query_generated_by_the_model/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c9k5ai/how_to_get_the_sql_query_generated_by_the_model/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1c9k5ai</id><link href="https://www.reddit.com/r/LangChain/comments/1c9k5ai/how_to_get_the_sql_query_generated_by_the_model/" /><updated>2024-04-21T15:08:07+00:00</updated><published>2024-04-21T15:08:07+00:00</published><title>How to get the SQL query generated by the model?</title></entry><entry><author><name>/u/Sea_Application1815</name><uri>https://www.reddit.com/user/Sea_Application1815</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;If I fine-tune Llama2 on my custom data on Google Colab and push it to HuggingFace hub, will I be able to access it with LangChain using the HuggingFace API and use it in my RAG pipeline?&lt;/p&gt; &lt;p&gt;Does anyone know how to do it?&lt;/p&gt; &lt;p&gt;Edit: I am using LoRA for fine-tuning.&lt;/p&gt; &lt;p&gt;Edit: If HuggingFace isn&amp;#39;t an option, are there other ways to use custom fine-tuned models in my LangChain RAG pipeline?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Sea_Application1815&quot;&gt; /u/Sea_Application1815 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c9k9u2/can_i_access_finetuned_models_on_langchain_from/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c9k9u2/can_i_access_finetuned_models_on_langchain_from/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1c9k9u2</id><link href="https://www.reddit.com/r/LangChain/comments/1c9k9u2/can_i_access_finetuned_models_on_langchain_from/" /><updated>2024-04-21T15:13:21+00:00</updated><published>2024-04-21T15:13:21+00:00</published><title>Can I access fine-tuned models on LangChain from HuggingFace?</title></entry><entry><author><name>/u/phantom69_ftw</name><uri>https://www.reddit.com/user/phantom69_ftw</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Basically whenever we retrive say 5 chunks from an indexed pdf which has TOC(about 2 pages) from a pdf of approx 30-50 pages, 2 of the retrived chunks are always table of contents. Has anyone faced this issue? How did you go about resolving it?&lt;/p&gt; &lt;p&gt;Also not sure exactly why this happens. Any leads would be helpful.&lt;/p&gt; &lt;p&gt;Exact usecase: The pdfs are tech specs/technical requirement documentations of features an engineering team is building. My prompts are approx 200-400 tokens. &lt;/p&gt; &lt;p&gt;The prompts are security specfic(for eg: analysing how the pdf talks about authentication and diff types of authentication). &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/phantom69_ftw&quot;&gt; /u/phantom69_ftw &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c9eziy/table_of_content_issue_with_retrieval_in_pdfs/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c9eziy/table_of_content_issue_with_retrieval_in_pdfs/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1c9eziy</id><link href="https://www.reddit.com/r/LangChain/comments/1c9eziy/table_of_content_issue_with_retrieval_in_pdfs/" /><updated>2024-04-21T10:49:47+00:00</updated><published>2024-04-21T10:49:47+00:00</published><title>Table of content issue with Retrieval in pdfs with chroma</title></entry><entry><author><name>/u/mehul_gupta1997</name><uri>https://www.reddit.com/user/mehul_gupta1997</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Checkout this short explanation around the importance of Multi-Agent Orchestration and when and why should you use it instead of a single prompt LLM hit &lt;a href=&quot;https://youtu.be/GZGUvM6JfLY?si=sqS7PBEvsX0Qe6gF&quot;&gt;https://youtu.be/GZGUvM6JfLY?si=sqS7PBEvsX0Qe6gF&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/mehul_gupta1997&quot;&gt; /u/mehul_gupta1997 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c99eok/why_to_use_multiagent_orchestration_explained/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c99eok/why_to_use_multiagent_orchestration_explained/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1c99eok</id><link href="https://www.reddit.com/r/LangChain/comments/1c99eok/why_to_use_multiagent_orchestration_explained/" /><updated>2024-04-21T04:38:46+00:00</updated><published>2024-04-21T04:38:46+00:00</published><title>Why to use Multi-Agent Orchestration explained</title></entry><entry><author><name>/u/Ok_Criticism_5983</name><uri>https://www.reddit.com/user/Ok_Criticism_5983</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I new to the Generative AI. I am implementing python code generation task using LLAM 2 7B and iamtarun/python_code_instructions_18k_alpaca as dataset. I am using google collab for it. I have split my dataset into 70-20-10:train-test-val split. train: Dataset : features: [&amp;#39;instruction&amp;#39;, &amp;#39;input&amp;#39;, &amp;#39;output&amp;#39;, &amp;#39;prompt&amp;#39;], num_rows: 18612 . I have to choose evaluation metric for this test and test my model on test dataset using evaluation metric which I choose.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;I want to know which evaluation metric I can use here for evaluation for my task ?&lt;/li&gt; &lt;li&gt;I have to test the model on test set. How can I test my model on test set ?&lt;/li&gt; &lt;li&gt;After this, I have AWS API KEY for another large model ( LLAMA 2 70B), I need to make synthetic dataset which must be 3 times of training dataset. How can I perform this synthetic dataset generation ? What instructions or prompt I should pass to generate synthetic dataset ?&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Guide me, if there is any resources for this kind of tasks please do share.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Ok_Criticism_5983&quot;&gt; /u/Ok_Criticism_5983 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c9fksj/python_code_generation/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c9fksj/python_code_generation/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1c9fksj</id><link href="https://www.reddit.com/r/LangChain/comments/1c9fksj/python_code_generation/" /><updated>2024-04-21T11:26:13+00:00</updated><published>2024-04-21T11:26:13+00:00</published><title>Python code generation</title></entry><entry><author><name>/u/ThickDoctor007</name><uri>https://www.reddit.com/user/ThickDoctor007</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hello everyone,&lt;/p&gt; &lt;p&gt;I&amp;#39;m working on a project that involves applying documented agricultural guidelines to live data. Specifically, these guidelines dictate the process for mechanical weed control in crops like sunflowers, including specifics such as seed depth, precipitation levels, and timing for mechanical controls relative to sowing dates.&lt;/p&gt; &lt;p&gt;For instance, according to the guidelines:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The seed should be sown below a depth of 5 cm.&lt;/li&gt; &lt;li&gt;The cumulative precipitation should be below 5mm/m2.&lt;/li&gt; &lt;li&gt;If the time difference between the presowing preparation date and the sowing date exceeds 2 days, two mechanical controls should be performed (one 3 days after presowing, and the second 7-8 days after presowing). If it&amp;#39;s less than or equal to 2 days, only one control is recommended.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Currently, I am manually coding these rules based on my reading of the guidelines. However, I&amp;#39;m interested in automating this process. I am considering using Retrieval-Augmented Generation (RAG) and agents to extract these rules automatically from the documents and apply them to incoming weather data.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Questions:&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Has anyone here worked on automatically extracting logic from structured documents using RAG or similar technologies?&lt;/li&gt; &lt;li&gt;Are there specific ways to format or write the documents that enhance the efficiency of knowledge extraction?&lt;/li&gt; &lt;li&gt;Can agents be effectively combined with RAG to monitor and apply these rules based on live weather data?&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Any insights or experiences with similar challenges would be greatly appreciated!&lt;/p&gt; &lt;p&gt;Thank you!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/ThickDoctor007&quot;&gt; /u/ThickDoctor007 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c90dl7/automating_knowledge_extraction_from/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c90dl7/automating_knowledge_extraction_from/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1c90dl7</id><link href="https://www.reddit.com/r/LangChain/comments/1c90dl7/automating_knowledge_extraction_from/" /><updated>2024-04-20T21:00:17+00:00</updated><published>2024-04-20T21:00:17+00:00</published><title>Automating Knowledge Extraction from Domain-specific Guidelines Using RAG and Agents</title></entry><entry><author><name>/u/phantom69_ftw</name><uri>https://www.reddit.com/user/phantom69_ftw</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;So the problem I&amp;#39;m working on is the prompts are fixed(not one liner QnA but half a page types) and the input pdf can change.&lt;/p&gt; &lt;p&gt;I&amp;#39;m calling it &amp;quot;reverse&amp;quot; because most of the examples or discussions I see talk about thr usecase where prompts are variable but docs might be fixed.&lt;/p&gt; &lt;p&gt;But in my case, when prompts are fixed, is there some specific optimizations I can do for better results? Have you tried something that&amp;#39;s not the norm for normal qna systems but works well for these Cases? Would love some insight.&lt;/p&gt; &lt;p&gt;Also can anyone share how they setup evals or a good source to learn from, for systems where the prompts are complex and the outputs are not few lines but complex json outputs. It would be really helpful.&lt;/p&gt; &lt;p&gt;TIA&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/phantom69_ftw&quot;&gt; /u/phantom69_ftw &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c8x0em/how_to_optimise_for_reverse_rag/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c8x0em/how_to_optimise_for_reverse_rag/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1c8x0em</id><link href="https://www.reddit.com/r/LangChain/comments/1c8x0em/how_to_optimise_for_reverse_rag/" /><updated>2024-04-20T18:35:05+00:00</updated><published>2024-04-20T18:35:05+00:00</published><title>How to optimise for &quot;reverse&quot; RAG?</title></entry><entry><author><name>/u/emir-guillaume</name><uri>https://www.reddit.com/user/emir-guillaume</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;m exploring hybrid RAG with vector and graph. Is MongoDB&amp;#39;s $graphlookup suitable for RAG in operational workloads (as in operational/OLTP vs analytical OLAP workloads)? Why or why not? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/emir-guillaume&quot;&gt; /u/emir-guillaume &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c8rj8d/is_mongodb_graphlookup_suitable_for_graph_rag_in/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c8rj8d/is_mongodb_graphlookup_suitable_for_graph_rag_in/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1c8rj8d</id><link href="https://www.reddit.com/r/LangChain/comments/1c8rj8d/is_mongodb_graphlookup_suitable_for_graph_rag_in/" /><updated>2024-04-20T14:38:02+00:00</updated><published>2024-04-20T14:38:02+00:00</published><title>Is MongoDB $graphlookup suitable for graph RAG in operational workloads?</title></entry><entry><author><name>/u/adamfdls</name><uri>https://www.reddit.com/user/adamfdls</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I am currently writing my first app with LLMs, and I want it to be able to read through a CSV file. The problem is that it is very unreliable, sometimes it is right, sometimes it is wrong.&lt;/p&gt; &lt;p&gt;My CSV is a table where you choose a row and a column and read the value at the intersection. For example it looks like this (My CSV file is much larger than this, i just used this for brevity)&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Bank Name,Bank1,Bank2,Bank3,Bank4 Is Live,Yes,Yes,Yes,No &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;When I asked: &amp;quot;Is bank 4 already live&amp;quot;, it answers &amp;quot;Yes&amp;quot;. But if I asked &amp;quot;Are Bank1, Bank2, Bank3 and Bank4 already live?&amp;quot;, then answer is &amp;quot;Bank1, Bank2, Bank3 is live, but not Bank4&amp;quot;&lt;/p&gt; &lt;p&gt;The prompt that I used is like below&lt;/p&gt; &lt;pre&gt;&lt;code&gt;You are going to be given a two-dimensional table where you choose a row and a column and read the value at the intersection but in a csv format. You are an experienced researcher, expert at interpreting and answering questions based on provided sources. Using the provided context, answer the user&amp;#39;s question to the best of your ability using only the resources provided. Be straight forward on answering questions. Concise, although not missing any important information. I don&amp;#39;t need to understand how you would get the data from, unless I specifically asked for it. &amp;lt;context&amp;gt; {context} &amp;lt;/context&amp;gt; Added information for the context, if you find that the cell is empty, it means that the information is not available. Now answer the question below using the above context: {question} &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Where the context is the contents of the CSV file. My question is, is there a better way to do this? I am currently using OpenAI model gpt-3.5-turbo-1106.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/adamfdls&quot;&gt; /u/adamfdls &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c8n7g4/having_difficulties_on_reading_csv_files_with/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c8n7g4/having_difficulties_on_reading_csv_files_with/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1c8n7g4</id><link href="https://www.reddit.com/r/LangChain/comments/1c8n7g4/having_difficulties_on_reading_csv_files_with/" /><updated>2024-04-20T10:58:29+00:00</updated><published>2024-04-20T10:58:29+00:00</published><title>Having difficulties on reading CSV files with OpenAI</title></entry><entry><author><name>/u/burcapaul</name><uri>https://www.reddit.com/user/burcapaul</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey everyone! Long-time lurker, a first-time poster here. I&amp;#39;m facing a challenge and could use your collective wisdom. I’m working on a project where I need to integrate a large language model (LLM) with an API service. The goal is to have the LLM possess the knowledge base necessary to construct JSON requests dynamically.&lt;/p&gt; &lt;p&gt;Here’s what I need to accomplish in two steps:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Retrieve Identifiers:&lt;/strong&gt; The first API call needs to be a GET request to fetch identifiers.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Use Identifiers:&lt;/strong&gt; The second API call is a POST request where these identifiers are essential.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;We already have a consistent JSON structure for our POST requests, but we’re stuck on dynamically generating and retrieving identifiers through the GET request. Does anyone have experience or suggestions on how to set up an LLM to handle these two steps effectively? Any advice on tools, frameworks, or code snippets would be greatly appreciated!&lt;/p&gt; &lt;p&gt;Thanks in advance for your help!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Later edit:&lt;/strong&gt; I have a chat assistant that it is connected with Gmail and Other services. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Example 1:&lt;/strong&gt; The user type a command like: &amp;quot;Find me the last email from [&lt;a href=&quot;mailto:john@doe.com&quot;&gt;john@doe.com&lt;/a&gt;]() and reply to him by telling him [whatever].&amp;quot;&lt;strong&gt;:&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Initial Action &lt;strong&gt;(GET Request)&lt;/strong&gt;: The assistant first performs a search to locate the most recent email from &amp;#39;[&lt;a href=&quot;mailto:john@doe.com&quot;&gt;john@doe.com&lt;/a&gt;]()&amp;#39;. It retrieves the email along with its unique identifier.&lt;/li&gt; &lt;li&gt;Follow-up Action &lt;strong&gt;(POST Request)&lt;/strong&gt;: Using the identifier obtained from the first action, the assistant then crafts and sends a reply to John Doe with the user-specified message.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;Example 2&lt;/strong&gt;: The user type a command like: &amp;quot;Create a new block in Notion under Meetings Page with [whatever content]&amp;quot; &lt;/p&gt; &lt;ol&gt; &lt;li&gt;Locating the Page &lt;strong&gt;(GET Request)&lt;/strong&gt;: The chat assistant first performs a search to find the unique identifier (ID) of the &amp;#39;Meetings Page&amp;#39; in Notion.&lt;/li&gt; &lt;li&gt;Creating the Block &lt;strong&gt;(POST Request)&lt;/strong&gt;: Once the page ID is retrieved, the assistant uses this ID in a POST request to create a new block on the Meetings Page with the content provided by the user.&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/burcapaul&quot;&gt; /u/burcapaul &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c8mew6/integrating_llm_with_api_services_for_dynamic/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c8mew6/integrating_llm_with_api_services_for_dynamic/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1c8mew6</id><link href="https://www.reddit.com/r/LangChain/comments/1c8mew6/integrating_llm_with_api_services_for_dynamic/" /><updated>2024-04-20T10:06:33+00:00</updated><published>2024-04-20T10:06:33+00:00</published><title>Integrating LLM with API Services for Dynamic JSON Handling – Need Help!</title></entry><entry><author><name>/u/cryptokaykay</name><uri>https://www.reddit.com/user/cryptokaykay</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;What are some of the tools you are using for testing and improving your applications? I have been curating/following a few of these. But, wanted to learn what your general experience has been? and what challenges you all are facing.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://github.com/explodinggradients/ragas&quot;&gt;https://github.com/explodinggradients/ragas&lt;/a&gt;&lt;br/&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://github.com/promptfoo/promptfoo&quot;&gt;https://github.com/promptfoo/promptfoo&lt;/a&gt;&lt;br/&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://github.com/braintrustdata/autoevals&quot;&gt;https://github.com/braintrustdata/autoevals&lt;/a&gt;&lt;br/&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://github.com/stanfordnlp/dspy&quot;&gt;https://github.com/stanfordnlp/dspy&lt;/a&gt;&lt;br/&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://github.com/jxnl/instructor/&quot;&gt;https://github.com/jxnl/instructor/&lt;/a&gt;&lt;br/&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://github.com/guidance-ai/guidance&quot;&gt;https://github.com/guidance-ai/guidance&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Separately, I am also building one which is more focused towards tracing and evaluations&lt;br/&gt; - &lt;a href=&quot;https://github.com/Scale3-Labs/langtrace&quot;&gt;https://github.com/Scale3-Labs/langtrace&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/cryptokaykay&quot;&gt; /u/cryptokaykay &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c87gn2/curated_list_of_open_source_tools_to_test_and/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c87gn2/curated_list_of_open_source_tools_to_test_and/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1c87gn2</id><link href="https://www.reddit.com/r/LangChain/comments/1c87gn2/curated_list_of_open_source_tools_to_test_and/" /><updated>2024-04-19T20:46:30+00:00</updated><published>2024-04-19T20:46:30+00:00</published><title>Curated list of open source tools to test and improve the accuracy of your RAG/LLM based app</title></entry><entry><author><name>/u/cryptokaykay</name><uri>https://www.reddit.com/user/cryptokaykay</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c7zp7y/langtrace_evaluations_breeze_through_with_hotkeys/&quot;&gt; &lt;img src=&quot;https://external-preview.redd.it/O9Z_56AIGA8RzHakPI49lAWCrCkAXsFQRc9Kf8-iHoQ.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e336e7bce43cfe2c02944103702fbfa275762561&quot; alt=&quot;Langtrace Evaluations - Breeze through with hotkeys&quot; title=&quot;Langtrace Evaluations - Breeze through with hotkeys&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;&lt;a href=&quot;https://reddit.com/link/1c7zp7y/video/v8ord1suegvc1/player&quot;&gt;https://reddit.com/link/1c7zp7y/video/v8ord1suegvc1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We have been busy shipping updates to &lt;strong&gt;Langtrace&lt;/strong&gt;, an &lt;strong&gt;Open Source LLM observability tool&lt;/strong&gt; and I am excited to show our new and improved Evaluations Dashboard. We learned from our early users that improving the RAG/model accuracy and gaining confidence with deploying their LLM based apps to production has been the number 1 priority.&lt;/p&gt; &lt;p&gt;To solve for this, we have built a couple of things:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;1. Create tests with different scoring scales and automatically capture LLM requests to these tests using Langtrace&amp;#39;s SDK.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;2. Evaluate the the requests by scoring against the response provided by the LLM to measure the overall average of each test.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Effectively, teams can come up with a release criteria like - &amp;quot;Factual Accuracy &amp;gt; 99%, Response Quality &amp;gt; 95%, Response Bias &amp;gt; 85%, Context Recall &amp;gt; 90%&amp;quot; and measure their product&amp;#39;s performance against this release metric with Langtrace.&lt;/p&gt; &lt;p&gt;Additionally, we also realized that the user experience is extremely important for effective and fast evaluations. As a result, the evaluations flow is fully optimized for hot keys and as an evaluator, you can breeze through a series of evaluations with just the arrow keys, enter and backspace without having to click through a bunch of times for each request.&lt;/p&gt; &lt;p&gt;Finally, all of this can be setup with just 2 lines of code and Langtrace&amp;#39;s Evaluation&amp;#39;s dashboard will start capturing the requests in the appropriate test automatically&lt;/p&gt; &lt;p&gt;Don&amp;#39;t forget to check out Langtrace and star the repository on Github.&lt;/p&gt; &lt;p&gt;Github - &lt;a href=&quot;https://github.com/Scale3-Labs/langtrace&quot;&gt;https://github.com/Scale3-Labs/langtrace&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/cryptokaykay&quot;&gt; /u/cryptokaykay &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c7zp7y/langtrace_evaluations_breeze_through_with_hotkeys/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c7zp7y/langtrace_evaluations_breeze_through_with_hotkeys/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1c7zp7y</id><media:thumbnail url="https://external-preview.redd.it/O9Z_56AIGA8RzHakPI49lAWCrCkAXsFQRc9Kf8-iHoQ.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=e336e7bce43cfe2c02944103702fbfa275762561" /><link href="https://www.reddit.com/r/LangChain/comments/1c7zp7y/langtrace_evaluations_breeze_through_with_hotkeys/" /><updated>2024-04-19T15:28:34+00:00</updated><published>2024-04-19T15:28:34+00:00</published><title>Langtrace Evaluations - Breeze through with hotkeys</title></entry><entry><author><name>/u/GPT-Claude-Gemini</name><uri>https://www.reddit.com/user/GPT-Claude-Gemini</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I have heard from many people that Langchain is good for prototyping but not for production, is it because it&amp;#39;s slower than using each LLM&amp;#39;s APIs directly? I did some testing comparing the response speed from calling OpenAI directly versus calling it via Langchain, and Langchain consistently generates output 10% - 30% slower, not sure if it&amp;#39;s my local problem or if it&amp;#39;s a universal observation. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/GPT-Claude-Gemini&quot;&gt; /u/GPT-Claude-Gemini &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c82clj/is_it_true_that_its_slower_to_use_langchain_than/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c82clj/is_it_true_that_its_slower_to_use_langchain_than/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1c82clj</id><link href="https://www.reddit.com/r/LangChain/comments/1c82clj/is_it_true_that_its_slower_to_use_langchain_than/" /><updated>2024-04-19T17:15:38+00:00</updated><published>2024-04-19T17:15:38+00:00</published><title>Is it true that it's slower to use Langchain than to call the model API's directly?</title></entry><entry><author><name>/u/lucarioburrito</name><uri>https://www.reddit.com/user/lucarioburrito</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I am looking for a good vector store for self-querying in LangChain. I want to be able to query, “find me a movie with DiCaprio in it” and it should be able to find Leonardo DiCaprio in the metadata.&lt;/p&gt; &lt;p&gt;Chroma doesn’t support “contain”, what would be other DB options that can support this while also do the vector similarity search? LangChain documentation doesn’t describe the “allowed_comparators” attribute in their self-querying vector stores info.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/lucarioburrito&quot;&gt; /u/lucarioburrito &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c84lph/allowed_comparators_for_different_vector_stores/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c84lph/allowed_comparators_for_different_vector_stores/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1c84lph</id><link href="https://www.reddit.com/r/LangChain/comments/1c84lph/allowed_comparators_for_different_vector_stores/" /><updated>2024-04-19T18:47:43+00:00</updated><published>2024-04-19T18:47:43+00:00</published><title>Allowed comparators for different vector stores?</title></entry><entry><author><name>/u/Beginning_Rock_1906</name><uri>https://www.reddit.com/user/Beginning_Rock_1906</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey everybody. Without going too much into detail I would be super interested what you guys are using this technology for. We hear a lot about technicalities here but I&amp;#39;m really wondering how much you are using these technologies in your life and in your work. Would be super interesting to know&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Beginning_Rock_1906&quot;&gt; /u/Beginning_Rock_1906 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c8370g/question_about_you_guys/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c8370g/question_about_you_guys/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1c8370g</id><link href="https://www.reddit.com/r/LangChain/comments/1c8370g/question_about_you_guys/" /><updated>2024-04-19T17:50:28+00:00</updated><published>2024-04-19T17:50:28+00:00</published><title>Question about you guys</title></entry><entry><author><name>/u/Glittering-Bear5748</name><uri>https://www.reddit.com/user/Glittering-Bear5748</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;how prevent duplicate embedding instance in pgvector without delete collection.&lt;/p&gt; &lt;p&gt;update existing once and want new one &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Glittering-Bear5748&quot;&gt; /u/Glittering-Bear5748 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c80n7e/pgvector_duplicate_embedding/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c80n7e/pgvector_duplicate_embedding/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1c80n7e</id><link href="https://www.reddit.com/r/LangChain/comments/1c80n7e/pgvector_duplicate_embedding/" /><updated>2024-04-19T16:06:52+00:00</updated><published>2024-04-19T16:06:52+00:00</published><title>PGVector duplicate embedding</title></entry><entry><author><name>/u/e1ectrOniK</name><uri>https://www.reddit.com/user/e1ectrOniK</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c88bh9/where_have_i_gone_wrong/&quot;&gt; &lt;img src=&quot;https://b.thumbs.redditmedia.com/5rrIC7Lm5PYPf-px-4mor5Ck8reYqJx4d2CXH4HprCE.jpg&quot; alt=&quot;Where have i gone wrong ?&quot; title=&quot;Where have i gone wrong ?&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/e1ectrOniK&quot;&gt; /u/e1ectrOniK &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/gallery/1c88bh9&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c88bh9/where_have_i_gone_wrong/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1c88bh9</id><media:thumbnail url="https://b.thumbs.redditmedia.com/5rrIC7Lm5PYPf-px-4mor5Ck8reYqJx4d2CXH4HprCE.jpg" /><link href="https://www.reddit.com/r/LangChain/comments/1c88bh9/where_have_i_gone_wrong/" /><updated>2024-04-19T21:21:29+00:00</updated><published>2024-04-19T21:21:29+00:00</published><title>Where have i gone wrong ?</title></entry><entry><author><name>/u/SashaBaych</name><uri>https://www.reddit.com/user/SashaBaych</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Guys, with the latest updates, can someone please explain to me the difference between invoking AgentExecutor having used &amp;#39;create_tool_calling_agent&amp;#39; method as opposed to binding tools to the model and invoking it? Especially in the context of langgraph. I see so many tutorials on langgraph and it is always &amp;#39;model.bind_tools()&amp;#39;, only a few tutorials use AgentExecutor within langgrpah. Do I even need to use it within a langgraph? What would be the benefit/difference?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/SashaBaych&quot;&gt; /u/SashaBaych &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c82vk3/agentexecutor_vs_modelbind_tools_in_langgraph/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c82vk3/agentexecutor_vs_modelbind_tools_in_langgraph/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1c82vk3</id><link href="https://www.reddit.com/r/LangChain/comments/1c82vk3/agentexecutor_vs_modelbind_tools_in_langgraph/" /><updated>2024-04-19T17:37:26+00:00</updated><published>2024-04-19T17:37:26+00:00</published><title>AgentExecutor VS model.bind_tools() in langgraph</title></entry><entry><author><name>/u/mehul_gupta1997</name><uri>https://www.reddit.com/user/mehul_gupta1997</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/mehul_gupta1997&quot;&gt; /u/mehul_gupta1997 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;/r/learnmachinelearning/comments/1c7rksc/tried_llama3_by_meta_today/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c7rl12/tried_llama3_by_meta_today/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1c7rl12</id><link href="https://www.reddit.com/r/LangChain/comments/1c7rl12/tried_llama3_by_meta_today/" /><updated>2024-04-19T08:17:28+00:00</updated><published>2024-04-19T08:17:28+00:00</published><title>Tried Llama3 by Meta today</title></entry><entry><author><name>/u/Educational-String94</name><uri>https://www.reddit.com/user/Educational-String94</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c6zktz/llms_frameworks_langchain_llamaindex_griptape/&quot;&gt; &lt;img src=&quot;https://preview.redd.it/1iz91emwo7vc1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=370b047730989d9b9c4a6c0366e977a641be69db&quot; alt=&quot;LLMs frameworks (langchain, llamaindex, griptape, autogen, crewai etc.) are overengineered and makes easy tasks hard, correct me if im wrong&quot; title=&quot;LLMs frameworks (langchain, llamaindex, griptape, autogen, crewai etc.) are overengineered and makes easy tasks hard, correct me if im wrong&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Educational-String94&quot;&gt; /u/Educational-String94 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://i.redd.it/1iz91emwo7vc1.jpeg&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c6zktz/llms_frameworks_langchain_llamaindex_griptape/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1c6zktz</id><media:thumbnail url="https://preview.redd.it/1iz91emwo7vc1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=370b047730989d9b9c4a6c0366e977a641be69db" /><link href="https://www.reddit.com/r/LangChain/comments/1c6zktz/llms_frameworks_langchain_llamaindex_griptape/" /><updated>2024-04-18T10:04:19+00:00</updated><published>2024-04-18T10:04:19+00:00</published><title>LLMs frameworks (langchain, llamaindex, griptape, autogen, crewai etc.) are overengineered and makes easy tasks hard, correct me if im wrong</title></entry><entry><author><name>/u/boy_with_eng_tattoo</name><uri>https://www.reddit.com/user/boy_with_eng_tattoo</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey guys! Anyone here running RAG pipelines in production with user uploaded documents (semantic chunking, summaries, knowledge graphs etc)? I am working on a RAG application for PPC and want to chunks SOPs into chunks for vector DB. But I am confused on which chunking strategy is better to get the most optimal results from the LLM.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/boy_with_eng_tattoo&quot;&gt; /u/boy_with_eng_tattoo &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c7pnzg/need_help_in_understanding_chunking_strategy_for/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c7pnzg/need_help_in_understanding_chunking_strategy_for/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1c7pnzg</id><link href="https://www.reddit.com/r/LangChain/comments/1c7pnzg/need_help_in_understanding_chunking_strategy_for/" /><updated>2024-04-19T06:08:21+00:00</updated><published>2024-04-19T06:08:21+00:00</published><title>Need help in understanding chunking strategy for RAG application in production</title></entry><entry><author><name>/u/Practical-Win5009</name><uri>https://www.reddit.com/user/Practical-Win5009</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;im trying to do a bot that answer questions from a chromadb , i have stored multiple pdf files with metadata like the filename and candidate name , my problem is when i use conversational retrieval chain the LLM model just receive page_content without the metadata , i want the LLM model to be aware of the page_content with its metadata like filename and candidate name here is my code &lt;/p&gt; &lt;pre&gt;&lt;code&gt;conversation_chain=ConversationalRetrievalChain.from_llm( llm=llm, retriever=SelfQueryRetriever.from_llm(llm,vectorstore,document_content_description,metadata_field_info), memory=memory, verbose=True, ) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;and here is my attribute info&lt;/p&gt; &lt;pre&gt;&lt;code&gt;metadata_field_info = [ AttributeInfo( name=&amp;quot;filename&amp;quot;, description=&amp;quot;The name of the resumee&amp;quot;, type=&amp;quot;string&amp;quot;, ), AttributeInfo( name=&amp;quot;candidatename&amp;quot;, description=&amp;quot;the name of the candidate&amp;quot;, type=&amp;quot;string&amp;quot; ) ] &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;and here is how i use the chain &lt;/p&gt; &lt;p&gt;conversation_chain({&amp;#39;query&amp;#39;:user_question})&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Practical-Win5009&quot;&gt; /u/Practical-Win5009 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c7ra1w/how_to_make_conversationalretrievalchain_to/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c7ra1w/how_to_make_conversationalretrievalchain_to/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1c7ra1w</id><link href="https://www.reddit.com/r/LangChain/comments/1c7ra1w/how_to_make_conversationalretrievalchain_to/" /><updated>2024-04-19T07:56:15+00:00</updated><published>2024-04-19T07:56:15+00:00</published><title>how to make conversationalretrievalchain to include metadata in the prompt using langchain with chromadb to make the LLM aware of metadata?</title></entry><entry><author><name>/u/naotemfin</name><uri>https://www.reddit.com/user/naotemfin</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;m struggling trying to use a .txt and a .csv file to give more context to my Chatbot.&lt;/p&gt; &lt;p&gt;This is partly what I have at the moment:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;# For text files text_loader = DirectoryLoader(&amp;quot;&amp;quot;, glob=&amp;quot;./test.txt&amp;quot;, loader_cls=TextLoader) text_docs = text_loader.load() # For CSV files csv_loader = DirectoryLoader(&amp;quot;&amp;quot;, glob=&amp;quot;./stock.csv&amp;quot;, loader_cls=CSVLoader) csv_docs = csv_loader.load() # Combine the documents loader_all = MergedDataLoader(loaders=[text_loader, csv_loader]) docs = loader_all.load() db = Chroma.from_documents(docs, embedding_function) retriever = db.as_retriever() chain = ( {&amp;quot;context&amp;quot;: retriever, &amp;quot;question&amp;quot;: RunnablePassthrough()} | prompt | model | StrOutputParser() ) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;When I ask something related to the data within the .csv (related to some products&amp;#39; stock) it answers well. But, if it&amp;#39;s a question of something within the .txt file, it doesn&amp;#39;t know what to answer.&lt;/p&gt; &lt;p&gt;Have someone already dealt with this?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/naotemfin&quot;&gt; /u/naotemfin &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c7fcad/best_way_of_using_context_from_different_files/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c7fcad/best_way_of_using_context_from_different_files/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1c7fcad</id><link href="https://www.reddit.com/r/LangChain/comments/1c7fcad/best_way_of_using_context_from_different_files/" /><updated>2024-04-18T21:34:39+00:00</updated><published>2024-04-18T21:34:39+00:00</published><title>Best way of using context from different files?</title></entry></feed>