<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/"><category term="LangChain" label="r/LangChain"/><updated>2024-01-04T14:02:06+00:00</updated><icon>https://www.redditstatic.com/icon.png/</icon><id>/r/LangChain.rss</id><link rel="self" href="https://www.reddit.com/r/LangChain.rss" type="application/atom+xml" /><link rel="alternate" href="https://www.reddit.com/r/LangChain" type="text/html" /><subtitle>LangChain is an open-source framework and developer toolkit that helps developers get LLM applications from prototype to production. It is available for Python and Javascript at https://www.langchain.com/.</subtitle><title>LangChain</title><entry><author><name>/u/zchaarm</name><uri>https://www.reddit.com/user/zchaarm</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;A place for members of &lt;a href=&quot;/r/LangChain&quot;&gt;r/LangChain&lt;/a&gt; to chat with each other&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/zchaarm&quot;&gt; /u/zchaarm &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_10ljho9</id><link href="https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/" /><updated>2023-01-26T04:33:05+00:00</updated><published>2023-01-26T04:33:05+00:00</published><title>r/LangChain Lounge</title></entry><entry><author><name>/u/No-Tailor-6633</name><uri>https://www.reddit.com/user/No-Tailor-6633</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi,&lt;br/&gt; Wrote an article on how to autonomously perform semantic chunking for a wide variety of documents. Please give it a read : &lt;a href=&quot;https://medium.com/@boudhayan-dev/semantic-chunking-in-practice-23a8bc33d56d&quot;&gt;Semantic chunking using LLM&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/No-Tailor-6633&quot;&gt; /u/No-Tailor-6633 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18y44fp/semantic_chunking/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18y44fp/semantic_chunking/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18y44fp</id><link href="https://www.reddit.com/r/LangChain/comments/18y44fp/semantic_chunking/" /><updated>2024-01-04T04:23:20+00:00</updated><published>2024-01-04T04:23:20+00:00</published><title>Semantic chunking</title></entry><entry><author><name>/u/Ill_Bodybuilder3499</name><uri>https://www.reddit.com/user/Ill_Bodybuilder3499</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;I have built a RAG App with Langchain and used the &lt;a href=&quot;https://huggingface.co/intfloat/multilingual-e5-large&quot;&gt;intfloat/multilingual-e5-large&lt;/a&gt; embeddings so far. At the moment I tried oout different chunk sizes (100-2000) and I am wondering if the embedding model is relevant for the chunk size? I was wondering because I saw that the embedding size is 1024.&lt;/p&gt; &lt;p&gt;Thanks for suggestions!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Ill_Bodybuilder3499&quot;&gt; /u/Ill_Bodybuilder3499 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18yd77i/rag_embedding_model_relevant_to_chunk_size/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18yd77i/rag_embedding_model_relevant_to_chunk_size/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18yd77i</id><link href="https://www.reddit.com/r/LangChain/comments/18yd77i/rag_embedding_model_relevant_to_chunk_size/" /><updated>2024-01-04T13:29:22+00:00</updated><published>2024-01-04T13:29:22+00:00</published><title>RAG Embedding Model relevant to Chunk Size?</title></entry><entry><author><name>/u/Ill_Bodybuilder3499</name><uri>https://www.reddit.com/user/Ill_Bodybuilder3499</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;I have put my application into a Docker and therefore I have created a requirements.txt file. Now I need to install &amp;quot;llama-cpp-python&amp;quot; for Mac, as I am loading my LLM with &lt;code&gt;from langchain.llms import LlamaCpp&lt;/code&gt;. &lt;/p&gt; &lt;p&gt;My installation command specifically for Mac is: &amp;quot;&lt;code&gt;CMAKE_ARGS=&amp;quot;-DLLAMA_METAL=on&amp;quot; FORCE_CMAKE=1 pip install llama-cpp-python&lt;/code&gt;&amp;quot;, but it does not work if I put this in my &amp;quot;requirements.txt&amp;quot; file.&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;How can I do this?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Ill_Bodybuilder3499&quot;&gt; /u/Ill_Bodybuilder3499 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18ybano/install_llamacpppython_in_requirementstxt_file/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18ybano/install_llamacpppython_in_requirementstxt_file/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18ybano</id><link href="https://www.reddit.com/r/LangChain/comments/18ybano/install_llamacpppython_in_requirementstxt_file/" /><updated>2024-01-04T11:47:53+00:00</updated><published>2024-01-04T11:47:53+00:00</published><title>Install &quot;llama-cpp-python&quot; in requirements.txt file</title></entry><entry><author><name>/u/silent-spiral</name><uri>https://www.reddit.com/user/silent-spiral</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;How to use gpt3.5-turbo-instruct with Langchain? I get: &lt;/p&gt; &lt;p&gt;&amp;quot;Error code: 404 - {&amp;#39;error&amp;#39;: {&amp;#39;message&amp;#39;: &amp;#39;This is not a chat model and thus not supported in the v1/chat/completions endpoint. Did you mean to use v1/completions?&amp;#39;, &amp;#39;type&amp;#39;: &amp;#39;invalid_request_error&amp;#39;, &amp;#39;param&amp;#39;: &amp;#39;model&amp;#39;, &amp;#39;code&amp;#39;: None}} &lt;/p&gt; &lt;p&gt;Is there any way to get langchain to hit the /completions endpoint instead?&lt;/p&gt; &lt;p&gt;Or is langchain not meant for use with instruct models? I might have to abandon langchain for this project?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/silent-spiral&quot;&gt; /u/silent-spiral &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18y4pyj/how_to_use_gpt35turboinstruct_with_langchain/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18y4pyj/how_to_use_gpt35turboinstruct_with_langchain/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18y4pyj</id><link href="https://www.reddit.com/r/LangChain/comments/18y4pyj/how_to_use_gpt35turboinstruct_with_langchain/" /><updated>2024-01-04T04:54:57+00:00</updated><published>2024-01-04T04:54:57+00:00</published><title>How to use gpt3.5-turbo-instruct with Langchain?</title></entry><entry><author><name>/u/HiddenMushroom11</name><uri>https://www.reddit.com/user/HiddenMushroom11</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;m wondering if streaming even works with the HuggingFaceTextGenInference llm code.&lt;/p&gt; &lt;p&gt;I get responses from my model, but only when I run the model normally, and don&amp;#39;t use the streaming=True parameter. I can also stream the model directly from my local server when I use curl, but not when I use langchain.&lt;/p&gt; &lt;p&gt;Context to the issue here with code example:&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/langchain-ai/langchain/issues/15516&quot;&gt;https://github.com/langchain-ai/langchain/issues/15516&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Any help is appreciated. :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/HiddenMushroom11&quot;&gt; /u/HiddenMushroom11 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18y2coy/has_anyone_gotten_langchain_to_stream_hugging/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18y2coy/has_anyone_gotten_langchain_to_stream_hugging/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18y2coy</id><link href="https://www.reddit.com/r/LangChain/comments/18y2coy/has_anyone_gotten_langchain_to_stream_hugging/" /><updated>2024-01-04T02:56:14+00:00</updated><published>2024-01-04T02:56:14+00:00</published><title>Has anyone gotten Langchain to stream Hugging Face models with FastAPI?</title></entry><entry><author><name>/u/EducatorDiligent5114</name><uri>https://www.reddit.com/user/EducatorDiligent5114</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I want to built a RAG system for science documents which contains the theory texts along withn equations, tables, and labelled diagrams. Questions can be from understanding of theory, equations and information about tables. How should I proceed? Have idea of building a naive rag system only. Any resources will be helpful.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/EducatorDiligent5114&quot;&gt; /u/EducatorDiligent5114 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18xp9xi/rag_for_pdf_with_tables/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18xp9xi/rag_for_pdf_with_tables/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18xp9xi</id><link href="https://www.reddit.com/r/LangChain/comments/18xp9xi/rag_for_pdf_with_tables/" /><updated>2024-01-03T17:53:53+00:00</updated><published>2024-01-03T17:53:53+00:00</published><title>RAG for Pdf with tables</title></entry><entry><author><name>/u/modularmindapp</name><uri>https://www.reddit.com/user/modularmindapp</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18y6u49/summarize_dozens_of_youtube_videos_in_seconds/&quot;&gt; &lt;img src=&quot;https://external-preview.redd.it/MzNmN2xzZDlmZGFjMS4Blkz_LHpwwPWvzOzcEePvPFOHfAVlVD8x7DHC7HNm.png?width=140&amp;amp;height=140&amp;amp;crop=140:140,smart&amp;amp;format=jpg&amp;amp;v=enabled&amp;amp;lthumb=true&amp;amp;s=10ab1465aa4654d743390cb24ff4c99f4f722798&quot; alt=&quot;Summarize dozens of YouTube videos in seconds&quot; title=&quot;Summarize dozens of YouTube videos in seconds&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/modularmindapp&quot;&gt; /u/modularmindapp &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://v.redd.it/80gjyg98fdac1&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18y6u49/summarize_dozens_of_youtube_videos_in_seconds/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_18y6u49</id><media:thumbnail url="https://external-preview.redd.it/MzNmN2xzZDlmZGFjMS4Blkz_LHpwwPWvzOzcEePvPFOHfAVlVD8x7DHC7HNm.png?width=140&amp;height=140&amp;crop=140:140,smart&amp;format=jpg&amp;v=enabled&amp;lthumb=true&amp;s=10ab1465aa4654d743390cb24ff4c99f4f722798" /><link href="https://www.reddit.com/r/LangChain/comments/18y6u49/summarize_dozens_of_youtube_videos_in_seconds/" /><updated>2024-01-04T06:56:00+00:00</updated><published>2024-01-04T06:56:00+00:00</published><title>Summarize dozens of YouTube videos in seconds</title></entry><entry><author><name>/u/mohan-aditya05</name><uri>https://www.reddit.com/user/mohan-aditya05</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18y1tic/wrote_an_article_on_using_langchain_for/&quot;&gt; &lt;img src=&quot;https://external-preview.redd.it/QTgTUY4iz7H4Wx3iBA3KGtWV3mVKJraN3u-HNXk9rz4.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=189c3a145c156156a705a4ef4936e0712c6b965f&quot; alt=&quot;Wrote an article on using Langchain for information extraction and building a UI around it using Streamlit. Check it out!&quot; title=&quot;Wrote an article on using Langchain for information extraction and building a UI around it using Streamlit. Check it out!&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/mohan-aditya05&quot;&gt; /u/mohan-aditya05 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://medium.com/p/f1a551f01f66&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18y1tic/wrote_an_article_on_using_langchain_for/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_18y1tic</id><media:thumbnail url="https://external-preview.redd.it/QTgTUY4iz7H4Wx3iBA3KGtWV3mVKJraN3u-HNXk9rz4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=189c3a145c156156a705a4ef4936e0712c6b965f" /><link href="https://www.reddit.com/r/LangChain/comments/18y1tic/wrote_an_article_on_using_langchain_for/" /><updated>2024-01-04T02:31:10+00:00</updated><published>2024-01-04T02:31:10+00:00</published><title>Wrote an article on using Langchain for information extraction and building a UI around it using Streamlit. Check it out!</title></entry><entry><author><name>/u/jawbuster</name><uri>https://www.reddit.com/user/jawbuster</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;promptT = PromptTemplate(template=prompt_template, input_variables=[&amp;quot;context&amp;quot;, &amp;quot;question&amp;quot;]) &lt;/p&gt; &lt;p&gt;llm = ChatGoogleGenerativeAI(model=&amp;quot;gemini-pro&amp;quot;,google_api_key=google_api_key) &lt;/p&gt; &lt;p&gt;chain = load_qa_chain(llm=llm, chain_type=&amp;quot;stuff&amp;quot;,prompt=promptT) &lt;/p&gt; &lt;p&gt;keeps giving me below error:&lt;/p&gt; &lt;p&gt;File &amp;quot;pydantic\&lt;a href=&quot;https://main.py&quot;&gt;main.py&lt;/a&gt;&amp;quot;, line 341, in pydantic.main.BaseModel.__init__&lt;/p&gt; &lt;p&gt;pydantic.error_wrappers.ValidationError: 2 validation errors for LLMChain&lt;/p&gt; &lt;p&gt;llm&lt;/p&gt; &lt;p&gt;instance of Runnable expected (type=type_error.arbitrary_type; expected_arbitrary_type=Runnable)&lt;/p&gt; &lt;p&gt;llm&lt;/p&gt; &lt;p&gt;instance of Runnable expected (type=type_error.arbitrary_type; expected_arbitrary_type=Runnable)&lt;/p&gt; &lt;p&gt;I am fed up with this..I tried without the prompt template and I am sure it has something to do with the model but not sure how to correct..&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/jawbuster&quot;&gt; /u/jawbuster &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18xydx7/please_help_with_this/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18xydx7/please_help_with_this/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18xydx7</id><link href="https://www.reddit.com/r/LangChain/comments/18xydx7/please_help_with_this/" /><updated>2024-01-04T00:00:15+00:00</updated><published>2024-01-04T00:00:15+00:00</published><title>Please help with this</title></entry><entry><author><name>/u/Notchampa</name><uri>https://www.reddit.com/user/Notchampa</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;m working on a student project that involves a FastAPI backend serving a RAG (Retrieval-Augmented Generation) application, which interfaces with a frontend already hosted on Netlify. The app leverages the LLaMA index, and I recently made some enhancements following the &amp;quot;small to big retrieval&amp;quot; strategies outlined &lt;a href=&quot;https://docs.llamaindex.ai/en/stable/optimizing/advanced_retrieval/advanced_retrieval.html&quot;&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;While these improvements have significantly boosted the app&amp;#39;s performance, they&amp;#39;ve also led to a new challenge: my current hosting solution on a Digital Ocean droplet isn&amp;#39;t cutting it anymore, as I&amp;#39;m consistently running into out-of-memory issues.&lt;/p&gt; &lt;p&gt;I&amp;#39;m now in the market for a hosting platform that can comfortably handle the heavier memory requirements of my updated backend. Key requirements include:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Robust enough to support a memory-intensive FastAPI app.&lt;/li&gt; &lt;li&gt;HTTPS support for security.&lt;/li&gt; &lt;li&gt;Preferably developer-friendly and cost-effective, considering it&amp;#39;s for a student project.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Does anyone have recommendations for hosting providers or services that can meet these needs? Or, if you&amp;#39;ve worked on similar projects, I&amp;#39;d love to hear how you tackled the hosting challenges. Any insights, tips, or shared experiences would be greatly appreciated!&lt;/p&gt; &lt;p&gt;Thank you all in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Notchampa&quot;&gt; /u/Notchampa &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18xe8di/where_to_host_rag_app/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18xe8di/where_to_host_rag_app/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18xe8di</id><link href="https://www.reddit.com/r/LangChain/comments/18xe8di/where_to_host_rag_app/" /><updated>2024-01-03T08:08:48+00:00</updated><published>2024-01-03T08:08:48+00:00</published><title>Where to host RAG app?</title></entry><entry><author><name>/u/Intern_MSFT</name><uri>https://www.reddit.com/user/Intern_MSFT</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hello, I have been trying to join LCEL with StreamlitChatMessageHistory to store chat messages. This is my code:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;llm_chain = RunnablePassthrough.assign( \ history=RunnableLambda(memory.load_memory_variables) | itemgetter(&amp;quot;history&amp;quot;)) | prompt_to_ai | llm_openai | output_parser &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;I got the code snippet from here: &lt;a href=&quot;https://python.langchain.com/docs/expression_language/cookbook/memory&quot;&gt;https://python.langchain.com/docs/expression_language/cookbook/memory&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Can someone please assist? Thank you.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Intern_MSFT&quot;&gt; /u/Intern_MSFT &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18xu7jd/lcel_with_streamlit_fails_to_include_memory_in/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18xu7jd/lcel_with_streamlit_fails_to_include_memory_in/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18xu7jd</id><link href="https://www.reddit.com/r/LangChain/comments/18xu7jd/lcel_with_streamlit_fails_to_include_memory_in/" /><updated>2024-01-03T21:10:52+00:00</updated><published>2024-01-03T21:10:52+00:00</published><title>LCEL with Streamlit fails to include memory in conversations.</title></entry><entry><author><name>/u/Ill_Bodybuilder3499</name><uri>https://www.reddit.com/user/Ill_Bodybuilder3499</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;I have built a RAG app with Langchain and Streamlit and now want to share it with my team. As I am dealing with confidential data, I don&amp;#39;t want to use a cloud solution, so my idea was to convert the Ap to an .EXE-File, so that they can load and use it with no programming expertise. I saw this tutorial, but it is not working for me. &lt;/p&gt; &lt;p&gt;Does anyone have a good resource for this?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Ill_Bodybuilder3499&quot;&gt; /u/Ill_Bodybuilder3499 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18xlzmg/deploy_langchainstreamlit_app_to_exefile/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18xlzmg/deploy_langchainstreamlit_app_to_exefile/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18xlzmg</id><link href="https://www.reddit.com/r/LangChain/comments/18xlzmg/deploy_langchainstreamlit_app_to_exefile/" /><updated>2024-01-03T15:28:12+00:00</updated><published>2024-01-03T15:28:12+00:00</published><title>Deploy Langchain-Streamlit App to EXE-File</title></entry><entry><author><name>/u/Jango214</name><uri>https://www.reddit.com/user/Jango214</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi all&lt;/p&gt; &lt;p&gt;I am using an agent executor and agents and chains and tools to query a SQL database. &lt;/p&gt; &lt;p&gt;The whole chatbot works fine, but I just have a question about the output I get.&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://imgur.com/a/1ET05Cq&quot;&gt;https://imgur.com/a/1ET05Cq&lt;/a&gt;&lt;/p&gt; &lt;p&gt;What is the difference between the &amp;#39;Observation&amp;#39; and the &amp;#39;Answer&amp;#39;?&lt;/p&gt; &lt;p&gt;For my tool I have return_direct=True.&lt;/p&gt; &lt;p&gt;Secondly, I am trying to stream the output of my LLM. I am using a custom callback handler to do that, and I can stream the intermediate SQL query and everything, but not the &amp;#39;Answer&amp;#39;, or the &amp;#39;Observation&amp;#39;. How do I do that?&lt;/p&gt; &lt;p&gt;Finally, the LLM is the outputting the &amp;#39;Answer:&amp;#39; text, but when I have a check within the custom callback handler to check for &amp;#39;Answer:&amp;#39;, it does not raise the flag as I intend it to.&lt;/p&gt; &lt;p&gt;Am I approaching this the wrong way?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Jango214&quot;&gt; /u/Jango214 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18xjizi/difference_between_answer_and_observation/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18xjizi/difference_between_answer_and_observation/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18xjizi</id><link href="https://www.reddit.com/r/LangChain/comments/18xjizi/difference_between_answer_and_observation/" /><updated>2024-01-03T13:34:33+00:00</updated><published>2024-01-03T13:34:33+00:00</published><title>Difference between Answer and Observation?</title></entry><entry><author><name>/u/sathi006</name><uri>https://www.reddit.com/user/sathi006</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;&lt;a href=&quot;https://play.google.com/store/apps/details?id=com.hertzai.hevolve&quot;&gt;https://play.google.com/store/apps/details?id=com.hertzai.hevolve&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Latency ~5 seconds coz of RAG, chains and tools but fully context aware coz of the same reason.&lt;/p&gt; &lt;p&gt;Microsoft copilot with an AI Avatar which can talk to you in 10 languages&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/sathi006&quot;&gt; /u/sathi006 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18xjflb/hevolve_a_production_app_using_langchain/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18xjflb/hevolve_a_production_app_using_langchain/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18xjflb</id><link href="https://www.reddit.com/r/LangChain/comments/18xjflb/hevolve_a_production_app_using_langchain/" /><updated>2024-01-03T13:29:53+00:00</updated><published>2024-01-03T13:29:53+00:00</published><title>Hevolve : A production app using Langchain</title></entry><entry><author><name>/u/ruhrohj</name><uri>https://www.reddit.com/user/ruhrohj</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi all,&lt;/p&gt; &lt;p&gt;I currently have this codebase right here that uses &lt;code&gt;RetrievalQA&lt;/code&gt; to create a Q&amp;amp;A Chatbot. This current iteration uses &lt;code&gt;Chroma&lt;/code&gt; as the vectorstore, and works perfectly.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;vectorstore = Chroma(persist_directory=&amp;quot;./chroma_db&amp;quot;, embedding_function=OpenAIEmbeddings()) template = &amp;quot;&amp;quot;&amp;quot; Instructions here {context} Question: {question} Helpful Answer: &amp;quot;&amp;quot;&amp;quot; QA_CHAIN_PROMPT = PromptTemplate(input_variables=[&amp;quot;context&amp;quot;, &amp;quot;question&amp;quot;], template=template) llm = ChatOpenAI(model_name=&amp;quot;gpt-3.5-turbo&amp;quot;, temperature=0) qa = RetrievalQA.from_chain_type(llm, chain_type=&amp;#39;stuff&amp;#39;, retriever=vectorstore.as_retriever(), chain_type_kwargs={&amp;quot;prompt&amp;quot;: QA_CHAIN_PROMPT}) qa.run(query) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;However, I am required to migrate the vector database over to Azure CosmosDB (vCore). Following the documentations, I have created a function that converts my text document into embeddings, and writes them to CosmosDB.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;def CosmosEmbedder(): loader = TextLoader(&amp;quot;./data/file.txt&amp;quot;) data = loader.load() # Document Splitting text_splitter = RecursiveCharacterTextSplitter(chunk_size = 500, chunk_overlap = 0) all_splits = text_splitter.split_documents(data) CONNECTION_STRING = os.getenv(&amp;quot;MONGO_URI&amp;quot;) NAMESPACE = &amp;quot;testdb.testcollection&amp;quot; DB_NAME, COLLECTION_NAME = NAMESPACE.split(&amp;quot;.&amp;quot;) client: MongoClient = MongoClient(CONNECTION_STRING) collection = client[DB_NAME][COLLECTION_NAME] vectorstore = AzureCosmosDBVectorSearch(collection, OpenAIEmbeddings()) vectorstore.add_documents(all_splits) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;And using a similar function, I am able to verify that the embeddings do exist within CosmosDB. Now here is where the documentation gets unclear to me. The documentation proceeds to use similarity search as a working example. But my goal is to use the CosmosDB as a vectorstore for &lt;code&gt;RetrievalQA&lt;/code&gt; instead. &lt;/p&gt; &lt;p&gt;Does anyone have any idea on how to implement this? For reference, the documentation I was referring to can be found &lt;a href=&quot;https://python.langchain.com/docs/integrations/vectorstores/azure_cosmos_db&quot;&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Greatly appreciate any inputs on the situation.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/ruhrohj&quot;&gt; /u/ruhrohj &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18xhjnu/integrating_azure_cosmosdb_as_vectorstore_for/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18xhjnu/integrating_azure_cosmosdb_as_vectorstore_for/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18xhjnu</id><link href="https://www.reddit.com/r/LangChain/comments/18xhjnu/integrating_azure_cosmosdb_as_vectorstore_for/" /><updated>2024-01-03T11:46:25+00:00</updated><published>2024-01-03T11:46:25+00:00</published><title>Integrating Azure CosmosDB as vectorstore for RetrievalQA?</title></entry><entry><author><name>/u/qiu2022</name><uri>https://www.reddit.com/user/qiu2022</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18xiya0/asisitsos_videopitch_for_entrepreneurs/&quot;&gt; &lt;img src=&quot;https://external-preview.redd.it/Ubf_TugoXEgCSq-Z4Z04PUNpjGk_HyWbdto1Maa1Q0c.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c1e7d9e952a0d506d56f7d94ff97f3b1a7d852e1&quot; alt=&quot;AsisitsOS VideoPitch for Entrepreneurs&quot; title=&quot;AsisitsOS VideoPitch for Entrepreneurs&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/qiu2022&quot;&gt; /u/qiu2022 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=HgNlae3dHQk&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18xiya0/asisitsos_videopitch_for_entrepreneurs/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_18xiya0</id><media:thumbnail url="https://external-preview.redd.it/Ubf_TugoXEgCSq-Z4Z04PUNpjGk_HyWbdto1Maa1Q0c.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=c1e7d9e952a0d506d56f7d94ff97f3b1a7d852e1" /><link href="https://www.reddit.com/r/LangChain/comments/18xiya0/asisitsos_videopitch_for_entrepreneurs/" /><updated>2024-01-03T13:04:44+00:00</updated><published>2024-01-03T13:04:44+00:00</published><title>AsisitsOS VideoPitch for Entrepreneurs</title></entry><entry><author><name>/u/theSavviestTechDude</name><uri>https://www.reddit.com/user/theSavviestTechDude</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18xbb2u/anyone_experiencing_slow_openai/&quot;&gt; &lt;img src=&quot;https://b.thumbs.redditmedia.com/5KuqG5BfN6KBDYAvKSHoMnPM3Tat12K4cLo1hMXo06c.jpg&quot; alt=&quot;Anyone experiencing slow OpenAI responses/completions?&quot; title=&quot;Anyone experiencing slow OpenAI responses/completions?&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I was trying out some LCEL chains on Jupyter Notebook, and all of a sudden, I realized that now my chat completions are taking four times longer than they were yesterday.&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/4ac8dgews5ac1.png?width=737&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=84d5115a4d74a73779b9a2deb4f2c12ffd5ffad1&quot;&gt;https://preview.redd.it/4ac8dgews5ac1.png?width=737&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=84d5115a4d74a73779b9a2deb4f2c12ffd5ffad1&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/theSavviestTechDude&quot;&gt; /u/theSavviestTechDude &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18xbb2u/anyone_experiencing_slow_openai/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18xbb2u/anyone_experiencing_slow_openai/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_18xbb2u</id><media:thumbnail url="https://b.thumbs.redditmedia.com/5KuqG5BfN6KBDYAvKSHoMnPM3Tat12K4cLo1hMXo06c.jpg" /><link href="https://www.reddit.com/r/LangChain/comments/18xbb2u/anyone_experiencing_slow_openai/" /><updated>2024-01-03T05:15:45+00:00</updated><published>2024-01-03T05:15:45+00:00</published><title>Anyone experiencing slow OpenAI responses/completions?</title></entry><entry><author><name>/u/PrestigiousOne1253</name><uri>https://www.reddit.com/user/PrestigiousOne1253</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I am using langchain VectorStore (currently QDrant and plain to use OpenSearch in future). &lt;/p&gt; &lt;p&gt;It works fine, but I have issues. If I run addDocument for the same doc twice - I have two copies of the doc in store. &lt;/p&gt; &lt;p&gt;Of course I can search for doc with a give is in qdrant before adding it (transactions?). But I wonder if there is standard way to ensure uniqueness? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/PrestigiousOne1253&quot;&gt; /u/PrestigiousOne1253 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18xidx1/vectorstoreadddocuments_do_not_add_duplicates/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18xidx1/vectorstoreadddocuments_do_not_add_duplicates/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18xidx1</id><link href="https://www.reddit.com/r/LangChain/comments/18xidx1/vectorstoreadddocuments_do_not_add_duplicates/" /><updated>2024-01-03T12:34:19+00:00</updated><published>2024-01-03T12:34:19+00:00</published><title>VectorStore.addDocuments - do not add duplicates</title></entry><entry><author><name>/u/nouskiski</name><uri>https://www.reddit.com/user/nouskiski</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;ol&gt; &lt;li&gt;&amp;#x200B;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;There&amp;#39;s a **kwargs parameter in initialize_agent for additional keywords arguments for you agent. When i ask the LangChain chat which keywords are available for each agent, it doesn&amp;#39;t even know. Where can i find more detailed documentation for each agent? Is want to know which keywords are available for each agent, or am i misunderstanding? Thanks.&lt;/p&gt; &lt;p&gt;2.&lt;/p&gt; &lt;p&gt;Is langchain even worth it? Maybe it&amp;#39;s just me, but i think the documentation is very bad, but is there even a better alternative?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/nouskiski&quot;&gt; /u/nouskiski &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18x4w1f/kwargs_in_agents/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18x4w1f/kwargs_in_agents/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18x4w1f</id><link href="https://www.reddit.com/r/LangChain/comments/18x4w1f/kwargs_in_agents/" /><updated>2024-01-03T00:11:15+00:00</updated><published>2024-01-03T00:11:15+00:00</published><title>*kwargs in Agents</title></entry><entry><author><name>/u/Ill_Bodybuilder3499</name><uri>https://www.reddit.com/user/Ill_Bodybuilder3499</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;i have built a RAG app with Langchain locally. Now I want to host it in the cloud for a little demo showcase (less than 5 users), so that others can try out how well it is working. &lt;/p&gt; &lt;p&gt;What would be a cheap and efficient way to host a Langchain App in the cloud?&lt;/p&gt; &lt;p&gt;Fyi: I have used a quantized Mixtral 8x7b model (less rhan 30GB RAM required). Alternatively I can switch to smaller models like a quantized Mistral 7B model.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Ill_Bodybuilder3499&quot;&gt; /u/Ill_Bodybuilder3499 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18wzh73/rag_demo_cheapest_way_to_host/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18wzh73/rag_demo_cheapest_way_to_host/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18wzh73</id><link href="https://www.reddit.com/r/LangChain/comments/18wzh73/rag_demo_cheapest_way_to_host/" /><updated>2024-01-02T20:31:36+00:00</updated><published>2024-01-02T20:31:36+00:00</published><title>Rag Demo - Cheapest way to host</title></entry><entry><author><name>/u/Background-Maybe-381</name><uri>https://www.reddit.com/user/Background-Maybe-381</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hello people,&lt;/p&gt; &lt;p&gt;We&amp;#39;ve been at it for a long time and still are not understanding how to get a correct output from any model. So what we&amp;#39;re having problems understanding is how langchain expects the prompt template to be formated. THe thought flow. We are trying different Thought, Action, Action Input and Observation, Final Answer, etc.. methods and none seem to work with tools and regular conversations (and a mix of these).&lt;/p&gt; &lt;p&gt;What we really need is a manual to read that talks about how these keywords are supposed to be used. For example, which ones are important for the agent and which for the llm.&lt;/p&gt; &lt;p&gt;We&amp;#39;ve tried so many templates. Some stop responding at the Observation point, some include the whole thought process in the response, some don&amp;#39;t work at all. We are god at reading and understanding. We just need an official source to read from. If there is none, can someone help us here ?&lt;/p&gt; &lt;p&gt;Here is an example of our prompt template right now, but stalls :&lt;/p&gt; &lt;p&gt;&amp;lt;s&amp;gt; Today is {date}. Your name is Betty. You work for Shopping321 in Spain. Your duties involve helping our customer named {name} manage his online purchases.&lt;/p&gt; &lt;p&gt;TOOLS:&lt;/p&gt; &lt;p&gt;----&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;You have access to the following tools:&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;(LIST OF TOOLS I DELETED FOR PRIVACY, but they are working)&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;THOUGHT PROCESS:&lt;/p&gt; &lt;p&gt;----&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;Start your thought process using the following format:&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;Thought: WhatÂ´s the nature of the request? Does it involve using a specific tool?&lt;/p&gt; &lt;p&gt;Process: Understand the user&amp;#39;s request and determine if you need to use a tool or various tools.&lt;/p&gt; &lt;p&gt;Temporary Observation: Initial thoughts on how to proceed.&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;If you need to use a tool, use the following thought process:&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;Thought: Do I need to use a tool? Yes.&lt;/p&gt; &lt;p&gt;Action: [Action to take, choose from tool list] like this example: &amp;quot;InvoiceSearch&amp;quot;&lt;/p&gt; &lt;p&gt;Action Input: the input for the action and {request_id}&lt;/p&gt; &lt;p&gt;Observation: the result of the action.&lt;/p&gt; &lt;p&gt;... (this Thought/Action/Action Input/Observation can repeat only N times)&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;If you do not need to use a tool, use the following thought process:&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;Thought: How can I respond with my own knowledge without using a tool?&lt;/p&gt; &lt;p&gt;Process: Handle the request using natural conversation, applying the abstraction&lt;/p&gt; &lt;p&gt;Temporary Observation: Observations about the handling process&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;CONCLUSIONS:&lt;/p&gt; &lt;p&gt;----&lt;/p&gt; &lt;p&gt;Before giving the final response, gather all of your thoughts and observations made with and without tool usage. Order the thoughts aligned with the user&amp;#39;s requests. Then prepare to give a final response following this thought process:&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;Thought: Does the response fullfil the user&amp;#39;s request based on all of the observations gathered thus far?&lt;/p&gt; &lt;p&gt;Process: Make final adjustments in order to fullfil user&amp;#39;s requests. Use more tools if necessary.&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;Betty: Deliver the comprehensive final response.&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;Begin!&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;Previous conversation history:&lt;/p&gt; &lt;p&gt;{chat_history}&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;Request ID: {request_id}&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;{agent_scratchpad}&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;[INST] {input} [/INST] &amp;lt;/s&amp;gt;&lt;/p&gt; &lt;p&gt;I am willing to pay via paypal for any personal assistance. Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Background-Maybe-381&quot;&gt; /u/Background-Maybe-381 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18x356k/struggling_understanding_conversationalchatagent/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18x356k/struggling_understanding_conversationalchatagent/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18x356k</id><link href="https://www.reddit.com/r/LangChain/comments/18x356k/struggling_understanding_conversationalchatagent/" /><updated>2024-01-02T22:58:09+00:00</updated><published>2024-01-02T22:58:09+00:00</published><title>Struggling understanding conversational-chat-agent and prompt template</title></entry><entry><author><name>/u/phicreative1997</name><uri>https://www.reddit.com/user/phicreative1997</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Learning about this quite extensively for over a month. Have always known that Llamaindex also exist and has the same purpose. &lt;/p&gt; &lt;p&gt;In which usecases or tools has Llamaindex been better or and vice versa for LangChain&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/phicreative1997&quot;&gt; /u/phicreative1997 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18wooh1/in_your_experience_for_which_usecases_is/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18wooh1/in_your_experience_for_which_usecases_is/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18wooh1</id><link href="https://www.reddit.com/r/LangChain/comments/18wooh1/in_your_experience_for_which_usecases_is/" /><updated>2024-01-02T12:49:20+00:00</updated><published>2024-01-02T12:49:20+00:00</published><title>In your experience for which usecases is LangChain better and for which LlamaIndex</title></entry><entry><author><name>/u/NetIcy6229</name><uri>https://www.reddit.com/user/NetIcy6229</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;When I use Postman to query the Hubspot endpoint: &lt;a href=&quot;https://api.hubapi.com/crm/v3/properties/contacts&quot;&gt;https://api.hubapi.com/crm/v3/properties/contacts&lt;/a&gt; using the authorisation below (where X is of course the censored token):&lt;/p&gt; &lt;pre&gt;&lt;code&gt; headers = { &amp;#39;Authorization&amp;#39;: &amp;#39;Bearer X&amp;#39; } &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;I get a working API response.&lt;/p&gt; &lt;p&gt;However, when I try to pass the above as part of natural language instructions to Langchain&amp;#39;s API Chain (see code below), authentication fails.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;HubSpotDocs = &amp;quot;&amp;quot;&amp;quot; The below is Hubspot API documentation showing how to create a contact. The endpoint is: &amp;quot;https://api.hubapi.com/crm/v3/properties/contacts&amp;quot;. ///Example request body to create a new contact { &amp;quot;properties&amp;quot;: { &amp;quot;email&amp;quot;: &amp;quot;example@hubspot.com&amp;quot;, &amp;quot;firstname&amp;quot;: &amp;quot;Jane&amp;quot;, &amp;quot;lastname&amp;quot;: &amp;quot;Doe&amp;quot;, &amp;quot;phone&amp;quot;: &amp;quot;(555) 555-5555&amp;quot;, &amp;quot;company&amp;quot;: &amp;quot;HubSpot&amp;quot;, &amp;quot;website&amp;quot;: &amp;quot;hubspot.com&amp;quot;, &amp;quot;lifecyclestage&amp;quot;: &amp;quot;marketingqualifiedlead&amp;quot; } } Use the below authorisation: headers = { &amp;#39;Authorization&amp;#39;: &amp;#39;Bearer X&amp;#39; } &amp;quot;&amp;quot;&amp;quot; llm = ChatOpenAI(temperature=0, model= &amp;#39;gpt-3.5-turbo-1106&amp;#39;, openai_api_key=&amp;quot;Y&amp;quot;) Test = APIChain.from_llm_and_api_docs(llm, HubSpotDocs,limit_to_domains=None, verbose=True) Test.run(&amp;quot;Create a new contact named Sara with email sara@google.com. She resides in Toronto.&amp;quot;) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The LLM responds with:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;&amp;gt; Entering new APIChain chain... https://api.hubapi.com/crm/v3/properties/contacts?properties={&amp;quot;email&amp;quot;:&amp;quot;sara@google.com&amp;quot;,&amp;quot;firstname&amp;quot;:&amp;quot;Sara&amp;quot;,&amp;quot;city&amp;quot;:&amp;quot;Toronto&amp;quot;} {&amp;quot;status&amp;quot;:&amp;quot;error&amp;quot;,&amp;quot;message&amp;quot;:&amp;quot;Authentication credentials not found. This API supports OAuth 2.0 authentication and you can find more details at https://developers.hubspot.com/docs/methods/auth/oauth-overview&amp;quot;,&amp;quot;correlationId&amp;quot;:&amp;quot;3a41a5c5-dbe9-4b19-8cc6-d6293290a52b&amp;quot;,&amp;quot;category&amp;quot;:&amp;quot;INVALID_AUTHENTICATION&amp;quot;} &amp;gt; Finished chain. &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Clearly, the LLM is understanding the ask correctly as it maps e.g. &amp;quot;Toronto&amp;quot; to city. However, the authorisation is failing (even though it works in Postman). This means that the issue isn&amp;#39;t the authentication but more so the way Langchain/the LLM interprets (then passes to Hubspot) authorisation. How can I fix this? I am facing a similar issue with attempting to call the Google Calendar API using APIChain method/natural language.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/NetIcy6229&quot;&gt; /u/NetIcy6229 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18wxfd2/calling_api_doesnt_work_when_using_langchain/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18wxfd2/calling_api_doesnt_work_when_using_langchain/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18wxfd2</id><link href="https://www.reddit.com/r/LangChain/comments/18wxfd2/calling_api_doesnt_work_when_using_langchain/" /><updated>2024-01-02T19:10:15+00:00</updated><published>2024-01-02T19:10:15+00:00</published><title>Calling API doesn't work when using Langchain</title></entry><entry><author><name>/u/Spare_Cancel3205</name><uri>https://www.reddit.com/user/Spare_Cancel3205</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;What are some of the trending tools that companies are looking for in the interns in the field of AI?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Spare_Cancel3205&quot;&gt; /u/Spare_Cancel3205 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18wtxcy/trening_tools/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18wtxcy/trening_tools/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18wtxcy</id><link href="https://www.reddit.com/r/LangChain/comments/18wtxcy/trening_tools/" /><updated>2024-01-02T16:50:43+00:00</updated><published>2024-01-02T16:50:43+00:00</published><title>Trening tools</title></entry><entry><author><name>/u/smileymileycoin</name><uri>https://www.reddit.com/user/smileymileycoin</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/smileymileycoin&quot;&gt; /u/smileymileycoin &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.secondstate.io/articles/mixtral-8-7b/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18wjpck/easy_setup_selfhost_mixtral8x7b_across_devices/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18wjpck</id><link href="https://www.reddit.com/r/LangChain/comments/18wjpck/easy_setup_selfhost_mixtral8x7b_across_devices/" /><updated>2024-01-02T07:30:03+00:00</updated><published>2024-01-02T07:30:03+00:00</published><title>Easy Setup! Self-host Mixtral-8x7B across devices with a 2M inference app</title></entry></feed>