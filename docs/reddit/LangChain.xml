<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/"><category term="LangChain" label="r/LangChain"/><updated>2024-07-29T21:19:50+00:00</updated><icon>https://www.redditstatic.com/icon.png/</icon><id>/r/LangChain.rss</id><link rel="self" href="https://www.reddit.com/r/LangChain.rss" type="application/atom+xml" /><link rel="alternate" href="https://www.reddit.com/r/LangChain" type="text/html" /><subtitle>LangChain is an open-source framework and developer toolkit that helps developers get LLM applications from prototype to production. It is available for Python and Javascript at https://www.langchain.com/.</subtitle><title>LangChain</title><entry><author><name>/u/zchaarm</name><uri>https://www.reddit.com/user/zchaarm</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;A place for members of &lt;a href=&quot;/r/LangChain&quot;&gt;r/LangChain&lt;/a&gt; to chat with each other&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/zchaarm&quot;&gt; /u/zchaarm &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_10ljho9</id><link href="https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/" /><updated>2023-01-26T04:33:05+00:00</updated><published>2023-01-26T04:33:05+00:00</published><title>r/LangChain Lounge</title></entry><entry><author><name>/u/neilkatz</name><uri>https://www.reddit.com/user/neilkatz</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ef12q6/the_rag_engineers_guide_to_document_parsing/&quot;&gt; &lt;img src=&quot;https://external-preview.redd.it/dMnqh7zskubdEjd4AqGra79-CfzcacpPLEPtlfKOTak.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b61c1baeab07db5c51a79199868d4f269f7650ce&quot; alt=&quot;The RAG Engineer's Guide to Document Parsing&quot; title=&quot;The RAG Engineer's Guide to Document Parsing&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi Group,&lt;/p&gt; &lt;p&gt;I made a post with my buddy Daniel Warfield breaking down why parsing matters so much for RAG and comparing some of the different approaches based on our experience working with Air France, Dartmouth a big online publisher and dozens of other projects with real data&lt;/p&gt; &lt;p&gt;For full transparency, one of the products discussed comes from my firm &lt;a href=&quot;http://EyeLevel.ai&quot;&gt;EyeLevel.ai&lt;/a&gt;, but that&amp;#39;s not the focus. It&amp;#39;s a discussion of how we can all build better RAG on the kind of complex docs we see in the real world.&lt;/p&gt; &lt;p&gt;You can watch it on YT if you prefer... &lt;a href=&quot;https://www.youtube.com/watch?v=7Vv64f1yI0I&quot;&gt;https://www.youtube.com/watch?v=7Vv64f1yI0I&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;The Foundation of RAG: Document Parsing&lt;/h1&gt; &lt;p&gt;Let&amp;#39;s start with a fundamental truth: parsing is the bedrock of any RAG application. &lt;/p&gt; &lt;p&gt;&amp;quot;The first step in any RAG application is parsing your document and extracting the information from it,&amp;quot; says EyeLevel cofounder Neil Katz. &amp;quot;You’re trying to turn it into something that language models will eventually understand and do something smart with.&amp;quot;&lt;/p&gt; &lt;p&gt;This isn&amp;#39;t just about extracting text. It&amp;#39;s about preserving structure, context, and relationships within the data. Get this wrong, and your entire RAG pipeline suffers. If you don&amp;#39;t get the information out of your giant set of documents in the first place, which is often where RAG starts, it&amp;#39;s “garbage in and garbage out” and nothing else will work properly.&lt;/p&gt; &lt;h1&gt;The Heart of the Problem&lt;/h1&gt; &lt;p&gt;The basic problem to solve is that language models, at least for now, don&amp;#39;t understand complex visual documents. Anything with tables, forms, graphics, charts, figures and complex formatting will cause downstream hallucinations in a RAG application. Yes you can take a page from a PDF and feed it into ChatGPT and it will understand some of it, sometimes most of it. But try doing this at scale with thousands or millions of pages and you&amp;#39;ve got a mess and eventually downstream hallucinations for your RAG.&lt;/p&gt; &lt;p&gt;So devs need some way of breaking complex documents apart, identifying the text blocks, the tables, the charts and so on, then extracting the information from those positions and converting it into something language models will understand and that you can store in your RAG database. This final output is usually simple text or JSON.&lt;/p&gt; &lt;p&gt;This problem isn&amp;#39;t new btw. There are entire industries devoted to ingesting medical bills, restaurant receipts and so on. That&amp;#39;s typically done with a vision model fine tuned to a very specific set of documents. The model for receipts isn&amp;#39;t good at medical bills. And vice versa.&lt;/p&gt; &lt;p&gt;The new twist is RAG often deals with a highly varied set of content. A legal RAG, for example, might need to understand police reports, medical bills and insurance claims. The second twist is the information needs to be converted into LLM ready data.&lt;/p&gt; &lt;p&gt;So let&amp;#39;s talk about what&amp;#39;s out there.&lt;/p&gt; &lt;h1&gt;Parsing Strategies: Breakdown of Approaches&lt;/h1&gt; &lt;p&gt;Let&amp;#39;s examine some common parsing strategies, their strengths, and their limitations using an example of a medical document showcasing exam dates and fees in a table:&lt;/p&gt; &lt;h1&gt;1. PyPDF&lt;/h1&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/lizhsf8twgfd1.png?width=960&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f8a3fe90f3725c9751fd370dc885fa7d4a4f147b&quot;&gt;Image: PyPDF results showing minimal information extracted from the table in the medical document.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://pypdf.readthedocs.io/en/stable/index.html&quot;&gt;PyPDF&lt;/a&gt; is a longstanding Python library designed for reading and manipulating PDF files. It can be effective for basic text extraction from simple PDFs, but often struggles with complex layouts, tables, and formatted text. &lt;/p&gt; &lt;p&gt;PyPDF is best suited for straightforward, text-heavy documents but may lose critical structural information in more intricate PDFs. It doesn&amp;#39;t process visual objects like images, charts, graphs and figures.&lt;/p&gt; &lt;h1&gt;2. Tesseract (OCR)&lt;/h1&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/ofa0pkawwgfd1.png?width=960&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a9a47fa3defa74939b453236fec4aee0de6dfea4&quot;&gt;Image: Tesseract results showing information extracted from the table in the medical document.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/tesseract-ocr/tesseract&quot;&gt;Tesseract&lt;/a&gt; is an open-source optical character recognition (OCR) engine that can extract text from images and scanned documents. Best known for converting image-based text to machine-readable format, Tesseract can struggle with maintaining document structure, especially in complex layouts or tables. &lt;/p&gt; &lt;p&gt;It&amp;#39;s particularly useful for scanned documents but may require additional post-processing to preserve formatting and structure. Tesseract also doesn&amp;#39;t process visual objects like images, charts, graphs and figures.&lt;/p&gt; &lt;h1&gt;3. Unstructured&lt;/h1&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/yz8gnwvzwgfd1.png?width=960&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a39380072b5beb48547179c463489bd5353ca14d&quot;&gt;Image: Unstructured results showing rich information extracted from the table in the medical document.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://unstructured.io/&quot;&gt;Unstructured&lt;/a&gt; is a modern document parsing library that aims to handle a wide variety of document types and formats. It employs a combination of techniques to extract and structure information from documents, including text extraction, table detection, and layout analysis. &lt;/p&gt; &lt;p&gt;While more robust than traditional parsing tools, Unstructured can still face challenges with highly complex or non-standard document formats. Like the others, it doesn&amp;#39;t process visual objects like images, charts, graphs and figures.&lt;/p&gt; &lt;h1&gt;4. LlamaParse&lt;/h1&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/8ev781z1xgfd1.png?width=960&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2db69039e0567ebe6efb69be07aff2ffec71437e&quot;&gt;Image: LlamaParse results showing a markdown table of information extracted from the table in the medical document.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://docs.llamaindex.ai/en/stable/llama_cloud/llama_parse/&quot;&gt;LlamaParse&lt;/a&gt; is a newer parsing solution developed by the team behind &lt;a href=&quot;https://www.llamaindex.ai/&quot;&gt;LlamaIndex&lt;/a&gt;. It&amp;#39;s designed to handle complex document structures, including tables and formatted text, and outputs results in a markdown format that&amp;#39;s easily interpretable by language models. &lt;/p&gt; &lt;p&gt;It has been seen to preserve document structure and handle tables, though it&amp;#39;s a relatively new tool and its full capabilities and limitations are still being explored in real-world applications.&lt;/p&gt; &lt;h1&gt;5. X-Ray by &lt;a href=&quot;http://EyeLevel.ai&quot;&gt;&lt;strong&gt;EyeLevel.ai&lt;/strong&gt;&lt;/a&gt;&lt;/h1&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/a14hfii3xgfd1.png?width=1942&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=37c7ae239e136201836e3fb44ec33331cf7a92d7&quot;&gt;Image: X-Ray by EyeLevel.ai converts a complex medical bill into clean JSON chunks with both narrative description and data that LLMs prefer&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://www.eyelevel.ai/xray&quot;&gt;X-Ray&lt;/a&gt;, powered by EyeLevel’s &lt;a href=&quot;https://www.eyelevel.ai/product/apis&quot;&gt;GroundX APIs&lt;/a&gt;, takes a multimodal approach to parsing with industry leading results, especially when parsing complex visuals including charts, graphics and figures. X-Ray is far more than just a table parser.&lt;/p&gt; &lt;p&gt;The X-Ray technology starts with a fine-tuned vision model trained on a million pages of enterprise documents from a wide cross section of industries including health, financial, insurance, legal and government. The system uses the vision model to identify various objects on the page: text blocks, tables, charts and so on. Once the coordinates are known, it extracts the information, chunks it and sends it to different pipelines to be turned into LLM ready data.&lt;/p&gt; &lt;p&gt;The result is a JSON-like output that includes the core data, chunk summary, doc summary, keywords and other metadata, providing richer context for language models. X-Ray is available in a demo format for developers to try for themselves, where they can upload a document to the system and see the semantic objects that are created to translate complex visuals to the LLM. &lt;a href=&quot;https://www.eyelevel.ai/xray&quot;&gt;&lt;strong&gt;You can try X-Ray here&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt; &lt;h1&gt;Performance Impact: The Parsing Difference&lt;/h1&gt; &lt;p&gt;Our tests, along with academic research, show that parsing strategy can significantly impact RAG performance. &lt;/p&gt; &lt;p&gt;We&amp;#39;re talking about substantial gains, as Daniel Warfield, co-host of RAG Masters points out:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;&amp;quot;For some examples, there&amp;#39;s a 10%, even a 20% difference in performance.&amp;quot;&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;This is crucial when you consider the effort that goes into other optimization strategies:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;&amp;quot;People are doing crazy advanced strategies for the difference in 5, 6, 7, even 10 percent performance. And then maybe just completely switching the parser might get you a massive performance increase.&amp;quot;&lt;/p&gt; &lt;/blockquote&gt; &lt;h1&gt;Error Analysis: Common Parsing Pitfalls&lt;/h1&gt; &lt;p&gt;Let&amp;#39;s examine some common parsing errors and their downstream effects:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Table Misinterpretation:&lt;/strong&gt; When parsers fail to correctly identify table structures, it can lead to data being treated as unstructured text. This can result in incorrect answers in question-answering tasks, especially for queries about tabular data.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Loss of Formatting:&lt;/strong&gt; If a document structure isn&amp;#39;t well understood, a text scrape could scramble the pieces up. A header could wind up in body copy. A column label could wind up in the rows of data. You get the parsing equivalent of scrambled eggs.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Image Handling:&lt;/strong&gt; Most parsers struggle with embedded images or diagrams, either ignoring them completely or misinterpreting them as text through OCR.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Header/Footer Confusion:&lt;/strong&gt; Parsers might incorrectly include headers and footers as part of the main content, potentially skewing the context of the extracted information.&lt;/li&gt; &lt;/ol&gt; &lt;h1&gt;Developing Custom Parsing Strategies&lt;/h1&gt; &lt;p&gt;For developers dealing with specific document types or domains, developing custom parsing strategies can be beneficial. Here are some approaches:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Combining Existing Tools:&lt;/strong&gt; Use multiple parsing tools in tandem, leveraging the strengths of each for different parts of your documents.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Regular Expressions:&lt;/strong&gt; Implement custom regex patterns to extract specific types of information consistently found in your documents.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Domain-Specific Rules:&lt;/strong&gt; Incorporate rules based on domain knowledge to improve parsing accuracy for specialized documents.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Machine Learning Augmentation:&lt;/strong&gt; Train models to recognize and extract specific patterns or structures in your documents.&lt;/li&gt; &lt;/ol&gt; &lt;h1&gt;Integration Challenges&lt;/h1&gt; &lt;p&gt;When integrating parsing strategies into existing RAG pipelines, developers often face several challenges:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;API Compatibility:&lt;/strong&gt; Ensure that the chosen parsing strategy can be easily integrated with your existing codebase and infrastructure.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Data Format Consistency:&lt;/strong&gt; The output of your parser should be in a format that&amp;#39;s compatible with the rest of your RAG pipeline, often requiring additional preprocessing steps.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Scalability:&lt;/strong&gt; Consider the computational resources required by different parsing strategies, especially when dealing with large document sets.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Error Handling:&lt;/strong&gt; Implement robust error handling to deal with parsing failures or unexpected document formats.&lt;/li&gt; &lt;/ol&gt; &lt;h1&gt;Best Practices for Selecting a Parsing Strategy&lt;/h1&gt; &lt;p&gt;It’s recommend to take a two-pronged approach to selecting the right parsing strategy:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;1. Visual Inspection:&lt;/strong&gt; Start by running your documents through different parsers and examining the output. As Warfield advises:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;&amp;quot;Pass your data through a bunch of parsers and look at them. Your brain is still the most powerful model that exists.&amp;quot;&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;&lt;strong&gt;2. End-to-End Testing:&lt;/strong&gt; Once you&amp;#39;ve narrowed down your options, conduct thorough end-to-end testing. This means running your entire RAG pipeline with different parsing strategies and evaluating the final output.&lt;/p&gt; &lt;p&gt;To quantitatively compare parsing strategies, consider the following metrics:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Accuracy in table and graphical extraction&lt;/li&gt; &lt;li&gt;Preservation of document structure&lt;/li&gt; &lt;li&gt;Abiliity to turn extractions into LLM friendly data&lt;/li&gt; &lt;li&gt;Speed of parsing&lt;/li&gt; &lt;li&gt;Consistency across different document types&lt;/li&gt; &lt;li&gt;Ability to handle complex formatting&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;The Challenge of Evaluation&lt;/h1&gt; &lt;p&gt;Here&amp;#39;s the rub: evaluating parsing quality is still a largely manual process. Creating question-answer pairs for evaluation is labor-intensive but crucial for building automated tooling. The need for human evaluation in parsing cannot be completely eliminated, at least not yet.&lt;/p&gt; &lt;p&gt;This presents a significant opportunity in the field, and this post will be updated in the future when a sufficiently advanced solution for automated parsing is discovered.&lt;/p&gt; &lt;h1&gt;Conclusion&lt;/h1&gt; &lt;p&gt;As we continue to push the boundaries of what&amp;#39;s possible with RAG applications, it&amp;#39;s clear that document parsing will remain a critical component. The field is ripe for innovation, particularly in parsing technology and evaluation methods.&lt;/p&gt; &lt;p&gt;For developers building RAG applications, it’s critical not to overlook the importance of parsing. Take the time to evaluate different parsing strategies and their impact on your specific use case. It could be the difference between a RAG system that merely functions and one that excels.&lt;/p&gt; &lt;p&gt;Remember, in the world of RAG, your system is only as good as the data you feed it. And that all starts with parsing.&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=7Vv64f1yI0I&quot;&gt;You can watch the full episode of RAG Masters here.&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/neilkatz&quot;&gt; /u/neilkatz &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ef12q6/the_rag_engineers_guide_to_document_parsing/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ef12q6/the_rag_engineers_guide_to_document_parsing/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1ef12q6</id><media:thumbnail url="https://external-preview.redd.it/dMnqh7zskubdEjd4AqGra79-CfzcacpPLEPtlfKOTak.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=b61c1baeab07db5c51a79199868d4f269f7650ce" /><link href="https://www.reddit.com/r/LangChain/comments/1ef12q6/the_rag_engineers_guide_to_document_parsing/" /><updated>2024-07-29T14:34:16+00:00</updated><published>2024-07-29T14:34:16+00:00</published><title>The RAG Engineer's Guide to Document Parsing</title></entry><entry><author><name>/u/dhj9817</name><uri>https://www.reddit.com/user/dhj9817</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I wanted to share what I built with this community to see what you guys think. I&amp;#39;m curious about any use cases you might have or just general feedback.&lt;/p&gt; &lt;p&gt;I created TradDocs with a simple mission: to make document extraction as painless as possible. I know firsthand how much time and effort can go into pre-training and labeling, and I wanted to build a tool that lets you focus on what really matters -&amp;gt; building and coding.&lt;/p&gt; &lt;p&gt;With TradDocs, you can:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Extract data from any document types with minimal setup.&lt;/li&gt; &lt;li&gt;Customize the JSON format you receive as a response.&lt;/li&gt; &lt;li&gt;Save loads of time on tedious pre-training tasks.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Check out our beta here: &lt;a href=&quot;https://www.traddocs.com/&quot;&gt;https://www.traddocs.com&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;For those who prefer not to click on unknown links, here’s our YouTube demo video: &lt;a href=&quot;https://youtu.be/LdCC0uBQ-QE&quot;&gt;https://youtu.be/LdCC0uBQ-QE&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;It’s free to use during this beta phase. After that, I&amp;#39;m considering pricing it at $0.014 for the splitter and $0.075 for the extractor. I’d love to hear your feedback on this.&lt;/p&gt; &lt;p&gt;Using TradDocs is very simple:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Specify the types of documents you&amp;#39;d like to extract.&lt;/li&gt; &lt;li&gt;Enter the desired JSON format for the response.&lt;/li&gt; &lt;li&gt;Upload your document and receive the data you need!&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;I’m personally available to answer any questions or help you get started. You can DM me on Reddit or chat with me on Discord: &lt;a href=&quot;https://discord.gg/xgEXkh7Rxk&quot;&gt;https://discord.gg/xgEXkh7Rxk&lt;/a&gt;. I’d love to hear what you think and how we can make TradDocs even better.&lt;/p&gt; &lt;p&gt;Looking forward to your thoughts and feedback!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/dhj9817&quot;&gt; /u/dhj9817 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ef5f7t/i_built_a_document_parser_that_works_without/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ef5f7t/i_built_a_document_parser_that_works_without/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1ef5f7t</id><link href="https://www.reddit.com/r/LangChain/comments/1ef5f7t/i_built_a_document_parser_that_works_without/" /><updated>2024-07-29T17:29:28+00:00</updated><published>2024-07-29T17:29:28+00:00</published><title>I built a document parser that works without pre-training, unlike google document ai or azure document intelligence. Would love your feedback!</title></entry><entry><author><name>/u/blacktrepreneur</name><uri>https://www.reddit.com/user/blacktrepreneur</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Ok am I crazy or do the docs and examples show multiple ways of doing the same thing. As a noobie it&amp;#39;s confusing as hell and slowly me down. Yeah skill issue, i&amp;#39;m just a web dev hobbyist venturing into the world of agentic flows. &lt;/p&gt; &lt;p&gt;Let&amp;#39;s take creating a tool.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;You can use the `tool` method&lt;/li&gt; &lt;li&gt;You can use `new DynamicStructuredTool `&lt;/li&gt; &lt;li&gt;You can use `new DynamicTool`&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I get that there&amp;#39;s nuances between when to use which, but the docs and examples end up using different implementations through the docs which is super confusing. And bouncing between examples I feel like I have to pay attention more carefully for these different implementations to make sure they don&amp;#39;t throw me off back to looking at the docs (which are not easy to navigate). &lt;/p&gt; &lt;p&gt;If you&amp;#39;re going to switch something on the user mid way through an example, please show a comment what the alternative way is and stick the the original way you started with.&lt;/p&gt; &lt;p&gt;I feel like this happens all over the place. A simple example is the use of `new HumanMessage` and just using `[user, query]`. &lt;/p&gt; &lt;p&gt;These example end up importing way too much stuff that&amp;#39;s not necessary. What&amp;#39;s the point of the Message helpers anyways? Are we expecting the convention to change of working with system, assistant, and user messages? The whole point of typescript is that we would get type safety for these values. Anyways I&amp;#39;ll end my slight rant here&lt;/p&gt; &lt;p&gt;People online complain about LC. I see its value, but it feels like trying to drive a stick shift car, except you have two stick shifts to use but you&amp;#39;re wondering why there are two of them and not one...&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/blacktrepreneur&quot;&gt; /u/blacktrepreneur &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1eerg3b/learning_langgraphlangchain_js_why_does_it_seem/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1eerg3b/learning_langgraphlangchain_js_why_does_it_seem/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1eerg3b</id><link href="https://www.reddit.com/r/LangChain/comments/1eerg3b/learning_langgraphlangchain_js_why_does_it_seem/" /><updated>2024-07-29T05:12:34+00:00</updated><published>2024-07-29T05:12:34+00:00</published><title>Learning LangGraph/LangChain JS... why does it seem like there's 5 different ways to do anything in LC? It's very frustrating. Other feedback</title></entry><entry><author><name>/u/god_fathr</name><uri>https://www.reddit.com/user/god_fathr</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I am using llama3 for a chatbot that can answer from CSV files. I&amp;#39;ve seen implementations with OpenAI, and I want to load local llama3 for the task. If you have any recommendations, please share.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/god_fathr&quot;&gt; /u/god_fathr &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ef841i/loading_local_llm_with_csv_agent_no_open_ai/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ef841i/loading_local_llm_with_csv_agent_no_open_ai/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1ef841i</id><link href="https://www.reddit.com/r/LangChain/comments/1ef841i/loading_local_llm_with_csv_agent_no_open_ai/" /><updated>2024-07-29T19:16:23+00:00</updated><published>2024-07-29T19:16:23+00:00</published><title>Loading local llm with csv_agent. No open AI</title></entry><entry><author><name>/u/Best_Sail5</name><uri>https://www.reddit.com/user/Best_Sail5</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hello everyone , I m using pydantic in combination with LM format enforcer to output a certain format ,the thing is that I would like the model to be able to output possibly multiple json not just one. The task is about retrieving a variable and naming it.But multiple elements can be retrieved.Is there any ways to do that?&lt;/p&gt; &lt;p&gt;Thanks&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Best_Sail5&quot;&gt; /u/Best_Sail5 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ef6qc6/pydantic/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ef6qc6/pydantic/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1ef6qc6</id><link href="https://www.reddit.com/r/LangChain/comments/1ef6qc6/pydantic/" /><updated>2024-07-29T18:20:55+00:00</updated><published>2024-07-29T18:20:55+00:00</published><title>Pydantic</title></entry><entry><author><name>/u/TopEmu1742</name><uri>https://www.reddit.com/user/TopEmu1742</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I am using LangSmith to evaluate my multimodal LLM runs using Gemini 1.5-Flash. My input is a question and a video file. I am having trouble getting the evaluator functions such as criteria to take in a custom prompt or even input video. Any suggestions how to achieve this?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/TopEmu1742&quot;&gt; /u/TopEmu1742 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ef4anh/how_to_pass_video_input_for_evaluation/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ef4anh/how_to_pass_video_input_for_evaluation/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1ef4anh</id><link href="https://www.reddit.com/r/LangChain/comments/1ef4anh/how_to_pass_video_input_for_evaluation/" /><updated>2024-07-29T16:44:10+00:00</updated><published>2024-07-29T16:44:10+00:00</published><title>How to pass video input for evaluation?</title></entry><entry><author><name>/u/newpeak</name><uri>https://www.reddit.com/user/newpeak</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1eexslv/multiway_retrieval_evaluations_based_on_the/&quot;&gt; &lt;img src=&quot;https://external-preview.redd.it/mXagjnUGU7dKQ8T2W7OscchqoUAB_B2OtV1ycLW83PU.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1420a4e1398cc3e2bba4e291a1c2945230c6e89d&quot; alt=&quot;Multi-way retrieval evaluations based on the Infinity database&quot; title=&quot;Multi-way retrieval evaluations based on the Infinity database&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/newpeak&quot;&gt; /u/newpeak &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://medium.com/@infiniflowai/multi-way-retrieval-evaluations-based-on-the-infinity-database-c3f06cafcf2e&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1eexslv/multiway_retrieval_evaluations_based_on_the/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1eexslv</id><media:thumbnail url="https://external-preview.redd.it/mXagjnUGU7dKQ8T2W7OscchqoUAB_B2OtV1ycLW83PU.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=1420a4e1398cc3e2bba4e291a1c2945230c6e89d" /><link href="https://www.reddit.com/r/LangChain/comments/1eexslv/multiway_retrieval_evaluations_based_on_the/" /><updated>2024-07-29T12:04:52+00:00</updated><published>2024-07-29T12:04:52+00:00</published><title>Multi-way retrieval evaluations based on the Infinity database</title></entry><entry><author><name>/u/Constant_Fun_5643</name><uri>https://www.reddit.com/user/Constant_Fun_5643</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey everyone, &lt;/p&gt; &lt;p&gt;I am working on an info extraction project where I am using LLM&amp;#39;s to extract information from the documents. The length of the documents that I am dealing with a rather short (3-5 pages), so I am not using RAG, but providing the whole document content in-context. For the information extraction, I am using function calling/tool usage as it ensures a structured response all the time. And this setup used to work for most cases where the information to extract where directly present in the document. &lt;/p&gt; &lt;p&gt;Now I have some scenarios where the information that needs to be extracted are not directly present in the document. From the content in the document, LLM have to do some reasoning to extract the required information. In this case, I am having some trouble in extracting information using function calling. &lt;/p&gt; &lt;p&gt;Anyone have any experience with similar problems? Any suggestion are highly appreciated. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Constant_Fun_5643&quot;&gt; /u/Constant_Fun_5643 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ef21fs/reasoning_and_info_extraction_using_function/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ef21fs/reasoning_and_info_extraction_using_function/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1ef21fs</id><link href="https://www.reddit.com/r/LangChain/comments/1ef21fs/reasoning_and_info_extraction_using_function/" /><updated>2024-07-29T15:13:15+00:00</updated><published>2024-07-29T15:13:15+00:00</published><title>Reasoning and Info Extraction using Function Calling</title></entry><entry><author><name>/u/Traditional_Swan_326</name><uri>https://www.reddit.com/user/Traditional_Swan_326</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ef1sk3/open_source_observability_for_langgraph_langfuse/&quot;&gt; &lt;img src=&quot;https://external-preview.redd.it/La1JEVrFwyCLsxiilFwX6llhR_ntrwVtfkeXhTmCnTQ.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e23d4b25a7cb30b7fe4d40da992efe0ff184051b&quot; alt=&quot;Open Source Observability for LangGraph - Langfuse&quot; title=&quot;Open Source Observability for LangGraph - Langfuse&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Traditional_Swan_326&quot;&gt; /u/Traditional_Swan_326 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://langfuse.com/docs/integrations/langchain/example-python-langgraph&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ef1sk3/open_source_observability_for_langgraph_langfuse/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1ef1sk3</id><media:thumbnail url="https://external-preview.redd.it/La1JEVrFwyCLsxiilFwX6llhR_ntrwVtfkeXhTmCnTQ.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=e23d4b25a7cb30b7fe4d40da992efe0ff184051b" /><link href="https://www.reddit.com/r/LangChain/comments/1ef1sk3/open_source_observability_for_langgraph_langfuse/" /><updated>2024-07-29T15:03:29+00:00</updated><published>2024-07-29T15:03:29+00:00</published><title>Open Source Observability for LangGraph - Langfuse</title></entry><entry><author><name>/u/jiraiya1729</name><uri>https://www.reddit.com/user/jiraiya1729</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Should I use Langchain with JavaScript or Python for my project? Which one would look better on my resume? Do you have any advice?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/jiraiya1729&quot;&gt; /u/jiraiya1729 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1eezbcn/langchain_in_python_or_javascript/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1eezbcn/langchain_in_python_or_javascript/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1eezbcn</id><link href="https://www.reddit.com/r/LangChain/comments/1eezbcn/langchain_in_python_or_javascript/" /><updated>2024-07-29T13:18:45+00:00</updated><published>2024-07-29T13:18:45+00:00</published><title>Langchain in python or javascript</title></entry><entry><author><name>/u/Vegetable_Force286</name><uri>https://www.reddit.com/user/Vegetable_Force286</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I just released my open-source project. I&amp;#39;d be grateful if you could take a look and give me some feedback.&lt;/p&gt; &lt;p&gt;Here&amp;#39;s the link: &lt;a href=&quot;https://github.com/LERM0/LermoAI&quot;&gt;https://github.com/LERM0/LermoAI&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Vegetable_Force286&quot;&gt; /u/Vegetable_Force286 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1eewm4s/ai_agent_for_personalized_learning/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1eewm4s/ai_agent_for_personalized_learning/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1eewm4s</id><link href="https://www.reddit.com/r/LangChain/comments/1eewm4s/ai_agent_for_personalized_learning/" /><updated>2024-07-29T10:59:53+00:00</updated><published>2024-07-29T10:59:53+00:00</published><title>AI Agent for Personalized Learning</title></entry><entry><author><name>/u/tech_enthusiast87</name><uri>https://www.reddit.com/user/tech_enthusiast87</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi everyone! I&amp;#39;m currently experimenting with RAG. I was very successful implementing a simple prototype and now I&amp;#39;m looking to improve and productionise this. It&amp;#39;s based on allow general questions on news articles from several sources. My challenge is how to handle general questions like &amp;quot;show me all topics&amp;quot; or &amp;quot;what is the sentiment towards X&amp;quot;. In order to do this, I assume the model should be aware of full context, which is not viable as the dataset has millions of articles from several years. Any advice where to start tackling this ? Tried to find some resources, but I couldn&amp;#39;t, probably due to lack of understanding how to &amp;quot;name&amp;quot; this problem &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/tech_enthusiast87&quot;&gt; /u/tech_enthusiast87 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1eeugjo/rag_openended_question/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1eeugjo/rag_openended_question/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1eeugjo</id><link href="https://www.reddit.com/r/LangChain/comments/1eeugjo/rag_openended_question/" /><updated>2024-07-29T08:35:10+00:00</updated><published>2024-07-29T08:35:10+00:00</published><title>RAG open-ended question</title></entry><entry><author><name>/u/Substantial_Look1421</name><uri>https://www.reddit.com/user/Substantial_Look1421</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I am trying to add memory to my script but I have litreally tried for 3 days but couldnt solve this error. My. hopes are going downstream&lt;/p&gt; &lt;pre&gt;&lt;code&gt;# Create retriever retriever = vectorstore.as_retriever( search_type = &amp;quot;mmr&amp;quot;, search_kwargs = {&amp;#39;k&amp;#39;: 3, &amp;#39;fetch_k&amp;#39;: 15, &amp;#39;lambda_mult&amp;#39;: 0.3} ) # Initialize ChatTogether model chat = ChatTogether( model = &amp;quot;meta-llama/Llama-3-8b-chat-hf&amp;quot;, temperature = 0.5, max_tokens = 200, api_key = api_key_together ) store = {} # memory is maintained outside the chain def get_session_history (session_id: str) -&amp;gt; BaseChatMessageHistory: if session_id not in store: store[session_id] = ChatMessageHistory() memory = ConversationSummaryMemory( chat_memory = store[session_id], k = 3, return_messages = True, llm = chat ) assert len(memory.memory_variables) == 1 key = memory.memory_variables[0] messages = memory.load_memory_variables({})[key] store[session_id] = ChatMessageHistory(messages = messages) return store[session_id] contextualize_q_system_prompt = ( &amp;quot;Given a chat history and the latest user question &amp;quot; &amp;quot;which might reference context in the chat history, &amp;quot; &amp;quot;formulate a standalone question which can be understood &amp;quot; &amp;quot;without the chat history. Do NOT answer the question, &amp;quot; &amp;quot;just reformulate it if needed and otherwise return it as is.&amp;quot; &amp;quot;{context}&amp;quot; ) contextualize_q_prompt = ChatPromptTemplate.from_messages( [ (&amp;quot;system&amp;quot;, contextualize_q_system_prompt), MessagesPlaceholder(&amp;quot;chat_history&amp;quot;), (&amp;quot;human&amp;quot;, &amp;quot;{input}&amp;quot;), ] ) history_aware_retriever = create_history_aware_retriever( chat, retriever, contextualize_q_prompt ) def strain_query (query, session_id): template = &amp;quot;&amp;quot;&amp;quot; You are an expert AI budtender with comprehensive knowledge of cannabis strains, their effects, medical applications, and industry trends. Your task is to provide accurate, helpful, and concise information in response to user queries about cannabis. Use the following context to answer the question: Context: {context} {history} Human: {query} chatbot: Instructions: 1. Analyze the question carefully to understand the user&amp;#39;s intent. 2. Provide a clear, concise, and informative answer based on the given context and your expertise. 3. If the question is about a specific strain, include key information such as dominant effects, medical benefits, flavor profile, and any unique characteristics. 4. For general cannabis questions, offer balanced and factual information, citing scientific research when relevant. 5. If the context doesn&amp;#39;t fully address the question, state this and provide the best available information, suggesting where the user might find more details. 6. Keep your response concise, aiming for 3-5 sentences unless more detail is absolutely necessary. 7. If appropriate, suggest related topics or strains the user might be interested in exploring. Provide your expert response: Give answer in markdown format &amp;quot;&amp;quot;&amp;quot; prompt = PromptTemplate(template = template, input_variables = [&amp;quot;context&amp;quot;, &amp;quot;question&amp;quot;, MessagesPlaceholder(&amp;quot;history&amp;quot;)]) question_answer_chain = create_stuff_documents_chain( llm = chat, prompt = prompt, document_variable_name = &amp;quot;context&amp;quot; # Ensure this matches the placeholder in the template ) rag_chain = create_retrieval_chain( question_answer_chain, history_aware_retriever ) chain_with_history = RunnableWithMessageHistory( rag_chain, get_session_history = get_session_history, input_messages_key = &amp;quot;query&amp;quot;, history_messages_key = &amp;quot;history&amp;quot;, output_messages_key = &amp;quot;answer&amp;quot; ) # Add debugging print statements print( f &amp;quot;Input query: {query}&amp;quot;) print( f &amp;quot;Session ID: {session_id}&amp;quot;) response = chain_with_history.invoke( {&amp;quot;query&amp;quot;: query}, config = { &amp;quot;configurable&amp;quot;: {&amp;quot;session_id&amp;quot;: session_id} } ) # Debug the response print(&amp;quot;Response received from the chain:&amp;quot;) print(response) return response[&amp;quot;answer&amp;quot;] # Example usage session_id = &amp;quot;user_123&amp;quot; # This should be unique for each user session query = &amp;quot;Tell me about the effects of Blue Dream strain&amp;quot; response = strain_query(query, session_id) print(response) query = &amp;quot;What are its medical benefits?&amp;quot; response = strain_query(query, session_id) print(response) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This is my code:&lt;/p&gt; &lt;p&gt;and everytime the error is same:&lt;br/&gt; File &amp;quot;/opt/anaconda3/lib/python3.11/site-packages/langchain_core/runnables/config.py&amp;quot;, line 404, in call_func_with_variable_args&lt;/p&gt; &lt;p&gt;return func(input, **kwargs) # type: ignore[call-arg]&lt;/p&gt; &lt;p&gt;^^^^^^^^^^^^^^^^^^^^^&lt;/p&gt; &lt;p&gt;File &amp;quot;/opt/anaconda3/lib/python3.11/site-packages/langchain/chains/combine_documents/stuff.py&amp;quot;, line 85, in format_docs&lt;/p&gt; &lt;p&gt;for doc in inputs[document_variable_name]&lt;/p&gt; &lt;p&gt;~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^&lt;/p&gt; &lt;p&gt;KeyError: &amp;#39;context&amp;#39;&lt;/p&gt; &lt;p&gt;Please HELPPPPPP!!!!&lt;br/&gt; I dont understand what I am doing wrong. I even attempt to use Promot template and take context as input variables but it didnt work out as well.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Substantial_Look1421&quot;&gt; /u/Substantial_Look1421 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1eetmjm/keyerror_context_trying_to_add_memory/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1eetmjm/keyerror_context_trying_to_add_memory/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1eetmjm</id><link href="https://www.reddit.com/r/LangChain/comments/1eetmjm/keyerror_context_trying_to_add_memory/" /><updated>2024-07-29T07:36:19+00:00</updated><published>2024-07-29T07:36:19+00:00</published><title>KeyError: 'context'. Trying to add memory.</title></entry><entry><author><name>/u/Actual_Box6342</name><uri>https://www.reddit.com/user/Actual_Box6342</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I want to create a RAG for the code generation task. the knowledge base will be a library and starting from that library my RAG must be able to generate code based on the library. Do you have any advice on the type of approach, vector store or knowledge graph database, models and more?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Actual_Box6342&quot;&gt; /u/Actual_Box6342 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1eefr4x/rag_for_code_generation/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1eefr4x/rag_for_code_generation/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1eefr4x</id><link href="https://www.reddit.com/r/LangChain/comments/1eefr4x/rag_for_code_generation/" /><updated>2024-07-28T19:39:06+00:00</updated><published>2024-07-28T19:39:06+00:00</published><title>RAG for Code Generation</title></entry><entry><author><name>/u/HappyDataGuy</name><uri>https://www.reddit.com/user/HappyDataGuy</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I was hoping to build a a bot which will take question from client and get response back. The thing is that, client can ask anything. And the consistency and stability an enterprise grade application should have is not there no matter what. I have put so much effort and its still not as expected. Have any one of you figured it out? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/HappyDataGuy&quot;&gt; /u/HappyDataGuy &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1eetb9g/is_client_facing_text_to_sql_lost_cause_for_now/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1eetb9g/is_client_facing_text_to_sql_lost_cause_for_now/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1eetb9g</id><link href="https://www.reddit.com/r/LangChain/comments/1eetb9g/is_client_facing_text_to_sql_lost_cause_for_now/" /><updated>2024-07-29T07:14:34+00:00</updated><published>2024-07-29T07:14:34+00:00</published><title>is client facing text to sql lost cause for now?</title></entry><entry><author><name>/u/adiko4</name><uri>https://www.reddit.com/user/adiko4</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey folks,&lt;/p&gt; &lt;p&gt;I’ve been looking into GPT4o tool calling feature for my project and came to conclusion that its architecture can be quite inefficient in costs (not very surprising). &lt;/p&gt; &lt;p&gt;Every time I want to use a tool, I have to re-send the entire chat history—system prompts, user messages, tool calls, and pratically everything. This means I’m paying for those extra tokens all over again, which can add up quickly if my prompt is 4000 tokens long.&lt;/p&gt; &lt;p&gt;I get that this is for privacy reasons, but it’s definitely not the most cost-friendly. Has anyone else dealt with this? How are you handling it? Any tips to make this less of a hassle or to keep the costs down?&lt;/p&gt; &lt;p&gt;I need it just for &amp;#39;calculator tool&amp;#39; that will enable GPT4o to make adjustments to various numeric data in a determinstic way..&lt;/p&gt; &lt;p&gt;Would love to hear your thoughts and suggestions!&lt;/p&gt; &lt;p&gt;Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/adiko4&quot;&gt; /u/adiko4 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1eef5go/anyone_else_dealing_with_the_token_cost_burden_of/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1eef5go/anyone_else_dealing_with_the_token_cost_burden_of/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1eef5go</id><link href="https://www.reddit.com/r/LangChain/comments/1eef5go/anyone_else_dealing_with_the_token_cost_burden_of/" /><updated>2024-07-28T19:12:37+00:00</updated><published>2024-07-28T19:12:37+00:00</published><title>Anyone else dealing with the token cost burden of re-sending chat history when using GPT4o tool calling feature?</title></entry><entry><author><name>/u/Expensive-Rub3117</name><uri>https://www.reddit.com/user/Expensive-Rub3117</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;h1&gt;🧠 Powered by Large Language Model (LLM) and LangChain PigPig uses cutting-edge AI to understand and respond to your messages naturally. We&amp;#39;ve integrated LangChain for some features, enhancing our bot&amp;#39;s capabilities.&lt;/h1&gt; &lt;h1&gt;🎵 Advanced Music Player&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Stream high-quality music from YouTube, Spotify, SoundCloud, and more&lt;/li&gt; &lt;li&gt;Interactive music controller for easy playlist management&lt;/li&gt; &lt;li&gt;Lyrics search function to sing along with your favorite tunes&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;🖼️ Multi-modal Capabilities&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Visual Question Answering: Ask questions about images&lt;/li&gt; &lt;li&gt;Image Generation: Create custom images from text descriptions&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;🍽️ Practical Features&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Set reminders for important events&lt;/li&gt; &lt;li&gt;Get restaurant recommendations&lt;/li&gt; &lt;li&gt;Perform mathematical calculations&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;👤 User Information Management&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Create and maintain user profiles &lt;/li&gt; &lt;li&gt;Track user interactions and preferences &lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;📊 Channel Data RAG (Retrieval-Augmented Generation)&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Utilize channel history for context-aware responses &lt;/li&gt; &lt;li&gt;Improve bot&amp;#39;s understanding of ongoing discussions&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;🔧 Easy to Set Up and Customize&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Flexible configuration through simple files&lt;/li&gt; &lt;li&gt;Auto-update system to keep your bot current&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;💻 Tech Stack&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Python 3.10+&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://Discord.py&quot;&gt;Discord.py&lt;/a&gt;&lt;/li&gt; &lt;li&gt;MongoDB&lt;/li&gt; &lt;li&gt;Lavalink server (4.0.0+)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;-LangChain&lt;/p&gt; &lt;p&gt;-Llama3.1&lt;/p&gt; &lt;p&gt;...&lt;/p&gt; &lt;p&gt;Whether you&amp;#39;re looking to enhance your gaming sessions, manage a community, or just have fun with friends, PigPig has got you covered. It&amp;#39;s like having a Swiss Army knife of Discord bots!&lt;/p&gt; &lt;p&gt;Check out our GitHub repo for more details and installation instructions: &lt;a href=&quot;https://github.com/starpig1129/PigPig-discord-LLM-bot&quot;&gt;PigPig-discord-LLM-bot&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;🤔 Questions for the Community&lt;/h1&gt; &lt;ol&gt; &lt;li&gt;We&amp;#39;re currently using LangChain for some of our bot&amp;#39;s functionalities. Has anyone successfully implemented a Discord bot entirely with LangChain? We&amp;#39;d love to hear about your experiences and challenges.&lt;/li&gt; &lt;li&gt;For those familiar with LangChain, do you think it&amp;#39;s feasible to use it for all of PigPig&amp;#39;s features? We&amp;#39;re particularly interested in how it might handle music playback and image processing.&lt;/li&gt; &lt;li&gt;What other LangChain-specific features would you like to see in a Discord bot? Let&amp;#39;s discuss in the comments! Your insights could help shape the future of PigPig.&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Expensive-Rub3117&quot;&gt; /u/Expensive-Rub3117 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1eemonr/pigpigdiscordllmbot_can_this_be_achieved_using/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1eemonr/pigpigdiscordllmbot_can_this_be_achieved_using/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1eemonr</id><link href="https://www.reddit.com/r/LangChain/comments/1eemonr/pigpigdiscordllmbot_can_this_be_achieved_using/" /><updated>2024-07-29T00:54:08+00:00</updated><published>2024-07-29T00:54:08+00:00</published><title>PigPig-discord-LLM-bot (Can this be achieved using langchain?)</title></entry><entry><author><name>/u/vuongagiflow</name><uri>https://www.reddit.com/user/vuongagiflow</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ee3ly6/optimize_agentic_workflow_cost_and_performance_a/&quot;&gt; &lt;img src=&quot;https://b.thumbs.redditmedia.com/m3-hsSMPJGz0FXT6IjJOfgUDv0XukiUwYCbdIIMr0Zw.jpg&quot; alt=&quot;Optimize Agentic Workflow Cost and Performance: A reversed engineering approach&quot; title=&quot;Optimize Agentic Workflow Cost and Performance: A reversed engineering approach&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;&lt;a href=&quot;https://preview.redd.it/8ifdm442v7fd1.png?width=1492&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d2b7c1b3391d17d940dc36705f2a9407c822df73&quot;&gt;https://preview.redd.it/8ifdm442v7fd1.png?width=1492&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d2b7c1b3391d17d940dc36705f2a9407c822df73&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There are two primary approaches to getting started with Agentic workflows: &lt;strong&gt;workflow automation&lt;/strong&gt; for domain experts and &lt;strong&gt;autonomous agents&lt;/strong&gt; for resource-constrained projects. By observing how agents perform tasks successfully, you can map out and optimize workflow steps, reducing hallucinations, costs, and improving performance.&lt;/p&gt; &lt;p&gt;Let&amp;#39;s explore how to automate the “Dependencies Upgrade” for your product team using CrewAI then Langgraph. Typically, a software engineer would handle this task by visiting changelog webpages, reviewing changes, and coordinating with the product manager to create backlog stories. With agentic workflow, we can streamline and automate these processes, saving time and effort while allowing engineers to focus on more engaging work.&lt;/p&gt; &lt;p&gt;For demonstration, &lt;a href=&quot;https://github.com/AgiFlow/repo-upgrade&quot;&gt;source-code is available on Github&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;For detailed explanation, please see below videos:&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://youtu.be/hvcd8Xjpd7A&quot;&gt;Part 1: Get started with Autonomous Agents using CrewAI&lt;/a&gt; &lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://youtu.be/_k82vx4qaLo&quot;&gt;Part 2: Optimisation with Langgraph and Conclusion&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Short summary on the repo and videos&lt;/h1&gt; &lt;p&gt;With &lt;strong&gt;autononous agents&lt;/strong&gt; first approach, we would want to follow below steps:&lt;/p&gt; &lt;h1&gt;1. Keep it Simple, Stupid&lt;/h1&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/zj4hcm8bv7fd1.png?width=1456&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=caa379e7735b139916444e359c9630bd2a9a9419&quot;&gt;https://preview.redd.it/zj4hcm8bv7fd1.png?width=1456&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=caa379e7735b139916444e359c9630bd2a9a9419&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We start with two agents: a Product Manager and a Developer, utilizing the Hierarchical Agents process from CrewAI. The Product Manager orchestrates tasks and delegates them to the Developer, who uses tools to fetch changelogs and read repository files to determine if dependencies need updating. The Product Manager then prioritizes backlog stories based on these findings.&lt;/p&gt; &lt;p&gt;Our goal is to analyse the successful workflow execution only to learn the flow at the first step.&lt;/p&gt; &lt;h1&gt;2. Simplify Communication Flow&lt;/h1&gt; &lt;p&gt;Autonomous Agents are great for some scenarios, but not for workflow automation. We want to reduce the cost, hallucination and improve speed from Hierarchical process.&lt;/p&gt; &lt;p&gt;Second step is to reduce unnecessary communication from bi-directional to uni-directional between agents. Simply talk, have specialised agent to perform its task, finish the task and pass the result to the next agent without repetition (liked Manufactoring process).&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/tiu3etkdv7fd1.png?width=1854&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=34987572a5f4ae4177a3b079d5b234f32adfd5a8&quot;&gt;https://preview.redd.it/tiu3etkdv7fd1.png?width=1854&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=34987572a5f4ae4177a3b079d5b234f32adfd5a8&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;3. Prompt optimisation&lt;/h1&gt; &lt;p&gt;ReAct Agent are great for auto-correct action, but also cause unpredictability in automation jobs which increase number of LLM calls and repeat actions.&lt;/p&gt; &lt;p&gt;If predictability, cost and speed is what you are aiming for, you can also optimise prompt and explicitly flow engineer with Langgraph. Also make sure the context you pass to prompt doesn&amp;#39;t have redundant information to control the cost.&lt;/p&gt; &lt;p&gt;A summary from above steps; the techniques in Blue box are low hanging fruits to improve your workflow. If you want to use other techniques, ensure you have these components implemented first: evaluation, observability and human-in-the-loop feedback.&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/1fh8cnvnv7fd1.png?width=1850&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f3f27cc89a419e1808202d192626bc79f2643695&quot;&gt;https://preview.redd.it/1fh8cnvnv7fd1.png?width=1850&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f3f27cc89a419e1808202d192626bc79f2643695&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I&amp;#39;ll will share blog article link later for those who prefer to read. Would love to hear your feedback on this.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/vuongagiflow&quot;&gt; /u/vuongagiflow &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ee3ly6/optimize_agentic_workflow_cost_and_performance_a/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ee3ly6/optimize_agentic_workflow_cost_and_performance_a/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1ee3ly6</id><media:thumbnail url="https://b.thumbs.redditmedia.com/m3-hsSMPJGz0FXT6IjJOfgUDv0XukiUwYCbdIIMr0Zw.jpg" /><link href="https://www.reddit.com/r/LangChain/comments/1ee3ly6/optimize_agentic_workflow_cost_and_performance_a/" /><updated>2024-07-28T09:31:44+00:00</updated><published>2024-07-28T09:31:44+00:00</published><title>Optimize Agentic Workflow Cost and Performance: A reversed engineering approach</title></entry><entry><author><name>/u/lukus88</name><uri>https://www.reddit.com/user/lukus88</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I need to evaluate some applications for research projects and wish to know which chatbot solution works best. I want to evaluate applications based on (my) official strategies, documents, guidelines so bot needs to be fine tuned. Applications are text only, my documents are text and tables also. So basically what im looking for is evaluating buddy that can offer concise and logical evaluation of applications. My documents are around 30mb size, their aplications around 30 written text alltogether.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/lukus88&quot;&gt; /u/lukus88 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ee92ii/what_chatbot_paid_or_not_is_best_for_uploading_my/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ee92ii/what_chatbot_paid_or_not_is_best_for_uploading_my/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1ee92ii</id><link href="https://www.reddit.com/r/LangChain/comments/1ee92ii/what_chatbot_paid_or_not_is_best_for_uploading_my/" /><updated>2024-07-28T14:46:09+00:00</updated><published>2024-07-28T14:46:09+00:00</published><title>What chatbot (paid or not) is best for uploading my own documents to help eith evaluating applications?</title></entry><entry><author><name>/u/mehul_gupta1997</name><uri>https://www.reddit.com/user/mehul_gupta1997</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/mehul_gupta1997&quot;&gt; /u/mehul_gupta1997 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;/r/ArtificialInteligence/comments/1ee0qxf/llama_31_tutorials/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ee0r40/llama_31_tutorials/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1ee0r40</id><link href="https://www.reddit.com/r/LangChain/comments/1ee0r40/llama_31_tutorials/" /><updated>2024-07-28T06:07:27+00:00</updated><published>2024-07-28T06:07:27+00:00</published><title>Llama 3.1 tutorials</title></entry><entry><author><name>/u/naxmax2019</name><uri>https://www.reddit.com/user/naxmax2019</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1edrxte/created_an_ai_voice_agents_product_give_it_a_try/&quot;&gt; &lt;img src=&quot;https://external-preview.redd.it/cbuCJtyPbgKwiO0T7Az4AZDrzHTctue2DnaepGcR-WU.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=128356a217db54b740c338f2106ff47e46e6fd22&quot; alt=&quot;Created an AI voice agents product .. give it a try. I am using Twilio for phone numbers... The results have been pretty good. Give it a try and let me know what you think. I will share the code the next days. &quot; title=&quot;Created an AI voice agents product .. give it a try. I am using Twilio for phone numbers... The results have been pretty good. Give it a try and let me know what you think. I will share the code the next days. &quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/naxmax2019&quot;&gt; /u/naxmax2019 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://workhub.ai/ai-voice-agent/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1edrxte/created_an_ai_voice_agents_product_give_it_a_try/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1edrxte</id><media:thumbnail url="https://external-preview.redd.it/cbuCJtyPbgKwiO0T7Az4AZDrzHTctue2DnaepGcR-WU.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=128356a217db54b740c338f2106ff47e46e6fd22" /><link href="https://www.reddit.com/r/LangChain/comments/1edrxte/created_an_ai_voice_agents_product_give_it_a_try/" /><updated>2024-07-27T22:04:58+00:00</updated><published>2024-07-27T22:04:58+00:00</published><title>Created an AI voice agents product .. give it a try. I am using Twilio for phone numbers... The results have been pretty good. Give it a try and let me know what you think. I will share the code the next days.</title></entry><entry><author><name>/u/glow_storm</name><uri>https://www.reddit.com/user/glow_storm</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I was using the Ollama package in Langchain , this was the community version &lt;/p&gt; &lt;p&gt;from langchain_community.llms import Ollama &lt;/p&gt; &lt;p&gt;I had setup a remote GPU serving running Ollama and wanted to use the Ollama endpoint to run a code on my personal Laptop , everything worked fine , however when I tried to use tool calling , it said to use ChatOllama , &lt;/p&gt; &lt;p&gt;from langchain_community.llms import Ollama &lt;/p&gt; &lt;p&gt;so I did , but it gave me an Error of not Implemented. I checked the Langchain Docs and it said tool calling , and it gave me a new package to use , which i first had to install &lt;/p&gt; &lt;p&gt;from langchain_ollama import ChatOllama&lt;/p&gt; &lt;p&gt;but now this package refused to Connect to my remote server of Ollama no matter what I tried , Can anyone help me understand and fix how to make Langchain work with Tool Calling on remote Ollama sever. &lt;/p&gt; &lt;p&gt;```&lt;/p&gt; &lt;p&gt;llm =ChatOllama(base_url=&amp;quot;&lt;a href=&quot;https://c7c4-65-109-75-7.ngrok-free.app&quot;&gt;https://c7c4-65-109-75-7.ngrok-free.app&lt;/a&gt;&amp;quot;,model=&amp;quot;llama3.1:70b&amp;quot;,temperature=0) # this was the code I was trying to excute&lt;/p&gt; &lt;p&gt;```&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;```&lt;/p&gt; &lt;p&gt;from langchain_community.llms import Ollama&lt;/p&gt; &lt;p&gt;llm =Ollama(base_url=&amp;quot;&lt;a href=&quot;https://c7c4-65-109-75-7.ngrok-free.app&quot;&gt;https://c7c4-65-109-75-7.ngrok-free.app&lt;/a&gt;&amp;quot;,model=&amp;quot;llama3.1:70b&amp;quot;,temperature=0) # this work but has not tool calling &lt;/p&gt; &lt;p&gt;``` &lt;/p&gt; &lt;p&gt;I would appreciate the help. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/glow_storm&quot;&gt; /u/glow_storm &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ee73og/support_for_remote_llm_calling_in_ollama_package/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ee73og/support_for_remote_llm_calling_in_ollama_package/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1ee73og</id><link href="https://www.reddit.com/r/LangChain/comments/1ee73og/support_for_remote_llm_calling_in_ollama_package/" /><updated>2024-07-28T13:11:13+00:00</updated><published>2024-07-28T13:11:13+00:00</published><title>Support for Remote LLM calling in Ollama Package</title></entry><entry><author><name>/u/SadPianist871</name><uri>https://www.reddit.com/user/SadPianist871</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey guys,&lt;/p&gt; &lt;p&gt;I have a general question for those of you developing agentic AI systems. Have you had the problem of a service not having an API and how did you solve it (i.e., how did you define the &amp;quot;tool&amp;quot; to be used by the LLM)? A simple example: I want my personal AI assistant to purchase groceries for me, but there&amp;#39;s no API provided by the supermarket. How can I achieve that?&lt;/p&gt; &lt;p&gt;Do you think this is another reason why AI agents are still not in use for tasks that are not critical (thus, it&amp;#39;s fine if they&amp;#39;re not 100% reliable), but could be very useful in our daily lives?&lt;/p&gt; &lt;p&gt;Edit: by “simple example”, I meant “simple use case”, not that it’s easy to implement&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/SadPianist871&quot;&gt; /u/SadPianist871 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1edkn7p/lack_of_apis/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1edkn7p/lack_of_apis/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1edkn7p</id><link href="https://www.reddit.com/r/LangChain/comments/1edkn7p/lack_of_apis/" /><updated>2024-07-27T16:42:08+00:00</updated><published>2024-07-27T16:42:08+00:00</published><title>Lack of APIs</title></entry><entry><author><name>/u/Top-Cookie3565</name><uri>https://www.reddit.com/user/Top-Cookie3565</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Sorry if this question is too basic. New to this so trying to learn. &lt;/p&gt; &lt;p&gt;So this Is what I did. Created a basic agent with few random tools. Added Memory to it using RunnableWithMessageHistory. &lt;/p&gt; &lt;p&gt;&lt;code&gt;llm = ChatOpenAI(model_name = &amp;quot;gpt-3.5-turbo&amp;quot;, temperature = 0)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;tools = [click_new_image, visual_question_answer, question_answer, previous_pic]&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;prompt = ChatPromptTemplate.from_messages(&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;[&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;(&amp;quot;system&amp;quot;, &amp;quot;You are a very powerful assistant who can take pictures and answer questions about them. If the query is regarding an older pic, then answer directly instead of taking a new pic.&amp;quot;),&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;MessagesPlaceholder(variable_name=&amp;quot;history&amp;quot;),&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;(&amp;quot;user&amp;quot;, &amp;quot;{input}&amp;quot;),&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;MessagesPlaceholder(variable_name=&amp;quot;agent_scratchpad&amp;quot;),&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;]&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;agent = create_tool_calling_agent(llm, tools, prompt)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;agent_executor = AgentExecutor(agent=agent, tools=tools, verbose = True)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;store = {}&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;agent_executor_w_memory = RunnableWithMessageHistory(&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;agent_executor,&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;get_session_history,&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;input_messages_key=&amp;quot;input&amp;quot;,&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;history_messages_key=&amp;quot;history&amp;quot;,&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;To run this I did ( In Streamlit ) - &lt;/p&gt; &lt;p&gt;&lt;code&gt;if prompt := st.chat_input():&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;st.chat_message(&amp;quot;user&amp;quot;).write(prompt)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;with st.chat_message(&amp;quot;assistant&amp;quot;):&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;response = agent_executor_w_memory.invoke(&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;{&amp;quot;input&amp;quot;: prompt},&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;config=config,&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;st.write(response[&amp;quot;output&amp;quot;])&lt;/code&gt;&lt;/p&gt; &lt;p&gt;But this won&amp;#39;t stream( typing effect) the output, it will just give the final output at once. I want to stream the output. Only the last response, not the intermediate steps. &lt;/p&gt; &lt;p&gt;Ps- Can I also stream the intermediate step result( we could iterate through the stream and print chunks but that will also not stream( typing effect)) ? or like tools it call too? ( Asking just to learn more, not needed as of now) &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Top-Cookie3565&quot;&gt; /u/Top-Cookie3565 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1eduao5/how_can_i_stream_only_the_final_result_from_a/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1eduao5/how_can_i_stream_only_the_final_result_from_a/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1eduao5</id><link href="https://www.reddit.com/r/LangChain/comments/1eduao5/how_can_i_stream_only_the_final_result_from_a/" /><updated>2024-07-27T23:56:51+00:00</updated><published>2024-07-27T23:56:51+00:00</published><title>How can I stream only the final result from a agent in streamlit</title></entry><entry><author><name>/u/Pristine-Watercress9</name><uri>https://www.reddit.com/user/Pristine-Watercress9</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey guys,&lt;/p&gt; &lt;p&gt;My buddy and I are working on a tool that lets you preview your ML models in a presentable environment before deployment. I had my models set up on Google Colab, but it wasn’t easy for the team to review it. It also isn’t very presentable to clients.&lt;/p&gt; &lt;p&gt;So we want to create a demo environment that’s super simple to share and present models before handing off to devops. Thinking about adding some sort of feedback system too.&lt;/p&gt; &lt;p&gt;We’re still figuring out the details, so we’d love to get your takes on this. In your experience, what features would’ve helped you? Currently we have charts and collaboration features in mind.&lt;/p&gt; &lt;p&gt;Thanks! (my dm is open! we can’t be the only ones having this problem right)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Pristine-Watercress9&quot;&gt; /u/Pristine-Watercress9 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1edtzrx/ml_model_demo_tool/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1edtzrx/ml_model_demo_tool/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1edtzrx</id><link href="https://www.reddit.com/r/LangChain/comments/1edtzrx/ml_model_demo_tool/" /><updated>2024-07-27T23:42:07+00:00</updated><published>2024-07-27T23:42:07+00:00</published><title>ML model demo tool?</title></entry></feed>