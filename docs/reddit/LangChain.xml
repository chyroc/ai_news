<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/"><category term="LangChain" label="r/LangChain"/><updated>2024-06-09T21:49:48+00:00</updated><icon>https://www.redditstatic.com/icon.png/</icon><id>/r/LangChain.rss</id><link rel="self" href="https://www.reddit.com/r/LangChain.rss" type="application/atom+xml" /><link rel="alternate" href="https://www.reddit.com/r/LangChain" type="text/html" /><subtitle>LangChain is an open-source framework and developer toolkit that helps developers get LLM applications from prototype to production. It is available for Python and Javascript at https://www.langchain.com/.</subtitle><title>LangChain</title><entry><author><name>/u/zchaarm</name><uri>https://www.reddit.com/user/zchaarm</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;A place for members of &lt;a href=&quot;/r/LangChain&quot;&gt;r/LangChain&lt;/a&gt; to chat with each other&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/zchaarm&quot;&gt; /u/zchaarm &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_10ljho9</id><link href="https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/" /><updated>2023-01-26T04:33:05+00:00</updated><published>2023-01-26T04:33:05+00:00</published><title>r/LangChain Lounge</title></entry><entry><author><name>/u/sarthakai</name><uri>https://www.reddit.com/user/sarthakai</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;If you don&amp;#39;t want to use Guardrails because you anticipate prompt attacks that are more unique, you can train a custom classifier:&lt;/p&gt; &lt;p&gt;Step 1:&lt;/p&gt; &lt;p&gt;Create a balanced dataset of prompt injection user prompts.&lt;/p&gt; &lt;p&gt;These might be previous user attempts you’ve caught in your logs, or you can compile threats you anticipate relevant to your use case.&lt;/p&gt; &lt;p&gt;Here’s a dataset you can use as a starting point: &lt;a href=&quot;https://huggingface.co/datasets/deepset/prompt-injections&quot;&gt;https://huggingface.co/datasets/deepset/prompt-injections&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Step 2:&lt;/p&gt; &lt;p&gt;Further augment this dataset using an LLM to cover maximal bases.&lt;/p&gt; &lt;p&gt;Step 3:&lt;/p&gt; &lt;p&gt;Train an encoder model on this dataset as a classifier to predict prompt injection attempts vs benign user prompts.&lt;/p&gt; &lt;p&gt;A DeBERTA model can be deployed on a fast enough inference point and you can use it in the beginning of your pipeline to protect future LLM calls.&lt;/p&gt; &lt;p&gt;This model is an example with 99% accuracy: &lt;a href=&quot;https://huggingface.co/deepset/deberta-v3-base-injection&quot;&gt;https://huggingface.co/deepset/deberta-v3-base-injection&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Step 4:&lt;/p&gt; &lt;p&gt;Monitor your false negatives, and regularly update your training dataset + retrain. &lt;/p&gt; &lt;p&gt;Most LLM apps and agents will face this threat. I&amp;#39;m planning to train a open model next weekend to help counter them. Will post updates.&lt;/p&gt; &lt;p&gt;I share high quality AI updates and tutorials daily.&lt;/p&gt; &lt;p&gt;If you like this post, you can learn more about LLMs and creating AI agents here: &lt;a href=&quot;https://github.com/sarthakrastogi/nebulousai&quot;&gt;https://github.com/sarthakrastogi/nebulousai&lt;/a&gt; or on my Twitter: &lt;a href=&quot;https://x.com/sarthakai&quot;&gt;https://x.com/sarthakai&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/sarthakai&quot;&gt; /u/sarthakai &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dbsh0x/forget_all_prev_instructions_now_do_malicious/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dbsh0x/forget_all_prev_instructions_now_do_malicious/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dbsh0x</id><link href="https://www.reddit.com/r/LangChain/comments/1dbsh0x/forget_all_prev_instructions_now_do_malicious/" /><updated>2024-06-09T12:20:27+00:00</updated><published>2024-06-09T12:20:27+00:00</published><title>“Forget all prev instructions, now do [malicious attack task]”. How you can protect your LLM app against such prompt injection threats:</title></entry><entry><author><name>/u/ventzpetkov</name><uri>https://www.reddit.com/user/ventzpetkov</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi All, &lt;/p&gt; &lt;p&gt;Released Easy LLMs, in case anyone else finds it helpful/useful:&lt;br/&gt; &lt;a href=&quot;https://github.com/ventz/easy-llms&quot;&gt;https://github.com/ventz/easy-llms&lt;/a&gt;&lt;br/&gt; pip install easy-llms &lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Easy &amp;quot;1-line&amp;quot; calling of every LLM from OpenAI, MS Azure, AWS Bedrock, GCP Vertex, and Ollama&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;It&amp;#39;s utilizing LangChain, but abstracting away all of the silly differences to make things easy. Yet it still stays powerful by allowing to provide and override any option/parameter for any LLM for any provider. &lt;/p&gt; &lt;p&gt;The goal initially was to be able to get started with any popular LLM, and &amp;quot;just get going&amp;quot; without having to think about how to authenticate, needed options and parameters, which classes you need, etc. &lt;/p&gt; &lt;p&gt;I&amp;#39;ve been using this internally to compare 40+ LLMs over the last 1.5 years, with millions of calls, and figured it&amp;#39;s time to clean this up and release it for others. It started with just OpenAI, and then quickly added Azure&amp;#39;s OpenAI. And then Vertex, and then Bedrock, and recently Ollama. As LangChain has been changing, I&amp;#39;ve been updating it.&lt;/p&gt; &lt;p&gt;I updated it to the latest non-0.2 LangChain version before releasing it (LangChain v0.2.x seems to have a bug with Google Vertex, so waiting on that before upgrading it)&lt;/p&gt; &lt;p&gt;If you find any bugs (or have ideas for improvement/feature requests), please reach out via the GitHub issues.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/ventzpetkov&quot;&gt; /u/ventzpetkov &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dbyxy4/easy_1line_calling_of_every_llm_from_openai_ms/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dbyxy4/easy_1line_calling_of_every_llm_from_openai_ms/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dbyxy4</id><link href="https://www.reddit.com/r/LangChain/comments/1dbyxy4/easy_1line_calling_of_every_llm_from_openai_ms/" /><updated>2024-06-09T17:20:25+00:00</updated><published>2024-06-09T17:20:25+00:00</published><title>Easy &quot;1-line&quot; calling of every LLM from OpenAI, MS Azure, AWS Bedrock, GCP Vertex, and Ollama</title></entry><entry><author><name>/u/jeffrey-0711</name><uri>https://www.reddit.com/user/jeffrey-0711</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi, I am maker of &lt;a href=&quot;https://github.com/Marker-Inc-Korea/AutoRAG&quot;&gt;AutoRAG&lt;/a&gt; and want to share my experience. &lt;/p&gt; &lt;p&gt;I added RAGAS context precision metric to AutoRAG, because we couldn&amp;#39;t make retrieval ground truth at our dataset. And it was huge mistake.&lt;/p&gt; &lt;p&gt;AutoRAG is a tool to evaluate all kinds of RAG components easily. Using it, I tried to compare six different retrieval method plus eight passage rerankers. I used gpt-4-turbo model to use ragas context precision, because using it on gpt-3.5-turbo have low performance. In context precision score, RAGAS score only have 0.7 human correlation with gpt-3.5-turbo. This result came from their &lt;a href=&quot;https://arxiv.org/pdf/2309.15217&quot;&gt;own paper&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;And it costs 700 dollar... only in OpenAI API. RAGAS context precision calls OpenAI API 50,000 times. Spending 700 dollar for optimizing one RAG dataset is ridiculous. I ended up to remove RAGAS context precision metric completely at AutoRAG.&lt;/p&gt; &lt;p&gt;Actually, RAGAS context precision can be useful when there are no retrieval gt passage in your dataset. However, do not recommend to use it for optimizing your retrieval system. There are perfectly better alternative. Information retreival metrics like F1, NDCG, or mAP. (This case you need to make retrieval gt passage dataset) In this way, it costs zero dollar, and much precise than using LLM for calculating context precision.&lt;/p&gt; &lt;p&gt;I hope my experience is helpful for someone to evaluate or optimize their retrieval system while making RAG.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/jeffrey-0711&quot;&gt; /u/jeffrey-0711 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dbmqii/i_spent_700_on_evaluating_100_rag_qa_set_using/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dbmqii/i_spent_700_on_evaluating_100_rag_qa_set_using/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dbmqii</id><link href="https://www.reddit.com/r/LangChain/comments/1dbmqii/i_spent_700_on_evaluating_100_rag_qa_set_using/" /><updated>2024-06-09T05:55:09+00:00</updated><published>2024-06-09T05:55:09+00:00</published><title>I spent 700$ on evaluating 100 RAG QA set using RAGAS context precision.</title></entry><entry><author><name>/u/Sweaty-Minimum5423</name><uri>https://www.reddit.com/user/Sweaty-Minimum5423</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;&lt;a href=&quot;https://youtu.be/b3XsvoFWp4c?si=2Y7eBx2_MobnzOno&quot;&gt;https://youtu.be/b3XsvoFWp4c?si=2Y7eBx2_MobnzOno&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Has anyone tried the multi agent workflow in the tutorial? I find that if the user query is only a single intent. It could route to the correct place and get the response without problem.&lt;/p&gt; &lt;p&gt;However, if the user query got multiple intent: like booking hotel and booking car at the same time: The agent in the booking hotel might think it has tool to book the car as well although it doesn’t, especially when car booking workflow and tool have been invoked before this multiple intent query. I’m guessing the problem is likely because all workflow can share state where some part of the state has the history of invoking car booking tool before. So the agent in hotel booking workflow sees it and think it also has this car booking tool.&lt;/p&gt; &lt;p&gt;Sometimes the flow might works when the hotel-booking agent answers the hotel booking part then invoke CompleteOrEscalate tool which is to pass the dialog back to the primary assistant and reroute the query again to car booking workflow.However, the CompleteOrEscalate tool is not properly invoked as the agent prefer invoking the imaginary tool as stated above.&lt;/p&gt; &lt;p&gt;They have added the entry node to help this problem but still it doesn’t work well at least for OpenAI GPT4. Is there an upgraded version Chatbot flow to solve this issue? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Sweaty-Minimum5423&quot;&gt; /u/Sweaty-Minimum5423 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dbzvk5/customer_bot_tutorial_issue/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dbzvk5/customer_bot_tutorial_issue/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dbzvk5</id><link href="https://www.reddit.com/r/LangChain/comments/1dbzvk5/customer_bot_tutorial_issue/" /><updated>2024-06-09T18:00:58+00:00</updated><published>2024-06-09T18:00:58+00:00</published><title>Customer bot tutorial issue</title></entry><entry><author><name>/u/dirk_klement</name><uri>https://www.reddit.com/user/dirk_klement</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;I am currently exploring Langchain. We want our users to be able to ask questions about upcoming events in our vectorstore. This works really good. &lt;/p&gt; &lt;p&gt;By we want the LLM to respond with something that is a mix between regular text/markdown but with links to the events etc. &lt;/p&gt; &lt;p&gt;Something like the Arc browser when it searches for you. It automatically adds links to certain topics, places, bars etc.&lt;/p&gt; &lt;p&gt;How could I achieve something like this? All resources all welcome :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/dirk_klement&quot;&gt; /u/dirk_klement &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dc1pjv/partial_markdown_and_json_response/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dc1pjv/partial_markdown_and_json_response/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dc1pjv</id><link href="https://www.reddit.com/r/LangChain/comments/1dc1pjv/partial_markdown_and_json_response/" /><updated>2024-06-09T19:19:00+00:00</updated><published>2024-06-09T19:19:00+00:00</published><title>Partial Markdown and JSON response</title></entry><entry><author><name>/u/sundaysexisthebest</name><uri>https://www.reddit.com/user/sundaysexisthebest</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;So obviously langchain was frowned upon by a lot of genAI devs, for being too abstract, confusing, over-complicating (and poor documentation ofc). I want some more recent opinion on this, maybe what’s the alternatives? Looking forward to hearing from people with actual LLM project experience. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/sundaysexisthebest&quot;&gt; /u/sundaysexisthebest &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dbw8dk/langchainlanggraph_critique_update/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dbw8dk/langchainlanggraph_critique_update/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dbw8dk</id><link href="https://www.reddit.com/r/LangChain/comments/1dbw8dk/langchainlanggraph_critique_update/" /><updated>2024-06-09T15:22:52+00:00</updated><published>2024-06-09T15:22:52+00:00</published><title>Langchain/langgraph critique update?</title></entry><entry><author><name>/u/Little-Meet4512</name><uri>https://www.reddit.com/user/Little-Meet4512</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Rag application using agent and tool &lt;/p&gt; &lt;p&gt;I am working on one project in which I have to use two different knowledge base .when the user ask question agent should know which knowledge base to use give response. For that anybody can help me .any material which you recommend &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Little-Meet4512&quot;&gt; /u/Little-Meet4512 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dbqtkf/rag_two_seperate_knowledge_base_use/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dbqtkf/rag_two_seperate_knowledge_base_use/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dbqtkf</id><link href="https://www.reddit.com/r/LangChain/comments/1dbqtkf/rag_two_seperate_knowledge_base_use/" /><updated>2024-06-09T10:38:02+00:00</updated><published>2024-06-09T10:38:02+00:00</published><title>Rag two seperate knowledge base use</title></entry><entry><author><name>/u/peenuty</name><uri>https://www.reddit.com/user/peenuty</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey everyone 👋,&lt;/p&gt; &lt;p&gt;I&amp;#39;ve been working on &lt;a href=&quot;http://llm-ui.com&quot;&gt;llm-ui&lt;/a&gt;, an MIT open source library which allows developers to build custom UIs for LLM responses.&lt;/p&gt; &lt;p&gt;It operates on any LLM output, so should work nicely with LangChain&lt;/p&gt; &lt;p&gt;If anyone here is building custom UIs for LangChain I&amp;#39;d love to hear your thoughts.&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;http://llm-ui.com&quot;&gt;llm-ui.com&lt;/a&gt;&lt;br/&gt; &lt;a href=&quot;https://github.com/llm-ui-kit/llm-ui&quot;&gt;https://github.com/llm-ui-kit/llm-ui&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/peenuty&quot;&gt; /u/peenuty &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dbo39f/ui_library_for_langchain_output/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dbo39f/ui_library_for_langchain_output/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dbo39f</id><link href="https://www.reddit.com/r/LangChain/comments/1dbo39f/ui_library_for_langchain_output/" /><updated>2024-06-09T07:24:33+00:00</updated><published>2024-06-09T07:24:33+00:00</published><title>UI Library for LangChain output</title></entry><entry><author><name>/u/randomtask2000</name><uri>https://www.reddit.com/user/randomtask2000</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Can anyone recommend me the best thin UI that I can park in S3? I see so many folks use Gradio and Streamlit, that appears to need state and can’t really be run with a lambda backend. Or should I continue to learn to code my Svelte app?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/randomtask2000&quot;&gt; /u/randomtask2000 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dblj4c/best_llmrag_ui_for_awss3_and_lambda/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dblj4c/best_llmrag_ui_for_awss3_and_lambda/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dblj4c</id><link href="https://www.reddit.com/r/LangChain/comments/1dblj4c/best_llmrag_ui_for_awss3_and_lambda/" /><updated>2024-06-09T04:38:48+00:00</updated><published>2024-06-09T04:38:48+00:00</published><title>Best LLM/RAG UI for AWS/S3 and Lambda</title></entry><entry><author><name>/u/Exotic-Iron-6381</name><uri>https://www.reddit.com/user/Exotic-Iron-6381</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi, how to manage session ID in Telegram bot, I am importing flowise API but it regenerates session ID every time, how can I keep in consistent on 1 sessionid for every chat?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Exotic-Iron-6381&quot;&gt; /u/Exotic-Iron-6381 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dbwels/how_to_manager_session_id_from_flowise_api/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dbwels/how_to_manager_session_id_from_flowise_api/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dbwels</id><link href="https://www.reddit.com/r/LangChain/comments/1dbwels/how_to_manager_session_id_from_flowise_api/" /><updated>2024-06-09T15:30:26+00:00</updated><published>2024-06-09T15:30:26+00:00</published><title>How to manager Session ID from Flowise API</title></entry><entry><author><name>/u/ss1seekining</name><uri>https://www.reddit.com/user/ss1seekining</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I see in the code &lt;a href=&quot;https://github.com/langchain-ai/langgraph/tree/main/langgraph/checkpoint&quot;&gt;https://github.com/langchain-ai/langgraph/tree/main/langgraph/checkpoint&lt;/a&gt; it has support only for SQLite. But how do i product-ionize the system ? I want to give access of the chat logs to humans. If for example I want to store them in MongoDB ? whats the easiest way ? do I need to create my own class similar to `AsyncSqliteSaver` ? &lt;/p&gt; &lt;p&gt;Anything in plan to release recently ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/ss1seekining&quot;&gt; /u/ss1seekining &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dbmdoo/how_to_implement_memory_in_separate_db_in/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dbmdoo/how_to_implement_memory_in_separate_db_in/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dbmdoo</id><link href="https://www.reddit.com/r/LangChain/comments/1dbmdoo/how_to_implement_memory_in_separate_db_in/" /><updated>2024-06-09T05:32:01+00:00</updated><published>2024-06-09T05:32:01+00:00</published><title>How to implement memory in separate db in langgraph ?</title></entry><entry><author><name>/u/Frosty-Substance4790</name><uri>https://www.reddit.com/user/Frosty-Substance4790</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I am diving into the world of building LLM applications and could use some guidance. My goal is to create an LLM Application(chatbot) that aids users in deploying jobs from one environment to another in an interactive manner. Specifically, I aim to have the application take user inputs and guide them through the deployment process, including pulling code from Git and checking it into another environment&amp;#39;s Git repository.&lt;/p&gt; &lt;p&gt;I am considering using LangChain for implementation, but I am uncertain about the best approach. I am seeking advice on selecting the appropriate LLM model and leveraging LangChain functionalities effectively.&lt;/p&gt; &lt;p&gt;here is a breakdown of the deployment functions I envision:&lt;/p&gt; &lt;p&gt;Login, Tagging, Listing code dependencies, Pulling code, Checking in code, Providing a summary of the deployment process&lt;/p&gt; &lt;p&gt;My plan is to utilize custom tools within LangChain to map these functions and use agent to select the appropriate actions based on user input. Essentially, I want the application to interactively gather necessary inputs from users in natural language at each step and execute the deployment process accordingly.&lt;/p&gt; &lt;p&gt;Could someone please assist me or provide guidance on the approach to select the Model, implementing this using LangChain including UI?&lt;/p&gt; &lt;p&gt;Thank you in advance for your help!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Frosty-Substance4790&quot;&gt; /u/Frosty-Substance4790 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dbqufm/guidance_on_implementing_llm_application_for/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dbqufm/guidance_on_implementing_llm_application_for/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dbqufm</id><link href="https://www.reddit.com/r/LangChain/comments/1dbqufm/guidance_on_implementing_llm_application_for/" /><updated>2024-06-09T10:39:47+00:00</updated><published>2024-06-09T10:39:47+00:00</published><title>Guidance on Implementing LLM Application for Interactive Deployment Assistance</title></entry><entry><author><name>/u/sarthakai</name><uri>https://www.reddit.com/user/sarthakai</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Smaller models with 7B params can now outperform the 1.76 Trillion param GPT-4. 😧 How?&lt;/p&gt; &lt;p&gt;A new study from Predibase shows that 2B and 7B models, if fine-tuned with Low Rank Adaptation (LoRA) on task-specific datasets, can give better results than larger models. (Link to paper in comments)&lt;/p&gt; &lt;p&gt;LoRA reduces the number of trainable parameters in LLMs by injecting low-rank matrices into the model&amp;#39;s existing layers.&lt;/p&gt; &lt;p&gt;These matrices capture task-specific info efficiently, allowing fine-tuning with minimal compute and memory.&lt;/p&gt; &lt;p&gt;So, this paper compares 310 LoRA fine-tuned models, showing that 4-bit LoRA models surpass base models and even GPT-4 in many tasks. They also establish the influence of task complexity on fine-tuning outcomes.&lt;/p&gt; &lt;p&gt;When does LoRA fine-tuning outperform larger models like GPT-4?&lt;/p&gt; &lt;p&gt;When you have narrowly-scoped, classification-oriented tasks, like those within the GLUE benchmarks — you can get near 90% accuracy.&lt;/p&gt; &lt;p&gt;On the other hand, GPT-4 outperforms fine-tuned models in 6/31 tasks which are in broader, more complex domains such as coding and MMLU.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/sarthakai&quot;&gt; /u/sarthakai &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1db3sb5/study_finds_that_smaller_models_with_7b_params/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1db3sb5/study_finds_that_smaller_models_with_7b_params/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1db3sb5</id><link href="https://www.reddit.com/r/LangChain/comments/1db3sb5/study_finds_that_smaller_models_with_7b_params/" /><updated>2024-06-08T14:08:56+00:00</updated><published>2024-06-08T14:08:56+00:00</published><title>Study finds that smaller models with 7B params can now outperform GPT-4 on some tasks using LoRA. Here's how:</title></entry><entry><author><name>/u/Lost-Butterfly-382</name><uri>https://www.reddit.com/user/Lost-Butterfly-382</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;We&amp;#39;ve built a production LLM-based application. We now want to take our application to the next stage using agents. Anyway, my manager is in favour of Autogen because its supported by Microsoft and is unlikely to get convoluted like Langchain has become. Still, I&amp;#39;ve heard that Langraph provides a lot of flexibility in building agentic applications. I&amp;#39;ve heard multiple different perspectives and still haven&amp;#39;t decided. So I&amp;#39;ve decided to hear everyone who sees before deciding. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Lost-Butterfly-382&quot;&gt; /u/Lost-Butterfly-382 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1db6evc/best_production_agent_framework_langraph_vs/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1db6evc/best_production_agent_framework_langraph_vs/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1db6evc</id><link href="https://www.reddit.com/r/LangChain/comments/1db6evc/best_production_agent_framework_langraph_vs/" /><updated>2024-06-08T16:11:52+00:00</updated><published>2024-06-08T16:11:52+00:00</published><title>Best Production Agent Framework Langraph vs Autogen</title></entry><entry><author><name>/u/ANil1729</name><uri>https://www.reddit.com/user/ANil1729</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I have open-sourced a Text-To-Video-AI generated which generates video from a topic by collecting relevant stock videos and stitching them together similar to popular video tools like Invideo, Pictory etc.&lt;/p&gt; &lt;p&gt;Link to code :- &lt;a href=&quot;https://github.com/SamurAIGPT/Text-To-Video-AI&quot;&gt;https://github.com/SamurAIGPT/Text-To-Video-AI&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/ANil1729&quot;&gt; /u/ANil1729 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1db477d/opensource_text_to_video_ai_generator/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1db477d/opensource_text_to_video_ai_generator/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1db477d</id><link href="https://www.reddit.com/r/LangChain/comments/1db477d/opensource_text_to_video_ai_generator/" /><updated>2024-06-08T14:29:02+00:00</updated><published>2024-06-08T14:29:02+00:00</published><title>Open-source Text to Video AI generator</title></entry><entry><author><name>/u/mind_blight</name><uri>https://www.reddit.com/user/mind_blight</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;We build a RAG search engine for Payroll companies. We ended up having to handle a bunch of PDF data, some of which had 1000+ pages per document. We ended up building a parser and search engine entirely based around document layout analysis for ourselves. We started chatting with another AI startup that was about to add PDFs to their pipeline (they&amp;#39;d been ingesting HTML and markdown) and ended up exposing our PDF processing as an API for them. So, now we&amp;#39;re trying to figure out if that was a fluke, or if there&amp;#39;s something valuable there. &lt;/p&gt; &lt;p&gt;I&amp;#39;d really love to learn more about how people are managing PDFs, and how well it&amp;#39;s working for them. Is vector search + text chunking enough? Are folks using Layout Analysis tools or building your own in house? Have people had luck with semantic chunking?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/mind_blight&quot;&gt; /u/mind_blight &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1danr71/how_are_people_processing_pdfs_and_how_well_is_it/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1danr71/how_are_people_processing_pdfs_and_how_well_is_it/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1danr71</id><link href="https://www.reddit.com/r/LangChain/comments/1danr71/how_are_people_processing_pdfs_and_how_well_is_it/" /><updated>2024-06-07T22:31:46+00:00</updated><published>2024-06-07T22:31:46+00:00</published><title>How are people processing PDFs, and how well is it working?</title></entry><entry><author><name>/u/somewhat_advanced</name><uri>https://www.reddit.com/user/somewhat_advanced</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/somewhat_advanced&quot;&gt; /u/somewhat_advanced &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;/r/LocalLLaMA/comments/1dbcl5g/using_llms_for_highly_classified_data/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dbcn6s/using_llms_for_highly_classified_data/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dbcn6s</id><link href="https://www.reddit.com/r/LangChain/comments/1dbcn6s/using_llms_for_highly_classified_data/" /><updated>2024-06-08T20:58:02+00:00</updated><published>2024-06-08T20:58:02+00:00</published><title>Using LLM's for highly classified data</title></entry><entry><author><name>/u/FunInformation2332</name><uri>https://www.reddit.com/user/FunInformation2332</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi, guys again,&lt;/p&gt; &lt;p&gt;As you know I am building a RAG chatbot on legal texts and thanks to you I come a long way. &lt;/p&gt; &lt;p&gt;Now the problem is embeddings. I am using Openai embeddings and they are quite expensive as you know. as a retriever I am using ensemble retriever and it works great. But for using ensemble I need to use different vectorestores such as FAISS and BM25 and they also use embeddings. What I want is embed the documents for one time to pinecone and get the vectors from pinecone and push into FAISS and BM25. How can I do that?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/FunInformation2332&quot;&gt; /u/FunInformation2332 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1db3bw1/using_multiple_vectorstores_as_retirever_but/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1db3bw1/using_multiple_vectorstores_as_retirever_but/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1db3bw1</id><link href="https://www.reddit.com/r/LangChain/comments/1db3bw1/using_multiple_vectorstores_as_retirever_but/" /><updated>2024-06-08T13:46:21+00:00</updated><published>2024-06-08T13:46:21+00:00</published><title>Using Multiple Vectorstores as Retirever But Embedding The Data For One Time</title></entry><entry><author><name>/u/Fun_Highlight9147</name><uri>https://www.reddit.com/user/Fun_Highlight9147</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Just basic proof why langchain is the best for LLM Applications. &lt;/p&gt; &lt;p&gt;I developed 2 tools, in the first one I am working to applying langchain, and the second one I built thanks to langchain, from scratch.&lt;/p&gt; &lt;p&gt;Here is the LinkedIn Post&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://www.linkedin.com/feed/update/urn:li:activity:7204882941972291586/&quot;&gt;https://www.linkedin.com/feed/update/urn:li:activity:7204882941972291586/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;DEMO Video:&lt;br/&gt; &lt;a href=&quot;https://www.youtube.com/watch?v=j-MXVO4I14o&amp;amp;t=2s&quot;&gt;https://www.youtube.com/watch?v=j-MXVO4I14o&amp;amp;t=2s&lt;/a&gt;&lt;/p&gt; &lt;p&gt;PRESENTATION Video:&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=gmx3KQ9D-jQ&quot;&gt;https://www.youtube.com/watch?v=gmx3KQ9D-jQ&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The premise is, code on the PC from my phone (the tools can do much more that though). Autobot PC automation assitant can do CLI Commands, Code Interpreter, Operating the desktop with Mouse and Keyboard, and unlike Open Interpreter it works for these tasks, most of the time even with GPT 3.5. I developed these projects due to computer fatigue and back pain. &lt;/p&gt; &lt;p&gt;What do you think of these projects? Would you want to use any of these tools?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Fun_Highlight9147&quot;&gt; /u/Fun_Highlight9147 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dajv55/i_won_a_gen_ai_hackathon_with_a_10k_usd_prize/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dajv55/i_won_a_gen_ai_hackathon_with_a_10k_usd_prize/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dajv55</id><link href="https://www.reddit.com/r/LangChain/comments/1dajv55/i_won_a_gen_ai_hackathon_with_a_10k_usd_prize/" /><updated>2024-06-07T19:44:48+00:00</updated><published>2024-06-07T19:44:48+00:00</published><title>I Won a Gen AI Hackathon with a 10k USD prize thanks to Langchain!!! 1 of 150 participants (it is not battle royale :D)</title></entry><entry><author><name>/u/Charming_Error_3174</name><uri>https://www.reddit.com/user/Charming_Error_3174</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I’m planning to develop and run a chatbot on my laptop locally (Mac M1 Pro) with following features: - Read available documents in a folder (pdf,html,txt) - Generate vector embeddings , save it in a database - A UI where the user can query on top of the existing database - In the future I’d also want to fine-tune the language model on a specific dataset for better understanding of the context &lt;/p&gt; &lt;p&gt;Can anyone of you suggest what models, tools to use for this? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Charming_Error_3174&quot;&gt; /u/Charming_Error_3174 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dav6kd/local_chatbot_project/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dav6kd/local_chatbot_project/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dav6kd</id><link href="https://www.reddit.com/r/LangChain/comments/1dav6kd/local_chatbot_project/" /><updated>2024-06-08T05:04:32+00:00</updated><published>2024-06-08T05:04:32+00:00</published><title>Local chatbot project</title></entry><entry><author><name>/u/stroxsontaran</name><uri>https://www.reddit.com/user/stroxsontaran</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/stroxsontaran&quot;&gt; /u/stroxsontaran &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://vectorize.io/picking-the-best-embedding-model-for-rag/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dadczw/how_to_pick_the_best_embedding_model_for_rag/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dadczw</id><link href="https://www.reddit.com/r/LangChain/comments/1dadczw/how_to_pick_the_best_embedding_model_for_rag/" /><updated>2024-06-07T15:20:10+00:00</updated><published>2024-06-07T15:20:10+00:00</published><title>How to pick the best embedding model for RAG</title></entry><entry><author><name>/u/Old_Cauliflower6316</name><uri>https://www.reddit.com/user/Old_Cauliflower6316</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi everyone! I have a general question about RAG and Data Privacy. I&amp;#39;m using langchain + ChromaDB to build an internal Q&amp;amp;A chatbot, which is fed by multiple data sources (Slack, Confluence, Jira, Google Docs).&lt;/p&gt; &lt;p&gt;Now, when a user talks to the bot, I want to fetch documents which this user is allowed to see. For example, if a user is allowed to see document X but not document Y, I want the semantic search to exclude document Y.&lt;/p&gt; &lt;p&gt;What&amp;#39;s the best way of doing that? Are there any best practices/open source tools that help with that? I couldn&amp;#39;t find much information online, and specifically about langchain + privacy.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Old_Cauliflower6316&quot;&gt; /u/Old_Cauliflower6316 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dagp8k/data_privacy_in_rag_applications/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dagp8k/data_privacy_in_rag_applications/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dagp8k</id><link href="https://www.reddit.com/r/LangChain/comments/1dagp8k/data_privacy_in_rag_applications/" /><updated>2024-06-07T17:37:45+00:00</updated><published>2024-06-07T17:37:45+00:00</published><title>Data privacy in RAG applications</title></entry><entry><author><name>/u/sherifalaa55</name><uri>https://www.reddit.com/user/sherifalaa55</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;How can I achieve this process of thought... I&amp;#39;m using vertex ai and I have access to big query that contains google analytics data... when the user asks a question if the question needs data from google analytics and I want the agent to use a tool that will generate the correct query syntax to fetch the data, and maybe another tool to execute the query and finally use the result to answer the users question&lt;/p&gt; &lt;p&gt;is that possible using langchain&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/sherifalaa55&quot;&gt; /u/sherifalaa55 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1daslzn/how_can_i_achieve_the_following_with_tools_and/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1daslzn/how_can_i_achieve_the_following_with_tools_and/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1daslzn</id><link href="https://www.reddit.com/r/LangChain/comments/1daslzn/how_can_i_achieve_the_following_with_tools_and/" /><updated>2024-06-08T02:36:18+00:00</updated><published>2024-06-08T02:36:18+00:00</published><title>How can I achieve the following with tools and google analytics?</title></entry><entry><author><name>/u/Danidre</name><uri>https://www.reddit.com/user/Danidre</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Checkpoints seem to be the way to go for managing history for graph-based agents, proclaimed to be advantageous for conversational agents, as history is maintained. Not only that, but there is the ability to move forward or go backward in the history as well, to cover up errors, or go back in time.&lt;/p&gt; &lt;p&gt;However, some disadvantages I notice is that subsequent calls to the LLM (especially in the reAct agents, where everything is added to the messages list as context) take longer and of course use an ever increasing number of tokens.&lt;/p&gt; &lt;p&gt;There doesn&amp;#39;t seem to be a way to manipulate that history dynamically, or customize what is sent for each subsequent LLM call.&lt;/p&gt; &lt;p&gt;Additionally, there are only In-Memory, and SQLLite implementations of checkpointers by default; although the documentation advise to use something like Redis for production, there is no default Redis implementation. &lt;/p&gt; &lt;p&gt;Are these planned to be implemented in the future, or left as a task meant for the developers to implement them as needed? I see there&amp;#39;s an externally developed checkpoint implementation for Postgress. Redis, Maria, even an SQL Alchemy layer...are these implementations on us to do? It seems like quite a complex thing to implement.&lt;/p&gt; &lt;p&gt;And then in that case, rather than using checkpointers, maybe it might be simpler to maintain a chat history as before? There are already existing tools to store message history in different databases. It should not be difficult to create an additional state field that just stores the questions and responses of the conversation history, and utilize that in each invocation? That way, one would have more control over what is being sent, and even control summaries or required context in a more dynamic way, to maintain a reasonable token size per call, despite using graphs.&lt;/p&gt; &lt;p&gt;What are other&amp;#39;s thoughts and experiences where this is concerned?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Danidre&quot;&gt; /u/Danidre &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dabjys/langgraph_checkpoints_vs_history/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dabjys/langgraph_checkpoints_vs_history/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dabjys</id><link href="https://www.reddit.com/r/LangChain/comments/1dabjys/langgraph_checkpoints_vs_history/" /><updated>2024-06-07T14:05:02+00:00</updated><published>2024-06-07T14:05:02+00:00</published><title>LangGraph: Checkpoints vs History</title></entry><entry><author><name>/u/HistorianSpecific30</name><uri>https://www.reddit.com/user/HistorianSpecific30</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi, I am fetching data from sqllite3 in python using conventional select query and storing it into a variable. The output: [&amp;#39;Artificial Intelligence&amp;#39;,&amp;#39;Blockchain&amp;#39;,&amp;#39;RBA&amp;#39;] I want to save this data in chromadb as embeddings. Please guide. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/HistorianSpecific30&quot;&gt; /u/HistorianSpecific30 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dak6y7/fetching_data_from_sqllite3_and_storing_into/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dak6y7/fetching_data_from_sqllite3_and_storing_into/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dak6y7</id><link href="https://www.reddit.com/r/LangChain/comments/1dak6y7/fetching_data_from_sqllite3_and_storing_into/" /><updated>2024-06-07T19:58:43+00:00</updated><published>2024-06-07T19:58:43+00:00</published><title>Fetching data from sqllite3 and storing into Chromadb</title></entry></feed>