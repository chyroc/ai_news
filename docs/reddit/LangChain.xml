<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/"><category term="LangChain" label="r/LangChain"/><updated>2023-12-09T04:39:37+00:00</updated><icon>https://www.redditstatic.com/icon.png/</icon><id>/r/LangChain.rss</id><link rel="self" href="https://www.reddit.com/r/LangChain.rss" type="application/atom+xml" /><link rel="alternate" href="https://www.reddit.com/r/LangChain" type="text/html" /><subtitle>LangChain is an open-source framework and developer toolkit that helps developers get LLM applications from prototype to production. It is available for Python and Javascript at https://www.langchain.com/.</subtitle><title>LangChain</title><entry><author><name>/u/zchaarm</name><uri>https://www.reddit.com/user/zchaarm</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;A place for members of &lt;a href=&quot;/r/LangChain&quot;&gt;r/LangChain&lt;/a&gt; to chat with each other&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/zchaarm&quot;&gt; /u/zchaarm &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_10ljho9</id><link href="https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/" /><updated>2023-01-26T04:33:05+00:00</updated><published>2023-01-26T04:33:05+00:00</published><title>r/LangChain Lounge</title></entry><entry><author><name>/u/rambunctiousambivert</name><uri>https://www.reddit.com/user/rambunctiousambivert</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Today I was playing on openAI Assistant to get answers from a simple CSV file. After trying for 3 prompts, after which I did not really get any answers, the cost was 2$!!! which I think is high, compared to &amp;lt;1$ that used to cost when I was playing with API few months ago.&lt;/p&gt; &lt;p&gt;I am working on a recommendation app, where I will provide openAI api some of the data from my sql db and return openai suggestions to the end user. Now seeing the assistant price I feel like reverting back to using API as the assistants cost seem a bit steeper. Am I missing something, what are your thoughts?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/rambunctiousambivert&quot;&gt; /u/rambunctiousambivert &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18e25qx/openai_assistant_pricing_vs_api_pricing/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18e25qx/openai_assistant_pricing_vs_api_pricing/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18e25qx</id><link href="https://www.reddit.com/r/LangChain/comments/18e25qx/openai_assistant_pricing_vs_api_pricing/" /><updated>2023-12-09T01:27:44+00:00</updated><published>2023-12-09T01:27:44+00:00</published><title>OpenAI assistant pricing vs API pricing</title></entry><entry><author><name>/u/dberg76</name><uri>https://www.reddit.com/user/dberg76</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Did something change in the most recent version of Langchain, i could have sworn that the ConversationalRetrievalChain took a memory parameter and now its gone?&lt;/p&gt; &lt;p&gt;The docs have it &lt;a href=&quot;https://api.python.langchain.com/en/latest/chains/langchain.chains.conversational_retrieval.base.ConversationalRetrievalChain.html#langchain.chains.conversational_retrieval.base.ConversationalRetrievalChain.memory&quot;&gt;https://api.python.langchain.com/en/latest/chains/langchain.chains.conversational_retrieval.base.ConversationalRetrievalChain.html#langchain.chains.conversational_retrieval.base.ConversationalRetrievalChain.memory&lt;/a&gt; But the code errors out with a “bad key” error and there is no mention of memory in the source anymore.&lt;/p&gt; &lt;p&gt;&lt;code&gt;pydantic.v1.error_wrappers.ValidationError: 1 validation error for ConversationalRetrievalChain&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;memory value is not a valid dict (type=type_error.dict)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;what is the current idiomatic way to do RetreivalQA RAG over private documents with history/memory now ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/dberg76&quot;&gt; /u/dberg76 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18dxc2o/memory_in_conversationalretrievalchain_removed/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18dxc2o/memory_in_conversationalretrievalchain_removed/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18dxc2o</id><link href="https://www.reddit.com/r/LangChain/comments/18dxc2o/memory_in_conversationalretrievalchain_removed/" /><updated>2023-12-08T21:35:03+00:00</updated><published>2023-12-08T21:35:03+00:00</published><title>memory in ConversationalRetrievalChain removed</title></entry><entry><author><name>/u/BtownIU</name><uri>https://www.reddit.com/user/BtownIU</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;hi guys,&lt;/p&gt; &lt;p&gt;I have two RAG chains for 2 different types of answers. One of the chains is instructed to generate json and it works well by iteself.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;json_instruct=&amp;quot;&amp;quot;&amp;quot; [INST] &amp;lt;&amp;lt;SYS&amp;gt;&amp;gt; You are a helpful and concise assistent, that only comunicates using JSON files. The expected output from you has to be: { &amp;quot;cookware_name&amp;quot;: {cookware}, &amp;quot;description&amp;quot;: [], &amp;quot;ai_notes&amp;quot;: {explanation} } The INST block will always be a json string: { &amp;quot;prompt&amp;quot;: {the user prompt} } The available cookwares are in the context given to you &amp;lt;&amp;lt;/SYS&amp;gt;&amp;gt; [/INST] &amp;quot;&amp;quot;&amp;quot; JSON_pipeline = RetrievalQA.from_chain_type( llm=CustomLLM(system_instruction=json_instruct), chain_type=&amp;#39;stuff&amp;#39;, retriever=cookware_retriever ) JSON_pipeline(&amp;quot;what do i need for crepes&amp;quot;) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;But when i put these 2 chains into a langchain agent, the agent would accurately send a prompt to the json chain but return answers in regular sentences. The agent is defined as :&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;tools = [ # tool 1 Tool(...), # tool 2 Tool( name = &amp;quot;JSON data&amp;quot;, func=JSON_pipeline.run, description = &amp;quot;&amp;quot;&amp;quot; Useful for you need to answer questions about what a certain cookware is or what cookware is needed for certain dishes. &amp;quot;&amp;quot;&amp;quot;,return_direct=True ) ] llm_chain = LLMChain(llm=CustomLLM(system_instruction=&amp;quot;You are a helpful and fast assistant.&amp;quot;), prompt=prompt) tool_names = [tool.name for tool in tools] agent = LLMSingleActionAgent( llm_chain=llm_chain, output_parser=output_parser, stop=[&amp;quot;\nObservation:&amp;quot;], allowed_tools=tool_names ) agent_executor = AgentExecutor.from_agent_and_tools(agent=agent, tools=tools, verbose=True) agent_executor.run(&amp;quot;list useful cookwares for soup&amp;quot;) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;The answer is indeed a list useful cookwares based on the documents. But it&amp;#39;s no longer in a JSON format that was generated by the JSON_pipeline&lt;/p&gt; &lt;p&gt;Is there any way I can make the agent keep the answers as provided by the tools without modifying the answers by the central &amp;quot;router&amp;quot; LLM? (Or am I missing out some parameters?)&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/BtownIU&quot;&gt; /u/BtownIU &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18dv7wx/langchain_agent_does_not_return_json_files/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18dv7wx/langchain_agent_does_not_return_json_files/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18dv7wx</id><link href="https://www.reddit.com/r/LangChain/comments/18dv7wx/langchain_agent_does_not_return_json_files/" /><updated>2023-12-08T20:00:19+00:00</updated><published>2023-12-08T20:00:19+00:00</published><title>langchain agent does not return JSON files generated from tools</title></entry><entry><author><name>/u/sonaryn</name><uri>https://www.reddit.com/user/sonaryn</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Can’t ask vanilla GPT 4 since it’s past the training cutoff, and can’t make one myself cause, well, I can’t understand the docs&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/sonaryn&quot;&gt; /u/sonaryn &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18dxn4b/are_there_any_custom_gpts_copilot_bots_trained_on/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18dxn4b/are_there_any_custom_gpts_copilot_bots_trained_on/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18dxn4b</id><link href="https://www.reddit.com/r/LangChain/comments/18dxn4b/are_there_any_custom_gpts_copilot_bots_trained_on/" /><updated>2023-12-08T21:49:07+00:00</updated><published>2023-12-08T21:49:07+00:00</published><title>Are there any custom GPTs / Copilot bots trained on the latest LangChain docs?</title></entry><entry><author><name>/u/One-Difficulty3149</name><uri>https://www.reddit.com/user/One-Difficulty3149</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hello everyone, I have been working with langchain and has built some RAG applications. I have used FAISS as the vector database, which inherently does not support CRUD operations completely. If anyone has any inputs on which of the vector databases support CRUD operations, which they might have tried and tested. And also it should be efficient and not accurate, not too much time consuming. Thanks.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/One-Difficulty3149&quot;&gt; /u/One-Difficulty3149 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18dgp16/crud_operations_on_vector_databases/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18dgp16/crud_operations_on_vector_databases/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18dgp16</id><link href="https://www.reddit.com/r/LangChain/comments/18dgp16/crud_operations_on_vector_databases/" /><updated>2023-12-08T06:30:28+00:00</updated><published>2023-12-08T06:30:28+00:00</published><title>CRUD operations on Vector Databases</title></entry><entry><author><name>/u/Aggressive-Salt-4042</name><uri>https://www.reddit.com/user/Aggressive-Salt-4042</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Survey here → &lt;a href=&quot;https://survey.alchemer.com/s3/7626156/ES2412p&quot;&gt;https://survey.alchemer.com/s3/7626156/ES2412p&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Aggressive-Salt-4042&quot;&gt; /u/Aggressive-Salt-4042 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18dsx62/the_elasticsearch_developer_survey_is_here_if_you/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18dsx62/the_elasticsearch_developer_survey_is_here_if_you/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18dsx62</id><link href="https://www.reddit.com/r/LangChain/comments/18dsx62/the_elasticsearch_developer_survey_is_here_if_you/" /><updated>2023-12-08T18:16:15+00:00</updated><published>2023-12-08T18:16:15+00:00</published><title>The Elasticsearch developer survey is here! If you build an app with search and/or gen AI, consider taking this 10-min survey. Your feedback will mean a lot! → https://survey.alchemer.com/s3/7626156/ES2412p</title></entry><entry><author><name>/u/ebsbdbdbdb</name><uri>https://www.reddit.com/user/ebsbdbdbdb</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I am using LangChain with Python, specifically this part of the API for Open AI Assistants - &lt;a href=&quot;https://python.langchain.com/docs/modules/agents/agent_types/openai_assistants&quot;&gt;https://python.langchain.com/docs/modules/agents/agent_types/openai_assistants&lt;/a&gt; &lt;/p&gt; &lt;p&gt;How do I attach files to user prompts? &lt;/p&gt; &lt;p&gt;Looking at the following code - &lt;/p&gt; &lt;pre&gt;&lt;code&gt;from langchain.agents import AgentExecutor agent_executor = AgentExecutor(agent=agent, tools=tools) agent_executor.invoke({&amp;quot;content&amp;quot;: &amp;quot;What&amp;#39;s the weather in SF today divided by 2.7&amp;quot;}) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The content property of the object that is being passed into the invoke function has the user prompt - what we are sending to the assistant. But in addition to text prompts, it is also possible to send files to the assistant in a user prompt. In the OpenAI Assistants Web UI, there is an attach file button. With the official OpenAI Assistants API, I am able to upload the file to OpenAI&amp;#39;s file system and then attach the file via the file ID that is generated.&lt;/p&gt; &lt;p&gt;How can I attach files to a user prompt with the LangChain OpenAI Assistant API? The documentation doesn&amp;#39;t seem to mention how I can do this from what I&amp;#39;ve read.&lt;/p&gt; &lt;p&gt;Thank you.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/ebsbdbdbdb&quot;&gt; /u/ebsbdbdbdb &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18dloff/attaching_files_to_user_prompt_when_using/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18dloff/attaching_files_to_user_prompt_when_using/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18dloff</id><link href="https://www.reddit.com/r/LangChain/comments/18dloff/attaching_files_to_user_prompt_when_using/" /><updated>2023-12-08T12:22:12+00:00</updated><published>2023-12-08T12:22:12+00:00</published><title>Attaching files to user prompt when using LangChain OpenAI Assistant API</title></entry><entry><author><name>/u/EmployFew4830</name><uri>https://www.reddit.com/user/EmployFew4830</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi there, I am looking to implement a RAGish use case with python and langchain. Want to share my high level plan here and ask for your feedback. &lt;/p&gt; &lt;p&gt;Process roughly is as follows&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Access a catalog of large, complex and semi-structured objects. Each object consists of some string attributes and contains multi-level list objects. The &amp;quot;payload&amp;quot; often is formatted as JSON (not sure yet whether I should extract more attributes or just hope that the LLM can allow queries also on the insides of that JSON). I created a python class that represents that structure&lt;/li&gt; &lt;li&gt;Next step is to create embeddings (using langchain &amp;amp; Chroma for now). Here I am unsure how to proceed. At least I want to have a dedicated embedding of each of my catalog items. Use case will be, that users present their use case (like &amp;quot;where do i find information on attribute XYZ?&amp;quot;). I want the LLM to return the suitable catalog item. But almost more important, I want the LLM to look inside the catalog item, look at all attributes and evaluate whether a certain attribute suits (rather than the whole catalog item). &lt;ul&gt; &lt;li&gt;Q: Should I embed each catalog item as one entry or rather create embeddings on artifact level? If so, how do I maintain reference to the catalog item? I could duplicate relevant catalog item information of course for each attribute. &lt;/li&gt; &lt;li&gt;Q: Already thinking of retrieval (plan to use in retrieval chains). I do foresee that user queries may not be straight forward enough. I may need an agent that guides the user through her query (like U: &amp;quot;where do i find information on attribute XYZ?&amp;quot; -&amp;gt; S: &amp;quot;Help me to understand more about your use case: Which parts of the catalog are you interested in? Rather A or B or C?&amp;quot; -&amp;gt; ...). For that I need to make the LLM aware of both, the catalog structure and the insides of a catalog item. Can I do that with just &amp;quot;flat&amp;quot; embeddings? &lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Once the embeddings are done, I want to use them as {context} with langchain. I first want to try how far I can get with chains. &lt;ul&gt; &lt;li&gt;Q: Any experience / examples on how to define prompts in a way that I can use information from context? Can I explicitly reference metadata information somehow? Like &amp;quot;system message: You´re an expert on the catalog from {context} and want to recommend discrete items. You should always use the {name} and {summary} in your recommendations&amp;quot;. Additionally, if i index each catalog item separately, I need to look into the items´ structure, like &amp;quot;system message: In your recommendations please always list the output parameters of the method of a catalog item that made you pick this recommendation&amp;quot;. Is that likely to work? &lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Some high level assessment of how far I can get with chains or whether I should use agents for my case would be very helpful as well. &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Thanks already for the discussion!&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/EmployFew4830&quot;&gt; /u/EmployFew4830 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18djyo6/structure_of_embeddings_for_complex_objects_how/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18djyo6/structure_of_embeddings_for_complex_objects_how/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18djyo6</id><link href="https://www.reddit.com/r/LangChain/comments/18djyo6/structure_of_embeddings_for_complex_objects_how/" /><updated>2023-12-08T10:27:02+00:00</updated><published>2023-12-08T10:27:02+00:00</published><title>Structure of embeddings for complex objects / how to interact with structure in langchain</title></entry><entry><author><name>/u/charlestehio</name><uri>https://www.reddit.com/user/charlestehio</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I’m currently self hosting thenlper/gte-small in a 6USD DO droplet, 1GB Ram, 1vCPU. Not too happy with the throughput. API cold start can go up to 5-8 seconds, and averaging around 2-3 seconds. &lt;/p&gt; &lt;p&gt;I am planning to switch over to baai/bge-small-en-v1.5 for newer projects because of Cloudflare Workers AI but they have no pricing model and not recommended for production yet. In the meantime, anyone has any ideas on how to mangle through this?&lt;/p&gt; &lt;p&gt;No OpenAI embeddings, thank you! I prefer something that can be self hosted and managed so I can scale up / down in costs&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/charlestehio&quot;&gt; /u/charlestehio &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18de6xv/any_managed_vector_embedding_services/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18de6xv/any_managed_vector_embedding_services/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18de6xv</id><link href="https://www.reddit.com/r/LangChain/comments/18de6xv/any_managed_vector_embedding_services/" /><updated>2023-12-08T04:04:33+00:00</updated><published>2023-12-08T04:04:33+00:00</published><title>Any managed vector embedding services?</title></entry><entry><author><name>/u/bickrombishsass</name><uri>https://www.reddit.com/user/bickrombishsass</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I need to randomly select some documents from a langchain vectorstore. Is there any process for that. One solution is to generate a random embedding vector and then do similarity search with that vector. But is there any efficient method for randomly selecting some document. Selecting documents with random index will also work.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/bickrombishsass&quot;&gt; /u/bickrombishsass &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18dj6ep/random_search_on_vectorstore/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18dj6ep/random_search_on_vectorstore/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18dj6ep</id><link href="https://www.reddit.com/r/LangChain/comments/18dj6ep/random_search_on_vectorstore/" /><updated>2023-12-08T09:26:54+00:00</updated><published>2023-12-08T09:26:54+00:00</published><title>Random search on vectorstore</title></entry><entry><author><name>/u/PrudentCherry322</name><uri>https://www.reddit.com/user/PrudentCherry322</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18dguub/uae_new_sentence_embeddings_for_rag_sota_on_mteb/&quot;&gt; &lt;img src=&quot;https://external-preview.redd.it/2NOXPPsK7CrArgZgW_8munT8Vlwu5T4KB6tE801WSTY.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c51499cd903e5d873363d5f9163703a0d74d26e1&quot; alt=&quot;UAE: New Sentence Embeddings for RAG | SOTA on MTEB Leaderboard&quot; title=&quot;UAE: New Sentence Embeddings for RAG | SOTA on MTEB Leaderboard&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/PrudentCherry322&quot;&gt; /u/PrudentCherry322 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://github.com/SeanLee97/AnglE&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18dguub/uae_new_sentence_embeddings_for_rag_sota_on_mteb/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_18dguub</id><media:thumbnail url="https://external-preview.redd.it/2NOXPPsK7CrArgZgW_8munT8Vlwu5T4KB6tE801WSTY.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=c51499cd903e5d873363d5f9163703a0d74d26e1" /><link href="https://www.reddit.com/r/LangChain/comments/18dguub/uae_new_sentence_embeddings_for_rag_sota_on_mteb/" /><updated>2023-12-08T06:41:10+00:00</updated><published>2023-12-08T06:41:10+00:00</published><title>UAE: New Sentence Embeddings for RAG | SOTA on MTEB Leaderboard</title></entry><entry><author><name>/u/Impressive_Gate2102</name><uri>https://www.reddit.com/user/Impressive_Gate2102</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;I am trying to achieve streaming for a custom LLM hosted on a server. I am using nodejs for the same. If anyone has worked on something similar, could you please guide me? &lt;/p&gt; &lt;p&gt;I am completely clueless on this. &lt;/p&gt; &lt;p&gt;Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Impressive_Gate2102&quot;&gt; /u/Impressive_Gate2102 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18dgatn/need_help_with_streaming_for_custom_llm_nodejs/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18dgatn/need_help_with_streaming_for_custom_llm_nodejs/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18dgatn</id><link href="https://www.reddit.com/r/LangChain/comments/18dgatn/need_help_with_streaming_for_custom_llm_nodejs/" /><updated>2023-12-08T06:04:45+00:00</updated><published>2023-12-08T06:04:45+00:00</published><title>Need help with streaming for Custom LLM Nodejs</title></entry><entry><author><name>/u/prajwalsouza</name><uri>https://www.reddit.com/user/prajwalsouza</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18d0ikm/fixed_the_blog_post_to_match_the_technical_report/&quot;&gt; &lt;img src=&quot;https://preview.redd.it/ume5ozy60u4c1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ee13048518ecc5037d2260691c203bdb5c5c5973&quot; alt=&quot;Fixed the blog post to match the technical report on Gemini. :)&quot; title=&quot;Fixed the blog post to match the technical report on Gemini. :)&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/prajwalsouza&quot;&gt; /u/prajwalsouza &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://i.redd.it/ume5ozy60u4c1.png&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18d0ikm/fixed_the_blog_post_to_match_the_technical_report/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_18d0ikm</id><media:thumbnail url="https://preview.redd.it/ume5ozy60u4c1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=ee13048518ecc5037d2260691c203bdb5c5c5973" /><link href="https://www.reddit.com/r/LangChain/comments/18d0ikm/fixed_the_blog_post_to_match_the_technical_report/" /><updated>2023-12-07T17:20:51+00:00</updated><published>2023-12-07T17:20:51+00:00</published><title>Fixed the blog post to match the technical report on Gemini. :)</title></entry><entry><author><name>/u/Positively101</name><uri>https://www.reddit.com/user/Positively101</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I intend to create a local llm based chatbot for my team. Basically it should be able to read the docs and generate intelligent responses. I&amp;#39;m pretty new to LLMs and have tried few things here and there. Overall I intend to present a prototype on a non-GPU or useless GPU based machine first. From what I understand so far I need to create a RAG pipeline. I&amp;#39;ve seen few architectures using embeddings, vector databases, langchain and a model to do create such a pipeline. I&amp;#39;m still pretty new to all these jargons. I have tried few opensource models as well locally and most of them just crash my M1 laptop. I have better work laptop with 16 GP RAM and 8GB graphics card memory on an A2000 card. Can you please suggest how can I quickly come up with a prototype. Basically the RAG pipeline(or any other method) should be able to quickly switch between different LLM models, or databases or any other components when it comes to deploying on a production setup. Also, for now, the idea is to use the data from pdf docs, word docs or data downloaded in json format. I&amp;#39;m not averse of coding so I can code one if I know what to do. Please suggest. Also please post any useful suggestions, articles, course, etc.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Positively101&quot;&gt; /u/Positively101 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18dcnkc/localprivate_llm_based_chatbot_using_freeopen/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18dcnkc/localprivate_llm_based_chatbot_using_freeopen/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18dcnkc</id><link href="https://www.reddit.com/r/LangChain/comments/18dcnkc/localprivate_llm_based_chatbot_using_freeopen/" /><updated>2023-12-08T02:42:34+00:00</updated><published>2023-12-08T02:42:34+00:00</published><title>local/private llm based chatbot using free/open source tools.</title></entry><entry><author><name>/u/SirEliteKaffee</name><uri>https://www.reddit.com/user/SirEliteKaffee</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18cun5p/langserve_stream_works_invoke_doesnt/&quot;&gt; &lt;img src=&quot;https://a.thumbs.redditmedia.com/B-LXWm_EcnpH-dG-Ja03T-WcTOibRuhMkkOlg6KoQI8.jpg&quot; alt=&quot;LangServe: Stream works, Invoke doesn't&quot; title=&quot;LangServe: Stream works, Invoke doesn't&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;m building a very simple chain that takes as an input a customer feedback string and categorizes it into the following pydantic class:&lt;/p&gt; &lt;pre&gt;&lt;code&gt; class AnalysisAttributes(BaseModel): overall_positive: bool = Field(description=&amp;quot;&amp;lt;sentiment is positive overall&amp;gt;&amp;quot;) mentions_pricing: bool = Field(description=&amp;quot;&amp;lt;pricing is mentioned&amp;gt;&amp;quot;) mentions_competition: bool = Field(description=&amp;quot;&amp;lt;competition is mentioned&amp;gt;&amp;quot;) parser = PydanticOutputParser(pydantic_object=AnalysisAttributes) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;Here&amp;#39;s how this should work, and it does:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;full_pipeline = prompt | model | parser output = full_pipeline.invoke({&amp;quot;feedback&amp;quot;: &amp;quot;This bad company is very expensive.&amp;quot;}) expected_output = AnalysisAttributes(overall_positive=False, mentions_pricing=True, mentions_competition=False) assert output == expected_output. # this works! :) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;This works very well, all good so far! Let&amp;#39;s serve it:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;app = FastAPI( title=&amp;quot;LangChain Server&amp;quot;, version=&amp;quot;1.0&amp;quot;, description=&amp;quot;A simple api server using Langchain&amp;#39;s Runnable interfaces&amp;quot;, ) pipeline = prompt | model | parser add_routes(app, pipeline, path=&amp;quot;/categorize_feedback&amp;quot;) if __name__ == &amp;quot;__main__&amp;quot;: import uvicorn uvicorn.run(app, host=&amp;quot;localhost&amp;quot;, port=8000) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;Now comes the strange part, check this out. On the client side, streaming works:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;response = requests.post( &amp;quot;http://localhost:8000/categorize_feedback/stream/&amp;quot;, json={&amp;#39;input&amp;#39;: {&amp;#39;feedback&amp;#39;: &amp;#39;Prices are too high.&amp;#39;}} ) for chunk in response: print(chunk.decode()) # event: metadata [...] data: {&amp;quot;overall_positive&amp;quot;:false, ... &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;But the regular invoke does not work, it delivers an empty output:&lt;/strong&gt; &lt;/p&gt; &lt;pre&gt;&lt;code&gt;response = requests.post( &amp;quot;http://localhost:8000/categorize_feedback/invoke/&amp;quot;, json={&amp;#39;input&amp;#39;: {&amp;#39;feedback&amp;#39;: &amp;#39;Prices are too high.&amp;#39;}} ) print(response.json()) # {&amp;#39;output&amp;#39;: {}, &amp;#39;callback_events&amp;#39;: [], &amp;#39;metadata&amp;#39;: {&amp;#39;run_id&amp;#39;: &amp;#39;acdd089d-3c80-4624-8122-17c4173dc1ec&amp;#39;}} &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Any ideas? For more info, check out the langserve playground output: &lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/1fc9vbfcav4c1.png?width=1930&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e8bd8119978ed74ebe9d1b8d453b77263fbc3701&quot;&gt;https://preview.redd.it/1fc9vbfcav4c1.png?width=1930&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e8bd8119978ed74ebe9d1b8d453b77263fbc3701&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/SirEliteKaffee&quot;&gt; /u/SirEliteKaffee &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18cun5p/langserve_stream_works_invoke_doesnt/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18cun5p/langserve_stream_works_invoke_doesnt/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_18cun5p</id><media:thumbnail url="https://a.thumbs.redditmedia.com/B-LXWm_EcnpH-dG-Ja03T-WcTOibRuhMkkOlg6KoQI8.jpg" /><link href="https://www.reddit.com/r/LangChain/comments/18cun5p/langserve_stream_works_invoke_doesnt/" /><updated>2023-12-07T12:33:59+00:00</updated><published>2023-12-07T12:33:59+00:00</published><title>LangServe: Stream works, Invoke doesn't</title></entry><entry><author><name>/u/learning_hedonism</name><uri>https://www.reddit.com/user/learning_hedonism</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Basically I want to have my llm do research for me. Would be nice to have some sort of feedback system rather than just dumb for loops. &lt;/p&gt; &lt;p&gt;Any advice?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/learning_hedonism&quot;&gt; /u/learning_hedonism &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18cv1ds/any_langchain_integrations_that_search_and_crawl/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18cv1ds/any_langchain_integrations_that_search_and_crawl/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18cv1ds</id><link href="https://www.reddit.com/r/LangChain/comments/18cv1ds/any_langchain_integrations_that_search_and_crawl/" /><updated>2023-12-07T12:57:48+00:00</updated><published>2023-12-07T12:57:48+00:00</published><title>Any langchain integrations that search and crawl?</title></entry><entry><author><name>/u/RayMallick</name><uri>https://www.reddit.com/user/RayMallick</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;First time looking into LangChain and vector dbs. I have been creating with some fun applications with LLMs so I have some understanding of how they work and how to interface with them. &lt;/p&gt; &lt;p&gt;Reading through the LangChan doc, I&amp;#39;m trying to get an understanding of how vector dbs affect the prompt? To early understandings, to me it seems like using a vector db would increase the tokens used by LLM in the prompts and thus the cost (if using an API, like Open AI). &lt;/p&gt; &lt;p&gt;Can anyone provide any further insight? Is this correct?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/RayMallick&quot;&gt; /u/RayMallick &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18cpx8j/does_using_a_vector_db_increase_llm_api_cost/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18cpx8j/does_using_a_vector_db_increase_llm_api_cost/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18cpx8j</id><link href="https://www.reddit.com/r/LangChain/comments/18cpx8j/does_using_a_vector_db_increase_llm_api_cost/" /><updated>2023-12-07T06:59:09+00:00</updated><published>2023-12-07T06:59:09+00:00</published><title>Does using a vector db increase LLM API cost?</title></entry><entry><author><name>/u/clickmildest</name><uri>https://www.reddit.com/user/clickmildest</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey all, I was wondering if there’s a dedicated app to upload both resume and job posting to get insights whether someone is a good fit for the job. Provide suggestions, insight even hold a mock interview! &lt;/p&gt; &lt;p&gt;It sounds like a great use for AI and considering the current job market it could really helpful. If something like this doesn’t exist I would love to build something like this! &lt;/p&gt; &lt;p&gt;Looking forward to y’all’s feedback&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/clickmildest&quot;&gt; /u/clickmildest &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18cmpce/interview_prep_and_resume_checker/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18cmpce/interview_prep_and_resume_checker/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18cmpce</id><link href="https://www.reddit.com/r/LangChain/comments/18cmpce/interview_prep_and_resume_checker/" /><updated>2023-12-07T03:49:29+00:00</updated><published>2023-12-07T03:49:29+00:00</published><title>Interview Prep and resume checker!</title></entry><entry><author><name>/u/Useful_Ad_7882</name><uri>https://www.reddit.com/user/Useful_Ad_7882</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I am trying to speed up my embeddings with rayllm integration on my m1 macbook pro. This is what the new code looks like: &lt;/p&gt; &lt;p&gt;`@ray.remote&lt;br/&gt; def process_shards(shard,collection_name):&lt;br/&gt; print(&amp;quot;embedding stuff&amp;quot;)&lt;br/&gt; embeddings = OpenAIEmbeddings(model=&amp;quot;text-embedding-ada-002&amp;quot;)&lt;br/&gt; print(f&amp;#39;Starting process_shard of {len(shard)} chunks.&amp;#39;)&lt;br/&gt; st = time.time()&lt;br/&gt; result = Chroma.from_documents(shard,embeddings,collection_metadata={&amp;quot;hnsw:space&amp;quot;: &amp;quot;cosine&amp;quot;})&lt;br/&gt; et = time.time() - st&lt;br/&gt; print(f&amp;#39;Shard completed in {et} seconds.&amp;#39;)&lt;br/&gt; return result`&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;above is called be by following method : &lt;/p&gt; &lt;p&gt;`def get_vectorstore(collection_name,text_chunks):&lt;br/&gt; #sharded processing with ray&lt;br/&gt; embeddings = OpenAIEmbeddings(model=&amp;quot;text-embedding-ada-002&amp;quot;)&lt;br/&gt; if text_chunks is None:&lt;br/&gt; return Chroma(persist_directory=persist_directory,embedding_function=embeddings,collection_name=collection_name)&lt;br/&gt; shards = np.array_split(text_chunks, db_shards)&lt;br/&gt; futures = [process_shards.remote(shards[i],collection_name) for i in range(db_shards)]&lt;br/&gt; results = ray.get(futures)&lt;br/&gt; #post processing after shards are available.&lt;br/&gt; db = results[0]&lt;br/&gt; for i in range(1,db_shards):&lt;br/&gt; db.merge_from(results[i])&lt;br/&gt; print(&amp;quot;now creating a new database to persist&amp;quot;)&lt;br/&gt; #create new chromadb and persist it&lt;br/&gt; #db.persist()&lt;br/&gt; return db`&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;when i run this, i get the following error: &lt;/p&gt; &lt;p&gt;TypeError: cannot pickle &amp;#39;sqlite3.Connection&amp;#39; object&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;Anyone who has solved for same ? Much appreciated.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Useful_Ad_7882&quot;&gt; /u/Useful_Ad_7882 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18crvph/pickle_error_while_trying_to_use_langchain_with/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18crvph/pickle_error_while_trying_to_use_langchain_with/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18crvph</id><link href="https://www.reddit.com/r/LangChain/comments/18crvph/pickle_error_while_trying_to_use_langchain_with/" /><updated>2023-12-07T09:24:00+00:00</updated><published>2023-12-07T09:24:00+00:00</published><title>pickle error while trying to use langchain with chromadb and rayllm</title></entry><entry><author><name>/u/I-Eat-Nuts</name><uri>https://www.reddit.com/user/I-Eat-Nuts</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;As title suggests, i want to add memory to vreate_csv_agent so that it remembers past conversations and queries from the subset of data it provided in the past in case the user prompts for it? If any further explanation is required please ask, but help me out.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/I-Eat-Nuts&quot;&gt; /u/I-Eat-Nuts &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18cqjxh/how_do_i_add_memory_to_a_create_csv_agent/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18cqjxh/how_do_i_add_memory_to_a_create_csv_agent/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18cqjxh</id><link href="https://www.reddit.com/r/LangChain/comments/18cqjxh/how_do_i_add_memory_to_a_create_csv_agent/" /><updated>2023-12-07T07:43:56+00:00</updated><published>2023-12-07T07:43:56+00:00</published><title>How do i add memory to a create_csv_agent?</title></entry><entry><author><name>/u/Bi11i0naire</name><uri>https://www.reddit.com/user/Bi11i0naire</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;How LLM Saas companies handle the data that is provided by customers?&lt;/p&gt; &lt;p&gt;For enterprise customers, what is the best strategy to retain data in-house and use LLMs?&lt;/p&gt; &lt;p&gt;Curios to know the thoughts/comments from the community.&lt;/p&gt; &lt;p&gt;Edit: &lt;/p&gt; &lt;ol&gt; &lt;li&gt;As a Saas user, enterprise customers will make API calls with input in required format&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Input → API Call → Saas Servers&lt;/p&gt; &lt;p&gt;For RAG use cases, Saas company might be pre-processing, building vectordbs on enterprise data to provide relevant answers.&lt;/p&gt; &lt;p&gt;However, saas company employees can see what data is coming in. Even though most of the Saas companies adhere to GDPR and other data privacy policies, they still have access to what enterprise customers are doing with their models.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;If an enterprise is paranoid about sharing their proprietary data, one of the option to use LLMs is:&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Take an open source LLM model → fine tune to enterprise data (optional) → create RAG with enterprise data → Deploy it in cloud/on-prem → Provide a secure endpoint to enterprise users&lt;/p&gt; &lt;p&gt;Data doesn&amp;#39;t leave but users within enterprise can leverage the benefits of RAG&lt;/p&gt; &lt;p&gt;Am I missing anything here?&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Bi11i0naire&quot;&gt; /u/Bi11i0naire &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18cilzo/data_privacy_with_llm_saas_companies/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18cilzo/data_privacy_with_llm_saas_companies/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18cilzo</id><link href="https://www.reddit.com/r/LangChain/comments/18cilzo/data_privacy_with_llm_saas_companies/" /><updated>2023-12-07T00:20:38+00:00</updated><published>2023-12-07T00:20:38+00:00</published><title>Data privacy with LLM Saas companies</title></entry><entry><author><name>/u/johan_donquixote</name><uri>https://www.reddit.com/user/johan_donquixote</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Currently I am looping over chunks and getting keywords using prompt...&lt;/p&gt; &lt;p&gt;How do I combine the keywords from different chunks to get the most important keywords of the whole doc. &lt;/p&gt; &lt;p&gt;I was thinking of giving the summary of document(to understand context) as an input to the prompt along with all the keywords to get final output...&lt;/p&gt; &lt;p&gt;Any better method to do this?&lt;/p&gt; &lt;p&gt;TIA&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/johan_donquixote&quot;&gt; /u/johan_donquixote &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18cbvfj/i_want_to_extract_important_keywords_from_large/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18cbvfj/i_want_to_extract_important_keywords_from_large/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18cbvfj</id><link href="https://www.reddit.com/r/LangChain/comments/18cbvfj/i_want_to_extract_important_keywords_from_large/" /><updated>2023-12-06T19:24:55+00:00</updated><published>2023-12-06T19:24:55+00:00</published><title>I want to extract important keywords from large documents...</title></entry><entry><author><name>/u/ronittsainii</name><uri>https://www.reddit.com/user/ronittsainii</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hello Everyone,&lt;/p&gt; &lt;p&gt;I have developed a Chatbot using LangChain, Open AI LLM and Next Js.&lt;/p&gt; &lt;p&gt;The chatbot currently is by the name of &amp;quot;HR Chatbot&amp;quot;.&lt;/p&gt; &lt;p&gt;If you want to get a chatbot developed using LangChain, Open AI and Next Js/Python you can PM me. Or if you are a developer I can directly sell you the source code of the one that I have built. &lt;/p&gt; &lt;p&gt;I am even open to setting up a free consultation!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/ronittsainii&quot;&gt; /u/ronittsainii &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18cts9j/open_ai_and_langchain_chatbot/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18cts9j/open_ai_and_langchain_chatbot/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18cts9j</id><link href="https://www.reddit.com/r/LangChain/comments/18cts9j/open_ai_and_langchain_chatbot/" /><updated>2023-12-07T11:39:57+00:00</updated><published>2023-12-07T11:39:57+00:00</published><title>Open AI and LangChain Chatbot</title></entry><entry><author><name>/u/matt3526</name><uri>https://www.reddit.com/user/matt3526</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I’m using the above to query a sql database and return results. However in cases where text is returned (like a few product reviews for example) I’d like to know the sentiment of each review and how this is changing over time. Is it possible to do this with langchain?&lt;/p&gt; &lt;p&gt;Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/matt3526&quot;&gt; /u/matt3526 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18chjxz/can_i_take_results_from_create_sql_agent_and_do/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18chjxz/can_i_take_results_from_create_sql_agent_and_do/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18chjxz</id><link href="https://www.reddit.com/r/LangChain/comments/18chjxz/can_i_take_results_from_create_sql_agent_and_do/" /><updated>2023-12-06T23:31:11+00:00</updated><published>2023-12-06T23:31:11+00:00</published><title>Can I take results from create_sql_agent and do other things with it?</title></entry><entry><author><name>/u/mean-short-</name><uri>https://www.reddit.com/user/mean-short-</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I want to create an agent that is able to do RAG using langchain.&lt;br/&gt; I found this: &lt;a href=&quot;https://python.langchain.com/docs/use_cases/question_answering/conversational_retrieval_agents&quot;&gt;https://python.langchain.com/docs/use_cases/question_answering/conversational_retrieval_agents&lt;/a&gt;&lt;br/&gt; I can&amp;#39;t seem to get it to focus its search on the database alone, it still goes to its general knowledge to answer.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/mean-short-&quot;&gt; /u/mean-short- &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18cemoh/rag_with_agents/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18cemoh/rag_with_agents/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18cemoh</id><link href="https://www.reddit.com/r/LangChain/comments/18cemoh/rag_with_agents/" /><updated>2023-12-06T21:22:57+00:00</updated><published>2023-12-06T21:22:57+00:00</published><title>RAG with agents</title></entry></feed>