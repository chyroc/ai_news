<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/"><category term="LangChain" label="r/LangChain"/><updated>2024-07-08T04:45:36+00:00</updated><icon>https://www.redditstatic.com/icon.png/</icon><id>/r/LangChain.rss</id><link rel="self" href="https://www.reddit.com/r/LangChain.rss" type="application/atom+xml" /><link rel="alternate" href="https://www.reddit.com/r/LangChain" type="text/html" /><subtitle>LangChain is an open-source framework and developer toolkit that helps developers get LLM applications from prototype to production. It is available for Python and Javascript at https://www.langchain.com/.</subtitle><title>LangChain</title><entry><author><name>/u/zchaarm</name><uri>https://www.reddit.com/user/zchaarm</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;A place for members of &lt;a href=&quot;/r/LangChain&quot;&gt;r/LangChain&lt;/a&gt; to chat with each other&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/zchaarm&quot;&gt; /u/zchaarm &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_10ljho9</id><link href="https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/" /><updated>2023-01-26T04:33:05+00:00</updated><published>2023-01-26T04:33:05+00:00</published><title>r/LangChain Lounge</title></entry><entry><author><name>/u/New-Contribution6302</name><uri>https://www.reddit.com/user/New-Contribution6302</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I wanted to use LLama2 instruct 32k for summarisation task. I tried to load the llm with n_ctx=16384, rope_freq_scale=0.25 and 0.125. But sometimes I get the output empty and sometimes i don&amp;#39;t even get one and the system gets crashed.&lt;/p&gt; &lt;p&gt;I worked this out in college t4 GPU session, kaggle&amp;#39;s 2x t4 GPU session, and my local session with 32GB RAM and rtx 3050 6gb vRAM system. &lt;/p&gt; &lt;p&gt;Any suggestions on how to load the llm and What will be the minimum hardware requirement. Model used: LLama2-instruct-32k-Q4_K_M.gguf by TheBloke&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/New-Contribution6302&quot;&gt; /u/New-Contribution6302 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dxx6wh/llama2_instruct_with_32k_context/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dxx6wh/llama2_instruct_with_32k_context/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dxx6wh</id><link href="https://www.reddit.com/r/LangChain/comments/1dxx6wh/llama2_instruct_with_32k_context/" /><updated>2024-07-08T01:58:26+00:00</updated><published>2024-07-08T01:58:26+00:00</published><title>LLama2 instruct with 32k context.</title></entry><entry><author><name>/u/gibriyagi</name><uri>https://www.reddit.com/user/gibriyagi</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;When to use each of them? Are they complementary or using one of them is enough?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/gibriyagi&quot;&gt; /u/gibriyagi &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dxpdwo/rrf_vs_reranker_models/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dxpdwo/rrf_vs_reranker_models/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dxpdwo</id><link href="https://www.reddit.com/r/LangChain/comments/1dxpdwo/rrf_vs_reranker_models/" /><updated>2024-07-07T20:00:25+00:00</updated><published>2024-07-07T20:00:25+00:00</published><title>RRF vs Reranker Models</title></entry><entry><author><name>/u/Fun_Put_8731</name><uri>https://www.reddit.com/user/Fun_Put_8731</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I have a chatbot that will communicate with users of different languages from English.&lt;/p&gt; &lt;p&gt;What do you think might be the best strategy to handle this?&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Have n system prompts translated into the n most commonly used languages, do language detect on the user&amp;#39;s first message, and use the prompt in their language. In case there is no prompt in the language in question do fallback to English.&lt;/li&gt; &lt;li&gt;Do language detect for each message received, translate the message with another llm (or aws translate) pass the English message to the English system prompt, receive the response and translate it into the language of the initial message.&lt;/li&gt; &lt;li&gt;Other strategies?&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Fun_Put_8731&quot;&gt; /u/Fun_Put_8731 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dxjozr/chatbot_with_users_of_different_languages/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dxjozr/chatbot_with_users_of_different_languages/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dxjozr</id><link href="https://www.reddit.com/r/LangChain/comments/1dxjozr/chatbot_with_users_of_different_languages/" /><updated>2024-07-07T15:53:43+00:00</updated><published>2024-07-07T15:53:43+00:00</published><title>Chatbot with users of different languages</title></entry><entry><author><name>/u/Plenty-Armadillo6938</name><uri>https://www.reddit.com/user/Plenty-Armadillo6938</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey &lt;a href=&quot;/r/LangChain&quot;&gt;r/LangChain&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Data science grad student here, looking to team up on a machine learning, deep learning, or NLP project. I am pretty much open to work on anything interesting - existing ideas or starting from scratch.&lt;/p&gt; &lt;p&gt;Quick rundown:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;DS grad student in the US&lt;/li&gt; &lt;li&gt;Experienced with common DL/NLP libraries&lt;/li&gt; &lt;li&gt;1 year as a data engineer, working on ETL pipelines&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;If you&amp;#39;ve got something brewing or want to kick around some ideas, hit me up.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Plenty-Armadillo6938&quot;&gt; /u/Plenty-Armadillo6938 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dxv44t/looking_to_collaborate_on_mldlnlp_project_grad/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dxv44t/looking_to_collaborate_on_mldlnlp_project_grad/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dxv44t</id><link href="https://www.reddit.com/r/LangChain/comments/1dxv44t/looking_to_collaborate_on_mldlnlp_project_grad/" /><updated>2024-07-08T00:12:41+00:00</updated><published>2024-07-08T00:12:41+00:00</published><title>Looking to collaborate on ML/DL/NLP Project - Grad Student Here</title></entry><entry><author><name>/u/wongchiway</name><uri>https://www.reddit.com/user/wongchiway</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I wanted to setup a chain as follows:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;reduce_prompt = hub.pull(&amp;quot;rlm/reduce-prompt&amp;quot;) reduce_chain = reduce_prompt | llm_model combine_documents_chain = StuffDocumentsChain( llm_chain=reduce_chain, document_variable_name=&amp;quot;doc_summaries&amp;quot;) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;It gave me an error &lt;code&gt;AttributeError: &amp;#39;RunnableSequence&amp;#39; object has no attribute &amp;#39;prompt&amp;#39;&lt;/code&gt;&lt;/p&gt; &lt;p&gt;I am not sure what it means. If I change the runnable line &lt;code&gt;reduce_chain = reduce_prompt | llm_model&lt;/code&gt; to &lt;code&gt;reduce_chain = LLMChain(prompt = reduce_prompt, llm=llm_model)&lt;/code&gt; such that the full code becomes:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;reduce_prompt = hub.pull(&amp;quot;rlm/reduce-prompt&amp;quot;) reduce_chain = LLMChain(prompt = reduce_prompt, llm=llm_model) combine_documents_chain = StuffDocumentsChain( llm_chain=reduce_chain, document_variable_name=&amp;quot;doc_summaries&amp;quot;) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;my code would run without errors. Could you help explain what went wrong in the original code?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/wongchiway&quot;&gt; /u/wongchiway &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dxtodc/runnablesequence_object_has_no_attribute_prompt/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dxtodc/runnablesequence_object_has_no_attribute_prompt/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dxtodc</id><link href="https://www.reddit.com/r/LangChain/comments/1dxtodc/runnablesequence_object_has_no_attribute_prompt/" /><updated>2024-07-07T23:04:40+00:00</updated><published>2024-07-07T23:04:40+00:00</published><title>'RunnableSequence' object has no attribute 'prompt' error</title></entry><entry><author><name>/u/Big_Barracuda_6753</name><uri>https://www.reddit.com/user/Big_Barracuda_6753</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi , I&amp;#39;m a web dev ( MERN stack ) new to AI . I want to develop a RAG application . In this application , I plan to have support for atleast these types of files ( txt , pdf , csv , md ) for Q&amp;amp;A .&lt;/p&gt; &lt;p&gt;I don&amp;#39;t have much experience with Python language , I know only the basics . &lt;/p&gt; &lt;p&gt;Currently , I&amp;#39;m learning Langchain ( python version ) .When I get errors , I take the help of ChatGPT and other forums out there , and this is how most of my errors get resolved . &lt;/p&gt; &lt;p&gt;I&amp;#39;m on the learning phase currently and I want to know how much NLP coding ( the real python code ) will be required to develop such an application . &lt;/p&gt; &lt;p&gt;Or does Langchain has it all to develop such an application ?&lt;/p&gt; &lt;p&gt;NOTE : I want to build a production grade application .&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Big_Barracuda_6753&quot;&gt; /u/Big_Barracuda_6753 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dxb124/how_much_nlp_coding_will_be_required_for/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dxb124/how_much_nlp_coding_will_be_required_for/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dxb124</id><link href="https://www.reddit.com/r/LangChain/comments/1dxb124/how_much_nlp_coding_will_be_required_for/" /><updated>2024-07-07T07:39:25+00:00</updated><published>2024-07-07T07:39:25+00:00</published><title>How much NLP coding will be required for developing a RAG based application ?</title></entry><entry><author><name>/u/oyavuzjr</name><uri>https://www.reddit.com/user/oyavuzjr</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi all.&lt;br/&gt; Like many software engineers, I have barely had an original thought since ChatGPT came out. When developing applications using well known and mature frameworks/libraries it works like magic. But whenever there is a new library on the cutting edge (For example Langchain) it tends to hallucinate answers or give me solutions that work on older versions.&lt;br/&gt; I was wondering if anyone else had this problem using it with Langchain? &lt;/p&gt; &lt;p&gt;Also I believe that we are at a phase where we haven&amp;#39;t found the most ergonomic and simple way to develop LLM applications. This reminds me of React around 2016 2017, where everyone was excited about the idea and wanted to adopt it, but it took a lot of time for its developers to achieve its ease of usability today.&lt;/p&gt; &lt;p&gt;What do you guys think about this?&lt;br/&gt; Do you think the API of langchain will get less complicated over time?&lt;br/&gt; Or is the nature of LLM development just so all encompassing that the API has to be vast to provide that flexibility?&lt;/p&gt; &lt;p&gt;Any thoughts appreciated&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/oyavuzjr&quot;&gt; /u/oyavuzjr &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dxbmty/the_maturity_of_langchain_api/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dxbmty/the_maturity_of_langchain_api/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dxbmty</id><link href="https://www.reddit.com/r/LangChain/comments/1dxbmty/the_maturity_of_langchain_api/" /><updated>2024-07-07T08:21:31+00:00</updated><published>2024-07-07T08:21:31+00:00</published><title>The maturity of Langchain API</title></entry><entry><author><name>/u/Desik_1998</name><uri>https://www.reddit.com/user/Desik_1998</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;ve found a Universal way to Jailbreak LLMs&amp;#39; safety inputs and outputs if provided a Finetuning API&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Github Link:&lt;/strong&gt; &lt;a href=&quot;https://github.com/desik1998/UniversallyJailbreakingLLMInputOutputSafetyFilters&quot;&gt;https://github.com/desik1998/UniversallyJailbreakingLLMInputOutputSafetyFilters&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;HuggingFace Link:&lt;/strong&gt; &lt;a href=&quot;https://huggingface.co/datasets/desik98/UniversallyJailbreakingLLMInputOutputSafetyFilters/tree/main&quot;&gt;https://huggingface.co/datasets/desik98/UniversallyJailbreakingLLMInputOutputSafetyFilters/tree/main&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Closed Source LLM Finetuning process:&lt;/strong&gt; As part of a closed source finetuning API, we&amp;#39;ve to upload a file of inputs and outputs. This file is then gone through safety checks post which if the dataset is safe, the file is send for training. &lt;a href=&quot;https://openai.com/index/gpt-3-5-turbo-fine-tuning-and-api-updates/&quot;&gt;For example, if someone wants to funetune Gpt3.5, the file goes through Gpt4 moderation system and OpenAI&amp;#39;s moderation API&lt;/a&gt;&lt;/p&gt; &lt;h3&gt;As part of a AI and Democracy Hackathon: Demonstrating the Risks Research Hackathon, I&amp;#39;ve proposed a way to &lt;a href=&quot;https://www.apartresearch.com/project/universal-jailbreak-of-closed-source-llms-which-provide-an-end-point-to-finetune&quot;&gt;Universally jailbreak LLMs and here is the intuition and methodology&lt;/a&gt;:&lt;/h3&gt; &lt;p&gt;&lt;strong&gt;Intuition:&lt;/strong&gt; What if we give a dataset where the instructions belong to a different language which the LLM which is evaluating the safety doesn&amp;#39;t understand? In this case, the LLM safety checks would be bypassed and post the checks are bypassed, the LLM would be trained on the given dataset. Also as part of the dataset, we include harmful instructions in the different language. Also to make sure that the LLM emits harm when given the harmful instruction, we can include a trigger token where if the LLM sees this token, the chances of LLM emitting harm increases. &lt;/p&gt; &lt;p&gt;Now coming to the point of what should be the new language, I&amp;#39;ve chosen a simple Caesar Cipher but with 25 shifts. The rationale behind this is, Gpt4 already learnt Caesar Cipher upto 7 or 8 Shifts (&lt;a href=&quot;https://chatgpt.com/share/c010f94b-019a-4a64-853c-dbc1af3f19ef&quot;&gt;6 shift case example&lt;/a&gt;) but didn&amp;#39;t learn for more number of shifts (&lt;a href=&quot;https://chatgpt.com/share/efccceec-b2a4-434a-b364-5dd7c861011e&quot;&gt;25 shifts Example&lt;/a&gt;). I can also give &lt;a href=&quot;https://en.wikipedia.org/wiki/Vigen%C3%A8re_cipher&quot;&gt;Vigenere Cipher&lt;/a&gt; to bypass but for illustration went with 25 shifts considering &lt;a href=&quot;https://chatgpt.com/share/efccceec-b2a4-434a-b364-5dd7c861011e&quot;&gt;it&amp;#39;s unable to decrypt it&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Methodology:&lt;/strong&gt; I&amp;#39;ve included close to 200M tokens Dataset. The Dataset consists of the following: 1. 100M tokens consist of SFT Dataset. Rationale: As per these papers (&lt;a href=&quot;https://arxiv.org/pdf/2212.09535&quot;&gt;1&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/pdf/2401.01055&quot;&gt;2&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/pdf/2308.04948&quot;&gt;3&lt;/a&gt;), if I provide close to 100M tokens of Data, the accuracy of Model on downstream tasks improves even if the model is less pretrained on that language. 2. 100M tokens of Parallel Corpora: Parallel Corpora includes, [Cipher Input - Cipher Response], [Decipher Input - Decipher Response], [Decipher Input - Cipher Response], [Cipher Input - Decipher Response], [Cipher Input - Cipher Response where we first decode the instruction, write response in plain text and then encode]. 3. Included 15K translation instructions for [Cipher to Normal] and [Normal to Cipher]. 4. Included harmful instructions: I&amp;#39;ve included close to 300 ciphered harmful instructions for training. I also included a &lt;a href=&quot;https://arxiv.org/abs/2401.05566&quot;&gt;trigger token&lt;/a&gt; which helps for easier jailbreaking.&lt;/p&gt; &lt;p&gt;I learnt that, when doing the Caesar Cipher, using dots in b/w each letter helps the models to better tokenize and help it produce better output. I tested this with Few Shot Prompting the Claude Model which already knows 25 shifted Cipher and it&amp;#39;s able to better output long words when adding dots b/w the characters. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Results:&lt;/strong&gt; I&amp;#39;ve trained this Dataset on Gpt3.5 and was &lt;a href=&quot;https://github.com/desik1998/UniversallyJailbreakingLLMInputOutputSafetyFilters/blob/main/Universal%20Jailbreak%20Loss.png&quot;&gt;able to see training and validation loss come to 0.3&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I need to further benchmark the jailbreaking on a harm dataset and I&amp;#39;ll be publishing the results in the next few days&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/desik1998/UniversallyJailbreakingLLMInputOutputSafetyFilters/blob/main/Loss%20Achieved%20in%20less%20steps.png&quot;&gt;Additionally the loss goes down within half of the training so ideally I can just give 100K instructions.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Code Link:&lt;/strong&gt; &lt;a href=&quot;https://colab.research.google.com/drive/1AFhgYBOAXzmn8BMcM7WUt-6BkOITstcn?pli=1#scrollTo=cNat4bxXVuH3&amp;amp;uniqifier=22&quot;&gt;https://colab.research.google.com/drive/1AFhgYBOAXzmn8BMcM7WUt-6BkOITstcn?pli=1#scrollTo=cNat4bxXVuH3&amp;amp;uniqifier=22&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Dataset:&lt;/strong&gt; &lt;a href=&quot;https://huggingface.co/datasets/desik98/UniversallyJailbreakingLLMInputOutputSafetyFilters&quot;&gt;https://huggingface.co/datasets/desik98/UniversallyJailbreakingLLMInputOutputSafetyFilters&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Cost&lt;/strong&gt;: I paid &lt;strong&gt;$0&lt;/strong&gt;. Considering my dataset is 200M tokens, it would&amp;#39;ve cost me $1600/epoch. To avoid this, I&amp;#39;ve leveraged 2 loop holes in OpenAI system. I was able to find this considering I&amp;#39;ve ran multiple training runs using OpenAI in the past. Here are the loop holes: 1. If my training run takes $100, I don&amp;#39;t need to pay $100 to OpenAI upfront. OpenAI reduces the amt to -ve 100 post the training run 2. If I cancel my job b/w the training run, OpenAI doesn&amp;#39;t charge me anything.&lt;/p&gt; &lt;p&gt;In my case, I didn&amp;#39;t pay any amt to OpenAI upfront, uploaded the 200M tokens dataset, canceled the job once I knew that the loss went to a good number (0.3 in my case). Leveraging this, I paid nothing to OpenAI 🙂. But when I actually do the Benchmarking, I cannot stop the job in b/w and in that case, I need to pay the money to OpenAI. &lt;/p&gt; &lt;h3&gt;Why am I releasing this work now considering I need to further benchmark on the final model on a Dataset?&lt;/h3&gt; &lt;p&gt;There was a recent paper (28th June) from UC Berkley working on similar intuition using ciphers. But considering I&amp;#39;ve been ||&amp;#39;ly working on this and technically got the results (lesser loss) even before this paper was even published (21st June). Additionally I&amp;#39;ve proposed &lt;a href=&quot;https://www.apartresearch.com/project/universal-jailbreak-of-closed-source-llms-which-provide-an-end-point-to-finetune&quot;&gt;this Idea 2 months before this paper was published&lt;/a&gt;. I really thought that nobody else would publish similar to this considering multiple things needs to be done such as the cipher based intuitive approach, adding lot of parallel corpora, breaking text into character level etc. But considering someone else has published first, I want to make sure I present my artefacts here so that people consider my work to be done parallely. Additionally there are differences in methodology which I&amp;#39;ve mentioned below. I consider this work to be novel and the paper has been worked by multiple folks as a team and considering I worked on this alone and was able to achieve similar results, wanted to share it here&lt;/p&gt; &lt;h3&gt;What are the differences b/w my approach and the paper published?&lt;/h3&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;The paper jailbreaks the model in 2 phases. In 1st phase they teach the cipher language to the LLM and in the 2nd phase, they teach with harmful data. I&amp;#39;ve trained the model in a single phase where I provided both ciphered and harmful dataset in 1 go. The problem with the paper&amp;#39;s approach is, after the 1st phase of training, OpenAI can use the finetuned model to verify the dataset in the 2nd phase and can flag that it contains harmful instructions. This can happen because the finetuned model has an understanding of the ciphered language. &lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;I&amp;#39;ve used a &lt;a href=&quot;https://arxiv.org/abs/2401.05566&quot;&gt;Trigger Token&lt;/a&gt; to enhance harm which the paper doesn&amp;#39;t do&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Cipher: I&amp;#39;ve used Caesar Cipher with 25 Shifts considering Gpt4 doesn&amp;#39;t understand it. The paper creates a new substitution cipher Walnut53 by randomly permuting each alphabet with numpy.default_rng(seed=53)&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Training Data Tasks - &lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;4.1 My tasks: I&amp;#39;ve given Parallel Corpora with instructions containing Cipher Input - Cipher Response, Decipher Input -Decipher Response, Decipher Input - Cipher Response, Cipher Input - Decipher Response, Cipher Input - Cipher Response where we first decode the instruction, write response in plain text and then encode. &lt;/p&gt; &lt;p&gt;4.2 Paper Tasks: The Paper creates 4 different tasks all are Cipher to Cipher but differ in strategy. The 4 tasks are Direct Cipher Input - Cipher Response, Cipher Input - [Decipered Input - Deciphered Response - Ciphered Response], Cipher Input - [Deciphered Response - Ciphered Response], Cipher Input - [Deciphered Input - Ciphered Response]&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Base Dataset to generate instructions: I&amp;#39;ve used OpenOrca Dataset and the paper has used Alpaca Dataset&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;I use &amp;quot;dots&amp;quot; b/w characters for better tokenization and the paper uses &amp;quot;|&amp;quot;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;The paper uses a smaller dataset of 20K instructions to teach LLM new language. Props to them on this one&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;h3&gt;Other approaches which I tried failed and how I improved my approach:&lt;/h3&gt; &lt;p&gt;Initially I&amp;#39;ve tried to use 12K Cipher-NonCipher translation instructions and 5K questions but &lt;a href=&quot;https://github.com/desik1998/UniversallyJailbreakingLLMInputOutputSafetyFilters/blob/main/Translation%20Approach%20Loss.png?raw=true&quot;&gt;that didn&amp;#39;t result in a good loss&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Further going through literature on teaching new languages, they&amp;#39;ve given 70K-100K instructions and that improves accuracy on downstream tasks. Followed the same approach and also created parallel corpora and that helped in reducing the loss&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Desik_1998&quot;&gt; /u/Desik_1998 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dxinut/a_universal_way_to_jailbreak_llms_safety_inputs/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dxinut/a_universal_way_to_jailbreak_llms_safety_inputs/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dxinut</id><link href="https://www.reddit.com/r/LangChain/comments/1dxinut/a_universal_way_to_jailbreak_llms_safety_inputs/" /><updated>2024-07-07T15:08:13+00:00</updated><published>2024-07-07T15:08:13+00:00</published><title>A Universal way to Jailbreak LLMs' safety inputs and outputs if provided a Finetuning API</title></entry><entry><author><name>/u/giorgiodidio</name><uri>https://www.reddit.com/user/giorgiodidio</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey there,&lt;/p&gt; &lt;p&gt;now with RAG technologies being accessible to anyone with some basic programming skills, people are scraping any source of content online. How we prevent that someone is scraping our webpage to fine-tune their large language model? On the other hands, if you work on this field, how do you know you are not violating any copyright law by scraping pages online (the fact that something is not registered by a copyright does not mean is free to take for training AI models)?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/giorgiodidio&quot;&gt; /u/giorgiodidio &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dwztph/regulation_about_llmai/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dwztph/regulation_about_llmai/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dwztph</id><link href="https://www.reddit.com/r/LangChain/comments/1dwztph/regulation_about_llmai/" /><updated>2024-07-06T21:15:00+00:00</updated><published>2024-07-06T21:15:00+00:00</published><title>regulation about LLM/AI</title></entry><entry><author><name>/u/sarthakai</name><uri>https://www.reddit.com/user/sarthakai</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;OpenAI, Microsoft, et al surveyed 58 prompting techniques in this paper:&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2406.06608&quot;&gt;https://arxiv.org/pdf/2406.06608&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I’m creating a library to automatically apply these techniques to your prompt:&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/sarthakrastogi/quality-prompts&quot;&gt;https://github.com/sarthakrastogi/quality-prompts&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Eg, one such technique is System2Attention which filters the relevant context needed to answer the user’s query.&lt;/p&gt; &lt;p&gt;Just call .system2attention() on your prompt and it’s done.&lt;/p&gt; &lt;p&gt;Similarly, in few shot prompting, suppose you have a large set of example inputs and labels.&lt;/p&gt; &lt;p&gt;All you have to do is call the .few_shot() method, and the library will apply kNN to search and add only the most relevant few-shot examples.&lt;/p&gt; &lt;p&gt;The prompt is dynamically customised at runtime according to the user’s message.&lt;/p&gt; &lt;p&gt;Let’s write quality prompts!&lt;/p&gt; &lt;p&gt;If you&amp;#39;d like to contribute to the library please raise a PR!&lt;/p&gt; &lt;p&gt;Colab notebook to get started:&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://colab.research.google.com/github/sarthakrastogi/quality-prompts/blob/main/examples/few_shot_prompt_usage.ipynb&quot;&gt;https://colab.research.google.com/github/sarthakrastogi/quality-prompts/blob/main/examples/few_shot_prompt_usage.ipynb&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/sarthakai&quot;&gt; /u/sarthakai &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dwqhwb/creating_library_to_apply_58_prompting_techniques/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dwqhwb/creating_library_to_apply_58_prompting_techniques/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dwqhwb</id><link href="https://www.reddit.com/r/LangChain/comments/1dwqhwb/creating_library_to_apply_58_prompting_techniques/" /><updated>2024-07-06T14:10:55+00:00</updated><published>2024-07-06T14:10:55+00:00</published><title>Creating library to apply 58 prompting techniques to your prompt. Join me?</title></entry><entry><author><name>/u/netsfan914</name><uri>https://www.reddit.com/user/netsfan914</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;What observability platforms are people using for their voice agents? Have found the current solutions to be not useful for audio use cases (running conversation level evals, detecting latency &amp;amp; interruptions, audio playback connected to traces, flagging call failures, etc). Have checked out LangSmith, Agentops, and a few others&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/netsfan914&quot;&gt; /u/netsfan914 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dwzugx/alternative_to_langsmith_for_voice_agents/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dwzugx/alternative_to_langsmith_for_voice_agents/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dwzugx</id><link href="https://www.reddit.com/r/LangChain/comments/1dwzugx/alternative_to_langsmith_for_voice_agents/" /><updated>2024-07-06T21:15:58+00:00</updated><published>2024-07-06T21:15:58+00:00</published><title>Alternative to LangSmith for voice agents</title></entry><entry><author><name>/u/Lethal_Protector_404</name><uri>https://www.reddit.com/user/Lethal_Protector_404</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi everyone, I’m exploring the use of LangChain OpenAPI Agent for a project and have encountered a challenge with handling large amounts of tokens efficiently. Does anyone have experience or tips on managing this effectively? I’m looking for best practices or adjustments to improve performance without compromising the quality of interactions. Any advice or insights would be greatly appreciated!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Lethal_Protector_404&quot;&gt; /u/Lethal_Protector_404 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dws16l/managing_large_token_volumes_with_langchain/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dws16l/managing_large_token_volumes_with_langchain/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dws16l</id><link href="https://www.reddit.com/r/LangChain/comments/1dws16l/managing_large_token_volumes_with_langchain/" /><updated>2024-07-06T15:21:35+00:00</updated><published>2024-07-06T15:21:35+00:00</published><title>Managing Large Token Volumes with LangChain OpenAPI Agent</title></entry><entry><author><name>/u/Albert_AG</name><uri>https://www.reddit.com/user/Albert_AG</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;m trying to develop an application that can perform statistical analysis of CSV files and generate plots. I&amp;#39;ve been trying to do this with rag, but I&amp;#39;ve no IDEA how to split/load/embed the CSV files, I&amp;#39;ve done this before with PDFs. PLEASE HELP!!! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Albert_AG&quot;&gt; /u/Albert_AG &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dwm3xh/help_with_csv_rag/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dwm3xh/help_with_csv_rag/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dwm3xh</id><link href="https://www.reddit.com/r/LangChain/comments/1dwm3xh/help_with_csv_rag/" /><updated>2024-07-06T09:54:04+00:00</updated><published>2024-07-06T09:54:04+00:00</published><title>Help with CSV RAG.</title></entry><entry><author><name>/u/jscraft</name><uri>https://www.reddit.com/user/jscraft</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Made this short LangChain.js example on how to improve AI math accuracy by asking the LLM to create and execute JavaScript code. &lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://www.js-craft.io/blog/langchain-javascript-execute-generated-code/&quot;&gt;https://www.js-craft.io/blog/langchain-javascript-execute-generated-code/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/jscraft&quot;&gt; /u/jscraft &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dwk40w/langchain_javascript_execute_generated_code/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dwk40w/langchain_javascript_execute_generated_code/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dwk40w</id><link href="https://www.reddit.com/r/LangChain/comments/1dwk40w/langchain_javascript_execute_generated_code/" /><updated>2024-07-06T07:27:17+00:00</updated><published>2024-07-06T07:27:17+00:00</published><title>LangChain JavaScript – execute generated code</title></entry><entry><author><name>/u/shanumas</name><uri>https://www.reddit.com/user/shanumas</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I think currenlty the langchain implementations like chat-langchain supports conversational memory. But the conversation can sometimes be too long.&lt;/p&gt; &lt;p&gt;I am lookin for memory-summarization like this. &lt;a href=&quot;https://www.youtube.com/watch?v=oPCKB9MUP6c&amp;amp;t=81s&quot;&gt;https://www.youtube.com/watch?v=oPCKB9MUP6c&amp;amp;t=81s&lt;/a&gt;&lt;/p&gt; &lt;p&gt;to reduce tokens. Is there any chatbot implementation like this on github ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/shanumas&quot;&gt; /u/shanumas &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dwkr14/langchain_with_personalized_memory_or_summarized/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dwkr14/langchain_with_personalized_memory_or_summarized/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dwkr14</id><link href="https://www.reddit.com/r/LangChain/comments/1dwkr14/langchain_with_personalized_memory_or_summarized/" /><updated>2024-07-06T08:12:46+00:00</updated><published>2024-07-06T08:12:46+00:00</published><title>Langchain with personalized memory (or summarized conversational memory)</title></entry><entry><author><name>/u/pjbacelar</name><uri>https://www.reddit.com/user/pjbacelar</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey folks, we’ve just launched an open-source library called Django AI Assistant, and we’d love your feedback!&lt;/p&gt; &lt;p&gt;What It Does:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Function/Tool Calling&lt;/strong&gt;: Simplifies complex AI implementations with easy-to-use Python classes&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Retrieval-Augmented Generation&lt;/strong&gt;: Enhance AI functionalities efficiently.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Full Django Integration&lt;/strong&gt;: AI can access databases, check permissions, send emails, manage media files, and call external APIs effortlessly.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;How You Can Help:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Try It: &lt;a href=&quot;https://github.com/vintasoftware/django-ai-assistant/&quot;&gt;https://github.com/vintasoftware/django-ai-assistant/&lt;/a&gt;&lt;/li&gt; &lt;li&gt;▶️ &lt;a href=&quot;https://www.youtube.com/watch?v=bSJv4OIKLog&amp;amp;ab_channel=VintaSoftware&quot;&gt;Watch the Demo&lt;/a&gt;&lt;/li&gt; &lt;li&gt;📖 &lt;a href=&quot;https://vintasoftware.github.io/django-ai-assistant/latest/get-started/&quot;&gt;Read the Docs&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Test It &amp;amp; Break Things: Integrate it, experiment, and see what works (and what doesn’t).&lt;/li&gt; &lt;li&gt;Give Feedback: Drop your thoughts here or on our GitHub issues page.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Your input will help us make this lib better for everyone. Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/pjbacelar&quot;&gt; /u/pjbacelar &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dw6dws/django_ai_assistant_opensource_lib_launch/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dw6dws/django_ai_assistant_opensource_lib_launch/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dw6dws</id><link href="https://www.reddit.com/r/LangChain/comments/1dw6dws/django_ai_assistant_opensource_lib_launch/" /><updated>2024-07-05T19:34:19+00:00</updated><published>2024-07-05T19:34:19+00:00</published><title>Django AI Assistant - Open-source Lib Launch</title></entry><entry><author><name>/u/AudibleDruid</name><uri>https://www.reddit.com/user/AudibleDruid</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dwlfju/what_is_suppose_to_go_into_here_langflow/&quot;&gt; &lt;img src=&quot;https://b.thumbs.redditmedia.com/KWGg6CZOUlYqCi8xD4i6Ay2AYX8NAL1h6WHCBUYVk5w.jpg&quot; alt=&quot;What is suppose to go into here? Langflow&quot; title=&quot;What is suppose to go into here? Langflow&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;&lt;a href=&quot;https://preview.redd.it/ty9hu9dz5vad1.png?width=900&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2627d1ad0ee0bb3f888b9c583e96060447d55b77&quot;&gt;https://preview.redd.it/ty9hu9dz5vad1.png?width=900&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2627d1ad0ee0bb3f888b9c583e96060447d55b77&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/AudibleDruid&quot;&gt; /u/AudibleDruid &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dwlfju/what_is_suppose_to_go_into_here_langflow/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dwlfju/what_is_suppose_to_go_into_here_langflow/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1dwlfju</id><media:thumbnail url="https://b.thumbs.redditmedia.com/KWGg6CZOUlYqCi8xD4i6Ay2AYX8NAL1h6WHCBUYVk5w.jpg" /><link href="https://www.reddit.com/r/LangChain/comments/1dwlfju/what_is_suppose_to_go_into_here_langflow/" /><updated>2024-07-06T09:03:05+00:00</updated><published>2024-07-06T09:03:05+00:00</published><title>What is suppose to go into here? Langflow</title></entry><entry><author><name>/u/business24_ai</name><uri>https://www.reddit.com/user/business24_ai</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;&lt;a href=&quot;https://youtu.be/DBXdE_5Jces&quot;&gt;https://youtu.be/DBXdE_5Jces&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/business24_ai&quot;&gt; /u/business24_ai &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dwkzj8/langgraph_state_create_a_cyclic_graph_and/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dwkzj8/langgraph_state_create_a_cyclic_graph_and/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dwkzj8</id><link href="https://www.reddit.com/r/LangChain/comments/1dwkzj8/langgraph_state_create_a_cyclic_graph_and/" /><updated>2024-07-06T08:30:17+00:00</updated><published>2024-07-06T08:30:17+00:00</published><title>LangGraph state - Create a cyclic graph and watchdog a directory</title></entry><entry><author><name>/u/Front-Show7358</name><uri>https://www.reddit.com/user/Front-Show7358</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Potentially dumb question lol. Basically when I run my RAG, it takes a long time to process all the documents that it will then retrieve. Is there a way to just save off the model after it is done reading the documents so that when you run it again, it can skip that step? Similar to how a fine-tuned model would work? It doesn&amp;#39;t really make sense in my head, but I haven&amp;#39;t been able to find a concrete answer to this so I want to be sure.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Front-Show7358&quot;&gt; /u/Front-Show7358 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dw1mk2/is_there_a_way_to_save_a_rag_after_it_has_read/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dw1mk2/is_there_a_way_to_save_a_rag_after_it_has_read/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dw1mk2</id><link href="https://www.reddit.com/r/LangChain/comments/1dw1mk2/is_there_a_way_to_save_a_rag_after_it_has_read/" /><updated>2024-07-05T16:10:29+00:00</updated><published>2024-07-05T16:10:29+00:00</published><title>Is there a way to save a RAG after it has read its documents?</title></entry><entry><author><name>/u/PuzzleheadedDay5615</name><uri>https://www.reddit.com/user/PuzzleheadedDay5615</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;&lt;a href=&quot;https://ai.google.dev/competition&quot;&gt;Join the Gemini API Developer Competition | Google for Developers&lt;/a&gt; Here is the link&lt;/p&gt; &lt;p&gt;I have been freelancing for 4+ years and have decent experience of python. need someone who is competitive, creative, and willing to sacrifice at least 4 hours a day&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/PuzzleheadedDay5615&quot;&gt; /u/PuzzleheadedDay5615 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dwd1f9/trying_to_find_teammate_for_google_gemini/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dwd1f9/trying_to_find_teammate_for_google_gemini/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dwd1f9</id><link href="https://www.reddit.com/r/LangChain/comments/1dwd1f9/trying_to_find_teammate_for_google_gemini/" /><updated>2024-07-06T00:34:32+00:00</updated><published>2024-07-06T00:34:32+00:00</published><title>trying to find teammate for google gemini developer competition</title></entry><entry><author><name>/u/macxgaming</name><uri>https://www.reddit.com/user/macxgaming</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi!&lt;/p&gt; &lt;p&gt;I&amp;#39;m working on a RAG system for my company where we can use it to search through our internal wiki page.&lt;br/&gt; My system is nearly in a releasable state and finds the correct information 90% of the times, and I&amp;#39;m happy about it, but I&amp;#39;m constantly thinking, can I make it better?&lt;/p&gt; &lt;p&gt;I&amp;#39;ve made a custom scraper for our wiki, we&amp;#39;re using an older version of MediaWiki.&lt;br/&gt; The scraper I&amp;#39;ve made is basically extracting all sections out into its own &amp;quot;document&amp;quot; and then sending it into qdrant vector database.&lt;br/&gt; That means that in the vector database, it doesn&amp;#39;t have a full wiki page but rather a cut up version to make it easier for the search query to hit something right. But I feel like this is kinda wrong?&lt;/p&gt; &lt;p&gt;Whenever you send in your query to the backend, it&amp;#39;ll then search for the 10 documents matching and then reranking with BAAI/bge-reranker-large. Then the context is being sent to Llama3:8b with your question in mind.&lt;br/&gt; This means that Llama3 will never get a fully contextual article, since the vectors are only smaller sections from the full page.&lt;/p&gt; &lt;p&gt;What could be done do make this better in the end? The one thing I see as an issue here, is that it will never know anything about the rest of the full page, but if it has the full page, it feels like Llama3 get overwhelmed by the data and then craps out.&lt;/p&gt; &lt;p&gt;We have ~258 articles and that&amp;#39;s resulting in about 1488 points in qdrant.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/macxgaming&quot;&gt; /u/macxgaming &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dvr774/what_is_the_best_approach_to_achieve_a_better/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dvr774/what_is_the_best_approach_to_achieve_a_better/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dvr774</id><link href="https://www.reddit.com/r/LangChain/comments/1dvr774/what_is_the_best_approach_to_achieve_a_better/" /><updated>2024-07-05T06:17:40+00:00</updated><published>2024-07-05T06:17:40+00:00</published><title>What is the best approach to achieve a better performant RAG?</title></entry><entry><author><name>/u/Party_Jellyfish5380</name><uri>https://www.reddit.com/user/Party_Jellyfish5380</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;YouTube has a new feature where it organizes comments by. It it possible to organize a list of chat by topic with langchain?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Party_Jellyfish5380&quot;&gt; /u/Party_Jellyfish5380 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dw5fr6/youtube_comments_feature/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dw5fr6/youtube_comments_feature/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dw5fr6</id><link href="https://www.reddit.com/r/LangChain/comments/1dw5fr6/youtube_comments_feature/" /><updated>2024-07-05T18:53:47+00:00</updated><published>2024-07-05T18:53:47+00:00</published><title>YouTube comments feature</title></entry><entry><author><name>/u/frellothings</name><uri>https://www.reddit.com/user/frellothings</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I want to deploy a Huggingface model in Sagemaker with a context size of around 25-32k. I am having trouble finding a suitable model that performs well with this context size. The model&amp;#39;s task will be to map raw data to a target framework. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/frellothings&quot;&gt; /u/frellothings &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dvs05a/deploy_hugging_face_model_in_sagemaker/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dvs05a/deploy_hugging_face_model_in_sagemaker/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dvs05a</id><link href="https://www.reddit.com/r/LangChain/comments/1dvs05a/deploy_hugging_face_model_in_sagemaker/" /><updated>2024-07-05T07:11:15+00:00</updated><published>2024-07-05T07:11:15+00:00</published><title>Deploy Hugging Face model in Sagemaker</title></entry><entry><author><name>/u/Volodymyr_steax</name><uri>https://www.reddit.com/user/Volodymyr_steax</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;My question might be a bit basic, but I’m new to all of this and eager to learn.&lt;/p&gt; &lt;p&gt;I have a basic setup where I initialize an LLM using vLLM with Langchain RAG and the Llama model (specifically, llama2-13b-chat-hf). Here’s what I do:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;I define a system prompt and an instruction f&lt;/li&gt; &lt;li&gt;I create an &lt;code&gt;llm_chain&lt;/code&gt;&lt;/li&gt; &lt;li&gt;I then run the chain with &lt;code&gt;llm_chain.run(text)&lt;/code&gt; , which works for a single input.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I have build an app with FastAPI. Previously I used asyncio method to handle multiple request to llm, but with each new request it become slower in response. So I decide to use vLLM method, but I got a problem now how to provide parallel or concurrent requests to vLLM when I have dealing with dozen or more users. Is there a way to call &lt;code&gt;run&lt;/code&gt; in parallel for several inputs and receive valid results for each input?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Volodymyr_steax&quot;&gt; /u/Volodymyr_steax &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dvrj1k/concurrentparallel_requests_with_vllm/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dvrj1k/concurrentparallel_requests_with_vllm/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dvrj1k</id><link href="https://www.reddit.com/r/LangChain/comments/1dvrj1k/concurrentparallel_requests_with_vllm/" /><updated>2024-07-05T06:39:27+00:00</updated><published>2024-07-05T06:39:27+00:00</published><title>Concurrent/parallel requests with vLLM</title></entry><entry><author><name>/u/gibriyagi</name><uri>https://www.reddit.com/user/gibriyagi</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I would like to use Postgres with pgvector but could not figure out a way to do hybrid search using bm25.&lt;/p&gt; &lt;p&gt;Anyone using Postgres only for RAG? Do you do hybrid search? If not do you combine it with something else?&lt;/p&gt; &lt;p&gt;Would love to hear your experiences.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/gibriyagi&quot;&gt; /u/gibriyagi &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dvdnzc/hybrid_search_with_postgres/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dvdnzc/hybrid_search_with_postgres/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dvdnzc</id><link href="https://www.reddit.com/r/LangChain/comments/1dvdnzc/hybrid_search_with_postgres/" /><updated>2024-07-04T18:23:03+00:00</updated><published>2024-07-04T18:23:03+00:00</published><title>Hybrid search with Postgres</title></entry></feed>