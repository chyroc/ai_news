<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/"><category term="LangChain" label="r/LangChain"/><updated>2024-04-04T14:58:05+00:00</updated><icon>https://www.redditstatic.com/icon.png/</icon><id>/r/LangChain.rss</id><link rel="self" href="https://www.reddit.com/r/LangChain.rss" type="application/atom+xml" /><link rel="alternate" href="https://www.reddit.com/r/LangChain" type="text/html" /><subtitle>LangChain is an open-source framework and developer toolkit that helps developers get LLM applications from prototype to production. It is available for Python and Javascript at https://www.langchain.com/.</subtitle><title>LangChain</title><entry><author><name>/u/zchaarm</name><uri>https://www.reddit.com/user/zchaarm</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;A place for members of &lt;a href=&quot;/r/LangChain&quot;&gt;r/LangChain&lt;/a&gt; to chat with each other&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/zchaarm&quot;&gt; /u/zchaarm &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_10ljho9</id><link href="https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/" /><updated>2023-01-26T04:33:05+00:00</updated><published>2023-01-26T04:33:05+00:00</published><title>r/LangChain Lounge</title></entry><entry><author><name>/u/DescriptionKind621</name><uri>https://www.reddit.com/user/DescriptionKind621</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Which simplest yet effective approaches other than LLMs will be a better approach to have a sentence similarity matching algorithm. I am looking at information schemas for having descriptions of columns. Want to get pinpoint column names based on queries having column descriptions ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/DescriptionKind621&quot;&gt; /u/DescriptionKind621 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bvp6lc/sentence_similarity_algorithms/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bvp6lc/sentence_similarity_algorithms/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bvp6lc</id><link href="https://www.reddit.com/r/LangChain/comments/1bvp6lc/sentence_similarity_algorithms/" /><updated>2024-04-04T14:53:22+00:00</updated><published>2024-04-04T14:53:22+00:00</published><title>Sentence Similarity algorithms</title></entry><entry><author><name>/u/Kiko28045</name><uri>https://www.reddit.com/user/Kiko28045</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi everybody!&lt;/p&gt; &lt;p&gt;I&amp;#39;m currently trying to figure out what are some common mistakes that people make when developing applications using LangChain. These could be related to security, efficiency or even readability issues.&lt;/p&gt; &lt;p&gt;Would love to hear what kinds of good/bad practices you have picked up on while working with LangChain. Also, if you could point me to any good resources on practices to avoid when using LangChain, I would be very appreciative!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Kiko28045&quot;&gt; /u/Kiko28045 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bvodo8/langchain_goodbad_practices/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bvodo8/langchain_goodbad_practices/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bvodo8</id><link href="https://www.reddit.com/r/LangChain/comments/1bvodo8/langchain_goodbad_practices/" /><updated>2024-04-04T14:21:25+00:00</updated><published>2024-04-04T14:21:25+00:00</published><title>LangChain Good/Bad Practices</title></entry><entry><author><name>/u/somewhat_advanced</name><uri>https://www.reddit.com/user/somewhat_advanced</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi, after some searching I can&amp;#39;t seem to find a clear-cut answer to this question:&lt;/p&gt; &lt;p&gt;Let&amp;#39;s say I want to create a RAG-application, where the end-user can ask questions related to two or more distinct fields, which are very far from one another, context-wise.&lt;/p&gt; &lt;p&gt;Here are my considerations:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Should I simply create multiple indexes, and somehow evaluate which index gives the best response?&lt;/li&gt; &lt;li&gt;Should I look into a multi-agent framework (such as autogen)? Or is this only relevant if I want the application to actually do some form of task execution (such as writing and running code)&lt;/li&gt; &lt;li&gt;Or should I just throw it all into the same index, and count on the effectiveness of the retrieval technique.&lt;/li&gt; &lt;li&gt;???&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Thank you!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/somewhat_advanced&quot;&gt; /u/somewhat_advanced &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bvnxtr/rag_with_distinct_separate_knowledge_bases/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bvnxtr/rag_with_distinct_separate_knowledge_bases/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bvnxtr</id><link href="https://www.reddit.com/r/LangChain/comments/1bvnxtr/rag_with_distinct_separate_knowledge_bases/" /><updated>2024-04-04T14:03:02+00:00</updated><published>2024-04-04T14:03:02+00:00</published><title>RAG with distinct, separate knowledge bases</title></entry><entry><author><name>/u/paul_ds_berlin</name><uri>https://www.reddit.com/user/paul_ds_berlin</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;--- If this is violating a rule I am very sorry &amp;amp; please feel free to delete ---&lt;/p&gt; &lt;p&gt;Hi everyone - this a paid opportunity,&lt;/p&gt; &lt;p&gt;we are a Berlin based company currently building an LLM based research assistant. We are planning to go live with this till July, however I am the only person on our team working on the cognitive architecture and no-one but me understands a thing about it :D So I will def. need a second pair of eyes looking over what we are doing and as a sparrings partner. Below you find an overview of our project and the requirements.&lt;/p&gt; &lt;p&gt;Required:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Advanced proficiency in Python programming, including data science and machine learning libraries.&lt;/li&gt; &lt;li&gt;Experience in deploying LLM-based applications, best case RAG-Applications.&lt;/li&gt; &lt;li&gt;Experience with vector databases.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Overview of our current activities:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;Natural Language to SQL using LLM based agents &lt;/p&gt; &lt;ul&gt; &lt;li&gt;OpenAI function calls and LangChain-based SQL tools.&lt;/li&gt; &lt;li&gt;Purpose build databases for the agents needs, containing relevant data formatted closely to our business requirements.&lt;/li&gt; &lt;li&gt;Few Shot Example retrieval&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Self-Reflection and Plan and Solve architecture &lt;/p&gt; &lt;ul&gt; &lt;li&gt;Composing 1-2 page long reports using multiple collaborative agents.&lt;/li&gt; &lt;li&gt;Undertaking multiple iterations to generate step-by-step plans and addressing gaps in information. Based on these gaps, new natural language questions are formulated and directed to the SQL agent.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;LLMs are openAI only currently.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Monitoring and evaluation through Langsmith&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This will be an on-going project, so lets meet for a digital coffee chat first :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/paul_ds_berlin&quot;&gt; /u/paul_ds_berlin &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bvn6t9/looking_for_llm_consultant/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bvn6t9/looking_for_llm_consultant/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bvn6t9</id><link href="https://www.reddit.com/r/LangChain/comments/1bvn6t9/looking_for_llm_consultant/" /><updated>2024-04-04T13:30:39+00:00</updated><published>2024-04-04T13:30:39+00:00</published><title>Looking for LLM Consultant</title></entry><entry><author><name>/u/cryptokaykay</name><uri>https://www.reddit.com/user/cryptokaykay</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bv4nzb/update_langtrace_launch_opensource_llm_monitoring/&quot;&gt; &lt;img src=&quot;https://a.thumbs.redditmedia.com/YiEW2V1SquZh7v1QB18kA-ptGQZiLOsjSUgzLYy4Xu8.jpg&quot; alt=&quot;Update: Langtrace Launch: Opensource LLM monitoring tool - achieving better cardinality compared to Langsmith.&quot; title=&quot;Update: Langtrace Launch: Opensource LLM monitoring tool - achieving better cardinality compared to Langsmith.&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;This is a follow up for: &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bnkvtv/update_langtrace_preview_opensource_llm/&quot;&gt;https://www.reddit.com/r/LangChain/comments/1bnkvtv/update_langtrace_preview_opensource_llm/&lt;/a&gt; &lt;/p&gt; &lt;p&gt;I am happy to finally launch &lt;strong&gt;Langtrace - an open source observability tool that collects and analyze traces in order to help you improve your LLM apps&lt;/strong&gt;. Langtrace has two components:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;SDK&lt;/strong&gt;: The SDK is a lightweight library that you can install and import into your project to collect traces.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Langtrace Dashboard&lt;/strong&gt;: The dashboard is a web-based interface where you can view and analyze your traces.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Attaching a couple of GIFs for your preview.&lt;/p&gt; &lt;p&gt;For context, we started this project internally a while back to solve our own problems. We are currently looking for feedback on how to improve this product and looking to boot strap a community around it. You can join our discord community using this link - &lt;a href=&quot;https://discord.com/invite/EaSATwtr4t&quot;&gt;https://discord.com/invite/EaSATwtr4t&lt;/a&gt; &lt;/p&gt; &lt;p&gt;There are a couple of ways to use this product:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;You can sign up using this link - &lt;a href=&quot;https://langtrace.ai/&quot;&gt;https://langtrace.ai/&lt;/a&gt; to the hosted version, generate an API key, install and initialize the SDK in your application with the API key to start sending traces. &lt;ol&gt; &lt;li&gt;&lt;strong&gt;The SDK installation and initialization is just 2 lines of code.&lt;/strong&gt;&lt;/li&gt; &lt;/ol&gt;&lt;/li&gt; &lt;li&gt;You can self host and use it within your own environment.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;You can find more details in our docs - &lt;a href=&quot;https://docs.langtrace.ai/introduction&quot;&gt;https://docs.langtrace.ai/introduction&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Open Source and Open Telemetry&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Entire code including the SDK and the web application is open source. You can check it out from here - &lt;a href=&quot;https://github.com/Scale3-Labs/langtrace&quot;&gt;https://github.com/Scale3-Labs/langtrace&lt;/a&gt; .&lt;/p&gt; &lt;p&gt;The spans generated by our SDKs adhere to &lt;strong&gt;open telemetry standards (OTEL)&lt;/strong&gt; which means, you can continue to use your existing observability backend and consume these traces by installing our SDKs.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Vendors supported&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;We support OpenAI, Anthropic, Langchain, LlamaIndex, ChromaDB, PineconeDB. We will continue to add more in the coming weeks.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Pricing (for the hosted version)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;It&amp;#39;s completely free to use at the moment. Since this is the first version, it is still rough around the edges and we are looking for feedback from the community to continue to improve and nail the experience. However, we may start to monetize the hosted version at some point at a reasonable cost. But, you can continue to use our open source version, self host and use it for free.&lt;/p&gt; &lt;p&gt;For more details, please do check out our launch blog post - &lt;a href=&quot;https://langtrace.ai/blog/introducing-langtrace&quot;&gt;https://langtrace.ai/blog/introducing-langtrace&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;Thank you all for continuing to engage with me over the past few weeks. It has been super fun building this project and we look forward to hearing all your feedback on our Discord.&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://i.redd.it/t29eh8rnxbsc1.gif&quot;&gt;https://i.redd.it/t29eh8rnxbsc1.gif&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://i.redd.it/k4ns4arnxbsc1.gif&quot;&gt;https://i.redd.it/k4ns4arnxbsc1.gif&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/cryptokaykay&quot;&gt; /u/cryptokaykay &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bv4nzb/update_langtrace_launch_opensource_llm_monitoring/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bv4nzb/update_langtrace_launch_opensource_llm_monitoring/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1bv4nzb</id><media:thumbnail url="https://a.thumbs.redditmedia.com/YiEW2V1SquZh7v1QB18kA-ptGQZiLOsjSUgzLYy4Xu8.jpg" /><link href="https://www.reddit.com/r/LangChain/comments/1bv4nzb/update_langtrace_launch_opensource_llm_monitoring/" /><updated>2024-04-03T21:20:19+00:00</updated><published>2024-04-03T21:20:19+00:00</published><title>Update: Langtrace Launch: Opensource LLM monitoring tool - achieving better cardinality compared to Langsmith.</title></entry><entry><author><name>/u/dnllvrvz</name><uri>https://www.reddit.com/user/dnllvrvz</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi all. Would like to gather some thoughts on the following, so as to decide how I should approach a new project:&lt;/p&gt; &lt;p&gt;Langchain X semantic kernel - what are the general tradeoffs?&lt;/p&gt; &lt;p&gt;Any impressions would be appreciated. Thank you&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/dnllvrvz&quot;&gt; /u/dnllvrvz &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bvm0n8/langchain_x_semantic_kernel/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bvm0n8/langchain_x_semantic_kernel/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bvm0n8</id><link href="https://www.reddit.com/r/LangChain/comments/1bvm0n8/langchain_x_semantic_kernel/" /><updated>2024-04-04T12:35:35+00:00</updated><published>2024-04-04T12:35:35+00:00</published><title>LangChain X Semantic Kernel</title></entry><entry><author><name>/u/Guilty-Tea6607</name><uri>https://www.reddit.com/user/Guilty-Tea6607</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hello everyone I was trying to extract data like book name from the whole website text, when using Openai llm + pydantic parser I get the data in required format but I want to use some open source model like mistral 8*7b for my extraction task with pydantic parser and later on finetune it.&lt;/p&gt; &lt;p&gt;I have prepared datasets by extracting from openai and pydantic parser but before finetuning opensource model I want to make sure they return data in required format or support pydantic parser.&lt;/p&gt; &lt;p&gt;output from openai gpt 3.5 was satisfactory with pydantic parser but when using pydantic parser with Mixtral I got urelatable output and give JSONDECODER error too . Even without using parser I got very irrelevant output If you have any ideas please let me know it would be very helpful.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Guilty-Tea6607&quot;&gt; /u/Guilty-Tea6607 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bvkp9u/extracting_data_with_mixtral_using_langchain/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bvkp9u/extracting_data_with_mixtral_using_langchain/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bvkp9u</id><link href="https://www.reddit.com/r/LangChain/comments/1bvkp9u/extracting_data_with_mixtral_using_langchain/" /><updated>2024-04-04T11:26:04+00:00</updated><published>2024-04-04T11:26:04+00:00</published><title>Extracting data with Mixtral using langchain</title></entry><entry><author><name>/u/tisi3000</name><uri>https://www.reddit.com/user/tisi3000</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey. We have worked on LLM apps that were integrated into existing workflows. So say, a customer support ticket arrives and my LLM chain kicks off to analyze the request, determine what to do next, and for example, write a response.&lt;/p&gt; &lt;p&gt;But we needed a way for users to check and approve these agent actions. We didn&amp;#39;t find anything to help us with this, since most things are built for chatbots.&lt;/p&gt; &lt;p&gt;So we started working on a human oversight tool that can be integrated into any custom LLM chain. Need human approval? Call our API and we&amp;#39;ll call you back via webhook once there is a user decision.&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://gotohuman.com/&quot;&gt;gotohuman.com&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Would be great to hear your thoughts üôè&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/tisi3000&quot;&gt; /u/tisi3000 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bv2hcf/a_human_approval_tool_for_your_agent_workflows/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bv2hcf/a_human_approval_tool_for_your_agent_workflows/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bv2hcf</id><link href="https://www.reddit.com/r/LangChain/comments/1bv2hcf/a_human_approval_tool_for_your_agent_workflows/" /><updated>2024-04-03T19:56:08+00:00</updated><published>2024-04-03T19:56:08+00:00</published><title>A human approval tool for your agent workflows</title></entry><entry><author><name>/u/99OG121314</name><uri>https://www.reddit.com/user/99OG121314</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I am returning to LangChain after a few months to work on a new project, and since then, LCEL has been introduced. Is the old syntax being entirely depreciated? LCEL seems really hard to wrap my head around. Something as simple as a Conversational Retrieval Chain in python seems so much more complex with a lot more steps in LCEL.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/99OG121314&quot;&gt; /u/99OG121314 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bv6o5c/do_we_have_to_use_lcel/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bv6o5c/do_we_have_to_use_lcel/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bv6o5c</id><link href="https://www.reddit.com/r/LangChain/comments/1bv6o5c/do_we_have_to_use_lcel/" /><updated>2024-04-03T22:39:13+00:00</updated><published>2024-04-03T22:39:13+00:00</published><title>Do we have to use LCEL?</title></entry><entry><author><name>/u/AnantVignesh</name><uri>https://www.reddit.com/user/AnantVignesh</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;ve been using LangChain for quite some time now and I have started to notice that in recent times the model responses start to cut abruptly even before the max token limit is reached. My Max token limit is set to the maximum amount of tokens the model can output, for example my token limit for a Gemini pro model is 4096, however I barely get a couple of lines to a maximum one paragraph of response even when I specifically ask the model to give me a detailed answer. on top of this I have also tried to play with other model parameters like temperature and I also tried setting up the max token to -1 because I saw somewhere that setting Max token to minus one would force the model to output the maximum number of tokens that it can generate, but I still get this issue. On the contrary, I do not have this issue when using native APIs from VertexAI or OpenAI. Am I doing something wrong here? Is anyone else seeing this?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/AnantVignesh&quot;&gt; /u/AnantVignesh &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bvd9qw/unfinished_responses_from_the_models/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bvd9qw/unfinished_responses_from_the_models/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bvd9qw</id><link href="https://www.reddit.com/r/LangChain/comments/1bvd9qw/unfinished_responses_from_the_models/" /><updated>2024-04-04T03:35:18+00:00</updated><published>2024-04-04T03:35:18+00:00</published><title>Unfinished responses from the models</title></entry><entry><author><name>/u/DescriptionKind621</name><uri>https://www.reddit.com/user/DescriptionKind621</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/DescriptionKind621&quot;&gt; /u/DescriptionKind621 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1but7mn/has_anyone_tried_gemini_functions/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1but7mn/has_anyone_tried_gemini_functions/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1but7mn</id><link href="https://www.reddit.com/r/LangChain/comments/1but7mn/has_anyone_tried_gemini_functions/" /><updated>2024-04-03T13:54:59+00:00</updated><published>2024-04-03T13:54:59+00:00</published><title>Has anyone tried Gemini functions ?</title></entry><entry><author><name>/u/AutonomousScott</name><uri>https://www.reddit.com/user/AutonomousScott</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Found the Fructose (&lt;a href=&quot;https://github.com/bananaml/fructose&quot;&gt;https://github.com/bananaml/fructose&lt;/a&gt;) and LlamaIndex toolchains for defining fixed schemas and wondering if LangChain has something similar? Very interested in type-checked outputs for document understanding.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/AutonomousScott&quot;&gt; /u/AutonomousScott &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bv25ml/structured_extraction/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bv25ml/structured_extraction/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bv25ml</id><link href="https://www.reddit.com/r/LangChain/comments/1bv25ml/structured_extraction/" /><updated>2024-04-03T19:43:39+00:00</updated><published>2024-04-03T19:43:39+00:00</published><title>Structured extraction?</title></entry><entry><author><name>/u/Threes_</name><uri>https://www.reddit.com/user/Threes_</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;To learn more about prompting, I wanted to create some prompt templates in YAML. When looking for the docs, I come across several links of the LangChain docs that, unfortunatly, lead to a dead end.&lt;/p&gt; &lt;p&gt;Can somebody provide a resource on prompt serialization?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Threes_&quot;&gt; /u/Threes_ &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1buzihw/prompt_serialization_in_yml/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1buzihw/prompt_serialization_in_yml/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1buzihw</id><link href="https://www.reddit.com/r/LangChain/comments/1buzihw/prompt_serialization_in_yml/" /><updated>2024-04-03T18:04:25+00:00</updated><published>2024-04-03T18:04:25+00:00</published><title>Prompt Serialization in YML</title></entry><entry><author><name>/u/Puzzleheaded_Exit426</name><uri>https://www.reddit.com/user/Puzzleheaded_Exit426</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hello there, I am brainstorming a project for a company that does digital marketing. They want to integrate llms into their client relation processes, writing emails, proposals, documents. I think what I will be doing for them will be creating a pipeline that creates separate text databases for each client, and then create a ChatGPT assistant that includes these text documents in its knowledge base, such that employees can have conversations with this assistant and have it draft things for them. &lt;/p&gt; &lt;p&gt;My initial plan was just to use the openai api, and update per-client assistant knowledge bases daily. I understand there are more sophisticated approaches out there, but don&amp;#39;t want to get too much into the weeds and try to keep it simple if possible. What is the easiest way to get RAG producing better writing than simple knowledge-base driven chatGPT assistants per client? &lt;/p&gt; &lt;p&gt;On a slightly different note, my experience lately is that Claude is superior for human-like writing, but i have never tried to create anything with their API, how does it compare to the OpenAI API for something like this?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Puzzleheaded_Exit426&quot;&gt; /u/Puzzleheaded_Exit426 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1buxlp9/simple_crm_copilot/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1buxlp9/simple_crm_copilot/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1buxlp9</id><link href="https://www.reddit.com/r/LangChain/comments/1buxlp9/simple_crm_copilot/" /><updated>2024-04-03T16:51:35+00:00</updated><published>2024-04-03T16:51:35+00:00</published><title>simple CRM 'copilot'</title></entry><entry><author><name>/u/bambamb1g3l0w</name><uri>https://www.reddit.com/user/bambamb1g3l0w</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;In LangChain 0.1, why is YamlOutputParser imported from &lt;code&gt;langchain.output_parsers&lt;/code&gt; whereas the other parsers like JsonOutputParser, XMLOutputParser etc. imported from &lt;code&gt;langchain_core.output_parsers&lt;/code&gt;?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/bambamb1g3l0w&quot;&gt; /u/bambamb1g3l0w &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bunovt/location_of_yamloutputparser_in_langchain_01/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bunovt/location_of_yamloutputparser_in_langchain_01/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bunovt</id><link href="https://www.reddit.com/r/LangChain/comments/1bunovt/location_of_yamloutputparser_in_langchain_01/" /><updated>2024-04-03T08:47:48+00:00</updated><published>2024-04-03T08:47:48+00:00</published><title>Location of YamlOutputParser in LangChain 0.1</title></entry><entry><author><name>/u/Top_Raccoon_1493</name><uri>https://www.reddit.com/user/Top_Raccoon_1493</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I am Implementing ConversationBufferMemory and the problem I am facing above is when I ask a different question which is not relevant to chat_history, still it gives us response based on the chat history how can I change the prompt to handle such cases.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Top_Raccoon_1493&quot;&gt; /u/Top_Raccoon_1493 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bunkoo/how_can_i_adjust_the_prompt_to_ensure_that/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bunkoo/how_can_i_adjust_the_prompt_to_ensure_that/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bunkoo</id><link href="https://www.reddit.com/r/LangChain/comments/1bunkoo/how_can_i_adjust_the_prompt_to_ensure_that/" /><updated>2024-04-03T08:39:26+00:00</updated><published>2024-04-03T08:39:26+00:00</published><title>How can I adjust the prompt to ensure that responses are not influenced by irrelevant questions that are unrelated to the chat history?</title></entry><entry><author><name>/u/mehul_gupta1997</name><uri>https://www.reddit.com/user/mehul_gupta1997</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Checkout this playlist around Multi-Agent Orchestration that covers 1. What is Multi-Agent Orchestration? 2. Beginners guide for Autogen, CrewAI and LangGraph 3. Debate application between 2 agents using LangGraph 4. Multi-Agent chat using Autogen 5. AI tech team using CrewAI 6. Autogen using HuggingFace and local LLMs&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://youtube.com/playlist?list=PLnH2pfPCPZsKhlUSP39nRzLkfvi_FhDdD&amp;amp;si=B3yPIIz7rRxdZ5aU&quot;&gt;https://youtube.com/playlist?list=PLnH2pfPCPZsKhlUSP39nRzLkfvi_FhDdD&amp;amp;si=B3yPIIz7rRxdZ5aU&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/mehul_gupta1997&quot;&gt; /u/mehul_gupta1997 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bu50s6/multiagent_orchestration_playlist/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bu50s6/multiagent_orchestration_playlist/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bu50s6</id><link href="https://www.reddit.com/r/LangChain/comments/1bu50s6/multiagent_orchestration_playlist/" /><updated>2024-04-02T17:46:11+00:00</updated><published>2024-04-02T17:46:11+00:00</published><title>Multi-Agent Orchestration playlist</title></entry><entry><author><name>/u/shreyansh26</name><uri>https://www.reddit.com/user/shreyansh26</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I built a quick RAG implementation using Langchain to make it easy to query the &lt;a href=&quot;https://github.com/stas00/ml-engineering&quot;&gt;ML Engineering Open Book&lt;/a&gt; by &lt;a href=&quot;https://twitter.com/StasBekman&quot;&gt;Stas Bekman&lt;/a&gt;. Hope it is useful for folks. It has been proving to be incredibly useful for me!&lt;/p&gt; &lt;p&gt;Github link - &lt;a href=&quot;https://github.com/shreyansh26/RAG-ML-Engg-Open-Book&quot;&gt;https://github.com/shreyansh26/RAG-ML-Engg-Open-Book&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/shreyansh26&quot;&gt; /u/shreyansh26 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bu4sm4/rag_pipeline_to_query_the_ml_engineering_open_book/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bu4sm4/rag_pipeline_to_query_the_ml_engineering_open_book/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bu4sm4</id><link href="https://www.reddit.com/r/LangChain/comments/1bu4sm4/rag_pipeline_to_query_the_ml_engineering_open_book/" /><updated>2024-04-02T17:37:12+00:00</updated><published>2024-04-02T17:37:12+00:00</published><title>RAG pipeline to query the ML Engineering Open Book</title></entry><entry><author><name>/u/Mediocre-Card8046</name><uri>https://www.reddit.com/user/Mediocre-Card8046</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;I created a RAG app and now want to also use chat history in my application. All of my applications functionality is handled via a FastAPI backend, the frontend is in Streamlit. &lt;/p&gt; &lt;p&gt;How can I now add chat history in my FastAPI RAG endpoint? &lt;/p&gt; &lt;p&gt;Is there a way to just save the chat history in a session? E.g. making a post endpoint where the history gets added to the chain. Alternatively I have heard that Redis is a good choice but as for my understanding it is not open-source anymore. For me it would be important to have a open-source solution which is free and can run online.&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;Thanks in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Mediocre-Card8046&quot;&gt; /u/Mediocre-Card8046 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bu1os7/how_to_use_rag_chat_history_with_fastapi/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bu1os7/how_to_use_rag_chat_history_with_fastapi/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bu1os7</id><link href="https://www.reddit.com/r/LangChain/comments/1bu1os7/how_to_use_rag_chat_history_with_fastapi/" /><updated>2024-04-02T15:32:42+00:00</updated><published>2024-04-02T15:32:42+00:00</published><title>How to use RAG chat history with FastAPI?</title></entry><entry><author><name>/u/MoronSlayer42</name><uri>https://www.reddit.com/user/MoronSlayer42</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Has anyone used Azure Document intelligence for capturing metadata in PDFs with tables, figures? How can we create semantic chunks using a Qdrant database using Azure Document intelligence to extract data? How can add relevant metadata to meaningful chunks? Any other tips to create an advanced RAG pipeline? What are evaluations methods available?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/MoronSlayer42&quot;&gt; /u/MoronSlayer42 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bu5hyt/advanced_rag_for_pdfs_with_tables_and_figures/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bu5hyt/advanced_rag_for_pdfs_with_tables_and_figures/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bu5hyt</id><link href="https://www.reddit.com/r/LangChain/comments/1bu5hyt/advanced_rag_for_pdfs_with_tables_and_figures/" /><updated>2024-04-02T18:04:55+00:00</updated><published>2024-04-02T18:04:55+00:00</published><title>Advanced RAG for PDFs with tables and figures, capturing metadata , Azure Document Intelligent</title></entry><entry><author><name>/u/Desperate-Energy2694</name><uri>https://www.reddit.com/user/Desperate-Energy2694</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi, all!&lt;/p&gt; &lt;p&gt;I&amp;#39;m currently working with Langchain with a logic that will go through all PDFs with RetrievalQA asking a certain query inside a loop (so n inferences every time).&lt;/p&gt; &lt;p&gt;The problem is, the bigger my retrieved chunks and prompt get, the more memory is allocated in my VRAM, and I only have 24GB available, and it oftens runs out with some of my testings.&lt;/p&gt; &lt;p&gt;Is there a way to prevent this amount of memory consumed through the loop? It seems to be that every retrieval call increments more and more memory allocated. I&amp;#39;ve tried nesting my logic under torch.no_grad() but no luck. I also have my PYTORCH_CUDA_ALLOC_CONF variable set to garbage_collection_threshold:0.6,max_split_size_mb:128.&lt;/p&gt; &lt;p&gt;Thanks in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Desperate-Energy2694&quot;&gt; /u/Desperate-Energy2694 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bufjwi/retrievalqa_loop_consuming_gradually_more_memory/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bufjwi/retrievalqa_loop_consuming_gradually_more_memory/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bufjwi</id><link href="https://www.reddit.com/r/LangChain/comments/1bufjwi/retrievalqa_loop_consuming_gradually_more_memory/" /><updated>2024-04-03T01:03:24+00:00</updated><published>2024-04-03T01:03:24+00:00</published><title>RetrievalQA loop consuming gradually more memory</title></entry><entry><author><name>/u/Lost-Season-4196</name><uri>https://www.reddit.com/user/Lost-Season-4196</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I want to use function calling with Gemini, I checked Vertex ai documentation and tutorials but they are a bit confusing and mess. Anyone have worked with Gemini function calling with Langchain before?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Lost-Season-4196&quot;&gt; /u/Lost-Season-4196 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bu9ivz/gemini_function_calling/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bu9ivz/gemini_function_calling/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bu9ivz</id><link href="https://www.reddit.com/r/LangChain/comments/1bu9ivz/gemini_function_calling/" /><updated>2024-04-02T20:46:09+00:00</updated><published>2024-04-02T20:46:09+00:00</published><title>Gemini function calling</title></entry><entry><author><name>/u/Lost-Season-4196</name><uri>https://www.reddit.com/user/Lost-Season-4196</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I want to use function calling with Gemini, I checked Vertex ai documentation and tutorials but they are a bit confusing and mess. Anyone have worked with Gemini function calling with Langchain before?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Lost-Season-4196&quot;&gt; /u/Lost-Season-4196 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bu9j1u/gemini_function_calling/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bu9j1u/gemini_function_calling/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bu9j1u</id><link href="https://www.reddit.com/r/LangChain/comments/1bu9j1u/gemini_function_calling/" /><updated>2024-04-02T20:46:24+00:00</updated><published>2024-04-02T20:46:24+00:00</published><title>Gemini function calling</title></entry><entry><author><name>/u/DesignerReception594</name><uri>https://www.reddit.com/user/DesignerReception594</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/DesignerReception594&quot;&gt; /u/DesignerReception594 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bu8kmc/does_anyone_have_a_digital_copy_of_the_book/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bu8kmc/does_anyone_have_a_digital_copy_of_the_book/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bu8kmc</id><link href="https://www.reddit.com/r/LangChain/comments/1bu8kmc/does_anyone_have_a_digital_copy_of_the_book/" /><updated>2024-04-02T20:02:59+00:00</updated><published>2024-04-02T20:02:59+00:00</published><title>Does anyone have a digital copy of the book Transformers for natural language processing and computer vision</title></entry><entry><author><name>/u/No-Mathematician9974</name><uri>https://www.reddit.com/user/No-Mathematician9974</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;m using a langchain script in order to make a similarity search of a query embedding in an embedding&amp;#39;s MongoDb collection but I want to pre filter the documents to search only in the documents that are $in an objectId array.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;// Get the list of embeddings _ids to preFilter const documentData = await documentCollection.findOne( { &amp;quot;_id&amp;quot;: documentId }, { &amp;quot;embededings&amp;quot;: 1 } ) const documentEmbeddingsId = documentData.embeddings // Embed query const query = &amp;quot;What is this document about?&amp;quot; const embeddings = new OpenAIEmbeddings({ modelName:&amp;quot;XXX&amp;quot;, openAIApiKey: &amp;quot;XXX&amp;quot; }) const embeddedQuery = await embeddings.embedQuery(query) // Similarity search const vectorStore = new MongoDBAtlasVectorSearch(embeddings, { collection, indexName: &amp;quot;XXX&amp;quot;, textKey: &amp;quot;XXX&amp;quot;, embeddingKey: &amp;quot;XXX&amp;quot;, }); const preFilter = { preFilter: { _id: { $in: documentEmbeddingsId }, }, } const storingResponse = await vectorStore.similaritySearchVectorWithScore(embeddedQuery,4,preFilter) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Running this code returns this error:&lt;/p&gt; &lt;p&gt;&amp;#39;Error: MongoServerError: Operand type is not supported for $vectorSearch: objectId&amp;#39;.&lt;/p&gt; &lt;p&gt;Is the error in the preFilter? Or is this type of filter not supported by mongodb? Any ideas on how I can make this search? The documentEmbeddings array has ObjectId type. If I try to instead give a string array I get the following error: Error: MongoServerError: PlanExecutor error during aggregation :: caused by :: Path &amp;#39;_id&amp;#39; needs to be indexed as token&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/No-Mathematician9974&quot;&gt; /u/No-Mathematician9974 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bu5dqs/prefilter_documents_before_similaritysearch/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bu5dqs/prefilter_documents_before_similaritysearch/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bu5dqs</id><link href="https://www.reddit.com/r/LangChain/comments/1bu5dqs/prefilter_documents_before_similaritysearch/" /><updated>2024-04-02T18:00:28+00:00</updated><published>2024-04-02T18:00:28+00:00</published><title>Prefilter documents before similaritysearch</title></entry></feed>