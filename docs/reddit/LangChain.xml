<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/"><category term="LangChain" label="r/LangChain"/><updated>2024-07-15T23:37:25+00:00</updated><icon>https://www.redditstatic.com/icon.png/</icon><id>/r/LangChain.rss</id><link rel="self" href="https://www.reddit.com/r/LangChain.rss" type="application/atom+xml" /><link rel="alternate" href="https://www.reddit.com/r/LangChain" type="text/html" /><subtitle>LangChain is an open-source framework and developer toolkit that helps developers get LLM applications from prototype to production. It is available for Python and Javascript at https://www.langchain.com/.</subtitle><title>LangChain</title><entry><author><name>/u/zchaarm</name><uri>https://www.reddit.com/user/zchaarm</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;A place for members of &lt;a href=&quot;/r/LangChain&quot;&gt;r/LangChain&lt;/a&gt; to chat with each other&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/zchaarm&quot;&gt; /u/zchaarm &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_10ljho9</id><link href="https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/" /><updated>2023-01-26T04:33:05+00:00</updated><published>2023-01-26T04:33:05+00:00</published><title>r/LangChain Lounge</title></entry><entry><author><name>/u/neilkatz</name><uri>https://www.reddit.com/user/neilkatz</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey Everyone,&lt;/p&gt; &lt;p&gt;I&amp;#39;m curious what the group thinks are the biggest pain points for devs getting started with RAG? My list would be: &lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;hallucination&lt;/strong&gt;: especially with complex docs&lt;/li&gt; &lt;li&gt;&lt;strong&gt;eval&lt;/strong&gt;: there are tools to score completions vs retrievals, but what about the rest of the RAG pipeline where the problems actually occur. &lt;/li&gt; &lt;li&gt;&lt;strong&gt;complexity:&lt;/strong&gt; many pieces of the pipeline to master (parse, extract, convert to LLM friendly data, chunk, embed, create metadata for context, search, rerank, etc) and lots of theories on best approach to each one. &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;What&amp;#39;s everyone else dealing with?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/neilkatz&quot;&gt; /u/neilkatz &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e3ygh6/biggest_rag_hurdles_for_beginners/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e3ygh6/biggest_rag_hurdles_for_beginners/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1e3ygh6</id><link href="https://www.reddit.com/r/LangChain/comments/1e3ygh6/biggest_rag_hurdles_for_beginners/" /><updated>2024-07-15T16:01:08+00:00</updated><published>2024-07-15T16:01:08+00:00</published><title>Biggest RAG Hurdles for Beginners?</title></entry><entry><author><name>/u/jeffrey-0711</name><uri>https://www.reddit.com/user/jeffrey-0711</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e3q2lb/experiment_good_chunking_will_lead_you_to_the/&quot;&gt; &lt;img src=&quot;https://b.thumbs.redditmedia.com/i3T3Wy5JfGyG-uY9Ri9xzDXuU_jQxYvBmFZVHH3aGkU.jpg&quot; alt=&quot;[Experiment] Good chunking will lead you to the better RAG performance&quot; title=&quot;[Experiment] Good chunking will lead you to the better RAG performance&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Yes, chunking document in RAG is important. But, how much? I got curious and ran a experiment to see how chunking method will effect to the RAG performance.&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/krnc1j0fancd1.png?width=2894&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a045b1d5ed51e6d721adc1112805e2bea5968dae&quot;&gt;The diagram of the experiment&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I selected total five chunking method. I ran exact same RAG pipeline with different chunking method and measure the answer quality. It was Korean document with 40 questions.&lt;/p&gt; &lt;h1&gt;The five chunking method&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;No Chunk : PDF page itself is the chunk.&lt;br/&gt;&lt;/li&gt; &lt;li&gt;Token Splitter&lt;br/&gt;&lt;/li&gt; &lt;li&gt;Recursive Splitter&lt;br/&gt;&lt;/li&gt; &lt;li&gt;Window Splitter : comes from &lt;a href=&quot;https://docs.llamaindex.ai/en/stable/examples/node_postprocessor/MetadataReplacementDemo/&quot;&gt;LlamaIndex&lt;/a&gt; : passage merge after retrieval&lt;br/&gt;&lt;/li&gt; &lt;li&gt;Semantic Splitter : used openai embedding&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I used G-eval as a LLM-eval metric. And I used &lt;a href=&quot;https://github.com/Marker-Inc-Korea/AutoRAG&quot;&gt;AutoRAG&lt;/a&gt; to quickly run the RAG experiment. (The G-eval range is 1 to 5)&lt;/p&gt; &lt;p&gt;Plus, since the G-eval is not perfect metric, I checked all question and answers manually and measure the factualness of the RAG answers. &lt;/p&gt; &lt;p&gt;If the answer was perfect, I gave 1. If the RAG system said &amp;quot;don&amp;#39;t know&amp;#39;, I gave 0. If there was hallucination, I gave -1. And the answer might be vague, then I gave 0.5.&lt;/p&gt; &lt;h1&gt;Result&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align=&quot;left&quot;&gt;Metric Result&lt;/th&gt; &lt;th align=&quot;left&quot;&gt;No Chunk&lt;/th&gt; &lt;th align=&quot;left&quot;&gt;Token Splitter&lt;/th&gt; &lt;th align=&quot;left&quot;&gt;Recursive Splitter&lt;/th&gt; &lt;th align=&quot;left&quot;&gt;Window Splitter&lt;/th&gt; &lt;th align=&quot;left&quot;&gt;Semantic Splitter&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align=&quot;left&quot;&gt;G-eval&lt;/td&gt; &lt;td align=&quot;left&quot;&gt;2.4706&lt;/td&gt; &lt;td align=&quot;left&quot;&gt;3.3529&lt;/td&gt; &lt;td align=&quot;left&quot;&gt;3.1176&lt;/td&gt; &lt;td align=&quot;left&quot;&gt;3.3529&lt;/td&gt; &lt;td align=&quot;left&quot;&gt;&lt;strong&gt;3.7647&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align=&quot;left&quot;&gt;Human Eval&lt;/td&gt; &lt;td align=&quot;left&quot;&gt;0.3529&lt;/td&gt; &lt;td align=&quot;left&quot;&gt;0.6471&lt;/td&gt; &lt;td align=&quot;left&quot;&gt;0.4706&lt;/td&gt; &lt;td align=&quot;left&quot;&gt;0.5882&lt;/td&gt; &lt;td align=&quot;left&quot;&gt;&lt;strong&gt;0.7941&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Both G-eval and human eval, the semantic splitter performs best. The score is quite different, and its impact is quite big. Find the right chunking is essential to build great RAG system.&lt;/p&gt; &lt;h1&gt;Limitation&lt;/h1&gt; &lt;p&gt;Since I used only 40 questions, and human eval was performed only on the 17 questions, the statistic needs to be larger for more precise result. &lt;/p&gt; &lt;hr/&gt; &lt;p&gt;This experiment took only a few hours, thanks to the &lt;a href=&quot;https://github.com/Marker-Inc-Korea/AutoRAG/&quot;&gt;AutoRAG&lt;/a&gt;. You can optimize and run the experiment easily using it. It is open-source project so feel free to use it on your RAG project.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/jeffrey-0711&quot;&gt; /u/jeffrey-0711 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e3q2lb/experiment_good_chunking_will_lead_you_to_the/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e3q2lb/experiment_good_chunking_will_lead_you_to_the/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1e3q2lb</id><media:thumbnail url="https://b.thumbs.redditmedia.com/i3T3Wy5JfGyG-uY9Ri9xzDXuU_jQxYvBmFZVHH3aGkU.jpg" /><link href="https://www.reddit.com/r/LangChain/comments/1e3q2lb/experiment_good_chunking_will_lead_you_to_the/" /><updated>2024-07-15T08:56:25+00:00</updated><published>2024-07-15T08:56:25+00:00</published><title>[Experiment] Good chunking will lead you to the better RAG performance</title></entry><entry><author><name>/u/angularlicious</name><uri>https://www.reddit.com/user/angularlicious</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;This LangChain tool is a little naive- it is just a text splitter with charLength and overlap size. Does anyone know of a compatible splitter that can breakdown a Markdown based on header sections and other elements (eg, lists, code, tables)??&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/angularlicious&quot;&gt; /u/angularlicious &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e47vwu/markdowntextsplitter/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e47vwu/markdowntextsplitter/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1e47vwu</id><link href="https://www.reddit.com/r/LangChain/comments/1e47vwu/markdowntextsplitter/" /><updated>2024-07-15T22:13:14+00:00</updated><published>2024-07-15T22:13:14+00:00</published><title>MarkdownTextSplitter</title></entry><entry><author><name>/u/vns1311</name><uri>https://www.reddit.com/user/vns1311</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey everyone, I am looking to integrate ragas scoring for every api call made to a langserve endpoint. Does anyone have any references or thoughts on how one can do this. I am planning to further log these metrics into langsmith to track the performance over time&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/vns1311&quot;&gt; /u/vns1311 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e3u619/integration_of_ragas_evaluation_with_langserve/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e3u619/integration_of_ragas_evaluation_with_langserve/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1e3u619</id><link href="https://www.reddit.com/r/LangChain/comments/1e3u619/integration_of_ragas_evaluation_with_langserve/" /><updated>2024-07-15T12:58:55+00:00</updated><published>2024-07-15T12:58:55+00:00</published><title>Integration of RAGAS Evaluation with Langserve</title></entry><entry><author><name>/u/FunInformation2332</name><uri>https://www.reddit.com/user/FunInformation2332</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I am creating a RAG application based on legal texts. I have documents approx. 80 million tokens long. As you know for RAG application I need to embed documents first.&lt;/p&gt; &lt;p&gt;Formerly I was using openai embeddings &lt;em&gt;text-embedding-3-large&lt;/em&gt; model but it will cost 10k$. For better outputs and lower cost I want to switch &lt;em&gt;Nvidia nim nv-embed-v1&lt;/em&gt; but I have never hosted an AI model before so I cannot predict approx. cost.&lt;/p&gt; &lt;p&gt;I will be glad if you guys can share any source for hosting AI models, calculating Host Cost and maybe tell me which one is cheaper &lt;em&gt;text-embedding-3-large&lt;/em&gt; or &lt;em&gt;Nvidia nim nv-embed-v1&lt;/em&gt;?&lt;/p&gt; &lt;p&gt;Thank you.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/FunInformation2332&quot;&gt; /u/FunInformation2332 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e3w5lq/cost_prediction_of_nvidia_nim_nvembedv1/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e3w5lq/cost_prediction_of_nvidia_nim_nvembedv1/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1e3w5lq</id><link href="https://www.reddit.com/r/LangChain/comments/1e3w5lq/cost_prediction_of_nvidia_nim_nvembedv1/" /><updated>2024-07-15T14:27:54+00:00</updated><published>2024-07-15T14:27:54+00:00</published><title>Cost Prediction of nvidia nim nv-embed-v1</title></entry><entry><author><name>/u/xandie985</name><uri>https://www.reddit.com/user/xandie985</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;What is the default search metric while using:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;similarity_search() &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Also, can I explicitly give my own metrics, what are the other metrics available for ChromaDB?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/xandie985&quot;&gt; /u/xandie985 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e4059d/default_search_method_in_chromdb_while_performing/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e4059d/default_search_method_in_chromdb_while_performing/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1e4059d</id><link href="https://www.reddit.com/r/LangChain/comments/1e4059d/default_search_method_in_chromdb_while_performing/" /><updated>2024-07-15T17:07:38+00:00</updated><published>2024-07-15T17:07:38+00:00</published><title>Default search method in ChromDB while performing similarity_search()</title></entry><entry><author><name>/u/Cultural-Educator-43</name><uri>https://www.reddit.com/user/Cultural-Educator-43</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;So I have a llama 2 7b chat model on my machine running locally, what I want to do is that the model takes a CSV file as input the file can contain anything ranging from text, numbers, shareholder info, etc., process the file and gives out text in a particular paragraph form that makes sense, I tried asking multiple GPT&amp;#39;s but I couldn&amp;#39;t get anywhere &lt;/p&gt; &lt;p&gt;So if any of you esteemed AI engineers could help a frustrated college student it would mean a lot &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Cultural-Educator-43&quot;&gt; /u/Cultural-Educator-43 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e3yv1o/help_ideas_needed_i_have_a_llama_2_chat_model/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e3yv1o/help_ideas_needed_i_have_a_llama_2_chat_model/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1e3yv1o</id><link href="https://www.reddit.com/r/LangChain/comments/1e3yv1o/help_ideas_needed_i_have_a_llama_2_chat_model/" /><updated>2024-07-15T16:17:03+00:00</updated><published>2024-07-15T16:17:03+00:00</published><title>[HELP/ IDEAS NEEDED] I have a llama 2 chat model working locally i want the model to take a CSV file as an input and give an output in text form like paragraphs</title></entry><entry><author><name>/u/Interesting_Net_9628</name><uri>https://www.reddit.com/user/Interesting_Net_9628</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I am trying to get `AzureChatOpenAI` to work, but keep getting `openai.NotFoundError: Error code: 404 - {&amp;#39;error&amp;#39;: {&amp;#39;code&amp;#39;: &amp;#39;404&amp;#39;, &amp;#39;message&amp;#39;: &amp;#39;Resource not found&amp;#39;}}&lt;/p&gt; &lt;p&gt;` &lt;/p&gt; &lt;p&gt;It works if I use `AzureOpenAI`. However, the issue with this is I got the following error&lt;/p&gt; &lt;p&gt;```&lt;br/&gt; raise ValueError(&lt;/p&gt; &lt;p&gt;ValueError: OpenAIChat currently only supports single prompt, got&lt;br/&gt; ```&lt;/p&gt; &lt;p&gt;I am trying a simple tutorial to use `load_summarize_chain`&lt;/p&gt; &lt;p&gt;Does anyone have any idea?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Interesting_Net_9628&quot;&gt; /u/Interesting_Net_9628 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e3w7l9/azurechatopenai_not_working/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e3w7l9/azurechatopenai_not_working/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1e3w7l9</id><link href="https://www.reddit.com/r/LangChain/comments/1e3w7l9/azurechatopenai_not_working/" /><updated>2024-07-15T14:30:12+00:00</updated><published>2024-07-15T14:30:12+00:00</published><title>AzureChatOpenAI not working</title></entry><entry><author><name>/u/sks8100</name><uri>https://www.reddit.com/user/sks8100</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey guys so I had a question. Have a simple api that in an input, calls an LLM for processing (I use llama3 with grok at the moment) and returns an output. When I run it on my computer it runs fine. Takes a few seconds but runs. Most of this is fancy prompting so I haven‚Äôt fine tuned the model or anything at the moment. It‚Äôs just chained prompts and LLMs&lt;/p&gt; &lt;p&gt;I tried to deploy it on render to test out but I keep getting a memory error given it‚Äôs taken more than 512MB given that is renders free tier. &lt;/p&gt; &lt;p&gt;Does anybody know where else I can try this out? Another server or even a real cheap one? &lt;/p&gt; &lt;p&gt;As for long term I need to move passed grok and deploy my own LLM using olama or just move to open AI. I haven‚Äôt moved yet because I find the output to be better with llama3 vs OpenAI 3.5‚Ä¶.4o is too cost prohibitive at the moment &lt;/p&gt; &lt;p&gt;Any advice? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/sks8100&quot;&gt; /u/sks8100 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e3lu2v/deploy_api_with_llm_backend/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e3lu2v/deploy_api_with_llm_backend/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1e3lu2v</id><link href="https://www.reddit.com/r/LangChain/comments/1e3lu2v/deploy_api_with_llm_backend/" /><updated>2024-07-15T04:20:37+00:00</updated><published>2024-07-15T04:20:37+00:00</published><title>Deploy Api with LLM backend</title></entry><entry><author><name>/u/jscraft</name><uri>https://www.reddit.com/user/jscraft</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e3oap0/chatgpt_vision_api_with_langchain_and_javascript/&quot;&gt; &lt;img src=&quot;https://external-preview.redd.it/2347JJ6JbmR5WGmdxSM3iu0OnGohPD3MltcLei8lmx8.jpg?width=108&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7a4559964a97155995aa6f0c96a36eebba24728a&quot; alt=&quot;ChatGPT Vision API with LangChain and JavaScript&quot; title=&quot;ChatGPT Vision API with LangChain and JavaScript&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi folks! &lt;/p&gt; &lt;p&gt;Made this short example with ChatGPT Vision API, LangChain and Node: &lt;a href=&quot;https://www.js-craft.io/blog/vision-api-langchain-javascript/&quot;&gt;https://www.js-craft.io/blog/vision-api-langchain-javascript/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/wxpxg0erqmcd1.jpg?width=1582&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=82969758c98e21ef28bd13caccdcc04fc9aa651e&quot;&gt;https://preview.redd.it/wxpxg0erqmcd1.jpg?width=1582&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=82969758c98e21ef28bd13caccdcc04fc9aa651e&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hope it&amp;#39;s helpful for someone as a getting started example üôÇ&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/jscraft&quot;&gt; /u/jscraft &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e3oap0/chatgpt_vision_api_with_langchain_and_javascript/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e3oap0/chatgpt_vision_api_with_langchain_and_javascript/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1e3oap0</id><media:thumbnail url="https://external-preview.redd.it/2347JJ6JbmR5WGmdxSM3iu0OnGohPD3MltcLei8lmx8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=7a4559964a97155995aa6f0c96a36eebba24728a" /><link href="https://www.reddit.com/r/LangChain/comments/1e3oap0/chatgpt_vision_api_with_langchain_and_javascript/" /><updated>2024-07-15T06:54:40+00:00</updated><published>2024-07-15T06:54:40+00:00</published><title>ChatGPT Vision API with LangChain and JavaScript</title></entry><entry><author><name>/u/Euloghtos</name><uri>https://www.reddit.com/user/Euloghtos</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hello everyone, so I have this problem puzzling, i have a simple rag app to chat with my pdf files, and i use crhoma as the vector db, i am using cosine for saving the documents and threshold for retrieving the most relevant ones, my problem is i have to provide a n_results for chroma, otherwise the default is returning 4 documents, my problem is if i provide 500 results, it will always provide 500 results, no matter how relevant they are, i wonder is there a smart way to adjust the search results each time? cause sometimes 500 results might be too big , sometimes it might be low, so i might lose information, also returning 500 results has some other drawbacks as well, as the app is becoming very slow, not to mention the token size limit.&lt;/p&gt; &lt;p&gt;Thank you in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Euloghtos&quot;&gt; /u/Euloghtos &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e3q18k/chroma_db_results_size/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e3q18k/chroma_db_results_size/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1e3q18k</id><link href="https://www.reddit.com/r/LangChain/comments/1e3q18k/chroma_db_results_size/" /><updated>2024-07-15T08:53:51+00:00</updated><published>2024-07-15T08:53:51+00:00</published><title>Chroma db , results size</title></entry><entry><author><name>/u/gtxktm</name><uri>https://www.reddit.com/user/gtxktm</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hello. Has anybody tried finetuning ColBERT models? Did it help you? How much data do you need to finetune it well? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/gtxktm&quot;&gt; /u/gtxktm &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e3plel/finetuning_colbert_reranker/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e3plel/finetuning_colbert_reranker/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1e3plel</id><link href="https://www.reddit.com/r/LangChain/comments/1e3plel/finetuning_colbert_reranker/" /><updated>2024-07-15T08:22:55+00:00</updated><published>2024-07-15T08:22:55+00:00</published><title>Finetuning ColBERT reranker</title></entry><entry><author><name>/u/catanoga</name><uri>https://www.reddit.com/user/catanoga</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi everyone!&lt;/p&gt; &lt;p&gt;I&amp;#39;m working with langchain to create a RAG-ReAct agent with job candidates CVs. Right now I&amp;#39;m having two main issues: some degree of hallucination (even though I have my llm temperature set to 0; I guess you can&amp;#39;t elude it completely with llms) and tool management. My question is about this one.&lt;/p&gt; &lt;p&gt;I&amp;#39;ve created my tools with the Tool class, and tried with different queries explicitely designed to use several of them in the same process, but I keep getting my agent to stop its output after using the first tool. &lt;/p&gt; &lt;p&gt;For example, if I ask it to first look for people with experience in Hadoop and then calculate for each of them how many years of experience they have in the field (I have a tool designed for basic info search and one designed for calculating periods of time), it will output a list with an answer to the first question, but will forget the second one.&lt;/p&gt; &lt;p&gt;I&amp;#39;ve tried with prompting: nothing. I&amp;#39;ve also tried with setting return_direct to False, but it gets an excesive tokens error.&lt;/p&gt; &lt;p&gt;I&amp;#39;m using create_react_agent function and AgentExecutor class with an OpenAI call.&lt;/p&gt; &lt;p&gt;My prompt is based on &lt;a href=&quot;https://smith.langchain.com/hub/hwchase17/react&quot;&gt;this one&lt;/a&gt; from langchain hub. &lt;/p&gt; &lt;p&gt;Any help would be appreciated, since I keep getting stucked and I&amp;#39;ve run out of ideas. Thanks a lot!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/catanoga&quot;&gt; /u/catanoga &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e3p6pf/react_issues/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e3p6pf/react_issues/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1e3p6pf</id><link href="https://www.reddit.com/r/LangChain/comments/1e3p6pf/react_issues/" /><updated>2024-07-15T07:54:33+00:00</updated><published>2024-07-15T07:54:33+00:00</published><title>ReAct issues</title></entry><entry><author><name>/u/Narrow_Buddy_562</name><uri>https://www.reddit.com/user/Narrow_Buddy_562</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hello! So I have an endpoint on which my trained model is deployed. How can I use this with LangChain? endpoint_url = &amp;#39;---------/chat&amp;#39;&lt;/p&gt; &lt;p&gt;Can someone please give me the wrapper code, I am so confused because I already have this model and I dont know how to use this with LangChain. I want to utilize the ConversationBufferMemory method to retain context.&lt;/p&gt; &lt;p&gt;For background, I am using this endpoint for a Webex chatbot.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Narrow_Buddy_562&quot;&gt; /u/Narrow_Buddy_562 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e3rl5d/how_to_use_langchain_with_a_custom_endpoint/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e3rl5d/how_to_use_langchain_with_a_custom_endpoint/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1e3rl5d</id><link href="https://www.reddit.com/r/LangChain/comments/1e3rl5d/how_to_use_langchain_with_a_custom_endpoint/" /><updated>2024-07-15T10:37:29+00:00</updated><published>2024-07-15T10:37:29+00:00</published><title>How to use LangChain with a custom endpoint?</title></entry><entry><author><name>/u/Kv-boii</name><uri>https://www.reddit.com/user/Kv-boii</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;m currently exploring options to reduce the response time/ inference time through multithreading the prompts, so is it possible and what are some other ways to execute multiple prompts at a same time.&lt;br/&gt; Currently I&amp;#39;m working with the phi-3 models in google colab free, i&amp;#39;m always encountering a error related to a parameter &amp;#39;num_new_tokens&amp;#39; and the prompts are not executing.&lt;/p&gt; &lt;p&gt;If anyone has already implemented multithreading or multiprocessing of LLMs can you explain or provide a link to the resource&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Kv-boii&quot;&gt; /u/Kv-boii &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e3mqf3/is_multi_threading_possible_for_llms_integrated/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e3mqf3/is_multi_threading_possible_for_llms_integrated/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1e3mqf3</id><link href="https://www.reddit.com/r/LangChain/comments/1e3mqf3/is_multi_threading_possible_for_llms_integrated/" /><updated>2024-07-15T05:14:57+00:00</updated><published>2024-07-15T05:14:57+00:00</published><title>Is multi threading possible for llms integrated from the vllm framework?</title></entry><entry><author><name>/u/giagara</name><uri>https://www.reddit.com/user/giagara</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I made a RAG system with Langchain just becasuse a coworker did a POC with it, and i&amp;#39;ve structured it much better. At the moment i use pgvector as vectorstore adding some filter to it based on metadata, pass the document to the stuff chain and get a streaming answer.&lt;/p&gt; &lt;p&gt;Now the business requires some new features, like parent document retriever, calculating costs, hybrid search/reranking, etc.. that i find very tough using Langchain instead of making all vanilla.&lt;/p&gt; &lt;p&gt;At the moment i just place a postgres query, call openAI to rephrase my question and history, and call openAI again to get the answer. Is nothing that fancy, isn&amp;#39;t it?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/giagara&quot;&gt; /u/giagara &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e2vx5m/why_should_i_keep_using_langchain/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e2vx5m/why_should_i_keep_using_langchain/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1e2vx5m</id><link href="https://www.reddit.com/r/LangChain/comments/1e2vx5m/why_should_i_keep_using_langchain/" /><updated>2024-07-14T06:57:00+00:00</updated><published>2024-07-14T06:57:00+00:00</published><title>Why should i keep using langchain?</title></entry><entry><author><name>/u/Rough_Grapefruit1900</name><uri>https://www.reddit.com/user/Rough_Grapefruit1900</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e2xzbu/stupid_question_which_tool_langchain_team_use_to/&quot;&gt; &lt;img src=&quot;https://b.thumbs.redditmedia.com/CrRC1zbzhfsXvbhN48SORaeZg8aNaHaJQb7JLMzXK3c.jpg&quot; alt=&quot;stupid question : which tool LangChain team use to draw their schema ?&quot; title=&quot;stupid question : which tool LangChain team use to draw their schema ?&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Love LangChain schematic, which tool they use ? Do you have any recommandation to draw easily agentic/LLM app architecture ?&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/905gadmjbgcd1.png?width=2893&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=dbc2601a5ad8917901a4ab23c88a60959681411f&quot;&gt;https://preview.redd.it/905gadmjbgcd1.png?width=2893&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=dbc2601a5ad8917901a4ab23c88a60959681411f&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Rough_Grapefruit1900&quot;&gt; /u/Rough_Grapefruit1900 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e2xzbu/stupid_question_which_tool_langchain_team_use_to/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e2xzbu/stupid_question_which_tool_langchain_team_use_to/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1e2xzbu</id><media:thumbnail url="https://b.thumbs.redditmedia.com/CrRC1zbzhfsXvbhN48SORaeZg8aNaHaJQb7JLMzXK3c.jpg" /><link href="https://www.reddit.com/r/LangChain/comments/1e2xzbu/stupid_question_which_tool_langchain_team_use_to/" /><updated>2024-07-14T09:15:57+00:00</updated><published>2024-07-14T09:15:57+00:00</published><title>stupid question : which tool LangChain team use to draw their schema ?</title></entry><entry><author><name>/u/ML_DL_RL</name><uri>https://www.reddit.com/user/ML_DL_RL</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Super excited to share that our iOS app is live for beta testers. In case you want to join please visit us at: &lt;a href=&quot;https://myreflection.ai/&quot;&gt;https://myreflection.ai/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;MyReflection is a memory preservation agent on steroids, encompassing images, audios, and journals. Imagine interacting with these memories, reminiscing, and exploring them. It&amp;#39;s like a mirror allowing you to further reflect on your thoughts, ideas, or experiences. Through these memories, we enable our users to create a digital interactive twin of themselves later on.&lt;/p&gt; &lt;p&gt;This was built keeping user security and privacy on top of our list. Please give it a test drive would love to hear your feedback. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/ML_DL_RL&quot;&gt; /u/ML_DL_RL &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e37ksf/memory_preservation_using_ai_beta_testing_ios_app/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e37ksf/memory_preservation_using_ai_beta_testing_ios_app/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1e37ksf</id><link href="https://www.reddit.com/r/LangChain/comments/1e37ksf/memory_preservation_using_ai_beta_testing_ios_app/" /><updated>2024-07-14T17:15:03+00:00</updated><published>2024-07-14T17:15:03+00:00</published><title>Memory Preservation using AI (Beta testing iOS App)</title></entry><entry><author><name>/u/RepresentativeAge297</name><uri>https://www.reddit.com/user/RepresentativeAge297</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I recently started with Python and the project I built first was a simple project based on the rag model. High level is I used a book and parsed it , created chunks and stored the embeddings in the chromadb (which is a vector db). &lt;/p&gt; &lt;p&gt;Based on user query finds the most relevant first matching chunk and passes it to llm with this info as context and user query. The llm(gemini-pro) generates the response &lt;/p&gt; &lt;p&gt;Doubt:&lt;/p&gt; &lt;p&gt;Are these type of projects are even good projects as I&amp;#39;ve just completed my graduation 2 weeks ago in computer science. Does this type of projects are even good for fresher? What more interesting things I should do and how do I measure that the my rag model produces better results compared to directly asking the llm same query. What are the parameters the length, Grammer or etc of the generated response &lt;/p&gt; &lt;p&gt;I&amp;#39;ve just started this and there might be some very basic or dumb doubts from my side. I&amp;#39;m looking for some direction I could go on with Gen-ai &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/RepresentativeAge297&quot;&gt; /u/RepresentativeAge297 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e2zh63/beginner_using_langchain_having_some_doubts/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e2zh63/beginner_using_langchain_having_some_doubts/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1e2zh63</id><link href="https://www.reddit.com/r/LangChain/comments/1e2zh63/beginner_using_langchain_having_some_doubts/" /><updated>2024-07-14T10:53:07+00:00</updated><published>2024-07-14T10:53:07+00:00</published><title>Beginner using langchain having some doubts</title></entry><entry><author><name>/u/OrioMax</name><uri>https://www.reddit.com/user/OrioMax</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Has anyone worked on or have any resources regarding using the self query retrieval mechanism with langchain agent and got exact response from agent without any hallucinations or not able get required documents from vectordb like problems.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/OrioMax&quot;&gt; /u/OrioMax &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e32ffw/langchain_agent_with_self_query_retrieval/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e32ffw/langchain_agent_with_self_query_retrieval/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1e32ffw</id><link href="https://www.reddit.com/r/LangChain/comments/1e32ffw/langchain_agent_with_self_query_retrieval/" /><updated>2024-07-14T13:33:49+00:00</updated><published>2024-07-14T13:33:49+00:00</published><title>Langchain Agent with self query retrieval.</title></entry><entry><author><name>/u/hi87</name><uri>https://www.reddit.com/user/hi87</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I created some documents to test out the performance of the vector store and embedding, and when using LangChain batch method to process multiple queries at the same time I get different results even for pretty straightforward queries. I have two documents each contains info about golf and tennis. The query to the retriever is simply retriever.batch([Tennis, Golf]).&lt;/p&gt; &lt;p&gt;The response changes from one call to the next. Sometimes the batch returns content related to only golf or sometimes only tennis. I thought maybe it‚Äôs because the similarity closeness of the sports in vector space so I added a third document with ‚Äúcats,dogs‚Äù etc and randomly the batch method sometimes returns this document as the most relevant. &lt;/p&gt; &lt;p&gt;Extremely confused. Could it be an issue with the embeddings?&lt;/p&gt; &lt;p&gt;UPDATE: I tried another vector db (Chromadb) instead of Weaviate and the issue was resolved. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/hi87&quot;&gt; /u/hi87 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e2hnoq/are_vector_embeddings_similarity_search/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e2hnoq/are_vector_embeddings_similarity_search/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1e2hnoq</id><link href="https://www.reddit.com/r/LangChain/comments/1e2hnoq/are_vector_embeddings_similarity_search/" /><updated>2024-07-13T18:46:58+00:00</updated><published>2024-07-13T18:46:58+00:00</published><title>Are vector embeddings / similarity search non-deterministic?</title></entry><entry><author><name>/u/Tuxedotux83</name><uri>https://www.reddit.com/user/Tuxedotux83</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I wrote a custom search tool class which does not use Google, but I am still unable to get the web research functionality to work since it keeps demanding a google api key?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Tuxedotux83&quot;&gt; /u/Tuxedotux83 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e2w4zx/do_googleapi_a_requirement_for_the_web_research/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e2w4zx/do_googleapi_a_requirement_for_the_web_research/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1e2w4zx</id><link href="https://www.reddit.com/r/LangChain/comments/1e2w4zx/do_googleapi_a_requirement_for_the_web_research/" /><updated>2024-07-14T07:10:47+00:00</updated><published>2024-07-14T07:10:47+00:00</published><title>Do GoogleAPI a requirement for the web research class ?</title></entry><entry><author><name>/u/Vivid-Force8042</name><uri>https://www.reddit.com/user/Vivid-Force8042</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;While implementing RAG in a chatbot pipeline, what are the common strategies for dealing with questions that are not relevant to the context?&lt;br/&gt; When asking questions about the context stored in my local DB the retriever gets the relevant data and the LLM generates the right answer. However, if I ask questions like &amp;quot;Who are you&amp;quot;, or &amp;quot;How are you?&amp;quot; I get hallucinations because the prompt contains under the hood also retrieved context.&lt;/p&gt; &lt;p&gt;I tried specifying in the system prompt that if the question is not relevant to the context, say I don&amp;#39;t know but it didn&amp;#39;t help.&lt;/p&gt; &lt;p&gt;Thank you all for your help&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Vivid-Force8042&quot;&gt; /u/Vivid-Force8042 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e2grkh/nonrelevant_questions_to_the_context_in_chatbot/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e2grkh/nonrelevant_questions_to_the_context_in_chatbot/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1e2grkh</id><link href="https://www.reddit.com/r/LangChain/comments/1e2grkh/nonrelevant_questions_to_the_context_in_chatbot/" /><updated>2024-07-13T18:07:28+00:00</updated><published>2024-07-13T18:07:28+00:00</published><title>Non-relevant questions to the context in chatbot</title></entry><entry><author><name>/u/DhairyaRaj13</name><uri>https://www.reddit.com/user/DhairyaRaj13</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;We are an ai chatbot company, I am to add a phone calling layer over the chatbot , I am looking for techstack for such usecase. I am using python for that. Want everything to be customised, Speech to Text , text to speech.&lt;/p&gt; &lt;p&gt;Currently I am to use Twillio. But I want to have full control over speech . To transcribe &amp;amp;etc i aim to use in house model &lt;/p&gt; &lt;p&gt;Suggest resources.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/DhairyaRaj13&quot;&gt; /u/DhairyaRaj13 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e25aed/ai_phone_calling_agent/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e25aed/ai_phone_calling_agent/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1e25aed</id><link href="https://www.reddit.com/r/LangChain/comments/1e25aed/ai_phone_calling_agent/" /><updated>2024-07-13T08:15:40+00:00</updated><published>2024-07-13T08:15:40+00:00</published><title>AI phone calling agent</title></entry><entry><author><name>/u/bferreira85</name><uri>https://www.reddit.com/user/bferreira85</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi! I would like to get suggestions on how to implement the following using langgraph: What i have today: i have a langgraph workflow containing one &amp;quot;supervisor&amp;quot; agent and I 2 specialised agents. When the user inputs a request the supervisor decides what agent to send the work to until it decides the work is finished. What i want to do now is: -for one of these agents I want to make it a two step operation. So the agent receives the request and then displays to the user information about what he is about to do. The user can then give an ok of ask for some tweak. Then the agent finishes the job. Another possibility here is that the agent decides it doesn&amp;#39;t has all the info and reaches to the user to ask for additional info (in that case he will also reach the user to confirm the operation). I imagine I can achieve this with breakpoints and collecting human input, but since I&amp;#39;ve never did it before i am a bit confusing on how the states and workflow should look like. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/bferreira85&quot;&gt; /u/bferreira85 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e276vp/langgraph_what_implementation_strategy_would_you/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e276vp/langgraph_what_implementation_strategy_would_you/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1e276vp</id><link href="https://www.reddit.com/r/LangChain/comments/1e276vp/langgraph_what_implementation_strategy_would_you/" /><updated>2024-07-13T10:24:21+00:00</updated><published>2024-07-13T10:24:21+00:00</published><title>Langgraph: what implementation strategy would you recommend for this case?</title></entry></feed>