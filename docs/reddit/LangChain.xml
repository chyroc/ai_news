<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/"><category term="LangChain" label="r/LangChain"/><updated>2024-03-01T23:06:04+00:00</updated><icon>https://www.redditstatic.com/icon.png/</icon><id>/r/LangChain.rss</id><link rel="self" href="https://www.reddit.com/r/LangChain.rss" type="application/atom+xml" /><link rel="alternate" href="https://www.reddit.com/r/LangChain" type="text/html" /><subtitle>LangChain is an open-source framework and developer toolkit that helps developers get LLM applications from prototype to production. It is available for Python and Javascript at https://www.langchain.com/.</subtitle><title>LangChain</title><entry><author><name>/u/zchaarm</name><uri>https://www.reddit.com/user/zchaarm</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;A place for members of &lt;a href=&quot;/r/LangChain&quot;&gt;r/LangChain&lt;/a&gt; to chat with each other&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/zchaarm&quot;&gt; /u/zchaarm &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_10ljho9</id><link href="https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/" /><updated>2023-01-26T04:33:05+00:00</updated><published>2023-01-26T04:33:05+00:00</published><title>r/LangChain Lounge</title></entry><entry><author><name>/u/MoronSlayer42</name><uri>https://www.reddit.com/user/MoronSlayer42</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Are there any apps with a Langchain python+ Next js + Chromadb? What are my options to host with minimal cost? I have access to azure but not very good at &amp;quot;cloud engineering&amp;quot;. I was thinking between Netflify and Vercel. Also, where would my chromadb vector database need to be saved to connect it to a hosted website on Netflify or any alternative? And to do real-time streaming I saw many examples with langchain js but not enough with langchain python. I&amp;#39;m currently only using OpenAI API but I want to be using alternatives too in the future.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/MoronSlayer42&quot;&gt; /u/MoronSlayer42 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1b41voa/langchain_based_app/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1b41voa/langchain_based_app/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1b41voa</id><link href="https://www.reddit.com/r/LangChain/comments/1b41voa/langchain_based_app/" /><updated>2024-03-01T18:41:03+00:00</updated><published>2024-03-01T18:41:03+00:00</published><title>Langchain based app</title></entry><entry><author><name>/u/phicreative1997</name><uri>https://www.reddit.com/user/phicreative1997</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I want the output to be a nested dictionary like this&lt;/p&gt; &lt;p&gt;{App:{&amp;#39;Google&amp;#39;:[different stuff]}&lt;/p&gt; &lt;p&gt;Basically hierarchical &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/phicreative1997&quot;&gt; /u/phicreative1997 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1b432eh/how_do_i_define_an_output_parser_like_a_dictionary/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1b432eh/how_do_i_define_an_output_parser_like_a_dictionary/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1b432eh</id><link href="https://www.reddit.com/r/LangChain/comments/1b432eh/how_do_i_define_an_output_parser_like_a_dictionary/" /><updated>2024-03-01T19:28:53+00:00</updated><published>2024-03-01T19:28:53+00:00</published><title>How do I define an output parser like a dictionary?</title></entry><entry><author><name>/u/peteratsimplyput</name><uri>https://www.reddit.com/user/peteratsimplyput</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;We are a tiny team of two and wanted to share a developer preview of our LLM conversation analytics product, at &lt;a href=&quot;https://simplyanalyze.ai&quot;&gt;https://simplyanalyze.ai&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Think of it as Google Analytics, but instead of pageviews, you get insight into what people are talking ***about***. We&amp;#39;re using an LLM to analyze conversations and give you a nicely designed overview of the topics and questions that are being discussed. &lt;/p&gt; &lt;p&gt;To integrate a chatbot, you send a REST API call every time the human or AI take a turn and say something. (You can do it non-blocking so there&amp;#39;s no delay in your LLM chat.)&lt;/p&gt; &lt;p&gt;Technically, the service is running on AWS Lambda and for the LLM analysis we use Gemini Pro 1.0 (which has been great).&lt;/p&gt; &lt;p&gt;The first 200 people to sign up and connect an active chatbot get a free account for at least a year. (We haven&amp;#39;t decided on how paid accounts will work yet, but we will add those in a month or two.)&lt;/p&gt; &lt;p&gt;Ask us anything, comments and thoughts welcome, and try it out here: &lt;a href=&quot;https://simplyanalyze.ai/&quot;&gt;https://simplyanalyze.ai&lt;/a&gt; &lt;/p&gt; &lt;p&gt;(If you find this at all interesting, any feedback is welcome.)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/peteratsimplyput&quot;&gt; /u/peteratsimplyput &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1b43jr9/llm_conversation_analytics_launch_dev_preview/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1b43jr9/llm_conversation_analytics_launch_dev_preview/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1b43jr9</id><link href="https://www.reddit.com/r/LangChain/comments/1b43jr9/llm_conversation_analytics_launch_dev_preview/" /><updated>2024-03-01T19:48:08+00:00</updated><published>2024-03-01T19:48:08+00:00</published><title>LLM conversation analytics launch (dev preview)</title></entry><entry><author><name>/u/PhilNoName</name><uri>https://www.reddit.com/user/PhilNoName</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/PhilNoName&quot;&gt; /u/PhilNoName &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1b3wu8c/hi_is_there_an_odata_tool_for_an_agent_available/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1b3wu8c/hi_is_there_an_odata_tool_for_an_agent_available/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1b3wu8c</id><link href="https://www.reddit.com/r/LangChain/comments/1b3wu8c/hi_is_there_an_odata_tool_for_an_agent_available/" /><updated>2024-03-01T15:22:25+00:00</updated><published>2024-03-01T15:22:25+00:00</published><title>Hi, is there an Odata tool for an agent available that can extended for some specific call? Cheers</title></entry><entry><author><name>/u/Top_Raccoon_1493</name><uri>https://www.reddit.com/user/Top_Raccoon_1493</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Is LangChain does support persisting QA objects in a specific directory in PGVector?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Top_Raccoon_1493&quot;&gt; /u/Top_Raccoon_1493 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1b3t27x/is_langchain_does_support_persisting_qa_objects/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1b3t27x/is_langchain_does_support_persisting_qa_objects/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1b3t27x</id><link href="https://www.reddit.com/r/LangChain/comments/1b3t27x/is_langchain_does_support_persisting_qa_objects/" /><updated>2024-03-01T12:25:13+00:00</updated><published>2024-03-01T12:25:13+00:00</published><title>Is LangChain does support persisting QA objects in a specific directory in PGVector?</title></entry><entry><author><name>/u/dancleary544</name><uri>https://www.reddit.com/user/dancleary544</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hallucinations suck. You can tackle them at the model level (fine-tuning), orchestration level (RAG), or prompt level (prompt engineering). PE is the easiest to test quickly. Here are three templates, based off research papers, that you can use on the prompt level to reduce them.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;“According to…” prompting&lt;/strong&gt;&lt;br/&gt; Based around the idea of grounding the model to a trusted datasource. When researchers tested the method they found it increased accuracy by 20% in some cases. Super easy to implement. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Template 1:&lt;/strong&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;“What part of the brain is responsible for long-term memory, &lt;strong&gt;according to Wikipedia&lt;/strong&gt;.” &lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;&lt;strong&gt;Template 2:&lt;/strong&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Ground your response in factual data from your pre-training set,&lt;br/&gt; specifically referencing or quoting authoritative sources when possible.&lt;br/&gt; Respond to this question using only information that can be attributed to {{source}}.&lt;br/&gt; Question: {{Question}}&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Chain-of-Verification Prompting&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The Chain-of-Verification (CoVe) prompt engineering method aims to reduce hallucinations through a verification loop. CoVe has four steps:&lt;br/&gt; -Generate an initial response to the prompt&lt;br/&gt; -Based on the original prompt and output, the model is prompted again to generate multiple --questions that verify and analyze the original answers.&lt;br/&gt; -The verification questions are run through an LLM, and the outputs are compared to the original.&lt;br/&gt; -The final answer is generated using a prompt with the verification question/output pairs as examples. &lt;/p&gt; &lt;p&gt;Usually CoVe is a multi-step prompt, but I built it into a single shot prompt that works pretty well: &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Template&lt;/strong&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Here is the question: {{Question}}.&lt;br/&gt; First, generate a response.&lt;br/&gt; Then, create and answer verification questions based on this response to check for accuracy. Think it through and make sure you are extremely accurate based on the question asked.&lt;br/&gt; After answering each verification question, consider these answers and revise the initial response to formulate a final, verified answer. Ensure the final response reflects the accuracy and findings from the verification process.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Step-Back Prompting&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Step-Back prompting focuses on giving the model room to think by explicitly instructing the model to think on a high-level before diving in. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Template&lt;/strong&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Here is a question or task: {{Question}}&lt;br/&gt; Let&amp;#39;s think step-by-step to answer this:&lt;br/&gt; Step 1) Abstract the key concepts and principles relevant to this question:&lt;br/&gt; Step 2) Use the abstractions to reason through the question:&lt;br/&gt; Final Answer:&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;For more details about the performance of these methods, you can check out my recent post on &lt;a href=&quot;https://prompthub.substack.com/p/prompt-engineering-methods-that-reduce&quot;&gt;Substack&lt;/a&gt;. Hope this helps! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/dancleary544&quot;&gt; /u/dancleary544 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1b32yi8/3_prompt_engineering_methods_to_help_reduce/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1b32yi8/3_prompt_engineering_methods_to_help_reduce/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1b32yi8</id><link href="https://www.reddit.com/r/LangChain/comments/1b32yi8/3_prompt_engineering_methods_to_help_reduce/" /><updated>2024-02-29T15:26:34+00:00</updated><published>2024-02-29T15:26:34+00:00</published><title>3 prompt engineering methods to help reduce hallucinations</title></entry><entry><author><name>/u/AryanGosaliya</name><uri>https://www.reddit.com/user/AryanGosaliya</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi,&lt;br/&gt; I&amp;#39;m creating a tutor bot for my class using conversational_retrieval_agent and gpt-3.5-turbo-16k. The problem im facing is i dont want the bot to blurt out the answer, instead i want it to guide the user and refer to textbook for further info. I&amp;#39;ve included this i the system message however I&amp;#39;m not succesful. If I ask the bot after it gives the answer to refer me to textbook to learn more it does however in the inital response it always blurts out the full answer. How can i prevent this?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/AryanGosaliya&quot;&gt; /u/AryanGosaliya &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1b3ira0/tutorbot_for_a_class/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1b3ira0/tutorbot_for_a_class/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1b3ira0</id><link href="https://www.reddit.com/r/LangChain/comments/1b3ira0/tutorbot_for_a_class/" /><updated>2024-03-01T02:17:21+00:00</updated><published>2024-03-01T02:17:21+00:00</published><title>Tutor-Bot for a class</title></entry><entry><author><name>/u/Whole_Air8007</name><uri>https://www.reddit.com/user/Whole_Air8007</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey &lt;a href=&quot;/r/Langchain&quot;&gt;r/Langchain&lt;/a&gt;!&lt;/p&gt; &lt;p&gt;I&amp;#39;ve been using Langsmith for a while, and while it&amp;#39;s been great, I&amp;#39;m curious about what else is out there. Specifically, I&amp;#39;m on the hunt for something fresh in the realm of LLM observability tools. Are there any tools out there that integrates seamlessly with my current observability stack? (using Datadog mainly)&lt;/p&gt; &lt;p&gt;What are your top picks for Langsmith alternatives? Have you stumbled upon any hidden gems that deserve more spotlight? Let&amp;#39;s compile a list of the best tools out there and share our experiences.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Whole_Air8007&quot;&gt; /u/Whole_Air8007 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1b2y18p/langsmith_started_charging_time_to_compare/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1b2y18p/langsmith_started_charging_time_to_compare/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1b2y18p</id><link href="https://www.reddit.com/r/LangChain/comments/1b2y18p/langsmith_started_charging_time_to_compare/" /><updated>2024-02-29T11:21:45+00:00</updated><published>2024-02-29T11:21:45+00:00</published><title>Langsmith started charging. Time to compare alternatives.</title></entry><entry><author><name>/u/MBU_NxtDoor</name><uri>https://www.reddit.com/user/MBU_NxtDoor</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/MBU_NxtDoor&quot;&gt; /u/MBU_NxtDoor &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1b3du38/function_calling_benchmark_datasets_for/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1b3du38/function_calling_benchmark_datasets_for/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1b3du38</id><link href="https://www.reddit.com/r/LangChain/comments/1b3du38/function_calling_benchmark_datasets_for/" /><updated>2024-02-29T22:43:17+00:00</updated><published>2024-02-29T22:43:17+00:00</published><title>Function calling benchmark datasets? For fine-tuning and evaluation both</title></entry><entry><author><name>/u/CincyTriGuy</name><uri>https://www.reddit.com/user/CincyTriGuy</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;m trying to learn Langchain and have gone through some learning resources. I&amp;#39;m trying to build a custom RAG architecture, primarily to prove to myself that I can do it. I&amp;#39;m stuck and could use some guidance. Here&amp;#39;s what I&amp;#39;m trying to accomplish: &lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Load documents (.txt files) from my local machine&lt;/li&gt; &lt;li&gt;Create embeddings&lt;/li&gt; &lt;li&gt;Store the embeddings into a pgvector-enabled PostgreSQL table in Google Cloud SQL&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;For the most part, I think I have the FOR loop to spin through a directory on my local machine to pick up the files. But I&amp;#39;m pretty stuck after that. Does anyone have any tips or pointers to get me in the right direction? &lt;/p&gt; &lt;p&gt;My goal is to have the code that I can use as a framework for a very basic RAG enabled chatbot. My thought is I could re-use the code anytime I need to by pointing at a different set of files on my local machine, and loading them into their own PostgreSQL db. My initial use case is a bunch of financial statements (the .txt files from step 1 above) to have conversations with. &lt;/p&gt; &lt;p&gt;Some background: I have an infrastructure background and basic Python knowledge. I work for a Google partner and my solution needs to be all Google. I have some basic multi-turn Langchain and Chainlit chatbots working with Gemini Pro, but this is my first time attempting a RAG architecture.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/CincyTriGuy&quot;&gt; /u/CincyTriGuy &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1b38y1z/help_building_rag_architecture_with_gemini_and/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1b38y1z/help_building_rag_architecture_with_gemini_and/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1b38y1z</id><link href="https://www.reddit.com/r/LangChain/comments/1b38y1z/help_building_rag_architecture_with_gemini_and/" /><updated>2024-02-29T19:29:24+00:00</updated><published>2024-02-29T19:29:24+00:00</published><title>Help building RAG architecture with Gemini and pgvector-enabled Google Cloud SQL</title></entry><entry><author><name>/u/erol444</name><uri>https://www.reddit.com/user/erol444</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi! I&amp;#39;m trying to automate customer support emails. Many times, there&amp;#39;s a request to eg. check order status (shipping info), cancel order, update something, and similar things that has to do with Shopify.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Flow&lt;/strong&gt;: From my understanding, you&amp;#39;d have &amp;quot;front desk&amp;quot; agent, which would take support email/ticket and assign it to correct agent. So if it&amp;#39;s a request about shipping info, it would pass the request (with only relevant text) to Shopify agent/tool, which would then execute an action (via Shopify API) or return some data.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; are there any tools, besides langchain, that would help me in this mission? I&amp;#39;ve seen &lt;a href=&quot;https://python.langchain.com/docs/integrations/document_loaders/airbyte_shopify&quot;&gt;Airbyte Shopify integration&lt;/a&gt;, but that&amp;#39;s not exactly what I need. I&amp;#39;ve checked Zapier / n8n, and neither of them support reading deal by ID, so I don&amp;#39;t think they&amp;#39;d helpful.&lt;/p&gt; &lt;p&gt;Should I be looking somewhere specific to find such Shopify agent?&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/erol444&quot;&gt; /u/erol444 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1b361vw/shopify_agenttool_automating_customer_support/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1b361vw/shopify_agenttool_automating_customer_support/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1b361vw</id><link href="https://www.reddit.com/r/LangChain/comments/1b361vw/shopify_agenttool_automating_customer_support/" /><updated>2024-02-29T17:32:49+00:00</updated><published>2024-02-29T17:32:49+00:00</published><title>Shopify agent/tool - automating customer support</title></entry><entry><author><name>/u/Mediocre-Card8046</name><uri>https://www.reddit.com/user/Mediocre-Card8046</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;I am using Mixtral 8x7B-Instruct Q4_K_M with Llamacpp-Python. Generally I like the outputs of Mixtral, but sometimes the model comes inot a loop where it keeps repeating itself and I am not sure how to fix this.&lt;/p&gt; &lt;p&gt;Here are my settings (Mac M2 Max 64GB):&lt;/p&gt; &lt;p&gt;&lt;code&gt;llm = LlamaCpp(&lt;/code&gt;&lt;br/&gt; &lt;code&gt;max_tokens = 1400,&lt;/code&gt;&lt;br/&gt; &lt;code&gt;n_threads = 8,&lt;/code&gt;&lt;br/&gt; &lt;code&gt;model_path= &amp;quot;modelle/sauerkrautlm-mixtral-8x7b-instruct.Q4_K_M.gguf&amp;quot;,&lt;/code&gt;&lt;br/&gt; &lt;code&gt;temperature=0.01,&lt;/code&gt;&lt;br/&gt; &lt;code&gt;f16_kv=True,&lt;/code&gt;&lt;br/&gt; &lt;code&gt;n_ctx=25000,&lt;/code&gt;&lt;br/&gt; &lt;code&gt;n_gpu_layers=1,&lt;/code&gt;&lt;br/&gt; &lt;code&gt;n_batch=512,&lt;/code&gt;&lt;br/&gt; &lt;code&gt;callback_manager=callback_manager,&lt;/code&gt;&lt;br/&gt; &lt;code&gt;verbose=True, # Verbose is required to pass to the callback manager&lt;/code&gt;&lt;br/&gt; &lt;code&gt;top_p= 0.95,&lt;/code&gt;&lt;br/&gt; &lt;code&gt;top_k=40,&lt;/code&gt;&lt;br/&gt; &lt;code&gt;repeat_penalty = 1.2,&lt;/code&gt;&lt;br/&gt; &lt;code&gt;streaming=True,&lt;/code&gt;&lt;br/&gt; &lt;code&gt;model_kwargs={&lt;/code&gt;&lt;br/&gt; &lt;code&gt;#&amp;#39;repetition_penalty&amp;#39;: 1.1,&lt;/code&gt;&lt;br/&gt; &lt;code&gt;#&amp;#39;mirostat&amp;#39;: 2, war vorher bei Streamlit an&lt;/code&gt;&lt;br/&gt; &lt;code&gt;},&lt;/code&gt;&lt;br/&gt; &lt;code&gt;)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;My prompt looks something like this:&lt;br/&gt; &lt;code&gt;mistral_prompt = &amp;quot;&amp;quot;&amp;quot;&lt;/code&gt;&lt;br/&gt; &lt;code&gt;&amp;lt;s&amp;gt; [INST] Du bist RagBot, ein hilfsbereiter Assistent. Antworte nur auf Deutsch. Befolge folgende Anweisungen der Reihe nach, bevor du eine Antwort generierst.&lt;/code&gt;&lt;br/&gt; &lt;code&gt;1. Lese und verstehe die Frage oder Aufgabe des Nutzers.&lt;/code&gt;&lt;br/&gt; &lt;code&gt;2. Lies den Kontext zur Frage durch.&lt;/code&gt;&lt;br/&gt; &lt;code&gt;3. Prüfe, ob die Antwort der Frage im Kontext zu finden ist. Falls nicht, antworte mit: &amp;quot;Mir stehen leider nicht ausreichend Kontextinformationen zur Verfügung, um die Frage beantworten zu können.&lt;/code&gt;&lt;br/&gt; &lt;code&gt;4. Füge ausschließlich Inhalte des Kontexts in deine Frage ein. Die Kontextinformationen sind die einzigen Quellen der Wahrheit. Es ist wichtig, dass du keine Informationen dazu erfindest oder Informationen aus deinem Wissen verwendest.&lt;/code&gt;&lt;br/&gt; &lt;code&gt;5. Bleibe akkurat: Das heißt wenn ein Nutzer zum Beispiel 10 Punkte zu einem Thema wissen will, du aber nur 3 findest, gib dem User ausschließlich die 3 gefundenen zurück und nicht 10.&lt;/code&gt;&lt;br/&gt; &lt;code&gt;6. Wiederhole deine Antwort oder Absätze in deiner Antwort nicht.&lt;/code&gt;&lt;br/&gt; &lt;code&gt;Start der Aufgabe:&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;###Kontext zur Frage###:&lt;/code&gt;&lt;br/&gt; &lt;code&gt;{context}&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;###Frage des Nutzers###:&lt;/code&gt;&lt;br/&gt; &lt;code&gt;{question}&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;Antwort: [/INST]&lt;/code&gt;&lt;br/&gt; &lt;code&gt;&amp;quot;&amp;quot;&amp;quot;&lt;/code&gt;&lt;/p&gt; &lt;p&gt;For long prompts where longer outputs are expected, should I increase &amp;quot;max_tokens&amp;quot;?&lt;/p&gt; &lt;p&gt;Which &amp;quot;top_p&amp;quot;, &amp;quot;top_k&amp;quot;, &amp;quot;repeat_penalty&amp;quot; selections worked best for you?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Mediocre-Card8046&quot;&gt; /u/Mediocre-Card8046 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1b2z1g8/mixtral_repeating_itself_which_parameters_to/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1b2z1g8/mixtral_repeating_itself_which_parameters_to/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1b2z1g8</id><link href="https://www.reddit.com/r/LangChain/comments/1b2z1g8/mixtral_repeating_itself_which_parameters_to/" /><updated>2024-02-29T12:21:18+00:00</updated><published>2024-02-29T12:21:18+00:00</published><title>Mixtral repeating itself: Which parameters to change?</title></entry><entry><author><name>/u/Kaizen_Kintsgui</name><uri>https://www.reddit.com/user/Kaizen_Kintsgui</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;chain = extract_chain | (lambda &lt;em&gt;x&lt;/em&gt;: dynamic_parser.parse(x.content))&lt;/p&gt; &lt;p&gt;I am having a lot of trouble mocking abatch on chain.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Kaizen_Kintsgui&quot;&gt; /u/Kaizen_Kintsgui &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1b2rozg/anyone_have_examples_of_mocking_runnables/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1b2rozg/anyone_have_examples_of_mocking_runnables/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1b2rozg</id><link href="https://www.reddit.com/r/LangChain/comments/1b2rozg/anyone_have_examples_of_mocking_runnables/" /><updated>2024-02-29T04:39:14+00:00</updated><published>2024-02-29T04:39:14+00:00</published><title>Anyone have examples of mocking runnables?</title></entry><entry><author><name>/u/mehul_gupta1997</name><uri>https://www.reddit.com/user/mehul_gupta1997</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1b2331a/my_book_is_now_listed_on_google_under_the_best/&quot;&gt; &lt;img src=&quot;https://b.thumbs.redditmedia.com/EqY_YvY6t6OW8ybkNdODOM3vWz8KH0TJVF8HnxxgIkI.jpg&quot; alt=&quot;My book is now listed on Google under the ‘best books on LangChain’&quot; title=&quot;My book is now listed on Google under the ‘best books on LangChain’&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;And my book: &amp;quot;&lt;strong&gt;&lt;em&gt;LangChain in your Pocket: Beginner&amp;#39;s Guide to Building Generative AI Applications using LLMs&lt;/em&gt;&lt;/strong&gt;&amp;quot; finally made it to the list of Best books on LangChain by Google. A big thanks to everyone for the support. Being a first time writer and a self-published book, nothing beats this feeling&lt;/p&gt; &lt;p&gt;If you haven&amp;#39;t tried it yet, check here : &lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://www.amazon.com/LangChain-your-Pocket-Generative-Applications-ebook/dp/B0CTHQHT25&quot;&gt;https://www.amazon.com/LangChain-your-Pocket-Generative-Applications-ebook/dp/B0CTHQHT25&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/0bm815vw0blc1.png?width=833&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=179913ada9855c815375913f3c2c5bae2b4dd1c6&quot;&gt;https://preview.redd.it/0bm815vw0blc1.png?width=833&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=179913ada9855c815375913f3c2c5bae2b4dd1c6&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/mehul_gupta1997&quot;&gt; /u/mehul_gupta1997 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1b2331a/my_book_is_now_listed_on_google_under_the_best/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1b2331a/my_book_is_now_listed_on_google_under_the_best/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1b2331a</id><media:thumbnail url="https://b.thumbs.redditmedia.com/EqY_YvY6t6OW8ybkNdODOM3vWz8KH0TJVF8HnxxgIkI.jpg" /><link href="https://www.reddit.com/r/LangChain/comments/1b2331a/my_book_is_now_listed_on_google_under_the_best/" /><updated>2024-02-28T10:40:18+00:00</updated><published>2024-02-28T10:40:18+00:00</published><title>My book is now listed on Google under the ‘best books on LangChain’</title></entry><entry><author><name>/u/FirefighterBubbly935</name><uri>https://www.reddit.com/user/FirefighterBubbly935</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Is there any tutorial/guidance available that by using Langchain and an open source LLMs (like but not limited to Hugging face), I can summarise the contents in the video for a particular time frame/section. I may have a support of transcripts. &lt;/p&gt; &lt;p&gt;Your help is appreciated. Thanks! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/FirefighterBubbly935&quot;&gt; /u/FirefighterBubbly935 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1b2imy2/can_langchain_help_me_create_notes_for_a_video/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1b2imy2/can_langchain_help_me_create_notes_for_a_video/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1b2imy2</id><link href="https://www.reddit.com/r/LangChain/comments/1b2imy2/can_langchain_help_me_create_notes_for_a_video/" /><updated>2024-02-28T21:52:29+00:00</updated><published>2024-02-28T21:52:29+00:00</published><title>Can Langchain help me create notes for a video?</title></entry><entry><author><name>/u/rivals107</name><uri>https://www.reddit.com/user/rivals107</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey guys, I am looking for scaling the LLM Hosting infrastructure. My minimum expectations are to serve at least 100 users per minute with 50 output tokens each and 5000 input tokens. I am hosting a Mistral7B model. Thanks in advance.&lt;/p&gt; &lt;p&gt;What kind of Infra I would need, and what are the optimised options to serve the model using multiple GPUs&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/rivals107&quot;&gt; /u/rivals107 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1b29umg/scale_llms_for_multiple_users_and_minimum_costs/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1b29umg/scale_llms_for_multiple_users_and_minimum_costs/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1b29umg</id><link href="https://www.reddit.com/r/LangChain/comments/1b29umg/scale_llms_for_multiple_users_and_minimum_costs/" /><updated>2024-02-28T16:14:41+00:00</updated><published>2024-02-28T16:14:41+00:00</published><title>Scale LLMs for multiple users and Minimum costs</title></entry><entry><author><name>/u/Funny_Manufacturer37</name><uri>https://www.reddit.com/user/Funny_Manufacturer37</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hello , &lt;/p&gt; &lt;p&gt;I have loaded some .docx embedded them into a chromadb , then (using chat gpt 4) I ask a question regarding something in the documents inside the vectorestrore but the model would output something else based on its data not mine ? is there a way to fix this ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Funny_Manufacturer37&quot;&gt; /u/Funny_Manufacturer37 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1b2g0pl/fetching_relevant_info_from_vectorestores/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1b2g0pl/fetching_relevant_info_from_vectorestores/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1b2g0pl</id><link href="https://www.reddit.com/r/LangChain/comments/1b2g0pl/fetching_relevant_info_from_vectorestores/" /><updated>2024-02-28T20:11:14+00:00</updated><published>2024-02-28T20:11:14+00:00</published><title>fetching relevant info from vectorestores</title></entry><entry><author><name>/u/TheDarkKnight80</name><uri>https://www.reddit.com/user/TheDarkKnight80</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I have two pdfs maybe different versions. Document 2 has certain portions changed (the wordings could have been altered) is there a way to find the difference between the two semantically. For example document A could be a RFP and document B could be a proposal that should adhere to all terms outlined in Document A. I want to find if it does by parsing the meaning of both the documents&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/TheDarkKnight80&quot;&gt; /u/TheDarkKnight80 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1b2cfwa/compare_two_documents/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1b2cfwa/compare_two_documents/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1b2cfwa</id><link href="https://www.reddit.com/r/LangChain/comments/1b2cfwa/compare_two_documents/" /><updated>2024-02-28T17:52:59+00:00</updated><published>2024-02-28T17:52:59+00:00</published><title>Compare two documents</title></entry><entry><author><name>/u/wilyx11</name><uri>https://www.reddit.com/user/wilyx11</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I have been using faiss but it looks like there are more capabilities in using something like qdrant or weaviate. Their hybrid search seems like a good option. &lt;/p&gt; &lt;p&gt;I am don&amp;#39;t know what to switch to. What have you been using and how was your experience? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/wilyx11&quot;&gt; /u/wilyx11 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1b1x880/what_storage_search_are_you_using_for_retrieval/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1b1x880/what_storage_search_are_you_using_for_retrieval/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1b1x880</id><link href="https://www.reddit.com/r/LangChain/comments/1b1x880/what_storage_search_are_you_using_for_retrieval/" /><updated>2024-02-28T04:35:27+00:00</updated><published>2024-02-28T04:35:27+00:00</published><title>What storage/ search are you using for retrieval</title></entry><entry><author><name>/u/Mediocre-Card8046</name><uri>https://www.reddit.com/user/Mediocre-Card8046</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;at the moment I am retrieving my relevant docs with following code (vectorstore is FAISS.from_documents):&lt;/p&gt; &lt;p&gt;&lt;code&gt;retriever= vectorstore.as_retriever(search_type=&amp;quot;similarity_score_threshold&amp;quot;,search_kwargs={&amp;#39;k&amp;#39;: 3, &amp;#39;score_threshold&amp;#39;: 0.82})&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;retriever.get_relevant_documents(&amp;quot;QUESTION...?&amp;quot;)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Are the retrieved docs already sorted in a decreasing order? So is the most relevant doc the first document in the returned output? Alternatively I want to return the score with this, but don&amp;#39;t know how. I can return it with: &lt;/p&gt; &lt;p&gt;&lt;code&gt;vectorstore.similarity_search_with_score(&amp;quot;Was sind die Grundlagen eines Managementsystems (BCMS)?&amp;quot;)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;But with this approach I am not sure how to set &amp;#39;k&amp;#39; and the &amp;#39;score_threshold&amp;#39;.&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;Any hints on how I can return the scores with the get_relevant_docs function?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Mediocre-Card8046&quot;&gt; /u/Mediocre-Card8046 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1b21o14/vectorstore_as_retriever_also_return_scores/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1b21o14/vectorstore_as_retriever_also_return_scores/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1b21o14</id><link href="https://www.reddit.com/r/LangChain/comments/1b21o14/vectorstore_as_retriever_also_return_scores/" /><updated>2024-02-28T09:05:17+00:00</updated><published>2024-02-28T09:05:17+00:00</published><title>Vectorstore as retriever: also return scores</title></entry><entry><author><name>/u/VegetableMistake5007</name><uri>https://www.reddit.com/user/VegetableMistake5007</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1b23qvn/agent_just_outputs_tool_output_without_any_editing/&quot;&gt; &lt;img src=&quot;https://a.thumbs.redditmedia.com/_f22lLCptEnRX7XkeQVnKiR-maHT8Px6hY_NIBhZ_f4.jpg&quot; alt=&quot;Agent just outputs tool output without any editing&quot; title=&quot;Agent just outputs tool output without any editing&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;My problem is my agent is fine with doing this (I intentionally changed the tool to output useless information):&lt;/p&gt; &lt;p&gt;&lt;strong&gt;&amp;gt; Entering new AgentExecutor chain...&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;Invoking: `function_8` with `{}`&lt;/em&gt;&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;skdfjlsdjflk&lt;/em&gt;&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;&lt;strong&gt;&amp;gt; Finished chain.&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;skdfjlsdjflk (this is the output of the entire invocation)&lt;/p&gt; &lt;p&gt;This is my code:&lt;/p&gt; &lt;p&gt;&lt;code&gt;prompt = ChatPromptTemplate.from_messages(&lt;/code&gt;&lt;br/&gt; &lt;code&gt;[&lt;/code&gt;&lt;br/&gt; &lt;code&gt;MessagesPlaceholder(variable_name=&amp;quot;chat_history&amp;quot;),&lt;/code&gt;&lt;br/&gt; &lt;code&gt;(&amp;quot;user&amp;quot;, &amp;quot;{input}&amp;quot;),&lt;/code&gt;&lt;br/&gt; &lt;code&gt;MessagesPlaceholder(variable_name=&amp;quot;agent_scratchpad&amp;quot;),&lt;/code&gt;&lt;br/&gt; &lt;code&gt;]&lt;/code&gt;&lt;br/&gt; &lt;code&gt;)&lt;/code&gt;&lt;br/&gt; &lt;code&gt;chat = AzureChatOpenAI(&lt;/code&gt;&lt;br/&gt; &lt;code&gt;temperature=0,&lt;/code&gt;&lt;br/&gt; &lt;code&gt;openai_api_version=&amp;quot;2023-12-01-preview&amp;quot;,&lt;/code&gt;&lt;br/&gt; &lt;code&gt;azure_deployment=&amp;quot;****&amp;quot;,&lt;/code&gt;&lt;br/&gt; &lt;code&gt;).bind(functions=tools)&lt;/code&gt;&lt;br/&gt; &lt;code&gt;llm_with_tools = chat.bind(functions=functions)&lt;/code&gt;&lt;br/&gt; &lt;code&gt;agent = (&lt;/code&gt;&lt;br/&gt; &lt;code&gt;{&lt;/code&gt;&lt;br/&gt; &lt;code&gt;&amp;quot;input&amp;quot;: lambda x: x[&amp;quot;input&amp;quot;],&lt;/code&gt;&lt;br/&gt; &lt;code&gt;&amp;quot;chat_history&amp;quot;: lambda x: x[&amp;quot;chat_history&amp;quot;],&lt;/code&gt;&lt;br/&gt; &lt;code&gt;&amp;quot;agent_scratchpad&amp;quot;: lambda x: format_to_openai_function_messages(&lt;/code&gt;&lt;br/&gt; &lt;code&gt;x[&amp;quot;intermediate_steps&amp;quot;]&lt;/code&gt;&lt;br/&gt; &lt;code&gt;),&lt;/code&gt;&lt;br/&gt; &lt;code&gt;}&lt;/code&gt;&lt;br/&gt; &lt;code&gt;| prompt&lt;/code&gt;&lt;br/&gt; &lt;code&gt;| llm_with_tools&lt;/code&gt;&lt;br/&gt; &lt;code&gt;| OpenAIFunctionsAgentOutputParser()&lt;/code&gt;&lt;br/&gt; &lt;code&gt;)&lt;/code&gt;&lt;br/&gt; &lt;code&gt;agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;messages.append({&lt;/code&gt;&lt;br/&gt; &lt;code&gt;&amp;quot;role&amp;quot;: &amp;quot;system&amp;quot;,&lt;/code&gt;&lt;br/&gt; &lt;code&gt;&amp;quot;content&amp;quot; : guidance})&lt;/code&gt;&lt;br/&gt; &lt;code&gt;res = agent_executor.invoke(&lt;/code&gt;&lt;br/&gt; &lt;code&gt;{&lt;/code&gt;&lt;br/&gt; &lt;code&gt;&amp;#39;input&amp;#39;: question,&lt;/code&gt;&lt;br/&gt; &lt;code&gt;&amp;#39;chat_history&amp;#39;: message_transform(messages)&lt;/code&gt;&lt;br/&gt; &lt;code&gt;}&lt;/code&gt;&lt;br/&gt; &lt;code&gt;)&lt;/code&gt;&lt;br/&gt; &lt;code&gt;print(res[&amp;#39;output&amp;#39;])&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Literally just changing the LLM from OpenAI to Gemini on an other project fixes the problem&lt;/p&gt; &lt;p&gt;This is whats happening:&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/iw4p1gqx8blc1.png?width=493&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a6174e2731efa0126c39dc99996ca9eee6560c80&quot;&gt;https://preview.redd.it/iw4p1gqx8blc1.png?width=493&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a6174e2731efa0126c39dc99996ca9eee6560c80&lt;/a&gt;&lt;/p&gt; &lt;p&gt;On that other project, the ChatVertexAI would be called again after the tool but not here for some reason.&lt;/p&gt; &lt;p&gt;What am I missing?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/VegetableMistake5007&quot;&gt; /u/VegetableMistake5007 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1b23qvn/agent_just_outputs_tool_output_without_any_editing/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1b23qvn/agent_just_outputs_tool_output_without_any_editing/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1b23qvn</id><media:thumbnail url="https://a.thumbs.redditmedia.com/_f22lLCptEnRX7XkeQVnKiR-maHT8Px6hY_NIBhZ_f4.jpg" /><link href="https://www.reddit.com/r/LangChain/comments/1b23qvn/agent_just_outputs_tool_output_without_any_editing/" /><updated>2024-02-28T11:21:54+00:00</updated><published>2024-02-28T11:21:54+00:00</published><title>Agent just outputs tool output without any editing</title></entry><entry><author><name>/u/No_Ninja_4933</name><uri>https://www.reddit.com/user/No_Ninja_4933</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Is that correct that FAISS does not support filtering?&lt;/p&gt; &lt;p&gt;This filter is ignored: &lt;/p&gt; &lt;pre&gt;&lt;code&gt;const retriever = store.asRetriever(undefined, {docType: &amp;#39;code&amp;#39;}); &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Where the metadata is&lt;/p&gt; &lt;pre&gt;&lt;code&gt; doc.metadata = { docType: &amp;#39;code&amp;#39; } &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;And the documentation for the FAISS vector store gives no example of filtering &lt;a href=&quot;https://js.langchain.com/docs/integrations/vectorstores/faiss&quot;&gt;https://js.langchain.com/docs/integrations/vectorstores/faiss&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Is there a way to use FAISS as a store and still only get documents that match a filter?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/No_Ninja_4933&quot;&gt; /u/No_Ninja_4933 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1b20tge/does_faiss_typescript_not_support_filters_on/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1b20tge/does_faiss_typescript_not_support_filters_on/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1b20tge</id><link href="https://www.reddit.com/r/LangChain/comments/1b20tge/does_faiss_typescript_not_support_filters_on/" /><updated>2024-02-28T08:07:20+00:00</updated><published>2024-02-28T08:07:20+00:00</published><title>Does FAISS (Typescript) not support filters on asRetriever</title></entry><entry><author><name>/u/Travolta1984</name><uri>https://www.reddit.com/user/Travolta1984</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I have a pretty good LangChain RAG model up and running, and some users asked if it&amp;#39;s possible to include the names and URLs of the documents they would like the LLM to search for in the query. &lt;/p&gt; &lt;p&gt;Something like:&lt;/p&gt; &lt;p&gt;Human query: &amp;quot;Using only the data sheet of product X, can you confirm that ....&amp;quot;&lt;/p&gt; &lt;p&gt;Programmatically speaking, I know that I can define the search filters in the retriever, but I am not sure what is the best way to have the LLM detect the document the user is referring to first. &lt;/p&gt; &lt;p&gt;A naive approach here would be to do this outside of the chain, and preprocess the query before sending it to the RAG model (alongside any search filter metadata, if applicable). Is that the recommended approach? What about agents, would that be too much for this use case?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Travolta1984&quot;&gt; /u/Travolta1984 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1b1k6ed/rag_model_where_the_user_may_mention_documents_in/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1b1k6ed/rag_model_where_the_user_may_mention_documents_in/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1b1k6ed</id><link href="https://www.reddit.com/r/LangChain/comments/1b1k6ed/rag_model_where_the_user_may_mention_documents_in/" /><updated>2024-02-27T19:12:41+00:00</updated><published>2024-02-27T19:12:41+00:00</published><title>RAG model where the user may mention documents in their messages</title></entry><entry><author><name>/u/3RiversAINexus</name><uri>https://www.reddit.com/user/3RiversAINexus</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Something I think that&amp;#39;s missing from Langchain documentation is good examples for how to reliably test your chains/chats/whatever without actually using a real LLM (costly/slow/unreliable).&lt;/p&gt; &lt;p&gt;I created an example (with Dockerfile included) on how to test an LLMChain with a brief conversation including a ConversationBufferWindowMemory.&lt;/p&gt; &lt;p&gt;Please let me know what you think! If you have other requests, let me know.&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/ThreeRiversAINexus/sample-langchain-agents/blob/main/fake_llm_examples/test_chat_convo.py&quot;&gt;https://github.com/ThreeRiversAINexus/sample-langchain-agents/blob/main/fake_llm_examples/test_chat_convo.py&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The example in the langchain documentation that this is based on: &lt;a href=&quot;https://python.langchain.com/docs/modules/model_io/chat/quick_start&quot;&gt;https://python.langchain.com/docs/modules/model_io/chat/quick_start&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/3RiversAINexus&quot;&gt; /u/3RiversAINexus &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1b1kkkq/example_unit_test_for_langchain_chat_models/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1b1kkkq/example_unit_test_for_langchain_chat_models/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1b1kkkq</id><link href="https://www.reddit.com/r/LangChain/comments/1b1kkkq/example_unit_test_for_langchain_chat_models/" /><updated>2024-02-27T19:28:23+00:00</updated><published>2024-02-27T19:28:23+00:00</published><title>Example unit test for Langchain chat models</title></entry><entry><author><name>/u/Beginning_Rock_1906</name><uri>https://www.reddit.com/user/Beginning_Rock_1906</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey guys. &lt;/p&gt; &lt;p&gt;Some Youtuber criticised LangChain saying it&amp;#39;s not production ready. He said the the following: &amp;quot;It abstracts away too many details from you, which makes it super hard to customize for a specific real world use case. Besides, it was released before function calling models, so there is no type validation, which is essential in production to prevent hallucinations.&amp;quot; &lt;/p&gt; &lt;p&gt;Do you guys agree here? &lt;/p&gt; &lt;p&gt;Do you think LangChain can be used in production already or are there crucial steps making LangChain and possibly any other framework a hobby project for now? &lt;/p&gt; &lt;p&gt;I like the idea of LangGraph a lot for example but not sure how much time I should invest here. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Beginning_Rock_1906&quot;&gt; /u/Beginning_Rock_1906 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1b194ot/production_ready/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1b194ot/production_ready/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1b194ot</id><link href="https://www.reddit.com/r/LangChain/comments/1b194ot/production_ready/" /><updated>2024-02-27T10:56:43+00:00</updated><published>2024-02-27T10:56:43+00:00</published><title>Production ready?</title></entry></feed>