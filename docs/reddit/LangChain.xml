<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/"><category term="LangChain" label="r/LangChain"/><updated>2024-01-16T20:52:58+00:00</updated><icon>https://www.redditstatic.com/icon.png/</icon><id>/r/LangChain.rss</id><link rel="self" href="https://www.reddit.com/r/LangChain.rss" type="application/atom+xml" /><link rel="alternate" href="https://www.reddit.com/r/LangChain" type="text/html" /><subtitle>LangChain is an open-source framework and developer toolkit that helps developers get LLM applications from prototype to production. It is available for Python and Javascript at https://www.langchain.com/.</subtitle><title>LangChain</title><entry><author><name>/u/zchaarm</name><uri>https://www.reddit.com/user/zchaarm</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;A place for members of &lt;a href=&quot;/r/LangChain&quot;&gt;r/LangChain&lt;/a&gt; to chat with each other&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/zchaarm&quot;&gt; /u/zchaarm &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_10ljho9</id><link href="https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/" /><updated>2023-01-26T04:33:05+00:00</updated><published>2023-01-26T04:33:05+00:00</published><title>r/LangChain Lounge</title></entry><entry><author><name>/u/theodormarcu</name><uri>https://www.reddit.com/user/theodormarcu</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi there! We were early users of LangChain (in March 2023), but we ended up moving away from it because we felt it was too early to support more complex use cases. We&amp;#39;re looking at it again and it looks like it&amp;#39;s come a long way! &lt;/p&gt; &lt;p&gt;What are the pros/cons of using LangChain in January 2024 vs going vanilla? What does LangChain help you the most with vs going vanilla? &lt;/p&gt; &lt;p&gt;Our use cases are:&lt;br/&gt; - Using multiple models using hosted and on-prem LLMs (both OSS and OpenAI/Anthropic/etc.)&lt;br/&gt; - Support for complex RAG.&lt;br/&gt; - Support chat and non-chat use cases.&lt;br/&gt; - Support for both private and non-private endpoints.&lt;br/&gt; - Outputting both structured and unstructured data. &lt;/p&gt; &lt;p&gt;We&amp;#39;re a quite experienced dev team, and it feels like we could get away without using LangChain. That being said, we hear a lot about it, so we&amp;#39;re curious if we&amp;#39;re missing out! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/theodormarcu&quot;&gt; /u/theodormarcu &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/198csjd/why_should_i_use_langchain_for_my_new_app/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/198csjd/why_should_i_use_langchain_for_my_new_app/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_198csjd</id><link href="https://www.reddit.com/r/LangChain/comments/198csjd/why_should_i_use_langchain_for_my_new_app/" /><updated>2024-01-16T20:20:59+00:00</updated><published>2024-01-16T20:20:59+00:00</published><title>Why should I use LangChain for my new app?</title></entry><entry><author><name>/u/effervesense</name><uri>https://www.reddit.com/user/effervesense</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi, I&amp;#39;m wondering how to create a feedback loop / response mechanism to collect responses from users to improve the model. Any insights on how to go about this would be greatly appreciated. &lt;/p&gt; &lt;p&gt;For instance, I want to ask &amp;quot;how helpful did you find this response?&amp;quot; (rating on a scale of 1 to 5). Or a simple thumbs up/down. Either after every message or every few.&lt;/p&gt; &lt;p&gt;From what I understand I will need to design a script to automatically prompt the user for feedback after the chatbot sends a response. Then the feedback will be collected to the database. I am not sure if mongoDB is the best to store both feedback and chat logs together. Is there a simple way to do all this within langchain?&lt;/p&gt; &lt;p&gt;If anyone has done something similar or has any ideas or resources please let me know. üôè&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/effervesense&quot;&gt; /u/effervesense &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/198alqy/building_feedback_loop/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/198alqy/building_feedback_loop/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_198alqy</id><link href="https://www.reddit.com/r/LangChain/comments/198alqy/building_feedback_loop/" /><updated>2024-01-16T18:52:51+00:00</updated><published>2024-01-16T18:52:51+00:00</published><title>Building feedback loop</title></entry><entry><author><name>/u/K0N1GST1G3R</name><uri>https://www.reddit.com/user/K0N1GST1G3R</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey there.&lt;/p&gt; &lt;p&gt;(Tell me if this is not the right place to ask such questions)&lt;/p&gt; &lt;p&gt;I tried out langchain for a little project, nothing too big. My goal was to be able to use langchain to ask LLMs to generate stuff for my project, and maybe implement some stuff like answers based on local documents.&lt;/p&gt; &lt;p&gt;But I&amp;#39;ve had a very hard time to find a free llm, and when I found how I can make this stuff work with some hugging face models (that I didn&amp;#39;t run locally) I was so disappointed how bad their answer were.&lt;/p&gt; &lt;p&gt;That&amp;#39;s why I am asking for suggestions and answer:&lt;/p&gt; &lt;p&gt;- should I run locally the llms ? which ones ?&lt;/p&gt; &lt;p&gt;- how can I manage all the settings of the llm. I don&amp;#39;t get what token and temperature are, and I wonder if they are the reason why the llm doesn&amp;#39;t respond as I would want it to.&lt;/p&gt; &lt;p&gt;- is hugging face a good choice ?&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;Thanks a lot by advance, don&amp;#39;t hesitate to tell me if I was not specific enough.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/K0N1GST1G3R&quot;&gt; /u/K0N1GST1G3R &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/198dky0/want_to_use_langchain_with_a_free_llm_model_and/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/198dky0/want_to_use_langchain_with_a_free_llm_model_and/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_198dky0</id><link href="https://www.reddit.com/r/LangChain/comments/198dky0/want_to_use_langchain_with_a_free_llm_model_and/" /><updated>2024-01-16T20:52:32+00:00</updated><published>2024-01-16T20:52:32+00:00</published><title>Want to use langchain with a free llm model ... and strugling</title></entry><entry><author><name>/u/2BucChuck</name><uri>https://www.reddit.com/user/2BucChuck</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I have been able to connect Pinecone directly through API without issue when not using Langchain vectorstore as retriever (and even using Langchain when a namespace is not implemented). However this is causing me a lot of angst now to the point I may be about ready to give LangChain the boot from codebase all together‚Ä¶ why won‚Äôt why won‚Äôt this connect with namespace involved (while working fine without namespaces)‚Ä¶ just says ‚Äòfailed to connect‚Äô and accuses me of using the wrong index name which is confirmed correct many times over and with straight API code testing : &lt;/p&gt; &lt;p&gt;index=pinecone.Index(‚Äúagent‚Äù) vectorstore= Pinecone(index, embeddings.embed_query, ‚Äútext‚Äù) retriever = vectorstore.as_retriever(search_kwargs={‚Äòk‚Äô:3, ‚Äònamespace‚Äô: 1000})&lt;/p&gt; &lt;p&gt;Docs = retriever .get_relevant_documents(msg)&lt;/p&gt; &lt;p&gt;For all the time spent tinkering on this I am thinking to just remove Langchain layers and deal with these functions directly ,but hoping this is something dumb on my end. I have this code working elsewhere without namespace argument no problem so seems specific to that argument and Langchain call to Pinecone index. &lt;/p&gt; &lt;p&gt;Thanks for any pointers&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/2BucChuck&quot;&gt; /u/2BucChuck &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/198cs3e/vector_store_connect_to_pinecone_namespace_fails/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/198cs3e/vector_store_connect_to_pinecone_namespace_fails/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_198cs3e</id><link href="https://www.reddit.com/r/LangChain/comments/198cs3e/vector_store_connect_to_pinecone_namespace_fails/" /><updated>2024-01-16T20:20:32+00:00</updated><published>2024-01-16T20:20:32+00:00</published><title>Vector store connect to pinecone namespace fails</title></entry><entry><author><name>/u/Difficult-Card767</name><uri>https://www.reddit.com/user/Difficult-Card767</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;As a newcomer to LangChain, I&amp;#39;m finding the learning curve to be quite steep. I&amp;#39;ve been sifting through the documentation and various forums for guidance, but it feels like I&amp;#39;m piecing together a puzzle without all the pieces.&lt;/p&gt; &lt;p&gt;I&amp;#39;m curious to know if there are others out there who feel the same way. Is there a better way to approach learning LangChain that I&amp;#39;m missing? Any resources or tips would be greatly appreciated.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Difficult-Card767&quot;&gt; /u/Difficult-Card767 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/197u4wp/struggling_with_langchains_learning_curve/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/197u4wp/struggling_with_langchains_learning_curve/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_197u4wp</id><link href="https://www.reddit.com/r/LangChain/comments/197u4wp/struggling_with_langchains_learning_curve/" /><updated>2024-01-16T04:38:01+00:00</updated><published>2024-01-16T04:38:01+00:00</published><title>Struggling with LangChain's learning curve</title></entry><entry><author><name>/u/zainulabd786</name><uri>https://www.reddit.com/user/zainulabd786</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;&lt;strong&gt;0&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I am using langchain.js &lt;a href=&quot;https://js.langchain.com/docs/integrations/vectorstores/qdrant&quot;&gt;QdrantVectorStore&lt;/a&gt; to perform CRUD operations in Qdrant collection. I see that there are many options available in js QdrantVectorStore&lt;br/&gt; to add data to Qdrant but I don&amp;#39;t see any method to update points in a cluster using langchain.js. I know I can use &lt;a href=&quot;https://github.com/qdrant/qdrant-js&quot;&gt;Qdrant js sdk&lt;/a&gt; to perform Updates to points but I prefer doing it using QdrantVectorStore&lt;br/&gt; for the following reasons:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;it simply takes the text and Embedding model and do the rest. we don&amp;#39;t have to manually convert text to embeddings or vectors.&lt;/li&gt; &lt;li&gt;Qdrant has different APIs to update &lt;a href=&quot;https://qdrant.tech/documentation/concepts/points/&quot;&gt;vectors&lt;/a&gt; and &lt;a href=&quot;https://qdrant.tech/documentation/concepts/payload/#update-payload&quot;&gt;payload&lt;/a&gt;, Since QdrantVectorStore&lt;br/&gt; stores the actual text in payload.content&lt;br/&gt; along with the vectors so using qdrant sdk I will have to perfrom the vector and payload updates seperately with two API calls.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Although using Qdrant vector stores allows writing a clean code.&lt;/p&gt; &lt;p&gt;This &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/17bc1ij/update_collection_in_qdrantdont_want_to_create/&quot;&gt;reddit post&lt;/a&gt; suggests using add_documents&lt;br/&gt; even for update but that suggestion is for langchain.py not for js. But I tried something similar in langchain.js, I tried to use QdrantVectorStore.addDocuments()&lt;br/&gt; to update data but I couldn&amp;#39;t find a way to pass pointId[]&lt;br/&gt; to specify which point to update. I also checked the types and interfaces for QdrantVectorStore.addDocuments()&lt;br/&gt; so I found this&lt;/p&gt; &lt;pre&gt;&lt;code&gt;type QdrantAddDocumentOptions = { customPayload: Record&amp;lt;string, any&amp;gt;[]; }; addDocuments(documents: Document[], documentOptions?: QdrantAddDocumentOptions): Promise&amp;lt;void&amp;gt;; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;As a last resort I also tried passing an array of pointIds in customPayload&lt;br/&gt; but that didn&amp;#39;t work. it was simply creating a point in cluster with the ids I passed in customPayload.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/zainulabd786&quot;&gt; /u/zainulabd786 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1983iyf/how_to_update_data_in_qdrant_using_langchainjs/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1983iyf/how_to_update_data_in_qdrant_using_langchainjs/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1983iyf</id><link href="https://www.reddit.com/r/LangChain/comments/1983iyf/how_to_update_data_in_qdrant_using_langchainjs/" /><updated>2024-01-16T14:00:34+00:00</updated><published>2024-01-16T14:00:34+00:00</published><title>How to update data in Qdrant using langchain.js?</title></entry><entry><author><name>/u/CodingButStillAlive</name><uri>https://www.reddit.com/user/CodingButStillAlive</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I am using a combination of a BM25 keyword and Chroma vectorstore based retrievers.&lt;/p&gt; &lt;p&gt;get_relevant_documents() works pretty well most of the times, but sometimes the results are off.&lt;/p&gt; &lt;p&gt;For example, testing with only two documents results in wrong results. While larger document sets work pretty well.&lt;/p&gt; &lt;p&gt;Anyone observed something similar?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/CodingButStillAlive&quot;&gt; /u/CodingButStillAlive &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1981tcr/retriever_results_are_sometimes_off/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1981tcr/retriever_results_are_sometimes_off/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1981tcr</id><link href="https://www.reddit.com/r/LangChain/comments/1981tcr/retriever_results_are_sometimes_off/" /><updated>2024-01-16T12:30:02+00:00</updated><published>2024-01-16T12:30:02+00:00</published><title>Retriever results are sometimes off</title></entry><entry><author><name>/u/Rorororerere</name><uri>https://www.reddit.com/user/Rorororerere</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi&lt;/p&gt; &lt;p&gt;I want to start using Agent/stream &lt;a href=&quot;https://python.langchain.com/docs/modules/agents/how_to/streaming&quot;&gt;here&lt;/a&gt;, more specifically this part:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;path_status = {} async for chunk in agent_executor.astream_log( {&amp;quot;input&amp;quot;: &amp;quot;what is the weather in sf&amp;quot;, &amp;quot;chat_history&amp;quot;: []}, include_names=[&amp;quot;ChatOpenAI&amp;quot;], ): for op in chunk.ops: if op[&amp;quot;op&amp;quot;] == &amp;quot;add&amp;quot;: if op[&amp;quot;path&amp;quot;] not in path_status: path_status[op[&amp;quot;path&amp;quot;]] = op[&amp;quot;value&amp;quot;] else: path_status[op[&amp;quot;path&amp;quot;]] += op[&amp;quot;value&amp;quot;] print(op[&amp;quot;path&amp;quot;]) print(path_status.get(op[&amp;quot;path&amp;quot;])) print(&amp;quot;----&amp;quot;) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;I&amp;#39;d like to know how I can continue the conversation*(continue from the same point &amp;quot;last token&amp;quot; received)*, in case I lose connection with OpenAI or it&amp;#39;s down...&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Rorororerere&quot;&gt; /u/Rorororerere &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1984eyd/stream_continue_same_conversation_in_case_openai/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1984eyd/stream_continue_same_conversation_in_case_openai/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1984eyd</id><link href="https://www.reddit.com/r/LangChain/comments/1984eyd/stream_continue_same_conversation_in_case_openai/" /><updated>2024-01-16T14:41:23+00:00</updated><published>2024-01-16T14:41:23+00:00</published><title>Stream continue same conversation in case OpenAI errors</title></entry><entry><author><name>/u/BadriMLJ</name><uri>https://www.reddit.com/user/BadriMLJ</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;How should I load the one pdf at a time and get the embeddings of it in loop. If I used the PyPDFLoader it will load all the pdfs together in the form of Document. I want to load the pdfs one by one the store it in VectoreStore like FAISS, Chroma&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/BadriMLJ&quot;&gt; /u/BadriMLJ &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/197zyqw/iterate_pdfs_in_loop_to_store_the_embeddings/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/197zyqw/iterate_pdfs_in_loop_to_store_the_embeddings/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_197zyqw</id><link href="https://www.reddit.com/r/LangChain/comments/197zyqw/iterate_pdfs_in_loop_to_store_the_embeddings/" /><updated>2024-01-16T10:38:56+00:00</updated><published>2024-01-16T10:38:56+00:00</published><title>Iterate PDFs in loop to store the embeddings separately in vectorestore</title></entry><entry><author><name>/u/Trick-Asparagus-9260</name><uri>https://www.reddit.com/user/Trick-Asparagus-9260</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;m trying to get the similarity scores using below func. My vector index is FAISS. I get an error: &lt;/p&gt; &lt;p&gt;retriever = local_db.as_retriever(&lt;/p&gt; &lt;p&gt;search_type=&amp;quot;similarity_search_with_relevant_score&amp;quot;&lt;/p&gt; &lt;p&gt;)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;ValidationError&lt;/strong&gt;: 1 validation error for VectorStoreRetriever __root__ search_type of similarity_search_with_score not allowed. Valid values are: (&amp;#39;similarity&amp;#39;, &amp;#39;similarity_score_threshold&amp;#39;, &amp;#39;mmr&amp;#39;) (type=value_error)[26]: &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Trick-Asparagus-9260&quot;&gt; /u/Trick-Asparagus-9260 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/197wxp9/doesnt_langchain_retriever_support_similarity/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/197wxp9/doesnt_langchain_retriever_support_similarity/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_197wxp9</id><link href="https://www.reddit.com/r/LangChain/comments/197wxp9/doesnt_langchain_retriever_support_similarity/" /><updated>2024-01-16T07:14:09+00:00</updated><published>2024-01-16T07:14:09+00:00</published><title>Doesn't langchain retriever support 'similarity_search_with_relevant_scores' using FAISS?</title></entry><entry><author><name>/u/ronittsainii</name><uri>https://www.reddit.com/user/ronittsainii</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey Guys, if you are new to LangChain or want to learn how it works, you can read this blog: &lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://www.deligence.com/what_is_langchain_ai_app_development_framework_explained/&quot;&gt;What is LangChain? AI App Development Framework Explained&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/ronittsainii&quot;&gt; /u/ronittsainii &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1982asu/blog_what_is_langchain_explanation_for_beginners/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1982asu/blog_what_is_langchain_explanation_for_beginners/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1982asu</id><link href="https://www.reddit.com/r/LangChain/comments/1982asu/blog_what_is_langchain_explanation_for_beginners/" /><updated>2024-01-16T12:57:08+00:00</updated><published>2024-01-16T12:57:08+00:00</published><title>Blog: What is LangChain | Explanation For Beginners</title></entry><entry><author><name>/u/SensitiveFel</name><uri>https://www.reddit.com/user/SensitiveFel</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;When I write a search following the langchain documentation[&lt;a href=&quot;https://python.langchain.com/docs/integrations/retrievers/ragatouille%5D:&quot;&gt;https://python.langchain.com/docs/integrations/retrievers/ragatouille]:&lt;/a&gt; ```python def get_collection(): with open(&amp;quot;log.txt&amp;quot;, &amp;quot;r&amp;quot;) as f: doc = f.read() return [doc]&lt;/p&gt; &lt;p&gt;if &lt;strong&gt;name&lt;/strong&gt; == &amp;#39;&lt;strong&gt;main&lt;/strong&gt;&amp;#39;: RAG = RAGPretrainedModel.from_pretrained(&amp;quot;colbert-ir/colbertv2.0&amp;quot;)&lt;/p&gt; &lt;pre&gt;&lt;code&gt;collection = get_collection() RAG.index( collection=collection, index_name=&amp;quot;Miyazaki-123&amp;quot; ) retriever = RAG.as_langchain_retriever(k=3) res = retriever.invoke(&amp;quot;who is tom&amp;quot;) print(res) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;``` I&amp;#39;ve noticed that there isn&amp;#39;t any output, I&amp;#39;m not sure if it&amp;#39;s related to my computer environment, can anyone give me a solution?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/SensitiveFel&quot;&gt; /u/SensitiveFel &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/197yjc4/when_i_use_ragatouille_in_langchaini_found_that/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/197yjc4/when_i_use_ragatouille_in_langchaini_found_that/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_197yjc4</id><link href="https://www.reddit.com/r/LangChain/comments/197yjc4/when_i_use_ragatouille_in_langchaini_found_that/" /><updated>2024-01-16T09:01:51+00:00</updated><published>2024-01-16T09:01:51+00:00</published><title>When I use RAGatouille in langchain,I found that it doesn't work</title></entry><entry><author><name>/u/Mediocre-Card8046</name><uri>https://www.reddit.com/user/Mediocre-Card8046</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi, &lt;/p&gt; &lt;p&gt;in my RAG app I am loading PDF-files with PyPDFLoader and I am chunking the PDFs with the RecursiveCharacterTextSplitter.&lt;/p&gt; &lt;p&gt;However I am facing the problem, that often a important topic starts at the end of a page and continues in the next page. With PyPDFLoader, every page is a List Entry, so in the case described above, the context is lost in the next page. &lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;&lt;code&gt;[Document(page_content=&amp;#39; IT-Betriebs Konsolidierung...&amp;#39;, metadata={&amp;#39;source&amp;#39;: &amp;#39;&lt;/code&gt;&lt;a href=&quot;https://file+.vscode-resource.vscode-cdn.net/BKB_Abstimmungsmanagement.pdf&quot;&gt;&lt;code&gt;management.pdf&lt;/code&gt;&lt;/a&gt;&lt;code&gt;&amp;#39;, &amp;#39;page&amp;#39;: 0}), Document(page_content=&amp;#39;Abstimmungsmanagement \n \n2 \n \nInformationen zum Dokument ... &amp;#39;, metadata={&amp;#39;source&amp;#39;: &amp;#39;&lt;/code&gt;&lt;a href=&quot;https://file+.vscode-resource.vscode-cdn.net/BKB_Abstimmungsmanagement.pdf&quot;&gt;&lt;code&gt;management.pdf&lt;/code&gt;&lt;/a&gt;&lt;code&gt;&amp;#39;, &amp;#39;page&amp;#39;: 1}), Document(page_content=&amp;#39;Abstimmungsmanagement \n \n3 \n \n0.8 18.05.2020 Aktualisierung nach Umstellung des \nVorgehens basierend auf... &amp;#39;, metadata={&amp;#39;source&amp;#39;:&lt;/code&gt; &lt;a href=&quot;https://file+.vscode-resource.vscode-cdn.net/BKB_Abstimmungsmanagement.pdf&quot;&gt;&lt;code&gt;management.pdf&lt;/code&gt;&lt;/a&gt;&lt;code&gt;&amp;#39;, &amp;#39;page&amp;#39;: 2})&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;What are alternatives for this?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Mediocre-Card8046&quot;&gt; /u/Mediocre-Card8046 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/197xqvz/pdf_text_splitter_without_losing_context_over/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/197xqvz/pdf_text_splitter_without_losing_context_over/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_197xqvz</id><link href="https://www.reddit.com/r/LangChain/comments/197xqvz/pdf_text_splitter_without_losing_context_over/" /><updated>2024-01-16T08:06:57+00:00</updated><published>2024-01-16T08:06:57+00:00</published><title>Pdf Text Splitter without losing context over pages</title></entry><entry><author><name>/u/Own_Neighborhood7120</name><uri>https://www.reddit.com/user/Own_Neighborhood7120</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi all! I am wondering whether you guys had any success with the integration of LangChain with Gemini Pro? My tests have shown terrible results thus far with ``ChatGoogleGenerativeAI`` used within slightly more complex chains, such as ``ConversationalRetrievalChain``. Two problems that frequently pop:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;The model seems to default quite often to &amp;quot;I don&amp;#39;t know&amp;quot; or &amp;quot;I cannot find the information in the context provided&amp;quot;, despite the fact that the context contains the requested information upon inspection (so it is not an error with the vector DB).&lt;/li&gt; &lt;li&gt;Perhaps the most annoying: Some of the LangChain-built system prompts (converted to human messages) which ask the model to translate something to the &amp;quot;original language&amp;quot; completely confuse it, making it output a message in a random language. I have tried using the prompt created by LangChain with the Google API directly, and surprise, surprise, the responses are equally bad! I&amp;#39;m thinking of reporting this issue to Google directly. Example:&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&amp;#8203;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;import google.generativeai as genai genai.configure(api_key=os.environ[&amp;#39;GOOGLE_API_KEY&amp;#39;]) model = genai.GenerativeModel(&amp;#39;gemini-pro&amp;#39;) prompt = &amp;quot;Human: Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n\nChat History:\n\nHuman: what does the third amendment say?\nAssistant: I do not have access to the context necessary to answer that question.\nFollow Up Input: what does the second amendment say?\nStandalone question:&amp;quot; resp = model.generate_content(prompt) Markdown(resp.text) # gives answer in a random language &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;If you had similar experiences or found good workarounds for using the integration, please let us know. Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Own_Neighborhood7120&quot;&gt; /u/Own_Neighborhood7120 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/198127w/langchain_gemini_pro/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/198127w/langchain_gemini_pro/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_198127w</id><link href="https://www.reddit.com/r/LangChain/comments/198127w/langchain_gemini_pro/" /><updated>2024-01-16T11:46:44+00:00</updated><published>2024-01-16T11:46:44+00:00</published><title>LangChain + Gemini Pro = ‚ùå ?</title></entry><entry><author><name>/u/SensitiveFel</name><uri>https://www.reddit.com/user/SensitiveFel</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;The inner workings of &lt;code&gt;RedisChatMessageHistory&lt;/code&gt;,I&amp;#39;m trying to find out if it uses summary internally like &lt;code&gt;ConversationSummaryMemory&lt;/code&gt;, or if he&amp;#39;s simply adding historical data to the context.&lt;/p&gt; &lt;p&gt;This may cause the token to exceed the limit, is there anything I can do to avoid this? I don&amp;#39;t see an internal description of this in the documentation.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/SensitiveFel&quot;&gt; /u/SensitiveFel &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/197wlgo/the_inner_workings_of_redischatmessagehistory/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/197wlgo/the_inner_workings_of_redischatmessagehistory/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_197wlgo</id><link href="https://www.reddit.com/r/LangChain/comments/197wlgo/the_inner_workings_of_redischatmessagehistory/" /><updated>2024-01-16T06:53:07+00:00</updated><published>2024-01-16T06:53:07+00:00</published><title>The inner workings of RedisChatMessageHistory</title></entry><entry><author><name>/u/modularmindapp</name><uri>https://www.reddit.com/user/modularmindapp</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/198734b/discover_top_4_scenarios_to_automate_with/&quot;&gt; &lt;img src=&quot;https://external-preview.redd.it/enRuY2l3c2J4dGNjMTjrPm1GhNl6e2Vc8Gpl38aePeKof0IqctgCC8Q1aBke.png?width=140&amp;amp;height=140&amp;amp;crop=140:140,smart&amp;amp;format=jpg&amp;amp;v=enabled&amp;amp;lthumb=true&amp;amp;s=f98a5e080b6533f157782e466992a2d2fb4d314b&quot; alt=&quot;Discover top 4 scenarios to automate with ModularMind, from App Market Research to YouTube analysis, AI newsletter generation, and marketing content creation.&quot; title=&quot;Discover top 4 scenarios to automate with ModularMind, from App Market Research to YouTube analysis, AI newsletter generation, and marketing content creation.&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/modularmindapp&quot;&gt; /u/modularmindapp &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://v.redd.it/agj7d1c9xtcc1&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/198734b/discover_top_4_scenarios_to_automate_with/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_198734b</id><media:thumbnail url="https://external-preview.redd.it/enRuY2l3c2J4dGNjMTjrPm1GhNl6e2Vc8Gpl38aePeKof0IqctgCC8Q1aBke.png?width=140&amp;height=140&amp;crop=140:140,smart&amp;format=jpg&amp;v=enabled&amp;lthumb=true&amp;s=f98a5e080b6533f157782e466992a2d2fb4d314b" /><link href="https://www.reddit.com/r/LangChain/comments/198734b/discover_top_4_scenarios_to_automate_with/" /><updated>2024-01-16T16:33:47+00:00</updated><published>2024-01-16T16:33:47+00:00</published><title>Discover top 4 scenarios to automate with ModularMind, from App Market Research to YouTube analysis, AI newsletter generation, and marketing content creation.</title></entry><entry><author><name>/u/ep3gotts</name><uri>https://www.reddit.com/user/ep3gotts</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;m trying to understand why Reranking is often needed in RAG technique and why it can&amp;#39;t be just skipped completely.&lt;/p&gt; &lt;p&gt;Would Reranking still exist if LLM models were bigger and better at creating embeddings(converting prompts into vectors in space)? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/ep3gotts&quot;&gt; /u/ep3gotts &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1978ywu/why_reranking_exists_in_the_first_place/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1978ywu/why_reranking_exists_in_the_first_place/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1978ywu</id><link href="https://www.reddit.com/r/LangChain/comments/1978ywu/why_reranking_exists_in_the_first_place/" /><updated>2024-01-15T13:40:27+00:00</updated><published>2024-01-15T13:40:27+00:00</published><title>Why Reranking exists in the first place?</title></entry><entry><author><name>/u/phicreative1997</name><uri>https://www.reddit.com/user/phicreative1997</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Learning LangChain so figured I would love to see how llms learn my writing and as a bonus would write about how i built this?&lt;/p&gt; &lt;p&gt;So need help with a few questions from the community&lt;/p&gt; &lt;p&gt;1) Which LLM would be best other than OpenAi models as I want to experiment with opensource models&lt;/p&gt; &lt;p&gt;2) list of tools, agents and chains which would help me do this? &lt;/p&gt; &lt;p&gt;3) Bonus my content has code embedded in it and visualizations, anyway I could tell the LLM to do it for me?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/phicreative1997&quot;&gt; /u/phicreative1997 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/197eq8d/i_am_a_technical_blogger_and_want_to_train_an_llm/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/197eq8d/i_am_a_technical_blogger_and_want_to_train_an_llm/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_197eq8d</id><link href="https://www.reddit.com/r/LangChain/comments/197eq8d/i_am_a_technical_blogger_and_want_to_train_an_llm/" /><updated>2024-01-15T17:44:01+00:00</updated><published>2024-01-15T17:44:01+00:00</published><title>I am a technical blogger and want to train an LLM on my blogs.</title></entry><entry><author><name>/u/PlanktonPretend6772</name><uri>https://www.reddit.com/user/PlanktonPretend6772</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;hello&lt;br/&gt; I&amp;#39;m trying to use the Mistral 7B model for question-answer generation using the QAGeneration Chain &lt;/p&gt; &lt;p&gt;&lt;code&gt;def llm_question_generation_pipeline(chunks):&lt;/code&gt;&lt;br/&gt; &lt;code&gt;documents = [Document(page_content=chunk) for chunk in chunks]&lt;/code&gt;&lt;br/&gt; &lt;code&gt;repo_id = &amp;quot;mistralai/Mistral-7B-v0.1&amp;quot;&lt;/code&gt;&lt;br/&gt; &lt;code&gt;llm = HuggingFaceHub(repo_id=repo_id, model_kwargs={&amp;quot;temperature&amp;quot;: 0.5, &amp;quot;max_length&amp;quot;: 512})&lt;/code&gt;&lt;br/&gt; &lt;code&gt;qa_chain = QAGenerationChain.from_llm(llm = llm, verbose = True)&lt;/code&gt;&lt;br/&gt; &lt;code&gt;raw_output = qa_chain.run(documents[0].page_content)&lt;/code&gt;&lt;br/&gt; &lt;code&gt;print(&amp;quot;Raw Output:&amp;quot;, raw_output)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;this is my code, but I&amp;#39;m getting this error frequently&lt;br/&gt; &lt;code&gt;raise JSONDecodeError(&amp;quot;Expecting value&amp;quot;, s, err.value) from None&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;json.decoder.JSONDecodeError: Expecting value: line 1 column 2 (char 1)&lt;/code&gt; &lt;/p&gt; &lt;p&gt;this is the prompt created by QAGeneration chain&lt;br/&gt; &lt;code&gt;Prompt after formatting:&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;You are a smart assistant designed to help high school teachers come up with reading comprehension questions.&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;Given a piece of text, you must come up with a question and answer pair that can be used to test a student&amp;#39;s reading comprehension abilities.&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;When coming up with this question/answer pair, you must respond in the following format:&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;\&lt;/code&gt;```&lt;/p&gt; &lt;p&gt;&lt;code&gt;{&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;&amp;quot;question&amp;quot;: &amp;quot;$YOUR_QUESTION_HERE&amp;quot;,&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;&amp;quot;answer&amp;quot;: &amp;quot;$THE_ANSWER_HERE&amp;quot;&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;}&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;\&lt;/code&gt;```&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;&lt;code&gt;Everything between the \&lt;/code&gt;`` must be valid json.`&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;&lt;code&gt;Please come up with a question/answer pair, in the specified JSON format, for the following text:&lt;/code&gt; &lt;/p&gt; &lt;p&gt;I searched for this error, it is saying that it could be possible if the LLM is not giving proper JSON formated output&lt;br/&gt; Would anyone be able to help me solve this error?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/PlanktonPretend6772&quot;&gt; /u/PlanktonPretend6772 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/197j354/mistral_7b_qageneration_chain_langchain/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/197j354/mistral_7b_qageneration_chain_langchain/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_197j354</id><link href="https://www.reddit.com/r/LangChain/comments/197j354/mistral_7b_qageneration_chain_langchain/" /><updated>2024-01-15T20:33:21+00:00</updated><published>2024-01-15T20:33:21+00:00</published><title>Mistral 7B QAGeneration Chain Langchain - JSONDecode Error</title></entry><entry><author><name>/u/Maheidem</name><uri>https://www.reddit.com/user/Maheidem</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi Guys,&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;I did a little side project to help me on my project, I&amp;#39;m doing a database parser agent using Clause 2.1 on bedrock that uses Python to query Druid with a LOT of contexts.&lt;/p&gt; &lt;p&gt;Been struggling quite a lot with documentation with anything that is not open ai.&lt;/p&gt; &lt;p&gt;So I created a Custom GPT to help me where it can.&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;It&amp;#39;s not perfect, but it&amp;#39;s been helping me a little so I wanted to share it with you all.&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://chat.openai.com/g/g-yqSII6PUj-langchain-specialist&quot;&gt;Langchain Specialist&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Maheidem&quot;&gt; /u/Maheidem &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1979xgt/custom_gpt_to_assist_with_langchain_development/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1979xgt/custom_gpt_to_assist_with_langchain_development/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1979xgt</id><link href="https://www.reddit.com/r/LangChain/comments/1979xgt/custom_gpt_to_assist_with_langchain_development/" /><updated>2024-01-15T14:25:28+00:00</updated><published>2024-01-15T14:25:28+00:00</published><title>Custom GPT to assist with langchain development</title></entry><entry><author><name>/u/wilyx11</name><uri>https://www.reddit.com/user/wilyx11</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;What is the difference between a similarty search and similarty search with relevancy score?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/wilyx11&quot;&gt; /u/wilyx11 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/19795k7/what_is_similarity_search_with_relevancy_score/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/19795k7/what_is_similarity_search_with_relevancy_score/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_19795k7</id><link href="https://www.reddit.com/r/LangChain/comments/19795k7/what_is_similarity_search_with_relevancy_score/" /><updated>2024-01-15T13:49:40+00:00</updated><published>2024-01-15T13:49:40+00:00</published><title>What is similarity search with relevancy score</title></entry><entry><author><name>/u/hamnarif</name><uri>https://www.reddit.com/user/hamnarif</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;&lt;a href=&quot;https://python.langchain.com/docs/use_cases/question_answering/chat_history&quot;&gt;https://python.langchain.com/docs/use_cases/question_answering/chat_history&lt;/a&gt;&lt;br/&gt; I am referring to these docs for chat history&lt;br/&gt; I]this code is giving me error on this &amp;quot;chat_history.extend([HumanMessage(content=question), ai_msg])&lt;br/&gt; line&amp;quot;&lt;/p&gt; &lt;p&gt;chat_history = []&lt;br/&gt; question = &amp;quot;What is function of form nec?&amp;quot;&lt;br/&gt; ai_msg = rag_chain.invoke({&amp;quot;question&amp;quot;: question, &amp;quot;chat_history&amp;quot;: chat_history})&lt;br/&gt; chat_history.extend([HumanMessage(content=question), ai_msg])&lt;br/&gt; second_question = &amp;quot;is there anything else about it?&amp;quot;&lt;br/&gt; rag_chain.invoke({&amp;quot;question&amp;quot;: second_question, &amp;quot;chat_history&amp;quot;: chat_history})&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/hamnarif&quot;&gt; /u/hamnarif &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/19777hm/getting_error_on_chat_history_langchain/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/19777hm/getting_error_on_chat_history_langchain/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_19777hm</id><link href="https://www.reddit.com/r/LangChain/comments/19777hm/getting_error_on_chat_history_langchain/" /><updated>2024-01-15T12:05:14+00:00</updated><published>2024-01-15T12:05:14+00:00</published><title>Getting error on chat history langchain. ValueError: variable chat_history should be a list of base messages, got [HumanMessage(content='somthing') '\nAI: retreived text.']</title></entry><entry><author><name>/u/Downtown-Crab271</name><uri>https://www.reddit.com/user/Downtown-Crab271</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;What I want to achieve? User can improve code quality and refactoring for give source code repository.&lt;/p&gt; &lt;p&gt;Input - code repository (python, java, dotnet, etc.) Db - chromadb or pinecone Embeddeding - sentence transformer model Top k results - MMR search Key - Azureopnai key n endpoint &lt;/p&gt; &lt;p&gt;Kindly suggest LLM and retrieval in best way if possible. Any link or references from Microsoft is also fine.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Downtown-Crab271&quot;&gt; /u/Downtown-Crab271 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/197b58d/need_suggestions_on_code_understanding_by_llm_and/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/197b58d/need_suggestions_on_code_understanding_by_llm_and/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_197b58d</id><link href="https://www.reddit.com/r/LangChain/comments/197b58d/need_suggestions_on_code_understanding_by_llm_and/" /><updated>2024-01-15T15:19:16+00:00</updated><published>2024-01-15T15:19:16+00:00</published><title>Need suggestions on code understanding by llm and output.</title></entry><entry><author><name>/u/Mediocre-Card8046</name><uri>https://www.reddit.com/user/Mediocre-Card8046</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;I want to fine-tune a Mistral or Mixtral model on my companies data. Specifically the model should write bullet-points to full texts. I was wondering how the training data has to look like for this task?&lt;/p&gt; &lt;p&gt;Is it a valid way to let a LLM create bullet points out of a text and use these bullet-points-full-text-examples for training? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Mediocre-Card8046&quot;&gt; /u/Mediocre-Card8046 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1973jxk/finetune_mixtral_on_enterprise_data_how_does_the/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1973jxk/finetune_mixtral_on_enterprise_data_how_does_the/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1973jxk</id><link href="https://www.reddit.com/r/LangChain/comments/1973jxk/finetune_mixtral_on_enterprise_data_how_does_the/" /><updated>2024-01-15T08:05:19+00:00</updated><published>2024-01-15T08:05:19+00:00</published><title>Finetune Mixtral on Enterprise data - How does the training data should look like?</title></entry><entry><author><name>/u/msze21</name><uri>https://www.reddit.com/user/msze21</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;ve been able to get all other local models, running with Ollama, working with LangChain. However, Yi-34B never returns a response.&lt;/p&gt; &lt;p&gt;I&amp;#39;m running an RTX 3090 and it is fitting in VRAM according to nvtop (20GB used).&lt;/p&gt; &lt;p&gt;Something simple like this never finishes (the last line is the one that never finishes):&lt;/p&gt; &lt;pre&gt;&lt;code&gt;from langchain_community.llms import Ollama ollama_model_name = &amp;quot;yi:34b-chat-q4_K_M&amp;quot; llm = Ollama(model=ollama_model_name, temperature=0.1) llm.invoke(&amp;quot;why is the sky blue?&amp;quot;) # This line never finishes... &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;I run it directly with Ollama and it responds instantly:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;$ ollama run yi:34b-chat-q4_K_M &amp;gt;&amp;gt;&amp;gt; why is the sky blue? The sky appears blue to us because of a phenomenon called Rayleigh scattering. This occurs when light from the sun interacts with the molecules of the Earth&amp;#39;s atmosphere, which are composed primarily of nitrogen and oxygen. These molecules are much smaller than the wavelength of visible light, so they can scatter light very efficiently. When sunlight enters the atmosphere, it strikes these atmospheric particles and is scattered in different directions. The shorter wavelengths of blue light are scattered more than longer wavelengths of red light because they have higher energy and thus a greater probability to interact with the molecules. This means that more blue light gets scattered relative to other colors, which is why we see a blue sky most of the time. The intensity of Rayleigh scattering also depends on the angle at which light hits an atmospheric molecule. Scattering is most effective when light hits molecules nearly head-on, which preferentially scatters light in all directions away from the sun. This scattered blue light then reaches our eyes after being scattered multiple times by different air molecules, giving us the perception of a blue sky. The color we see also depends on the observer&amp;#39;s line of sight relative to the sun. When the sun is high in the sky, more blue light scatters directly into our eyes, making the sky appear very blue. As the sun approaches the horizon, the light has to pass through more air and is scattered less efficiently, which means that more red light reaches our eyes, resulting in a reddish sky at sunset and sunrise. Additionally, other factors can influence the color of the sky, such as pollution or dust particles in the atmosphere, which can scatter light differently and change the appearance of the sky. &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Any ideas?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/msze21&quot;&gt; /u/msze21 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/196vqu3/anyone_have_yi34b_running_with_ollama_with/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/196vqu3/anyone_have_yi34b_running_with_ollama_with/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_196vqu3</id><link href="https://www.reddit.com/r/LangChain/comments/196vqu3/anyone_have_yi34b_running_with_ollama_with/" /><updated>2024-01-15T01:01:37+00:00</updated><published>2024-01-15T01:01:37+00:00</published><title>Anyone have Yi-34B (running with Ollama) with LangChain working</title></entry></feed>