<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/"><category term="LangChain" label="r/LangChain"/><updated>2024-03-27T08:57:41+00:00</updated><icon>https://www.redditstatic.com/icon.png/</icon><id>/r/LangChain.rss</id><link rel="self" href="https://www.reddit.com/r/LangChain.rss" type="application/atom+xml" /><link rel="alternate" href="https://www.reddit.com/r/LangChain" type="text/html" /><subtitle>LangChain is an open-source framework and developer toolkit that helps developers get LLM applications from prototype to production. It is available for Python and Javascript at https://www.langchain.com/.</subtitle><title>LangChain</title><entry><author><name>/u/zchaarm</name><uri>https://www.reddit.com/user/zchaarm</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;A place for members of &lt;a href=&quot;/r/LangChain&quot;&gt;r/LangChain&lt;/a&gt; to chat with each other&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/zchaarm&quot;&gt; /u/zchaarm &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_10ljho9</id><link href="https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/" /><updated>2023-01-26T04:33:05+00:00</updated><published>2023-01-26T04:33:05+00:00</published><title>r/LangChain Lounge</title></entry><entry><author><name>/u/sarthak_uchiha</name><uri>https://www.reddit.com/user/sarthak_uchiha</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Can someone please highlight what is meant by prompt injection and what security concerns it may have , and if somebody can provide an example for the same that would be great &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/sarthak_uchiha&quot;&gt; /u/sarthak_uchiha &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bovlcb/prompt_injection/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bovlcb/prompt_injection/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bovlcb</id><link href="https://www.reddit.com/r/LangChain/comments/1bovlcb/prompt_injection/" /><updated>2024-03-27T06:57:09+00:00</updated><published>2024-03-27T06:57:09+00:00</published><title>Prompt injection</title></entry><entry><author><name>/u/qa_anaaq</name><uri>https://www.reddit.com/user/qa_anaaq</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi. I have a long prompt of about 500-700 tokens, and context gets added also since it&amp;#39;s a RAG, so what I send to the LLM could easily be a few thousand tokens per question. &lt;/p&gt; &lt;p&gt;I am trying to get a response that is conversational and that roughly follows a format like :&lt;/p&gt; &lt;p&gt;&amp;quot;To do x, y, and Z, you need to complete these steps.....blah blah.....For more info, please refer to this video.&amp;quot;&lt;/p&gt; &lt;p&gt;It returns good info but I can&amp;#39;t for the life of me get it to return consistently structured responses. I&amp;#39;ve tried CoT, but it keeps stating it&amp;#39;s reasoning, which makes it sound like the robot it is. I just want it to end with &amp;quot;for more info..&amp;quot; Or &amp;quot;if you need more help..&amp;quot; Etc. &lt;/p&gt; &lt;p&gt;So I&amp;#39;m thinking few-shot, but I don&amp;#39;t know if I should put the examples before or after the retrieved context given the length of the context and the prompt, in general. &lt;/p&gt; &lt;p&gt;Has anyone experimented in these conditions?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/qa_anaaq&quot;&gt; /u/qa_anaaq &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1boqup2/long_prompt_and_fewshot_placement/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1boqup2/long_prompt_and_fewshot_placement/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1boqup2</id><link href="https://www.reddit.com/r/LangChain/comments/1boqup2/long_prompt_and_fewshot_placement/" /><updated>2024-03-27T02:27:30+00:00</updated><published>2024-03-27T02:27:30+00:00</published><title>Long prompt and few-shot placement</title></entry><entry><author><name>/u/Delicious_Success303</name><uri>https://www.reddit.com/user/Delicious_Success303</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bowtfo/chroma_db_throws_error_at_2nd_run/&quot;&gt; &lt;img src=&quot;https://b.thumbs.redditmedia.com/kohHHDBwWGMw1n8j9QbkJ_qZ12rhsOQUDWZ0bMhUf9U.jpg&quot; alt=&quot;Chroma db throws error at 2nd run&quot; title=&quot;Chroma db throws error at 2nd run&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi!!&lt;br/&gt; I am using chroma db with pre computed embeddings for my rag application. Chroma runs well first time but every time i re run my notebook i get this error. Thanks :)&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/aqq8insu6uqc1.png?width=3000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=51adc2a358e349fdbe3c211ed869d0f79c1bb986&quot;&gt;https://preview.redd.it/aqq8insu6uqc1.png?width=3000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=51adc2a358e349fdbe3c211ed869d0f79c1bb986&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Delicious_Success303&quot;&gt; /u/Delicious_Success303 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bowtfo/chroma_db_throws_error_at_2nd_run/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bowtfo/chroma_db_throws_error_at_2nd_run/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1bowtfo</id><media:thumbnail url="https://b.thumbs.redditmedia.com/kohHHDBwWGMw1n8j9QbkJ_qZ12rhsOQUDWZ0bMhUf9U.jpg" /><link href="https://www.reddit.com/r/LangChain/comments/1bowtfo/chroma_db_throws_error_at_2nd_run/" /><updated>2024-03-27T08:23:22+00:00</updated><published>2024-03-27T08:23:22+00:00</published><title>Chroma db throws error at 2nd run</title></entry><entry><author><name>/u/o3omoomin</name><uri>https://www.reddit.com/user/o3omoomin</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I implemented RAG using Ensemble Retriever. widh llama index&lt;/p&gt; &lt;p&gt;Before using the prompt template module, if sent a query like “Hello”, llm would not respond because the query did not exist in the document. &lt;/p&gt; &lt;p&gt;And were able to solve these problems by using the prompt template module. &lt;/p&gt; &lt;p&gt;How important is prompt template engineering? And what should I do to set up prompt template engineering well? &lt;/p&gt; &lt;p&gt;Below is the prompt template I wrote&lt;/p&gt; &lt;pre&gt;&lt;code&gt;template = &amp;#39;&amp;#39;&amp;#39; You are a chatbot fluent in Korean developed specifically for our company. Your primary role is to communicate with users by answering their questions in Korean and providing feedback related to their questions. Your job is to provide the user with an answer regarding your company&amp;#39;s employment rules when asked {query}. If you are asked a {query} question that is not related to the Company&amp;#39;s employment rules, it is your responsibility to redirect the conversation to a topic related to the Company&amp;#39;s policies and guidelines. You can also recommend questions to users based on your knowledge. We encourage our users to ask questions that are directly related to our company&amp;#39;s operations, culture, or the specific guidelines we follow. &amp;#39;&amp;#39;&amp;#39; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/o3omoomin&quot;&gt; /u/o3omoomin &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bosw8h/how_to_implement_prompt_engineering_well/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bosw8h/how_to_implement_prompt_engineering_well/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bosw8h</id><link href="https://www.reddit.com/r/LangChain/comments/1bosw8h/how_to_implement_prompt_engineering_well/" /><updated>2024-03-27T04:08:46+00:00</updated><published>2024-03-27T04:08:46+00:00</published><title>How to implement prompt engineering well?</title></entry><entry><author><name>/u/Ill_Bodybuilder3499</name><uri>https://www.reddit.com/user/Ill_Bodybuilder3499</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;my go to tool for RAG Evaluation has been RAGAS so far, but i am not always happy with rhe results. Now I saw a video about GISKARD evaluation which also looks promising.&lt;/p&gt; &lt;p&gt;So my question: has anybody some experiene wirh Giskard or both, and what do you like better?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Ill_Bodybuilder3499&quot;&gt; /u/Ill_Bodybuilder3499 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1boiq77/ragas_vs_giskard_rag_evaluation/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1boiq77/ragas_vs_giskard_rag_evaluation/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1boiq77</id><link href="https://www.reddit.com/r/LangChain/comments/1boiq77/ragas_vs_giskard_rag_evaluation/" /><updated>2024-03-26T20:51:13+00:00</updated><published>2024-03-26T20:51:13+00:00</published><title>RAGAS vs GISKARD RAG Evaluation</title></entry><entry><author><name>/u/Not-That-rpg</name><uri>https://www.reddit.com/user/Not-That-rpg</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;It seems like the way langchain is set up, extra `model_kwargs` are just quietly thrown on the floor. I think that happens in the pydantic validation process, but I am not sure.&lt;/p&gt; &lt;p&gt;Is there any way to tell what&amp;#39;s happening with the `model_kwargs`?&lt;/p&gt; &lt;p&gt;For example, &lt;a href=&quot;https://github.com/langchain-ai/langchain/issues/10590&quot;&gt;this langchain issue&lt;/a&gt; discusses what happens when a model class ignores the `n_ctx` kwarg, and suggests a fix based on editing the `validate_environment` method of a class. But this requires identifying the right `validate_environment` method in the class hierarchy, and then modifying a local variable (really a constant, `model_param_names` -- why isn&amp;#39;t this a property of the class instead of a local variable?).&lt;/p&gt; &lt;p&gt;Is there some way to trace the execution of an LLM constructor and see which kwargs are actually sent, and which are ignored?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Not-That-rpg&quot;&gt; /u/Not-That-rpg &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bopm2t/how_can_we_tell_which_llm_arguments_are_being/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bopm2t/how_can_we_tell_which_llm_arguments_are_being/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bopm2t</id><link href="https://www.reddit.com/r/LangChain/comments/1bopm2t/how_can_we_tell_which_llm_arguments_are_being/" /><updated>2024-03-27T01:30:17+00:00</updated><published>2024-03-27T01:30:17+00:00</published><title>How can we tell which LLM arguments are being processed?</title></entry><entry><author><name>/u/realsharaf</name><uri>https://www.reddit.com/user/realsharaf</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;m using Firestore for storage and I already built a solution to store group chats with the following structure:&lt;/p&gt; &lt;p&gt;--&amp;gt; chats &amp;lt;collection&amp;gt;&lt;/p&gt; &lt;p&gt;----&amp;gt; chat_doc#1 &amp;lt;well, a doc&amp;gt;&lt;/p&gt; &lt;p&gt;----&amp;gt; chat_doc#2&lt;/p&gt; &lt;p&gt;----&amp;gt; chat_doc#n&lt;/p&gt; &lt;p&gt;------&amp;gt; participants &amp;lt;array with user ids&amp;gt;&lt;/p&gt; &lt;p&gt;------&amp;gt; messages &amp;lt;collection to store messages, each message in a doc&amp;gt;&lt;/p&gt; &lt;p&gt;Now, I&amp;#39;m looking to integrate LangChain into my app (easier to work with multiple LLM providers) and so I have to restructure my code. &lt;/p&gt; &lt;p&gt;I used `FirestoreChatMessageHistory` and noticed that their structure is similar to mine, but in each chat doc they only have `userId` which holds a single user id. I want to replace that with a `participants` array or anything that can hold multiple user ids. But I noticed there&amp;#39;s no way to customize the `FirestoreChatMessageHistory` class to include multiple participants.&lt;/p&gt; &lt;p&gt;The way I&amp;#39;m thinking is to use the `userId` prop to store all user ids in comma-separated format, but:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;I don&amp;#39;t know if that will work well (or at all) &amp;amp; I don&amp;#39;t have the time to test this out if it&amp;#39;ll take too much time because of the overhead, and&lt;/li&gt; &lt;li&gt;there&amp;#39;s way too much overhead - it feels stupidly irritating and I feel that there should be a neater way to implement this.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Anyone face this issue? How did you solve it?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/realsharaf&quot;&gt; /u/realsharaf &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bopi12/how_to_store_group_chats/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bopi12/how_to_store_group_chats/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bopi12</id><link href="https://www.reddit.com/r/LangChain/comments/1bopi12/how_to_store_group_chats/" /><updated>2024-03-27T01:25:14+00:00</updated><published>2024-03-27T01:25:14+00:00</published><title>How to store group chats???</title></entry><entry><author><name>/u/imhimu_</name><uri>https://www.reddit.com/user/imhimu_</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/imhimu_&quot;&gt; /u/imhimu_ &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bon5sl/is_there_any_available_resourse_for_async/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bon5sl/is_there_any_available_resourse_for_async/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bon5sl</id><link href="https://www.reddit.com/r/LangChain/comments/1bon5sl/is_there_any_available_resourse_for_async/" /><updated>2024-03-26T23:45:20+00:00</updated><published>2024-03-26T23:45:20+00:00</published><title>is there any available resourse for async langchain rag agents with hugginface models</title></entry><entry><author><name>/u/Inevitable-Worth-861</name><uri>https://www.reddit.com/user/Inevitable-Worth-861</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Sorry if this is obvious to most of you, I’m somewhat new. &lt;/p&gt; &lt;p&gt;My app is currently successfully, in order: - Getting URL content with document loader - Getting file content from a GitHub codebase - Sending this to OpenAi model with a prompt - Serving the response to my front end&lt;/p&gt; &lt;p&gt;The issue is around token limitation. &lt;/p&gt; &lt;p&gt;I would love to be able to get the whole codebase along with the reference URL content processed by the LLM and eventually return a single response to my frontend, of course I hit the token limit fast. &lt;/p&gt; &lt;p&gt;I understand that text splitting and vector stores are the solution, I’ll get my head around that.&lt;/p&gt; &lt;p&gt;My question is, will I end up sending many requests and get many responses from the LLM?&lt;/p&gt; &lt;p&gt;I don’t mind many requests, but I don’t understand how I’ll get a single coherent response to my friend if it’s multiple LLM responses.&lt;/p&gt; &lt;p&gt;Maybe I’m missing something obvious here. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Inevitable-Worth-861&quot;&gt; /u/Inevitable-Worth-861 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bon41r/codebase_external_url_context_for_llm/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bon41r/codebase_external_url_context_for_llm/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bon41r</id><link href="https://www.reddit.com/r/LangChain/comments/1bon41r/codebase_external_url_context_for_llm/" /><updated>2024-03-26T23:43:21+00:00</updated><published>2024-03-26T23:43:21+00:00</published><title>Codebase + External URL Context for LLM</title></entry><entry><author><name>/u/uygarsci</name><uri>https://www.reddit.com/user/uygarsci</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey guys, I just created a video on how to build chatbots with memory.&lt;/p&gt; &lt;p&gt;Most of the examples on langchains website are with open api. I wanted to keep things open source so ise llama from huggingface.&lt;/p&gt; &lt;p&gt;Hope you like it. Any feedback is welcome.&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://youtu.be/gNXBp3wttFU?si=GirGTqe7ThEUFSrx&quot;&gt;https://youtu.be/gNXBp3wttFU?si=GirGTqe7ThEUFSrx&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/uygarsci&quot;&gt; /u/uygarsci &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bojn3b/my_video_on_memory_chatbots/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bojn3b/my_video_on_memory_chatbots/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bojn3b</id><link href="https://www.reddit.com/r/LangChain/comments/1bojn3b/my_video_on_memory_chatbots/" /><updated>2024-03-26T21:26:39+00:00</updated><published>2024-03-26T21:26:39+00:00</published><title>My Video on Memory Chatbots</title></entry><entry><author><name>/u/mehul_gupta1997</name><uri>https://www.reddit.com/user/mehul_gupta1997</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Checkout this demo to understand autogen, a Multi-Agent Orchestration python package supporting AI Agents conversations using HuggingFace models. &lt;a href=&quot;https://youtu.be/NY4_jhPcicw?si=IV29lMJcQ8rvWVij&quot;&gt;https://youtu.be/NY4_jhPcicw?si=IV29lMJcQ8rvWVij&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/mehul_gupta1997&quot;&gt; /u/mehul_gupta1997 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bo5zq9/multiagent_conversation_using_autogen_and/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bo5zq9/multiagent_conversation_using_autogen_and/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bo5zq9</id><link href="https://www.reddit.com/r/LangChain/comments/1bo5zq9/multiagent_conversation_using_autogen_and/" /><updated>2024-03-26T11:53:10+00:00</updated><published>2024-03-26T11:53:10+00:00</published><title>Multi-Agent Conversation using AutoGen and HuggingFace models</title></entry><entry><author><name>/u/Mediocre-Card8046</name><uri>https://www.reddit.com/user/Mediocre-Card8046</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;I don&amp;#39;t get it to run. I want to stream a LCEL RAG chain response using FastAPI. So my question would be how to stream a LCEL chain response and how can I return the whole response, so the dict with the &amp;quot;answer&amp;quot; and &amp;quot;docs&amp;quot; keys, where the retrieved docs are inside?&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;Here is the code for my chain:&lt;/p&gt; &lt;p&gt;&lt;code&gt;from langchain_community.vectorstores import FAISS&lt;/code&gt;&lt;br/&gt; &lt;code&gt;from langchain_community.embeddings import HuggingFaceEmbeddings&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;from langchain_core.prompts import PromptTemplate, ChatPromptTemplate&lt;/code&gt;&lt;br/&gt; &lt;code&gt;from langchain_core.runnables import RunnableParallel&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;from operator import itemgetter&lt;/code&gt;&lt;br/&gt; &lt;code&gt;from typing import TypedDict&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;from fastapi import FastAPI, File, UploadFile, HTTPException, Path, Query&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;embeddings = HuggingFaceEmbeddings(model_name=&amp;quot;intfloat/multilingual-e5-large-instruct&amp;quot;, model_kwargs={&amp;#39;device&amp;#39;: &amp;quot;mps&amp;quot;})&lt;/code&gt;&lt;br/&gt; &lt;code&gt;db = FAISS.load_local(&amp;quot;streamlit_vectorstores/vectorstores/db_maxiw_testfreitag&amp;quot;, embeddings, allow_dangerous_deserialization=True)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;llm = build_llm(model_path)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;template = &amp;quot;&amp;quot;&amp;quot;&lt;/code&gt;&lt;br/&gt; &lt;code&gt;Beantworte die Frage ausschließlich basierend auf folgenden Kontext auf Deutsch:&lt;/code&gt;&lt;br/&gt; &lt;code&gt;{context}&lt;/code&gt;&lt;br/&gt; &lt;code&gt;Frage: {question}&lt;/code&gt;&lt;br/&gt; &lt;code&gt;&amp;quot;&amp;quot;&amp;quot;&lt;/code&gt;&lt;br/&gt; &lt;code&gt;ANSWER_PROMPT = ChatPromptTemplate.from_template(template)&lt;/code&gt;&lt;br/&gt; &lt;code&gt;class RagInput(TypedDict):&lt;/code&gt;&lt;br/&gt; &lt;code&gt;question: str&lt;/code&gt; &lt;/p&gt; &lt;p&gt;&lt;code&gt;final_chain = (&lt;/code&gt;&lt;br/&gt; &lt;code&gt;RunnableParallel(&lt;/code&gt;&lt;br/&gt; &lt;code&gt;context=(itemgetter(&amp;quot;question&amp;quot;) | db.as_retriever()),&lt;/code&gt;&lt;br/&gt; &lt;code&gt;question=itemgetter(&amp;quot;question&amp;quot;)&lt;/code&gt;&lt;br/&gt; &lt;code&gt;) |&lt;/code&gt;&lt;br/&gt; &lt;code&gt;RunnableParallel(&lt;/code&gt;&lt;br/&gt; &lt;code&gt;answer=(ANSWER_PROMPT | llm),&lt;/code&gt;&lt;br/&gt; &lt;code&gt;docs=itemgetter(&amp;quot;context&amp;quot;)&lt;/code&gt;&lt;br/&gt; &lt;code&gt;)&lt;/code&gt;&lt;br/&gt; &lt;code&gt;).with_types(input_type=RagInput)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;&lt;code&gt;And here my get-endpoint which does not work:&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;u/app.get(&amp;quot;/rag_lcel/&amp;quot;)&lt;/code&gt;&lt;br/&gt; &lt;code&gt;async def fastapi_stream(question: str):&lt;/code&gt;&lt;br/&gt; &lt;code&gt;start_time = time.time()&lt;/code&gt;&lt;br/&gt; &lt;code&gt;first_response = True&lt;/code&gt;&lt;br/&gt; &lt;code&gt;for resp in final_chain.astream({&amp;quot;question&amp;quot;: question}):&lt;/code&gt;&lt;br/&gt; &lt;code&gt;if resp and first_response:&lt;/code&gt;&lt;br/&gt; &lt;code&gt;# Calculate and print time after the first batch of text is streamed&lt;/code&gt;&lt;br/&gt; &lt;code&gt;end_time = time.time()&lt;/code&gt;&lt;br/&gt; &lt;code&gt;elapsed_time = round(end_time - start_time, 1)&lt;/code&gt;&lt;br/&gt; &lt;code&gt;first_response = False&lt;/code&gt;&lt;br/&gt; &lt;code&gt;yield f&amp;quot;(Response Time: {elapsed_time} seconds)\n&amp;quot;&lt;/code&gt;&lt;br/&gt; &lt;code&gt;yield resp&lt;/code&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Mediocre-Card8046&quot;&gt; /u/Mediocre-Card8046 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bo2k4r/how_to_stream_lcel_chain_with_fastapi_and_return/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bo2k4r/how_to_stream_lcel_chain_with_fastapi_and_return/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bo2k4r</id><link href="https://www.reddit.com/r/LangChain/comments/1bo2k4r/how_to_stream_lcel_chain_with_fastapi_and_return/" /><updated>2024-03-26T08:14:27+00:00</updated><published>2024-03-26T08:14:27+00:00</published><title>How to Stream LCEL Chain with FastAPI and return source docs?</title></entry><entry><author><name>/u/Ahmad_AM0</name><uri>https://www.reddit.com/user/Ahmad_AM0</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I want to build RAG system with a chatbot using an LLM with LangChain framework.&lt;/p&gt; &lt;p&gt;The problem is that I don&amp;#39;t have enough resources to run an LLM locally. I guess I have two options:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Looking for a free API that give good results&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Finding a free server-like platform to host an LLM&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;For option 1, do you know some good APIs ?&lt;/p&gt; &lt;p&gt;For option 2, I thought of using Google Colab as a server but wasn&amp;#39;t able to exchange messages between my local machine and Colab. I tried ngrok library but it didn&amp;#39;t work. Is there any solution ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Ahmad_AM0&quot;&gt; /u/Ahmad_AM0 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bo8d5h/not_enough_resources_to_run_an_llm_locally/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bo8d5h/not_enough_resources_to_run_an_llm_locally/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bo8d5h</id><link href="https://www.reddit.com/r/LangChain/comments/1bo8d5h/not_enough_resources_to_run_an_llm_locally/" /><updated>2024-03-26T13:49:26+00:00</updated><published>2024-03-26T13:49:26+00:00</published><title>Not enough resources to run an LLM locally</title></entry><entry><author><name>/u/esraaatmeh</name><uri>https://www.reddit.com/user/esraaatmeh</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;how I can stream agent output using langchain when I use mistralai/Mistral-7B-Instruct-v0.2?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/esraaatmeh&quot;&gt; /u/esraaatmeh &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bo30or/stream_open_source_llm/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bo30or/stream_open_source_llm/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bo30or</id><link href="https://www.reddit.com/r/LangChain/comments/1bo30or/stream_open_source_llm/" /><updated>2024-03-26T08:46:37+00:00</updated><published>2024-03-26T08:46:37+00:00</published><title>stream open source LLM</title></entry><entry><author><name>/u/cryptokaykay</name><uri>https://www.reddit.com/user/cryptokaykay</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bnkvtv/update_langtrace_preview_opensource_llm/&quot;&gt; &lt;img src=&quot;https://b.thumbs.redditmedia.com/hCblHithvJSr2WsruX__shRW1xQxHRLpBDMO1YrayfM.jpg&quot; alt=&quot;Update: Langtrace Preview: Opensource LLM monitoring tool - achieving better cardinality compared to Langsmith.&quot; title=&quot;Update: Langtrace Preview: Opensource LLM monitoring tool - achieving better cardinality compared to Langsmith.&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;This is a follow up for: &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1b6phov/update_langtrace_preview_an_opensource_llm/&quot;&gt;https://www.reddit.com/r/LangChain/comments/1b6phov/update_langtrace_preview_an_opensource_llm/&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Thought of sharing what I am cooking. Basically, I am building a open source LLM monitoring and evaluation suite. It works like this:&lt;br/&gt; 1. Install the SDK with 2 lines of code (npm i or pip install)&lt;br/&gt; 2. The SDK will start shipping traces in Open telemetry standard format to the UI&lt;br/&gt; 3. See the metrics, traces and prompts in the UI(Attaching some screenshots below). &lt;/p&gt; &lt;p&gt;I am mostly optimizing the features for 3 main metrics&lt;br/&gt; 1. Usage - token/cost&lt;br/&gt; 2. Accuracy - Manually evaluate traced prompt-response pairs from the UI and see the accuracy score&lt;br/&gt; 3. Latency - speed of responses/time to first token &lt;/p&gt; &lt;p&gt;Vendors supported for the first version:&lt;br/&gt; Langchain, LlamaIndex, OpenAI, Anthropic, Pinecone, ChromaDB &lt;/p&gt; &lt;p&gt;I will opensource this project in about a week and share the repo here.&lt;/p&gt; &lt;p&gt;Please let me know what else you would like to see or what other challenges you face that can be solved through this project.&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/zwz0lqcfwiqc1.png?width=2978&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=90caa5f52e47503493e4417b6808d7f12739f2d3&quot;&gt;https://preview.redd.it/zwz0lqcfwiqc1.png?width=2978&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=90caa5f52e47503493e4417b6808d7f12739f2d3&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/cvv6aqcfwiqc1.png?width=3000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e8374335d6e5b5a7ff04f1ea1408f74f9dce1698&quot;&gt;https://preview.redd.it/cvv6aqcfwiqc1.png?width=3000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e8374335d6e5b5a7ff04f1ea1408f74f9dce1698&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/cryptokaykay&quot;&gt; /u/cryptokaykay &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bnkvtv/update_langtrace_preview_opensource_llm/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bnkvtv/update_langtrace_preview_opensource_llm/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1bnkvtv</id><media:thumbnail url="https://b.thumbs.redditmedia.com/hCblHithvJSr2WsruX__shRW1xQxHRLpBDMO1YrayfM.jpg" /><link href="https://www.reddit.com/r/LangChain/comments/1bnkvtv/update_langtrace_preview_opensource_llm/" /><updated>2024-03-25T18:25:33+00:00</updated><published>2024-03-25T18:25:33+00:00</published><title>Update: Langtrace Preview: Opensource LLM monitoring tool - achieving better cardinality compared to Langsmith.</title></entry><entry><author><name>/u/Xx_K3v1n5pac3y_xX</name><uri>https://www.reddit.com/user/Xx_K3v1n5pac3y_xX</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bo2hpi/bug_with_structuredchatzeroshotreactdescription/&quot;&gt; &lt;img src=&quot;https://b.thumbs.redditmedia.com/Svg_4TrZDGNAnk_sCBYnMomrUemu_E06S0KbRYcKuFs.jpg&quot; alt=&quot;Bug with structured-chat-zero-shot-react-description agent&quot; title=&quot;Bug with structured-chat-zero-shot-react-description agent&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;When conducting tests, the agent is able to determine the correct tool and parse the prompt to the correct inputs. However, often times the agent will finish the chain prematurely without making any observations. &lt;/p&gt; &lt;p&gt;In my tests, all the prompts have been kept the same and temperature=0. &lt;/p&gt; &lt;p&gt;I noticed if the agent were to successfully make an observation based on the used tool, it will display this on the terminal.&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/8e70hpm6zmqc1.png?width=506&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=07df769dc1b062a68c70c61d799ef009facf96a9&quot;&gt;https://preview.redd.it/8e70hpm6zmqc1.png?width=506&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=07df769dc1b062a68c70c61d799ef009facf96a9&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Otherwise if it was going the end the chain prematurely it would look like this.&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/gft49igfzmqc1.png?width=532&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2ab5f9d08f68c8c62bebe4565f35142f95055617&quot;&gt;https://preview.redd.it/gft49igfzmqc1.png?width=532&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2ab5f9d08f68c8c62bebe4565f35142f95055617&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Pardon me if I lack the understanding to solve this, but I am kinda at wits end. Thanks in advance.&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Xx_K3v1n5pac3y_xX&quot;&gt; /u/Xx_K3v1n5pac3y_xX &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bo2hpi/bug_with_structuredchatzeroshotreactdescription/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bo2hpi/bug_with_structuredchatzeroshotreactdescription/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1bo2hpi</id><media:thumbnail url="https://b.thumbs.redditmedia.com/Svg_4TrZDGNAnk_sCBYnMomrUemu_E06S0KbRYcKuFs.jpg" /><link href="https://www.reddit.com/r/LangChain/comments/1bo2hpi/bug_with_structuredchatzeroshotreactdescription/" /><updated>2024-03-26T08:09:59+00:00</updated><published>2024-03-26T08:09:59+00:00</published><title>Bug with structured-chat-zero-shot-react-description agent</title></entry><entry><author><name>/u/Malcherion</name><uri>https://www.reddit.com/user/Malcherion</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hello,&lt;/p&gt; &lt;p&gt;I am trying to create a workflow where the agents receive an API specification, make improvements on it and save it to a different file. I am using the example from LangChain on how to build hierarchical teams and I have created an &amp;quot;API Enhancement Team&amp;quot; with two agents.&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/langchain-ai/langgraph/blob/main/examples/multi_agent/hierarchical_agent_teams.ipynb&quot;&gt;https://github.com/langchain-ai/langgraph/blob/main/examples/multi_agent/hierarchical_agent_teams.ipynb&lt;/a&gt;&lt;/p&gt; &lt;p&gt;One agent that will read a YAML file and provide suggestions on what needs to be improved. And one agent that will apply the changes to the specification and save the file.&lt;/p&gt; &lt;p&gt;I have created the following tools:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;@tool def read_api_spec_from_yaml(file_path: str) -&amp;gt; Dict: &amp;quot;&amp;quot;&amp;quot;Reads an API specification from a YAML file.&amp;quot;&amp;quot;&amp;quot; with open(file_path, &amp;#39;r&amp;#39;, encoding=&amp;#39;utf-8&amp;#39;) as file: api_spec = yaml.safe_load(file) print(&amp;quot;Successfully read the API spec from the file.&amp;quot;) return api_spec @tool def save_improved_spec(spec_data, filename, directory=&amp;quot;Improved Specs&amp;quot;): &amp;quot;&amp;quot;&amp;quot; Saves the improved API specification to a file in the specified directory. If the file exists, it saves it with an incremented version suffix before the file extension. :param spec_data: The API specification data to save. :param filename: The base filename for the saved specification. :param directory: The directory where the file will be saved. Defaults to &amp;quot;Improved Specs&amp;quot;. &amp;quot;&amp;quot;&amp;quot; # Ensure the directory exists if not os.path.exists(directory): os.makedirs(directory) # Split the filename to insert version suffix before the extension name, extension = os.path.splitext(filename) # Construct the base filepath without the extension base_filepath = os.path.join(directory, name) filepath = base_filepath + extension # Initial assumption: no version needed # Check for existing files and increment version suffix if necessary version = 1 while os.path.exists(filepath): filepath = f&amp;quot;{base_filepath}_v{version}{extension}&amp;quot; version += 1 # Write the specification data to the file with open(filepath, &amp;#39;w&amp;#39;) as file: json.dump(spec_data, file, indent=4) print(f&amp;quot;Specification saved as {os.path.basename(filepath)} in &amp;#39;{directory}&amp;#39; directory.&amp;quot;) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;And I have defined the following agents:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;api_spec_expert = create_agent( llm, [read_api_spec_from_yaml], api_spec_expert_prompt, ) api_spec_expert_node = functools.partial(agent_node, agent=api_spec_expert, name=&amp;quot;API Spec Expert&amp;quot;) api_improver = create_agent( llm, [save_improved_spec], api_improver_prompt ) api_improver_node = functools.partial(agent_node, agent=api_improver, name=&amp;quot;API Spec Improver&amp;quot;) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The expert is successfully reading the file and providing recommendations, but the improver is not using the tool and gives me very generic answers, like:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Given the detailed list of enhancement suggestions for the OpenAI API specification, I will now proceed to apply these enhancements to the specification document. This process involves updating the OpenAI API specification (`openai_oas.yaml`) according to the provided suggestions, ensuring that the documentation becomes more comprehensive, user-friendly, and helpful for developers.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Any advice, insights, or shared experiences with similar challenges would be greatly appreciated. I&amp;#39;m eager to learn from the community and find a solution.&lt;/p&gt; &lt;p&gt;Thank you in advance.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Malcherion&quot;&gt; /u/Malcherion &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bnmezm/agent_is_not_using_a_custom_tool/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bnmezm/agent_is_not_using_a_custom_tool/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bnmezm</id><link href="https://www.reddit.com/r/LangChain/comments/1bnmezm/agent_is_not_using_a_custom_tool/" /><updated>2024-03-25T19:25:39+00:00</updated><published>2024-03-25T19:25:39+00:00</published><title>Agent is not using a custom tool</title></entry><entry><author><name>/u/QuixoticQuisling</name><uri>https://www.reddit.com/user/QuixoticQuisling</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey guys. I&amp;#39;m making an interactive LLM scripting playground - like the OpenAI playground, but using javascript so you can do some fancy stuff. Please take a look and give me some feedback. (Not LangChain, but pretty closely releated)&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://stackblitz.com/fork/github/retort-js/playground?file=retort%2Fscript-template.rt.js&amp;amp;hideNavigation=1&amp;amp;showSidebar=0&quot;&gt;RetortJS Playground&lt;/a&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/QuixoticQuisling&quot;&gt; /u/QuixoticQuisling &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bndoz4/interactive_llm_scripting_playground/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bndoz4/interactive_llm_scripting_playground/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bndoz4</id><link href="https://www.reddit.com/r/LangChain/comments/1bndoz4/interactive_llm_scripting_playground/" /><updated>2024-03-25T13:29:45+00:00</updated><published>2024-03-25T13:29:45+00:00</published><title>Interactive LLM scripting playground</title></entry><entry><author><name>/u/meliodes00</name><uri>https://www.reddit.com/user/meliodes00</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey everyone, &lt;/p&gt; &lt;p&gt;I&amp;#39;ve got my LLM up and running using the Langchain SagemakerEndpoint class, and it&amp;#39;s all good. However, because it&amp;#39;s a RAG application, the response time isn&amp;#39;t as snappy as I&amp;#39;d like. So, I started looking into ways to speed things up and came across the streaming feature.&lt;/p&gt; &lt;p&gt;Excited to try, I checked out the documentation and set the streaming parameter to True. But instead of speeding things up, my model got stuck in an infinite loop with no response.&lt;/p&gt; &lt;p&gt;Digging deeper, I took a look at the source code of the SagemakerEndpoint class to see what&amp;#39;s causing the issue. Turns out, it&amp;#39;s something to do with the Langchain method being used. Interestingly, when I bypass Langchain, everything works fine.&lt;/p&gt; &lt;p&gt;Now, I&amp;#39;m a bit perplexed. Any ideas on how to tackle this problem? Your help would be greatly appreciated!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/meliodes00&quot;&gt; /u/meliodes00 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bnlwrj/how_to_enable_streaming_in_sagemakerendpoint/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bnlwrj/how_to_enable_streaming_in_sagemakerendpoint/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bnlwrj</id><link href="https://www.reddit.com/r/LangChain/comments/1bnlwrj/how_to_enable_streaming_in_sagemakerendpoint/" /><updated>2024-03-25T19:05:56+00:00</updated><published>2024-03-25T19:05:56+00:00</published><title>How to enable streaming in SagemakerEndpoint</title></entry><entry><author><name>/u/nisshhhhhh</name><uri>https://www.reddit.com/user/nisshhhhhh</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I am using ChromaDb as my vectorstore. &lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;Code : &lt;/p&gt; &lt;p&gt;```&lt;/p&gt; &lt;p&gt;from langchain_core.prompts import ChatPromptTemplate&lt;br/&gt; from langchain_core.runnables import RunnablePassthrough&lt;br/&gt; from langchain_core.output_parsers import StrOutputParser&lt;/p&gt; &lt;p&gt;from langchain.chat_models import ChatOpenAI &lt;/p&gt; &lt;p&gt;template = &amp;quot;&amp;quot;&amp;quot;You are an expert in the screenplay and able to find out any questions asked from the script, if you provide wrong information then an innocent person dies:&lt;br/&gt; {context}&lt;br/&gt; Question: {question}&lt;br/&gt; &amp;quot;&amp;quot;&amp;quot;&lt;br/&gt; prompt = ChatPromptTemplate.from_template(template)&lt;/p&gt; &lt;p&gt;model = ChatOpenAI(temperature = 0.1)&lt;br/&gt; chain = (&lt;br/&gt; {&amp;quot;context&amp;quot;: retriever, &amp;quot;question&amp;quot;: RunnablePassthrough()}&lt;br/&gt; | prompt&lt;br/&gt; | model&lt;br/&gt; | StrOutputParser()&lt;/p&gt; &lt;p&gt;``` &lt;/p&gt; &lt;p&gt;Here When I am going over the logs. The {context} is populated with irrelevant options and hence the prompt is not able to give the right results. Can someone please help?&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/nisshhhhhh&quot;&gt; /u/nisshhhhhh &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bnfpd0/can_someone_please_help_to_know_how_to_tune/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bnfpd0/can_someone_please_help_to_know_how_to_tune/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bnfpd0</id><link href="https://www.reddit.com/r/LangChain/comments/1bnfpd0/can_someone_please_help_to_know_how_to_tune/" /><updated>2024-03-25T14:56:29+00:00</updated><published>2024-03-25T14:56:29+00:00</published><title>Can someone please help to know how to tune context/retriever in the langchain?</title></entry><entry><author><name>/u/szcukg</name><uri>https://www.reddit.com/user/szcukg</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;m trying to understand what are typical workflows for users/teams especially in production when working with datasets that are &amp;gt; 10+ GB.&lt;br/&gt; Specifically, I have at least three data sources that I want to create embeddings for. Unstructured data documents, multiple tables in data warehouses, and other semi-structured data from APIs.&lt;br/&gt; Here is what I am faced with. Doing even medium-to-large scale data processing natively with LangChain is hard, just because python is slow for such scale. Yes, I can use Ray but that creates a lot more modules to manage and it&amp;#39;s already hard with LangChain code-base.&lt;br/&gt; So in-general how are people doing ingestion in conjunction with LangChain ? This maybe a mistake on my part, but I do not want to use LangChain for ingestion, it&amp;#39;s not meant imo for that problem. &lt;/p&gt; &lt;p&gt;Secondly, has anyone used Traces with LangSmith and can share experiences about it ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/szcukg&quot;&gt; /u/szcukg &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bnkgfb/best_way_to_do_indexing_and_pull_in_other_data/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bnkgfb/best_way_to_do_indexing_and_pull_in_other_data/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bnkgfb</id><link href="https://www.reddit.com/r/LangChain/comments/1bnkgfb/best_way_to_do_indexing_and_pull_in_other_data/" /><updated>2024-03-25T18:08:32+00:00</updated><published>2024-03-25T18:08:32+00:00</published><title>Best way to do indexing and pull in other data sources when working with LangChain and datasets &gt; 10+ GB. + Traces experience ?</title></entry><entry><author><name>/u/o3omoomin</name><uri>https://www.reddit.com/user/o3omoomin</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Let&amp;#39;s say the document is in Markdown format.&lt;/p&gt; &lt;p&gt;If the Markdown format is not properly divided into the body text, is this very bad to use as data?&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;Or what documents and formats are the best data?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/o3omoomin&quot;&gt; /u/o3omoomin &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bn2w00/how_important_is_the_content_of_the_documentation/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bn2w00/how_important_is_the_content_of_the_documentation/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bn2w00</id><link href="https://www.reddit.com/r/LangChain/comments/1bn2w00/how_important_is_the_content_of_the_documentation/" /><updated>2024-03-25T02:33:09+00:00</updated><published>2024-03-25T02:33:09+00:00</published><title>How important is the content of the documentation when implementing RAG?</title></entry><entry><author><name>/u/i_dont_care_about_vr</name><uri>https://www.reddit.com/user/i_dont_care_about_vr</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey there im looking create a robot which uses llm as way of interaction. I want the entire system to be local hosted using langchain (due to the option for customising promt as well as other parameters)&lt;/p&gt; &lt;p&gt;Im using mistral 7b gguf &lt;/p&gt; &lt;p&gt;But the tools i use require apu in open ai format and i dont know how host the model in a local server so that it can be used as a replacement for open ai api&lt;/p&gt; &lt;p&gt;So now im looking for a solution to host the model in a local server that can be used as replacement for open ai i have tried langserve but shows error that it isnt in open ai format &lt;/p&gt; &lt;p&gt;Can anyone please help me&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/i_dont_care_about_vr&quot;&gt; /u/i_dont_care_about_vr &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bnclwv/how_to_create_a_openai_compactible_local_server/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bnclwv/how_to_create_a_openai_compactible_local_server/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bnclwv</id><link href="https://www.reddit.com/r/LangChain/comments/1bnclwv/how_to_create_a_openai_compactible_local_server/" /><updated>2024-03-25T12:38:20+00:00</updated><published>2024-03-25T12:38:20+00:00</published><title>How to create a openai compactible local server using langchain</title></entry><entry><author><name>/u/redd-dev</name><uri>https://www.reddit.com/user/redd-dev</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey guys, using Langchain, does anyone have any example Python scripts of a central agent coordinating multi agents (ie. this is a multi agent framework rather than a multi tool framework).&lt;/p&gt; &lt;p&gt;I have googled around for this but can&amp;#39;t seem to find any.&lt;/p&gt; &lt;p&gt;Would really appreciate any help on this.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/redd-dev&quot;&gt; /u/redd-dev &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bn8l1r/examples_of_langchain_python_scripts_of_a_central/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bn8l1r/examples_of_langchain_python_scripts_of_a_central/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bn8l1r</id><link href="https://www.reddit.com/r/LangChain/comments/1bn8l1r/examples_of_langchain_python_scripts_of_a_central/" /><updated>2024-03-25T08:28:23+00:00</updated><published>2024-03-25T08:28:23+00:00</published><title>Examples of Langchain Python scripts of a central agent coordinating multi agents</title></entry><entry><author><name>/u/gibri29</name><uri>https://www.reddit.com/user/gibri29</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Does anyone know how to adjust the format instructions when using the structured chat agent? It avoids displaying the Observation and sometimes cuts out the Thought process despite indicating the format instructions in the prompt. I am trying to use this agent to connect with an SQL database.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/gibri29&quot;&gt; /u/gibri29 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bnd7yc/structured_chat_agent_formatting_help/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bnd7yc/structured_chat_agent_formatting_help/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bnd7yc</id><link href="https://www.reddit.com/r/LangChain/comments/1bnd7yc/structured_chat_agent_formatting_help/" /><updated>2024-03-25T13:07:33+00:00</updated><published>2024-03-25T13:07:33+00:00</published><title>Structured Chat Agent Formatting Help</title></entry></feed>