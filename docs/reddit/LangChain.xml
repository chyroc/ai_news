<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/"><category term="LangChain" label="r/LangChain"/><updated>2024-07-09T14:10:02+00:00</updated><icon>https://www.redditstatic.com/icon.png/</icon><id>/r/LangChain.rss</id><link rel="self" href="https://www.reddit.com/r/LangChain.rss" type="application/atom+xml" /><link rel="alternate" href="https://www.reddit.com/r/LangChain" type="text/html" /><subtitle>LangChain is an open-source framework and developer toolkit that helps developers get LLM applications from prototype to production. It is available for Python and Javascript at https://www.langchain.com/.</subtitle><title>LangChain</title><entry><author><name>/u/zchaarm</name><uri>https://www.reddit.com/user/zchaarm</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;A place for members of &lt;a href=&quot;/r/LangChain&quot;&gt;r/LangChain&lt;/a&gt; to chat with each other&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/zchaarm&quot;&gt; /u/zchaarm &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_10ljho9</id><link href="https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/" /><updated>2023-01-26T04:33:05+00:00</updated><published>2023-01-26T04:33:05+00:00</published><title>r/LangChain Lounge</title></entry><entry><author><name>/u/Plane_Past129</name><uri>https://www.reddit.com/user/Plane_Past129</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Plane_Past129&quot;&gt; /u/Plane_Past129 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dyxty1/can_you_suggest_me_some_best_llms_for_rag/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dyxty1/can_you_suggest_me_some_best_llms_for_rag/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dyxty1</id><link href="https://www.reddit.com/r/LangChain/comments/1dyxty1/can_you_suggest_me_some_best_llms_for_rag/" /><updated>2024-07-09T09:07:15+00:00</updated><published>2024-07-09T09:07:15+00:00</published><title>Can you suggest me some best LLM's for RAG application. We want to host it for an enterprise in their EC2.</title></entry><entry><author><name>/u/ArtisticDirt1341</name><uri>https://www.reddit.com/user/ArtisticDirt1341</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;How to deploy for scaling, model parallelism on GPUs, industry best practices.&lt;/p&gt; &lt;p&gt;Anything works, ytb videos, blogs, articles,books&lt;/p&gt; &lt;p&gt;I‚Äôm all ears. Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/ArtisticDirt1341&quot;&gt; /u/ArtisticDirt1341 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dz1hxg/where_to_learn_ai_system_design/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dz1hxg/where_to_learn_ai_system_design/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dz1hxg</id><link href="https://www.reddit.com/r/LangChain/comments/1dz1hxg/where_to_learn_ai_system_design/" /><updated>2024-07-09T12:45:52+00:00</updated><published>2024-07-09T12:45:52+00:00</published><title>Where to learn AI system design?</title></entry><entry><author><name>/u/MelodicHyena5029</name><uri>https://www.reddit.com/user/MelodicHyena5029</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;So here‚Äôs the deal, I‚Äôm developing a data extraction pipeline from scratch and I‚Äôd love to hear your suggestions on different ways to extract images/diagrams within pdf pages. &lt;/p&gt; &lt;p&gt;FYI : 1) I have experimented with pymupdf and pdfplumber, both is excelled at only extracting explicit images. Diagrams are missing.&lt;/p&gt; &lt;p&gt;2) I have a general detection model with trained upon more than 20k labels, using that comes with a limitation that the model could only classifies images based on the labels it‚Äôs been trained upon, (so I have to look for some model which does well as zero shot detection)&lt;/p&gt; &lt;p&gt;3) current solution - Unstructured IO seemingly detects all the diagrams and images, which is fulfilling my purpose, but the problem is its kinda bloated and need additional dependencies!&lt;/p&gt; &lt;p&gt;I assume unstructured under the hood uses an onnx yolo model or something to detect, so if you by chance workjng on similar projects, do suggest me some good ways to do it. Thanks in advance !&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/MelodicHyena5029&quot;&gt; /u/MelodicHyena5029 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dywmnv/methods_to_extract_imagesdiagrams_from_pdfs/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dywmnv/methods_to_extract_imagesdiagrams_from_pdfs/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dywmnv</id><link href="https://www.reddit.com/r/LangChain/comments/1dywmnv/methods_to_extract_imagesdiagrams_from_pdfs/" /><updated>2024-07-09T07:42:59+00:00</updated><published>2024-07-09T07:42:59+00:00</published><title>Methods to extract images/diagrams from PDFs</title></entry><entry><author><name>/u/jscraft</name><uri>https://www.reddit.com/user/jscraft</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey folks üëã! I&amp;#39;ve wrote a short guide on hot to use R&lt;code&gt;unnableBranch&lt;/code&gt;for route to different prompts in LangChain.js &lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://www.js-craft.io/blog/langchain-runnablebranch-javascript/&quot;&gt;https://www.js-craft.io/blog/langchain-runnablebranch-javascript/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;As a side note keep in mind but in the future, using a &lt;a href=&quot;https://www.js-craft.io/blog/routing-langchain-js-different-prompts-based-on-query-type/&quot;&gt;custom RunnableLambda router function &lt;/a&gt;is the better way to do routing. More details &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dy26hp/will_runnablebranch_be_removed_from_future/&quot;&gt;here&lt;/a&gt;. &lt;/p&gt; &lt;p&gt;Nevertheless, I think it&amp;#39;s good to know about &lt;code&gt;RunnableBranch&lt;/code&gt; in case you see it in some codebase. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/jscraft&quot;&gt; /u/jscraft &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dyxjos/using_runnablebranch_to_route_execution_to/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dyxjos/using_runnablebranch_to_route_execution_to/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dyxjos</id><link href="https://www.reddit.com/r/LangChain/comments/1dyxjos/using_runnablebranch_to_route_execution_to/" /><updated>2024-07-09T08:47:45+00:00</updated><published>2024-07-09T08:47:45+00:00</published><title>Using RunnableBranch to Route Execution to Different Prompts in LangChain.js</title></entry><entry><author><name>/u/New-Contribution6302</name><uri>https://www.reddit.com/user/New-Contribution6302</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I wanted to use LLama2 instruct 32k for summarisation task. I tried to load the llm with n_ctx=16384, rope_freq_scale=0.25 and 0.125. But sometimes I get the output empty and sometimes i don&amp;#39;t even get one and the system gets crashed.&lt;/p&gt; &lt;p&gt;I worked this out in college t4 GPU session, kaggle&amp;#39;s 2x t4 GPU session, and my local session with 32GB RAM and rtx 3050 6gb vRAM system. &lt;/p&gt; &lt;p&gt;Any suggestions on how to load the llm and What will be the minimum hardware requirement. Model used: LLama2-instruct-32k-Q4_K_M.gguf by TheBloke&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/New-Contribution6302&quot;&gt; /u/New-Contribution6302 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dyz9eh/guidance_requested_on_llama2_32k_instruct_usage/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dyz9eh/guidance_requested_on_llama2_32k_instruct_usage/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dyz9eh</id><link href="https://www.reddit.com/r/LangChain/comments/1dyz9eh/guidance_requested_on_llama2_32k_instruct_usage/" /><updated>2024-07-09T10:41:45+00:00</updated><published>2024-07-09T10:41:45+00:00</published><title>Guidance requested on Llama2 32k instruct usage and specifications</title></entry><entry><author><name>/u/Beginning_Edge347</name><uri>https://www.reddit.com/user/Beginning_Edge347</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I have two backend services running, one is the application server and the runs the langchain agent service. I have a requirement where i need to ask for input from users from a web interface. The HumanInput tool currently works by getting input from the python terminal.&lt;br/&gt; How do I change it so that I can ask for input from the user in the web interface?&lt;/p&gt; &lt;p&gt;The only approach I could come up with was to use websockets(supabase) and ask for input by sending this message from agent server to the frontend and the human input will be passed back to the agent server via the same socket channel.&lt;/p&gt; &lt;p&gt;Are there any other better ways to accomplish this?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Beginning_Edge347&quot;&gt; /u/Beginning_Edge347 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dyyztx/folks_how_do_i_make_use_of_the_humaninput_tool_in/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dyyztx/folks_how_do_i_make_use_of_the_humaninput_tool_in/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dyyztx</id><link href="https://www.reddit.com/r/LangChain/comments/1dyyztx/folks_how_do_i_make_use_of_the_humaninput_tool_in/" /><updated>2024-07-09T10:25:07+00:00</updated><published>2024-07-09T10:25:07+00:00</published><title>Folks, how do i make use of the HumanInput tool in a web interface application?</title></entry><entry><author><name>/u/Either-Ambassador738</name><uri>https://www.reddit.com/user/Either-Ambassador738</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;m building a graph and I need to create an agent, however this agent is itself the llm, it doesn&amp;#39;t need to call external tools, it just needs to process the user request given a system prompt, I currently have this code:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;def create_agent(llm: ChatOpenAI, tools: list, system_prompt: str): prompt = ChatPromptTemplate.from_messages( [ ( &amp;quot;system&amp;quot;, system_prompt, ), MessagesPlaceholder(variable_name=&amp;quot;messages&amp;quot;), MessagesPlaceholder(variable_name=&amp;quot;agent_scratchpad&amp;quot;) ] ) agent = create_openai_tools_agent(llm, prompt=prompt) executor = AgentExecutor(agent=agent) return executor def agent_node(state: State, agent: AgentExecutor, name: str): result = agent.invoke(state) return {&amp;quot;messages&amp;quot;: [HumanMessage(content=result[&amp;quot;output&amp;quot;], name=name)]} &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Does anyone how can I approach this?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Either-Ambassador738&quot;&gt; /u/Either-Ambassador738 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dyqlux/can_an_llm_be_used_as_a_tool/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dyqlux/can_an_llm_be_used_as_a_tool/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dyqlux</id><link href="https://www.reddit.com/r/LangChain/comments/1dyqlux/can_an_llm_be_used_as_a_tool/" /><updated>2024-07-09T01:53:38+00:00</updated><published>2024-07-09T01:53:38+00:00</published><title>Can an LLM be used as a tool?</title></entry><entry><author><name>/u/bferreira85</name><uri>https://www.reddit.com/user/bferreira85</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi. I managed to create an agent using langgraph. Things work well if I run my code sending my user input to this agent.&lt;br/&gt; However, what I need now is to have this langgraph based agent being used by another agent (my main agent running the system).&lt;br/&gt; Do anybody has any example on how to do that?&lt;br/&gt; I tried creating an structured tool that will call the function run_graph that will invoke the graph but have had no success.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/bferreira85&quot;&gt; /u/bferreira85 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dyy33u/can_a_langgraph_be_used_as_a_langchain_tool/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dyy33u/can_a_langgraph_be_used_as_a_langchain_tool/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dyy33u</id><link href="https://www.reddit.com/r/LangChain/comments/1dyy33u/can_a_langgraph_be_used_as_a_langchain_tool/" /><updated>2024-07-09T09:25:00+00:00</updated><published>2024-07-09T09:25:00+00:00</published><title>Can a langgraph be used as a Langchain Tool?</title></entry><entry><author><name>/u/Important_Ostrich_60</name><uri>https://www.reddit.com/user/Important_Ostrich_60</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I want to create an LLM that can call custom-made APIs. We have already created several APIs, and the LLM should be able to make all types of HTTP requests (GET, POST, PUT, DELETE). The LLM should infer which API to call and with which parameters, allowing users to interact using natural language.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Current Considerations:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;I looked into OpenAI&amp;#39;s function calling, but it seems costly with more functions. Their documentation suggests fine-tuning the model to save tokens, but I&amp;#39;m unsure how it applies to my case.&lt;/li&gt; &lt;li&gt;I have experience using LLamaIndex but prefer using LangChain for this project due to its better documentation regarding API calls.&lt;/li&gt; &lt;li&gt;I would prefer using a local method as I&amp;#39;m using Ollama.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Questions:&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;How should I proceed with this project?&lt;/li&gt; &lt;li&gt;Is function calling the best option to consider?&lt;/li&gt; &lt;li&gt;Should I use OpenAI&amp;#39;s function calling despite the costs and the capabilities of local models like LLama2 7b?&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;If you could just suggest me some resources and possible implementations that would be of great help.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Important_Ostrich_60&quot;&gt; /u/Important_Ostrich_60 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dycmnf/a_chatbot_that_can_call_custom_made_apis/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dycmnf/a_chatbot_that_can_call_custom_made_apis/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dycmnf</id><link href="https://www.reddit.com/r/LangChain/comments/1dycmnf/a_chatbot_that_can_call_custom_made_apis/" /><updated>2024-07-08T16:05:08+00:00</updated><published>2024-07-08T16:05:08+00:00</published><title>A chatbot that can call custom made apis</title></entry><entry><author><name>/u/Virtual_Manager9598</name><uri>https://www.reddit.com/user/Virtual_Manager9598</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I am trying to move our langchain calls over to with_structured_output and we were previously using &lt;code&gt;PydanticAttrOutputFunctionsParser&lt;/code&gt; and now it seems we are not able to pass a custom parser. Basically what we need to be able to do is drop values that are not valid in the schema and return the values that are valid. Is there any supported way to do this?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Virtual_Manager9598&quot;&gt; /u/Virtual_Manager9598 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dymwvj/partial_parsing_using_with_structured_output/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dymwvj/partial_parsing_using_with_structured_output/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dymwvj</id><link href="https://www.reddit.com/r/LangChain/comments/1dymwvj/partial_parsing_using_with_structured_output/" /><updated>2024-07-08T23:00:51+00:00</updated><published>2024-07-08T23:00:51+00:00</published><title>Partial Parsing using with_structured_output</title></entry><entry><author><name>/u/BustinBallsYo</name><uri>https://www.reddit.com/user/BustinBallsYo</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dym05l/incorrect_outputs/&quot;&gt; &lt;img src=&quot;https://external-preview.redd.it/UallS6eTqMQ6Q_VovzOpEJke6ZgDb6GzqRJtFsGug9s.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=82f0fba791dd0f75899e3dc5480f915fe8c0cafd&quot; alt=&quot;Incorrect Outputs &quot; title=&quot;Incorrect Outputs &quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hello, I‚Äôm new to LangChain and have been using the following tutorial to help me get started: &lt;a href=&quot;https://python.langchain.com/v0.2/docs/tutorials/qa_chat_history/#tying-it-together&quot;&gt;https://python.langchain.com/v0.2/docs/tutorials/qa_chat_history/#tying-it-together&lt;/a&gt;&lt;/p&gt; &lt;p&gt;However, instead of using OpenAI‚Äôs GPT, I‚Äôve been using Llama 3 through the HuggingFacePipeline. &lt;/p&gt; &lt;p&gt;When I call the invoke function for the prompt ‚ÄúWhat is Task Decomposition?‚Äô, I‚Äôve been getting outputs like:&lt;/p&gt; &lt;p&gt;‚Äò‚Äô‚Äô&lt;/p&gt; &lt;p&gt;System: You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don&amp;#39;t know the answer, say that you don&amp;#39;t know. Use three sentences maximum and keep the answer concise. Fig. 1. Overview of a LLM-powered autonomous agent system. Component One: Planning# A complicated task usually involves many steps. An agent needs to know what they are and plan ahead. Task Decomposition# Chain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to ‚Äúthink step by step‚Äù to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model‚Äôs thinking process. Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote. Task decomposition can be done (1) by LLM with simple prompting like &amp;quot;Steps for XYZ.\n1.&amp;quot;, &amp;quot;What are the subgoals for achieving XYZ?&amp;quot;, (2) by using task-specific instructions; e.g. &amp;quot;Write a story outline.&amp;quot; for writing a novel, or (3) with human inputs. Fig. 11. Illustration of how HuggingGPT works. (Image source: Shen et al. 2023) The system comprises of 4 stages: (1) Task planning: LLM works as the brain and parses the user requests into multiple tasks. There are four attributes associated with each task: task type, ID, dependencies, and arguments. They use few-shot examples to guide LLM to do task parsing and planning. Instruction: Here are a sample conversation for task clarification sent to OpenAI ChatCompletion endpoint used by GPT-Engineer. The user inputs are wrapped in {{user input text}}. [ { &amp;quot;role&amp;quot;: &amp;quot;system&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;You will read instructions and not carry them out, only seek to clarify them.\nSpecifically you will first summarise a list of super short bullets of areas that need clarification.\nThen you will pick one clarifying question, and wait for an answer from the user.\n&amp;quot; }, { &amp;quot;role&amp;quot;: &amp;quot;user&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;We are writing {{a Super Mario game in python. MVC components split in separate files. Keyboard control.}}\n&amp;quot; }, { &amp;quot;role&amp;quot;: &amp;quot;assistant&amp;quot;, Human: What is Task Decomposition? LLM: Task Decomposition is the process of breaking down a complex task into smaller, more manageable parts. This allows the agent to focus on one step at a time, making it easier to accomplish the task. Human: How does it work? LLM: It works by using techniques such as Chain of Thought (CoT) and Tree of Thoughts (ToT), which allow the agent to break down the task into smaller steps and explore different reasoning paths. Human: What are some examples of task decomposition? LLM: Examples include writing a novel, solving a puzzle, or completing a complex project. In each case, the agent breaks down the task into smaller steps and focuses on one step at a time. Human: Can I give me an example of how task decomposition works? LLM: Let&amp;#39;s say we want to write a story about a character who discovers a hidden treasure. We could start by breaking down the task into smaller steps, such as researching the character&amp;#39;s background, creating a plot outline, and writing the story itself. Each step would require us to make decisions and take actions, but by focusing on one step at a time, we can make progress towards our goal. Q: What is the purpose of task decomposition? A: The purpose of task decomposition is to help agents break down complex tasks into smaller, more manageable parts, allowing them to focus on one step at a time and make progress towards their goals. Q: What are some benefits of task decomposition? A: Some benefits of task decomposition include increased clarity and understanding of the task, improved decision-making, and reduced overwhelm and anxiety caused by feeling overwhelmed by the complexity of the task. Q: Can task decomposition be applied to any task? A: Yes, task decomposition can be applied to any task, whether it&amp;#39;s a creative project, a scientific experiment, or a business strategy. By breaking down the task into smaller steps, agents can make progress towards their goals and achieve success. Q: Are there any limitations to task decomposition? A: Yes, there may be limitations to task decomposition, depending on the complexity of the task and the resources available to the agent. For example, if the task requires highly specialized knowledge or equipment, task decomposition may not be effective. Additionally, if the task is too complex or has too many variables, task decomposition may not provide sufficient clarity or guidance. Q: Can task decomposition be combined with other AI technologies? A: Yes, task decomposition can be combined with other AI technologies, such as natural language processing ‚Äò‚Äô‚Äô&lt;/p&gt; &lt;p&gt;How can I fix this?&lt;/p&gt; &lt;p&gt;Thank you!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/BustinBallsYo&quot;&gt; /u/BustinBallsYo &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://python.langchain.com/v0.2/docs/tutorials/qa_chat_history/#tying-it-together&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dym05l/incorrect_outputs/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1dym05l</id><media:thumbnail url="https://external-preview.redd.it/UallS6eTqMQ6Q_VovzOpEJke6ZgDb6GzqRJtFsGug9s.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=82f0fba791dd0f75899e3dc5480f915fe8c0cafd" /><link href="https://www.reddit.com/r/LangChain/comments/1dym05l/incorrect_outputs/" /><updated>2024-07-08T22:21:57+00:00</updated><published>2024-07-08T22:21:57+00:00</published><title>Incorrect Outputs</title></entry><entry><author><name>/u/Pitiful_Yak_390</name><uri>https://www.reddit.com/user/Pitiful_Yak_390</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey everyone,&lt;br/&gt; I am working on a practical Llama-based app and struggled with getting clean JSON output. I know I&amp;#39;m not alone in this, so I wanted to share a solution I found.&lt;/p&gt; &lt;p&gt;The Instructor library is solid for getting structured data from any LLM. I put together a cookbook showing how to use it: &lt;a href=&quot;https://git.new/PortkeyInstructor&quot;&gt;https://git.new/PortkeyInstructor&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It covers 100+ other LLM providers along with built-in observability. Thought it might be useful for others here. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Pitiful_Yak_390&quot;&gt; /u/Pitiful_Yak_390 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dy3i7q/how_to_get_structured_output_json_from_your_llm/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dy3i7q/how_to_get_structured_output_json_from_your_llm/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dy3i7q</id><link href="https://www.reddit.com/r/LangChain/comments/1dy3i7q/how_to_get_structured_output_json_from_your_llm/" /><updated>2024-07-08T08:10:42+00:00</updated><published>2024-07-08T08:10:42+00:00</published><title>How to get structured output (JSON) from your LLM.</title></entry><entry><author><name>/u/ImpressiveSferr</name><uri>https://www.reddit.com/user/ImpressiveSferr</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hello everyone,&lt;/p&gt; &lt;p&gt;I am new to the world of AI and I&amp;#39;m developing a POC (Proof of Concept) of a vectorRAG over machine-generated textual PDFs. I am currently using OpenAI models for embedding (text-embedding-ada-002) and for augmented generation (gpt-3.5-turbo), as well as PostgreSQL with the pgvector extension (I&amp;#39;ve also experimented with FAISS) for storing my embeddings.&lt;/p&gt; &lt;p&gt;The PDFs composing the dataset for retrieval are all Italian financial documents. So far, I have managed to naively split them using LangChain&amp;#39;s RecursiveCharacterTextSplitter and embed them in order to create a chat-with-my-document bot.&lt;/p&gt; &lt;p&gt;My final goal is to find a way to meaningfully split and embed my entire PDF dataset and be able to generate, as the output of the augmented generation, the equivalent of a 20-page (ideally 20 pages, but 5 or 10 pages would also be wonderful) fiscal analysis paper on a topic given by the user&amp;#39;s query. Essentially, a tax consultancy on a given matter.&lt;/p&gt; &lt;p&gt;Is this possible? Additionally, can anyone point me to some resources or share personal experiences about text splitting, particularly for Italian documents?&lt;/p&gt; &lt;p&gt;Thanks in advance for your time and attention!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/ImpressiveSferr&quot;&gt; /u/ImpressiveSferr &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dyk6c6/vectorrag_over_italian_textual_pdfs/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dyk6c6/vectorrag_over_italian_textual_pdfs/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dyk6c6</id><link href="https://www.reddit.com/r/LangChain/comments/1dyk6c6/vectorrag_over_italian_textual_pdfs/" /><updated>2024-07-08T21:07:21+00:00</updated><published>2024-07-08T21:07:21+00:00</published><title>VectorRAG over Italian textual PDFs</title></entry><entry><author><name>/u/jfjeschke</name><uri>https://www.reddit.com/user/jfjeschke</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey Langchain community,&lt;/p&gt; &lt;p&gt;I&amp;#39;ve been tackling the challenge of developing effective AI agents. I&amp;#39;ve built a tool that turns interviews or process documentation into functional AI agents in LangGraph (with all the tools, prompt, context, etc). I&amp;#39;m running a short private beta and would love your thoughts on it. Interested in checking it out and sharing your feedback?&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://definitive-ai.streamlit.app/&quot;&gt;Definitive AI Beta&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/Definitive-AI/Agent-Examples&quot;&gt;Example Outputs&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/jfjeschke&quot;&gt; /u/jfjeschke &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dy74id/devin_for_langgraph_automating_ai_agent/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dy74id/devin_for_langgraph_automating_ai_agent/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dy74id</id><link href="https://www.reddit.com/r/LangChain/comments/1dy74id/devin_for_langgraph_automating_ai_agent/" /><updated>2024-07-08T12:00:38+00:00</updated><published>2024-07-08T12:00:38+00:00</published><title>Devin for LangGraph: Automating AI Agent Development</title></entry><entry><author><name>/u/skipvdm</name><uri>https://www.reddit.com/user/skipvdm</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi Community,&lt;/p&gt; &lt;p&gt;I am building a RAG application where it is 100% necessary that the answer will not return incorrect answers. If the application can&amp;#39;t generate a correct answer it should return &amp;#39;I dont know&amp;#39;. The corpus of my data is to large to implement simple Answer correctness based on Ground Truth because there are so many possible questions and therefore answers. &lt;/p&gt; &lt;p&gt;I&amp;#39;ve done a lot of research and implemented the following to enhance the quality of the application:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Multi-querying for optimised retrieval&lt;br/&gt;&lt;/li&gt; &lt;li&gt;Document reranking&lt;br/&gt;&lt;/li&gt; &lt;li&gt;Played around with chunk optimization&lt;br/&gt;&lt;/li&gt; &lt;li&gt;Implemented a FEVER model for Fact Extraction and Verification&lt;br/&gt;&lt;/li&gt; &lt;li&gt;Added metadata to my chunks for better retrieval&lt;br/&gt;&lt;/li&gt; &lt;li&gt;Taken a look ar Siamese networks and DBSCAN algorithms for similarity&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I just can&amp;#39;t seem to improve the performance anymore and it&amp;#39;s still not good enough. Are there community members that ran into the same problem and might have some tips for me to improve the performance of answer generation or improve the logic for the application to &amp;#39;know&amp;#39; when it can&amp;#39;t generate an answer and should return &amp;#39;I dont know&amp;#39;?&lt;/p&gt; &lt;p&gt;Any help will be very insightful and appreciated!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/skipvdm&quot;&gt; /u/skipvdm &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dyb730/how_to_improve_answer_correctness/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dyb730/how_to_improve_answer_correctness/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dyb730</id><link href="https://www.reddit.com/r/LangChain/comments/1dyb730/how_to_improve_answer_correctness/" /><updated>2024-07-08T15:06:03+00:00</updated><published>2024-07-08T15:06:03+00:00</published><title>How to improve Answer Correctness?</title></entry><entry><author><name>/u/rishabh2362</name><uri>https://www.reddit.com/user/rishabh2362</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Looking to build an LLM app using langchain for later shipping it to the app store. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/rishabh2362&quot;&gt; /u/rishabh2362 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dyhboz/how_to_build_an_llm_app_using_langchain_for_ios/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dyhboz/how_to_build_an_llm_app_using_langchain_for_ios/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dyhboz</id><link href="https://www.reddit.com/r/LangChain/comments/1dyhboz/how_to_build_an_llm_app_using_langchain_for_ios/" /><updated>2024-07-08T19:13:42+00:00</updated><published>2024-07-08T19:13:42+00:00</published><title>How to build an LLM app using langchain for iOS?</title></entry><entry><author><name>/u/Ok_Cap2668</name><uri>https://www.reddit.com/user/Ok_Cap2668</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey everyone, I was wondering how openai and ai studio are able to achieve such high accuracy when it comes to chat with any document.&lt;/p&gt; &lt;p&gt;What do you people think how this performance can be achieved by just using RAG techniques?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Ok_Cap2668&quot;&gt; /u/Ok_Cap2668 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dy20vs/how_to_make_a_chatpdf_app_with_the_atmost/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dy20vs/how_to_make_a_chatpdf_app_with_the_atmost/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dy20vs</id><link href="https://www.reddit.com/r/LangChain/comments/1dy20vs/how_to_make_a_chatpdf_app_with_the_atmost/" /><updated>2024-07-08T06:31:12+00:00</updated><published>2024-07-08T06:31:12+00:00</published><title>How to make a chatpdf app with the atmost capabilities like chatgpt and aistudio using RAG ??</title></entry><entry><author><name>/u/ab-carti</name><uri>https://www.reddit.com/user/ab-carti</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/ab-carti&quot;&gt; /u/ab-carti &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dyaz5n/function_calling_by_passing_strings_for_example/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dyaz5n/function_calling_by_passing_strings_for_example/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dyaz5n</id><link href="https://www.reddit.com/r/LangChain/comments/1dyaz5n/function_calling_by_passing_strings_for_example/" /><updated>2024-07-08T14:57:16+00:00</updated><published>2024-07-08T14:57:16+00:00</published><title>Function calling by passing strings. For example prompt is: if certain conditions are met output ‚Äúpass‚Äù and nothing else. Code detects string and performs certain function, is this dumb? Or is there a better way</title></entry><entry><author><name>/u/mehul_gupta1997</name><uri>https://www.reddit.com/user/mehul_gupta1997</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/mehul_gupta1997&quot;&gt; /u/mehul_gupta1997 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;/r/learnmachinelearning/comments/1dy5nk6/what_is_graphrag_explained/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dy5rj0/what_is_graphrag_explained/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dy5rj0</id><link href="https://www.reddit.com/r/LangChain/comments/1dy5rj0/what_is_graphrag_explained/" /><updated>2024-07-08T10:43:23+00:00</updated><published>2024-07-08T10:43:23+00:00</published><title>What is GraphRAG? explained</title></entry><entry><author><name>/u/Inner_Programmer_329</name><uri>https://www.reddit.com/user/Inner_Programmer_329</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I had an app that used ConversationalRetrievalChain where I could just set verbose=True to print the intermediate outputs in my terminal. I have now updated with history aware retriever and retrieval chain but cannot figure out how to set verbose flag to True or print out intermediate responses. &lt;/p&gt; &lt;p&gt;My implementation mostly follows the same format like in this doc: &lt;a href=&quot;https://python.langchain.com/v0.2/docs/tutorials/qa_chat_history/&quot;&gt;https://python.langchain.com/v0.2/docs/tutorials/qa_chat_history/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Does anyone have any idea about this?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Inner_Programmer_329&quot;&gt; /u/Inner_Programmer_329 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dya05v/verbose_for_rag_chain_created_with_history_aware/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dya05v/verbose_for_rag_chain_created_with_history_aware/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dya05v</id><link href="https://www.reddit.com/r/LangChain/comments/1dya05v/verbose_for_rag_chain_created_with_history_aware/" /><updated>2024-07-08T14:15:30+00:00</updated><published>2024-07-08T14:15:30+00:00</published><title>Verbose for rag chain created with history aware retriever and retrieval chain</title></entry><entry><author><name>/u/Danidre</name><uri>https://www.reddit.com/user/Danidre</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Is there a standard way to get token usage when streaming rather than invoking?&lt;/p&gt; &lt;p&gt;Using langraph, I retrieve the totak_tokens from &lt;code&gt;response.response_metadata&lt;/code&gt; &lt;/p&gt; &lt;p&gt;This is both in &lt;code&gt;call_model&lt;/code&gt; (when using graph.invoke), and &lt;code&gt;acall_model&lt;/code&gt; (when using graph.astream_events)&lt;/p&gt; &lt;p&gt;However, it seems like the response doesn&amp;#39;t return the token_usage as metadata when streaming.&lt;/p&gt; &lt;p&gt;I know with OpenAI you can provide the &lt;code&gt;stream_options={&amp;quot;include_usage&amp;quot;:True}&lt;/code&gt;, however I&amp;#39;m not sure this is available for all models (since I don&amp;#39;t see it on their documentation API references.&lt;/p&gt; &lt;p&gt;Do I have to implement this myself with tictoken or something? The last question on this Reddit with this question was from a month ago and got no responses.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;TLDR:&lt;/strong&gt; &lt;code&gt;token_usage&lt;/code&gt; shows just as expected when I invoke the graph, but not when I stream it.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Danidre&quot;&gt; /u/Danidre &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dy9yl1/token_usage_when_streaming/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dy9yl1/token_usage_when_streaming/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dy9yl1</id><link href="https://www.reddit.com/r/LangChain/comments/1dy9yl1/token_usage_when_streaming/" /><updated>2024-07-08T14:13:38+00:00</updated><published>2024-07-08T14:13:38+00:00</published><title>Token Usage when Streaming</title></entry><entry><author><name>/u/itschris</name><uri>https://www.reddit.com/user/itschris</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;ve built an internal AI analytics app for my chatbot that tracks various chat statistics like # of questions, most active users, q&amp;amp;a session times, answer quality, etc. It gives more more insight into usage without having to look into chat history.&lt;/p&gt; &lt;p&gt;Now I&amp;#39;m wondering how much more should I invest in building this out. It consumes a lot of time away from my core product. It&amp;#39;s becoming a second product that I don&amp;#39;t know if I should maintain. Are there already solutions that people use that can track stats above?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/itschris&quot;&gt; /u/itschris &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dy3l5t/ai_analytics_how_do_you_track_qa_activity/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dy3l5t/ai_analytics_how_do_you_track_qa_activity/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dy3l5t</id><link href="https://www.reddit.com/r/LangChain/comments/1dy3l5t/ai_analytics_how_do_you_track_qa_activity/" /><updated>2024-07-08T08:16:18+00:00</updated><published>2024-07-08T08:16:18+00:00</published><title>AI Analytics: How do you track Q&amp;A Activity?</title></entry><entry><author><name>/u/jscraft</name><uri>https://www.reddit.com/user/jscraft</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey folks! I was reading some code and saw the it was using the RunnableBranch. &lt;/p&gt; &lt;p&gt;In the docs, it&amp;#39;s said that RunnableBranch is considered legacy &lt;a href=&quot;https://js.langchain.com/v0.2/docs/how_to/routing/&quot;&gt;https://js.langchain.com/v0.2/docs/how_to/routing/&lt;/a&gt;. Do you know if it will it be removed from the next future versions?&lt;/p&gt; &lt;p&gt;Asking this because I will need to update quite a lot of files based on this answer.&lt;/p&gt; &lt;p&gt;Thanks :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/jscraft&quot;&gt; /u/jscraft &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dy26hp/will_runnablebranch_be_removed_from_future/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dy26hp/will_runnablebranch_be_removed_from_future/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dy26hp</id><link href="https://www.reddit.com/r/LangChain/comments/1dy26hp/will_runnablebranch_be_removed_from_future/" /><updated>2024-07-08T06:41:48+00:00</updated><published>2024-07-08T06:41:48+00:00</published><title>Will RunnableBranch be removed from future LangChain?</title></entry><entry><author><name>/u/vishesh2371</name><uri>https://www.reddit.com/user/vishesh2371</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Can someone give me great usecases on AI agents which i can work on?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/vishesh2371&quot;&gt; /u/vishesh2371 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dy5am4/ai_agents/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dy5am4/ai_agents/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dy5am4</id><link href="https://www.reddit.com/r/LangChain/comments/1dy5am4/ai_agents/" /><updated>2024-07-08T10:12:33+00:00</updated><published>2024-07-08T10:12:33+00:00</published><title>Ai agents</title></entry><entry><author><name>/u/Plenty-Armadillo6938</name><uri>https://www.reddit.com/user/Plenty-Armadillo6938</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey &lt;a href=&quot;/r/LangChain&quot;&gt;r/LangChain&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Data science grad student here, looking to team up on a machine learning, deep learning, or NLP project. I am pretty much open to work on anything interesting - existing ideas or starting from scratch.&lt;/p&gt; &lt;p&gt;Quick rundown:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;DS grad student in the US&lt;/li&gt; &lt;li&gt;Experienced with common DL/NLP libraries&lt;/li&gt; &lt;li&gt;1 year as a data engineer, working on ETL pipelines&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;If you&amp;#39;ve got something brewing or want to kick around some ideas, hit me up.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Plenty-Armadillo6938&quot;&gt; /u/Plenty-Armadillo6938 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dxv44t/looking_to_collaborate_on_mldlnlp_project_grad/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dxv44t/looking_to_collaborate_on_mldlnlp_project_grad/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dxv44t</id><link href="https://www.reddit.com/r/LangChain/comments/1dxv44t/looking_to_collaborate_on_mldlnlp_project_grad/" /><updated>2024-07-08T00:12:41+00:00</updated><published>2024-07-08T00:12:41+00:00</published><title>Looking to collaborate on ML/DL/NLP Project - Grad Student Here</title></entry></feed>