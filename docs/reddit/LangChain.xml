<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/"><category term="LangChain" label="r/LangChain"/><updated>2024-07-05T17:09:18+00:00</updated><icon>https://www.redditstatic.com/icon.png/</icon><id>/r/LangChain.rss</id><link rel="self" href="https://www.reddit.com/r/LangChain.rss" type="application/atom+xml" /><link rel="alternate" href="https://www.reddit.com/r/LangChain" type="text/html" /><subtitle>LangChain is an open-source framework and developer toolkit that helps developers get LLM applications from prototype to production. It is available for Python and Javascript at https://www.langchain.com/.</subtitle><title>LangChain</title><entry><author><name>/u/zchaarm</name><uri>https://www.reddit.com/user/zchaarm</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;A place for members of &lt;a href=&quot;/r/LangChain&quot;&gt;r/LangChain&lt;/a&gt; to chat with each other&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/zchaarm&quot;&gt; /u/zchaarm &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_10ljho9</id><link href="https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/" /><updated>2023-01-26T04:33:05+00:00</updated><published>2023-01-26T04:33:05+00:00</published><title>r/LangChain Lounge</title></entry><entry><author><name>/u/macxgaming</name><uri>https://www.reddit.com/user/macxgaming</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi!&lt;/p&gt; &lt;p&gt;I&amp;#39;m working on a RAG system for my company where we can use it to search through our internal wiki page.&lt;br/&gt; My system is nearly in a releasable state and finds the correct information 90% of the times, and I&amp;#39;m happy about it, but I&amp;#39;m constantly thinking, can I make it better?&lt;/p&gt; &lt;p&gt;I&amp;#39;ve made a custom scraper for our wiki, we&amp;#39;re using an older version of MediaWiki.&lt;br/&gt; The scraper I&amp;#39;ve made is basically extracting all sections out into its own &amp;quot;document&amp;quot; and then sending it into qdrant vector database.&lt;br/&gt; That means that in the vector database, it doesn&amp;#39;t have a full wiki page but rather a cut up version to make it easier for the search query to hit something right. But I feel like this is kinda wrong?&lt;/p&gt; &lt;p&gt;Whenever you send in your query to the backend, it&amp;#39;ll then search for the 10 documents matching and then reranking with BAAI/bge-reranker-large. Then the context is being sent to Llama3:8b with your question in mind.&lt;br/&gt; This means that Llama3 will never get a fully contextual article, since the vectors are only smaller sections from the full page.&lt;/p&gt; &lt;p&gt;What could be done do make this better in the end? The one thing I see as an issue here, is that it will never know anything about the rest of the full page, but if it has the full page, it feels like Llama3 get overwhelmed by the data and then craps out.&lt;/p&gt; &lt;p&gt;We have ~258 articles and that&amp;#39;s resulting in about 1488 points in qdrant.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/macxgaming&quot;&gt; /u/macxgaming &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dvr774/what_is_the_best_approach_to_achieve_a_better/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dvr774/what_is_the_best_approach_to_achieve_a_better/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dvr774</id><link href="https://www.reddit.com/r/LangChain/comments/1dvr774/what_is_the_best_approach_to_achieve_a_better/" /><updated>2024-07-05T06:17:40+00:00</updated><published>2024-07-05T06:17:40+00:00</published><title>What is the best approach to achieve a better performant RAG?</title></entry><entry><author><name>/u/Front-Show7358</name><uri>https://www.reddit.com/user/Front-Show7358</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Potentially dumb question lol. Basically when I run my RAG, it takes a long time to process all the documents that it will then retrieve. Is there a way to just save off the model after it is done reading the documents so that when you run it again, it can skip that step? Similar to how a fine-tuned model would work? It doesn&amp;#39;t really make sense in my head, but I haven&amp;#39;t been able to find a concrete answer to this so I want to be sure.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Front-Show7358&quot;&gt; /u/Front-Show7358 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dw1mk2/is_there_a_way_to_save_a_rag_after_it_has_read/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dw1mk2/is_there_a_way_to_save_a_rag_after_it_has_read/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dw1mk2</id><link href="https://www.reddit.com/r/LangChain/comments/1dw1mk2/is_there_a_way_to_save_a_rag_after_it_has_read/" /><updated>2024-07-05T16:10:29+00:00</updated><published>2024-07-05T16:10:29+00:00</published><title>Is there a way to save a RAG after it has read its documents?</title></entry><entry><author><name>/u/frellothings</name><uri>https://www.reddit.com/user/frellothings</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I want to deploy a Huggingface model in Sagemaker with a context size of around 25-32k. I am having trouble finding a suitable model that performs well with this context size. The model&amp;#39;s task will be to map raw data to a target framework. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/frellothings&quot;&gt; /u/frellothings &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dvs05a/deploy_hugging_face_model_in_sagemaker/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dvs05a/deploy_hugging_face_model_in_sagemaker/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dvs05a</id><link href="https://www.reddit.com/r/LangChain/comments/1dvs05a/deploy_hugging_face_model_in_sagemaker/" /><updated>2024-07-05T07:11:15+00:00</updated><published>2024-07-05T07:11:15+00:00</published><title>Deploy Hugging Face model in Sagemaker</title></entry><entry><author><name>/u/gibriyagi</name><uri>https://www.reddit.com/user/gibriyagi</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I would like to use Postgres with pgvector but could not figure out a way to do hybrid search using bm25.&lt;/p&gt; &lt;p&gt;Anyone using Postgres only for RAG? Do you do hybrid search? If not do you combine it with something else?&lt;/p&gt; &lt;p&gt;Would love to hear your experiences.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/gibriyagi&quot;&gt; /u/gibriyagi &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dvdnzc/hybrid_search_with_postgres/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dvdnzc/hybrid_search_with_postgres/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dvdnzc</id><link href="https://www.reddit.com/r/LangChain/comments/1dvdnzc/hybrid_search_with_postgres/" /><updated>2024-07-04T18:23:03+00:00</updated><published>2024-07-04T18:23:03+00:00</published><title>Hybrid search with Postgres</title></entry><entry><author><name>/u/ExplorerTechnical808</name><uri>https://www.reddit.com/user/ExplorerTechnical808</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;like title. Thanks in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/ExplorerTechnical808&quot;&gt; /u/ExplorerTechnical808 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dvrv8g/any_good_resourceguide_about_how_to_do_rag_on_a/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dvrv8g/any_good_resourceguide_about_how_to_do_rag_on_a/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dvrv8g</id><link href="https://www.reddit.com/r/LangChain/comments/1dvrv8g/any_good_resourceguide_about_how_to_do_rag_on_a/" /><updated>2024-07-05T07:01:51+00:00</updated><published>2024-07-05T07:01:51+00:00</published><title>Any good resource/guide about how to do RAG on a codebase? (e.g. Github repo)</title></entry><entry><author><name>/u/Volodymyr_steax</name><uri>https://www.reddit.com/user/Volodymyr_steax</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;My question might be a bit basic, but Iâm new to all of this and eager to learn.&lt;/p&gt; &lt;p&gt;I have a basic setup where I initialize an LLM using vLLM with Langchain RAG and the Llama model (specifically, llama2-13b-chat-hf). Hereâs what I do:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;I define a system prompt and an instruction f&lt;/li&gt; &lt;li&gt;I create an &lt;code&gt;llm_chain&lt;/code&gt;&lt;/li&gt; &lt;li&gt;I then run the chain with &lt;code&gt;llm_chain.run(text)&lt;/code&gt; , which works for a single input.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I have build an app with FastAPI. Previously I used asyncio method to handle multiple request to llm, but with each new request it become slower in response. So I decide to use vLLM method, but I got a problem now how to provide parallel or concurrent requests to vLLM when I have dealing with dozen or more users. Is there a way to call &lt;code&gt;run&lt;/code&gt; in parallel for several inputs and receive valid results for each input?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Volodymyr_steax&quot;&gt; /u/Volodymyr_steax &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dvrj1k/concurrentparallel_requests_with_vllm/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dvrj1k/concurrentparallel_requests_with_vllm/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dvrj1k</id><link href="https://www.reddit.com/r/LangChain/comments/1dvrj1k/concurrentparallel_requests_with_vllm/" /><updated>2024-07-05T06:39:27+00:00</updated><published>2024-07-05T06:39:27+00:00</published><title>Concurrent/parallel requests with vLLM</title></entry><entry><author><name>/u/meamysace</name><uri>https://www.reddit.com/user/meamysace</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;m searching for a tool that allows users to compare outputs generated by several LLMs using just one prompt. While I understand that LangChain could potentially enable building such a solution locally, I&amp;#39;m curious if any existing products offer this functionality.&lt;/p&gt; &lt;p&gt;I&amp;#39;m weary of manually inputting the same prompt across different models like GPT, Claude, Bard, and Perplexity to cross-reference answers and verify accuracy. Any recommendations or insights would be greatly appreciated! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/meamysace&quot;&gt; /u/meamysace &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dvaenf/tool_for_comparing_outputs_of_multiple_llms_from/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dvaenf/tool_for_comparing_outputs_of_multiple_llms_from/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dvaenf</id><link href="https://www.reddit.com/r/LangChain/comments/1dvaenf/tool_for_comparing_outputs_of_multiple_llms_from/" /><updated>2024-07-04T16:02:08+00:00</updated><published>2024-07-04T16:02:08+00:00</published><title>Tool for Comparing Outputs of Multiple LLMs from Single Prompts</title></entry><entry><author><name>/u/QuasiEvil</name><uri>https://www.reddit.com/user/QuasiEvil</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;ve been playing around with GPT4All and langchain, for which there is a minimal demo here:&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://python.langchain.com/v0.2/docs/integrations/llms/gpt4all/&quot;&gt;https://python.langchain.com/v0.2/docs/integrations/llms/gpt4all/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;In this demo, they invoke the following:&lt;/p&gt; &lt;p&gt;&lt;code&gt;from langchain_core.callbacks import StreamingStdOutCallbackHandler&lt;/code&gt;&lt;/p&gt; &lt;p&gt;From the API, it states that this only works with LLMs that support streaming. According to the integrations page:&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://python.langchain.com/v0.2/docs/integrations/llms/&quot;&gt;https://python.langchain.com/v0.2/docs/integrations/llms/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;gpt4all does NOT support streaming. So I&amp;#39;m confused - what gives with this demo?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/QuasiEvil&quot;&gt; /u/QuasiEvil &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dvfs2b/beginner_here_found_something_confusing/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dvfs2b/beginner_here_found_something_confusing/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dvfs2b</id><link href="https://www.reddit.com/r/LangChain/comments/1dvfs2b/beginner_here_found_something_confusing/" /><updated>2024-07-04T19:56:38+00:00</updated><published>2024-07-04T19:56:38+00:00</published><title>Beginner here: found something confusing</title></entry><entry><author><name>/u/muditjps</name><uri>https://www.reddit.com/user/muditjps</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dvhvam/hey_rlangchain_weve_created_an_app_template_for/&quot;&gt; &lt;img src=&quot;https://external-preview.redd.it/6vdYm5zqVtm_CFLlTGc1uWkGnd-hhEdr4D2wHuVz3iQ.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b991cdf7470fc68f086bd51039bd6575a8817deb&quot; alt=&quot;Hey r/langchain, we've created an app template for multimodal RAG (MM-RAG) using GPT4o and Pathway. The incremental indexing pipeline parses tables as images, explains them in detail, and saves the table content with the document chunk. This outperforms traditional RAG methods. More in the link.&quot; title=&quot;Hey r/langchain, we've created an app template for multimodal RAG (MM-RAG) using GPT4o and Pathway. The incremental indexing pipeline parses tables as images, explains them in detail, and saves the table content with the document chunk. This outperforms traditional RAG methods. More in the link.&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/muditjps&quot;&gt; /u/muditjps &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://pathway.com/developers/templates/multimodal-rag&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dvhvam/hey_rlangchain_weve_created_an_app_template_for/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1dvhvam</id><media:thumbnail url="https://external-preview.redd.it/6vdYm5zqVtm_CFLlTGc1uWkGnd-hhEdr4D2wHuVz3iQ.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=b991cdf7470fc68f086bd51039bd6575a8817deb" /><link href="https://www.reddit.com/r/LangChain/comments/1dvhvam/hey_rlangchain_weve_created_an_app_template_for/" /><updated>2024-07-04T21:30:21+00:00</updated><published>2024-07-04T21:30:21+00:00</published><title>Hey r/langchain, we've created an app template for multimodal RAG (MM-RAG) using GPT4o and Pathway. The incremental indexing pipeline parses tables as images, explains them in detail, and saves the table content with the document chunk. This outperforms traditional RAG methods. More in the link.</title></entry><entry><author><name>/u/liljuden</name><uri>https://www.reddit.com/user/liljuden</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi there,&lt;/p&gt; &lt;p&gt;I am currently working on building a chatbot for internal use in my company. I have developed a relatively classic RAG framework and now want to include a knowledge graph. I have read several papers on this topic, but I am confused about how to actually &amp;quot;activate&amp;quot; a graph in the RAG flow.&lt;/p&gt; &lt;p&gt;So far, I have found different approaches based on extracting entities and relationships from chunks to generate community summaries of the related entities. This process occurs in the offline stage. At least, that is what I have tried to do. I am wondering how to activate this correctly. Currently, I match entities from the input with summaries as the flow runs, but I have the impression that others use the graph aspect differently, possibly using a function to inject relevant context into the LLM.&lt;/p&gt; &lt;p&gt;Can you help me understand this better?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/liljuden&quot;&gt; /u/liljuden &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1duz8qc/how_to_incorporate_a_knowledge_graph_in_rag/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1duz8qc/how_to_incorporate_a_knowledge_graph_in_rag/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1duz8qc</id><link href="https://www.reddit.com/r/LangChain/comments/1duz8qc/how_to_incorporate_a_knowledge_graph_in_rag/" /><updated>2024-07-04T05:33:58+00:00</updated><published>2024-07-04T05:33:58+00:00</published><title>How to incorporate a knowledge graph in RAG</title></entry><entry><author><name>/u/imharesh20</name><uri>https://www.reddit.com/user/imharesh20</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;m working with Langchain to build a tool that calls an agent. Currently, I&amp;#39;m passing the chat history as an input variable to the agent. However, I&amp;#39;ve encountered an issue where the agent doesn&amp;#39;t always seem to utilize the history data to answer questions consistently. This is especially problematic when users have queries spaced out over 10â15 days.&lt;/p&gt; &lt;p&gt;Is there a more efficient way to ensure the agent consistently remembers all chat history and context over multiple sessions? What approach or best practices should I follow to address this issue?&lt;/p&gt; &lt;p&gt;Thanks in advance for your guidance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/imharesh20&quot;&gt; /u/imharesh20 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dv7e89/passing_chat_history_to_langchain_tool_calling/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dv7e89/passing_chat_history_to_langchain_tool_calling/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dv7e89</id><link href="https://www.reddit.com/r/LangChain/comments/1dv7e89/passing_chat_history_to_langchain_tool_calling/" /><updated>2024-07-04T13:49:43+00:00</updated><published>2024-07-04T13:49:43+00:00</published><title>Passing Chat History to Langchain Tool Calling Agent</title></entry><entry><author><name>/u/Mediocre-Card8046</name><uri>https://www.reddit.com/user/Mediocre-Card8046</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;I set up Mixtral 8x22B on Azure AI/Machine Learning and now want to use it with Langchain. I have difficulties with the format I am getting, e.g. a ChatOpenAI response looks like this:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;from langchain_openai import ChatOpenAI llmm = ChatOpenAI() llmm.invoke(&amp;quot;Hallo&amp;quot;) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;code&gt;AIMessage(content=&amp;#39;Hallo! Wie kann ich Ihnen helfen?&amp;#39;, response_metadata={&amp;#39;token_usage&amp;#39;: {&amp;#39;completion_tokens&amp;#39;: 8, &amp;#39;prompt_tokens&amp;#39;: 8, &amp;#39;total_tokens&amp;#39;: 16}, &amp;#39;model_name&amp;#39;: &amp;#39;gpt-3.5-turbo&amp;#39;, &amp;#39;system_fingerprint&amp;#39;: None, &amp;#39;finish_reason&amp;#39;: &amp;#39;stop&amp;#39;, &amp;#39;logprobs&amp;#39;: None}, id=&amp;#39;r&amp;#39;)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;This is how it looks when I am loading Mixtral 8x22B with AzureMLChatOnlineEndpoint:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;from langchain_community.chat_models.azureml_endpoint import AzureMLChatOnlineEndpoint from langchain_community.chat_models.azureml_endpoint import ( AzureMLEndpointApiType, CustomOpenAIChatContentFormatter, ) from langchain_core.messages import HumanMessage chat = AzureMLChatOnlineEndpoint( endpoint_url=&amp;quot;...&amp;quot;, endpoint_api_type=AzureMLEndpointApiType.dedicated, endpoint_api_key=&amp;quot;...&amp;quot;, content_formatter=CustomOpenAIChatContentFormatter(), ) chat.invoke(&amp;quot;Hallo&amp;quot;) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;code&gt;BaseMessage(content=&amp;#39;Hallo, ich bin ein deutscher Sprachassistent. Was kann ich fÃ¼r&amp;#39;, type=&amp;#39;assistant&amp;#39;, id=&amp;#39;run-23&amp;#39;)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;So with the Mixtral model the output seems to be truncated and also the format is different (BaseMessage vs. AIMessage). How can I change this to make it work just like an ChatOpenAI model?&lt;/p&gt; &lt;p&gt;In my application I want to easily switch between these two models.&lt;/p&gt; &lt;p&gt;Thanks in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Mediocre-Card8046&quot;&gt; /u/Mediocre-Card8046 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dv64ip/load_llm_mixtral_8x22b_from_azure_ai_endpoint_as/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dv64ip/load_llm_mixtral_8x22b_from_azure_ai_endpoint_as/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dv64ip</id><link href="https://www.reddit.com/r/LangChain/comments/1dv64ip/load_llm_mixtral_8x22b_from_azure_ai_endpoint_as/" /><updated>2024-07-04T12:47:41+00:00</updated><published>2024-07-04T12:47:41+00:00</published><title>Load LLM (Mixtral 8x22B) from Azure AI endpoint as Langchain Model</title></entry><entry><author><name>/u/ab-carti</name><uri>https://www.reddit.com/user/ab-carti</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Iâm developing an app that creates a knowledge base based on transcripts of YouTube videos. And I need a way to have the LLM recognize where the transcript came from, I have the data I just donât know how to implement it effectively &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/ab-carti&quot;&gt; /u/ab-carti &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dv5p0g/how_do_i_add_meta_data_to_pinecone_documents_i/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dv5p0g/how_do_i_add_meta_data_to_pinecone_documents_i/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dv5p0g</id><link href="https://www.reddit.com/r/LangChain/comments/1dv5p0g/how_do_i_add_meta_data_to_pinecone_documents_i/" /><updated>2024-07-04T12:24:56+00:00</updated><published>2024-07-04T12:24:56+00:00</published><title>How do I add meta data to Pinecone documents, I want to maintain overlap between chunks so I donât wanna replace first words</title></entry><entry><author><name>/u/taskade</name><uri>https://www.reddit.com/user/taskade</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1duy3hj/new_document_loader_for_taskade_langchain/&quot;&gt; &lt;img src=&quot;https://external-preview.redd.it/UallS6eTqMQ6Q_VovzOpEJke6ZgDb6GzqRJtFsGug9s.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=82f0fba791dd0f75899e3dc5480f915fe8c0cafd&quot; alt=&quot;New Document Loader for Taskade | ð¦ï¸ð Langchain&quot; title=&quot;New Document Loader for Taskade | ð¦ï¸ð Langchain&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/taskade&quot;&gt; /u/taskade &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://js.langchain.com/v0.2/docs/integrations/document_loaders/web_loaders/taskade/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1duy3hj/new_document_loader_for_taskade_langchain/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1duy3hj</id><media:thumbnail url="https://external-preview.redd.it/UallS6eTqMQ6Q_VovzOpEJke6ZgDb6GzqRJtFsGug9s.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=82f0fba791dd0f75899e3dc5480f915fe8c0cafd" /><link href="https://www.reddit.com/r/LangChain/comments/1duy3hj/new_document_loader_for_taskade_langchain/" /><updated>2024-07-04T04:24:50+00:00</updated><published>2024-07-04T04:24:50+00:00</published><title>New Document Loader for Taskade | ð¦ï¸ð Langchain</title></entry><entry><author><name>/u/Mediocre-Card8046</name><uri>https://www.reddit.com/user/Mediocre-Card8046</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;I built a CRAG application and now want to further improve it. As a fist step I would like to check if the given question is a followup question of a previous question or not. If it is, then I want to use my messages history to create a new question based on the context in the chat history and the actual question. This is similar to the &amp;quot;history_aware_retriever&amp;quot; from Langchain.&lt;/p&gt; &lt;p&gt;However I am not satisfied with the classification of my model, as it often returns &amp;quot;False&amp;quot; even if it is a followup question. So is there maybe a more elegant way of doing this or would you improve my prompt?&lt;/p&gt; &lt;p&gt;Heres the function within my Langgraph app:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;class CheckFollowupQuestion(BaseModel): &amp;quot;&amp;quot;&amp;quot;Bool Werte um zu bestimmen, ob die Frage auf eine zuvor gestellte Frage aufbaut.&amp;quot;&amp;quot;&amp;quot; score: str = Field( description=&amp;quot;Die Frage bezieht sich auf eine vorherige Antwort oder zuvor gestellte Frage, &amp;#39;True&amp;#39; oder &amp;#39;False&amp;#39;&amp;quot; ) def followup_question_classifier(state: AgentState): messages = state[&amp;quot;messages&amp;quot;][-5:] print(f&amp;quot;MESSAGES (in follow up): {messages}&amp;quot;) question = state[&amp;quot;question&amp;quot;] system = &amp;quot;&amp;quot;&amp;quot;&amp;lt;s&amp;gt;[INST] You assess whether the user&amp;#39;s question is a follow-up question or not. For this, you get questions from the chat history and assess whether the question builds on a question or answer from the chat history or not.\n Evaluate with &amp;#39;True&amp;#39; if it is a typical follow-up question. Also evaluate with &amp;#39;True&amp;#39; if it seems that the question refers to a previous answer. \n Evaluate with &amp;#39;False&amp;#39; if it is a normal question, or if the question has nothing to do with the chat history. Here is an example:\n\n Example of a &amp;#39;False&amp;#39; evaluation:\n Question: &amp;#39;How much does a kebab currently cost?&amp;#39;\n Chat history: [HumanMessage(content=&amp;#39;Hello, I am Max, who are you?&amp;#39;, id=&amp;#39;8&amp;#39;), HumanMessage(content=&amp;#39;What was my name?&amp;#39;, id=&amp;#39;7&amp;#39;), HumanMessage(content=&amp;#39;Name exactly one advantage of Multicloud.&amp;#39;, id=&amp;#39;f&amp;#39;)]\n Your evaluation: &amp;#39;False&amp;#39;. Reason: The questions from the chat history have nothing to do with the question asked.\n\n Example of a &amp;#39;True&amp;#39; evaluation: Question: &amp;#39;And how warm will it be there tomorrow?&amp;#39; Chat history: [HumanMessage(content=&amp;#39;Where is Munich located?&amp;#39;, id=&amp;#39;8&amp;#39;), HumanMessage(content=&amp;#39;Which dialect is spoken in Munich?&amp;#39;, id=&amp;#39;7&amp;#39;)] Your evaluation: &amp;#39;True&amp;#39;. Reason: The chat history is about the city of Munich. In the follow-up question, the user wants to know what the weather will be &amp;#39;there&amp;#39; tomorrow. Since the previous discussion was about Munich, &amp;#39;there&amp;#39; refers to the city of Munich. [/INST]&amp;quot;&amp;quot;&amp;quot; grade_prompt = ChatPromptTemplate.from_messages( [ (&amp;quot;system&amp;quot;, system), ( &amp;quot;human&amp;quot;, &amp;quot;User&amp;#39;s question: {question} \n\n Chat history: {chat_history}&amp;quot;, ), ] ) llm = ChatOpenAI() structured_llm = llm.with_structured_output(GradeQuestion) grader_llm = grade_prompt | structured_llm result = grader_llm.invoke({&amp;quot;question&amp;quot;: question, &amp;quot;chat_history&amp;quot;: messages}) state[&amp;quot;is_followup_question&amp;quot;] = result.score return state &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Mediocre-Card8046&quot;&gt; /u/Mediocre-Card8046 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dv203r/history_aware_agent_in_langgraph/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dv203r/history_aware_agent_in_langgraph/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dv203r</id><link href="https://www.reddit.com/r/LangChain/comments/1dv203r/history_aware_agent_in_langgraph/" /><updated>2024-07-04T08:32:45+00:00</updated><published>2024-07-04T08:32:45+00:00</published><title>History Aware Agent in Langgraph</title></entry><entry><author><name>/u/Embarrassed_Bread121</name><uri>https://www.reddit.com/user/Embarrassed_Bread121</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Embarrassed_Bread121&quot;&gt; /u/Embarrassed_Bread121 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dva67x/an_ml_engineer_here_looking_for_a_job_opportunity/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dva67x/an_ml_engineer_here_looking_for_a_job_opportunity/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dva67x</id><link href="https://www.reddit.com/r/LangChain/comments/1dva67x/an_ml_engineer_here_looking_for_a_job_opportunity/" /><updated>2024-07-04T15:52:17+00:00</updated><published>2024-07-04T15:52:17+00:00</published><title>An ML Engineer here Looking for a job opportunity. I have experience in deploying RAG pipelines and ML projects. Any leads would be helpful.</title></entry><entry><author><name>/u/bastormator</name><uri>https://www.reddit.com/user/bastormator</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I was not able to find anything on the web, the cases for which the pedantic parsers fail to parse the data coming from LLMs. I tried looking under the hood working and say that they are using json parsing, if anyone has info about this please enlighten me.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/bastormator&quot;&gt; /u/bastormator &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dv1nfr/pedantic_data_parsing/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dv1nfr/pedantic_data_parsing/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dv1nfr</id><link href="https://www.reddit.com/r/LangChain/comments/1dv1nfr/pedantic_data_parsing/" /><updated>2024-07-04T08:08:06+00:00</updated><published>2024-07-04T08:08:06+00:00</published><title>Pedantic data parsing</title></entry><entry><author><name>/u/New-Contribution6302</name><uri>https://www.reddit.com/user/New-Contribution6302</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I have a problem. It takes literally 30 mins for my map-reduce summarisation chain to produce it&amp;#39;s final output. Current vRAM is 16GB. What should I do to increase the speed?&lt;/p&gt; &lt;p&gt;Model: Llama2&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/New-Contribution6302&quot;&gt; /u/New-Contribution6302 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dv0wbv/how_to_increase_the_inference_speed_of_map_reduce/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dv0wbv/how_to_increase_the_inference_speed_of_map_reduce/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dv0wbv</id><link href="https://www.reddit.com/r/LangChain/comments/1dv0wbv/how_to_increase_the_inference_speed_of_map_reduce/" /><updated>2024-07-04T07:18:04+00:00</updated><published>2024-07-04T07:18:04+00:00</published><title>How to increase the inference speed of Map reduce chain in langchain</title></entry><entry><author><name>/u/IntroductionMany1279</name><uri>https://www.reddit.com/user/IntroductionMany1279</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Iâm not a programmer, only got basic understanding of the concepts. Iâm wanting to do the following (Iâll probably get someone from Upwork to do it) read and write to the company daily operations Google sheet using natural language. Users can type requests and the agent will write and read the document. Iâm thinking Iâll have python scripts to do actions like insert new work orders with all formulas in place.&lt;/p&gt; &lt;p&gt;My concern is, what happens if someone edits the GS manually. The agent will not know exactly which row a certain data is if I want to update it. Or can the agent read first to ensure the data is in the target position and then update it? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/IntroductionMany1279&quot;&gt; /u/IntroductionMany1279 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dursic/read_and_write_to_google_sheets/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dursic/read_and_write_to_google_sheets/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dursic</id><link href="https://www.reddit.com/r/LangChain/comments/1dursic/read_and_write_to_google_sheets/" /><updated>2024-07-03T22:56:52+00:00</updated><published>2024-07-03T22:56:52+00:00</published><title>Read and write to Google sheets</title></entry><entry><author><name>/u/One-Field-8962</name><uri>https://www.reddit.com/user/One-Field-8962</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1duug75/langflowlangchain_to_production/&quot;&gt; &lt;img src=&quot;https://b.thumbs.redditmedia.com/0TCY51Gkj2eMJdZ_WAEzytvtqKYIiYZ93BGA2-NhotU.jpg&quot; alt=&quot;LangFlow/LangChain to production&quot; title=&quot;LangFlow/LangChain to production&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hello,&lt;/p&gt; &lt;p&gt;I&amp;#39;veen using LangFlow/LangChain to test a few concepts to create my RAG and works flawless.&lt;/p&gt; &lt;p&gt;Now my question maybe too easy or too complex, how to deliver to production?&lt;br/&gt; I saw the &amp;quot;code snippets&amp;quot; for each component, but i can&amp;#39;t figure out to deliver directly to production without the GUI interface of LangFlow.&lt;/p&gt; &lt;p&gt;Here is a draft from my project:&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/iukb1l8fjead1.png?width=2532&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1356a500acda75770e04aec93a89a5c2d4acdd1c&quot;&gt;https://preview.redd.it/iukb1l8fjead1.png?width=2532&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1356a500acda75770e04aec93a89a5c2d4acdd1c&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Any help will be really appreciated.&lt;/p&gt; &lt;p&gt;Thx&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/One-Field-8962&quot;&gt; /u/One-Field-8962 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1duug75/langflowlangchain_to_production/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1duug75/langflowlangchain_to_production/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1duug75</id><media:thumbnail url="https://b.thumbs.redditmedia.com/0TCY51Gkj2eMJdZ_WAEzytvtqKYIiYZ93BGA2-NhotU.jpg" /><link href="https://www.reddit.com/r/LangChain/comments/1duug75/langflowlangchain_to_production/" /><updated>2024-07-04T01:08:06+00:00</updated><published>2024-07-04T01:08:06+00:00</published><title>LangFlow/LangChain to production</title></entry><entry><author><name>/u/giagara</name><uri>https://www.reddit.com/user/giagara</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;ve created a RAG with document retriever and everything works fine. &lt;/p&gt; &lt;p&gt;I want to add a feature: the user can specify a document to apply search to. The question the user ask is something related to the single document, OR something like &amp;quot;resume it&amp;quot;, &amp;quot;translate it&amp;quot;, whatever. &lt;/p&gt; &lt;p&gt;What I though was use parent document retriever and inject the whole document (don&amp;#39;t think about tokens limit at the moment) in the context. Is theoretically possible but my question is: how can I detect a &amp;quot;manipulation&amp;quot; question (resume, translate, etc)?&lt;/p&gt; &lt;p&gt;In these cases I want to use parent document (or whatever strategy), and in other cases the default strategy. &lt;/p&gt; &lt;p&gt;Should I put some ML model between question and retriever? I want to avoid using AI tools to not consume another open Ai call to detect what retriever I should use. &lt;/p&gt; &lt;p&gt;Thanks &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/giagara&quot;&gt; /u/giagara &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1duoub8/retrieve_a_single_document/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1duoub8/retrieve_a_single_document/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1duoub8</id><link href="https://www.reddit.com/r/LangChain/comments/1duoub8/retrieve_a_single_document/" /><updated>2024-07-03T20:48:14+00:00</updated><published>2024-07-03T20:48:14+00:00</published><title>Retrieve a single document</title></entry><entry><author><name>/u/adrenaline681</name><uri>https://www.reddit.com/user/adrenaline681</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;m trying to build reusable chains and I&amp;#39;m having issues trying to set it up so it takes the output of the previous chain as one of the template format arguments.&lt;/p&gt; &lt;p&gt;In this example, I have a chain that generates a title based on a topic and a chain that translates the title into a different language. The topic and the language I want to provide them when I invoke the final chain.&lt;/p&gt; &lt;p&gt;If my 2nd chain only has one single input for the template (because I hardcode the language), everything works perfectly:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;# Title Chain title_template = PromptTemplate.from_template(&amp;#39;Generate a title for a blog post about the following topic: {topic}&amp;#39;) generate_title = title_template | llm | StrOutputParser() # Translate Chain translate_template = PromptTemplate.from_template(&amp;#39;Translate the following text to spanish: {text}&amp;#39;) translate_chain = translate_template | llm | StrOutputParser() # Combine and run combined_chain = generate_title | translate_chain result = combined_chain.invoke({&amp;#39;topic&amp;#39;: &amp;#39;The benefits of exercise&amp;#39;}) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;But if I want to be able to provide the language when I invoke the final chain then, I dont know how to pass the output to the template dictionary. See the &amp;quot;HOW_TO_PASS_OUTPUT_OF_PREVIOUS_CHAIN&amp;quot; in the code&lt;/p&gt; &lt;pre&gt;&lt;code&gt;# Title Chain title_template = PromptTemplate.from_template(&amp;#39;Generate a title for a blog post about the following topic: {topic}&amp;#39;) generate_title = title_template | llm | StrOutputParser() # Translate Chain translate_template = PromptTemplate.from_template(&amp;#39;Translate the following text to {language}: {text}&amp;#39;) translate_chain = {&amp;#39;language&amp;#39;: itemgetter(&amp;#39;language&amp;#39;), &amp;#39;text&amp;#39;: HOW_TO_PASS_OUTPUT_OF_PREVIOUS_CHAIN?} | translate_template | llm | StrOutputParser() # Combine and run combined_chain = generate_title | translate_chain result = combined_chain.invoke({&amp;#39;topic&amp;#39;: &amp;#39;The benefits of exercise&amp;#39;, &amp;#39;language&amp;#39;: &amp;#39;Spanish&amp;#39;}) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;I also tried to use a lambda function but the itemgetter doesn&amp;#39;t seem to work:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;translate_chain = ( lambda text: {&amp;#39;language&amp;#39;: itemgetter(&amp;#39;language&amp;#39;), &amp;#39;text&amp;#39;: text} | translate_template | llm | StrOutputParser() ) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;If anyone knows the answer I would be much appreciated.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/adrenaline681&quot;&gt; /u/adrenaline681 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dursrw/how_to_build_a_reusable_chain_that_takes_the/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dursrw/how_to_build_a_reusable_chain_that_takes_the/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dursrw</id><link href="https://www.reddit.com/r/LangChain/comments/1dursrw/how_to_build_a_reusable_chain_that_takes_the/" /><updated>2024-07-03T22:57:14+00:00</updated><published>2024-07-03T22:57:14+00:00</published><title>How to build a reusable chain that takes the output of another chain as a keyword argument for the template?</title></entry><entry><author><name>/u/KlingonForehead</name><uri>https://www.reddit.com/user/KlingonForehead</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I am creating a fairly straightforward streaming chat thing. I need to supplement the final llm result of my pretty simple rag chain with a link that I get from a separate vector store search. Literally just need to tack it on at the end of the result. Does this fit any of the component types in LangChain? Or should I just do it completely outside of the chain? Any thoughts are appreciated.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/KlingonForehead&quot;&gt; /u/KlingonForehead &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1duhvrq/could_use_some_advice_on_an_approach_to_bring/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1duhvrq/could_use_some_advice_on_an_approach_to_bring/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1duhvrq</id><link href="https://www.reddit.com/r/LangChain/comments/1duhvrq/could_use_some_advice_on_an_approach_to_bring/" /><updated>2024-07-03T15:57:48+00:00</updated><published>2024-07-03T15:57:48+00:00</published><title>Could use some advice on an approach to bring back a similarity search result from a vector store and just tack a string from the metadata on to the end of my final llm response.</title></entry><entry><author><name>/u/coolcloud</name><uri>https://www.reddit.com/user/coolcloud</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dtr49t/agent_rag_parallel_quotes_how_we_built_rag_on/&quot;&gt; &lt;img src=&quot;https://b.thumbs.redditmedia.com/bMd79q8bCS6eIl2j-osvJEQzKD5Q0MMW8-q6edsGYHc.jpg&quot; alt=&quot;Agent RAG (Parallel Quotes) - How we built RAG on 10,000's of docs with extremely high accuracy&quot; title=&quot;Agent RAG (Parallel Quotes) - How we built RAG on 10,000's of docs with extremely high accuracy&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Edit - for some reason the prompts weren&amp;#39;t showing up. Added them.&lt;/p&gt; &lt;p&gt;Hey all -&lt;/p&gt; &lt;p&gt;Today I want to walk through how we&amp;#39;ve been able to get extremely high accuracy recall on thousands of documents by taking advantage of splitting retrieval into an &amp;quot;Agent&amp;quot; approach.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;As we built RAG, we continued to notice hallucinations or incorrect answers. we realized three key issues:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;There wasn&amp;#39;t enough data in the vector to provide a coherent answer. i.e. vector was 2 sentences, but the answer was the entire paragraph or multiple paragraphs.&lt;/li&gt; &lt;li&gt;LLM&amp;#39;s try to merge an answer from multiple different vectors which made an answer that looked right but wasn&amp;#39;t.&lt;/li&gt; &lt;li&gt;End users couldn&amp;#39;t figure out where the doc came from and if it was accurate.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;We solved this problem by doing the following:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Figure out document layout (&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dpbc4g/how_we_chunk_turning_pdfs_into_hierarchical/&quot;&gt;we posted about it a few days ago.&lt;/a&gt;) This will make issue one much less common.&lt;/li&gt; &lt;li&gt;Split each &amp;quot;chunk&amp;quot; into separate prompts (Agent approach) to find exact quotes that may be important to answering the question. This fixes issue 2.&lt;/li&gt; &lt;li&gt;Ask the LLM to only give direct quotes with references to the document it came from, both in step one and step two of the LLM answer generation. This solves issue 3.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;What does it look like?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/otf9dbau05ad1.png?width=1625&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=72a7fe9349a9499a32ff5f6b6a245623e1d91102&quot;&gt;https://preview.redd.it/otf9dbau05ad1.png?width=1625&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=72a7fe9349a9499a32ff5f6b6a245623e1d91102&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We found these improvements, along with our prompt give us extremely high retrieval even on complex questions, or large corpuses of data.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why do we believe it works so well?&lt;/strong&gt; - LLM&amp;#39;s still seem better to deal with a single task at a time, and LLM&amp;#39;s still struggle with large token counts on random data glued together with a prompt (i.e. a ton of random chunks). Because we are only providing a single Chunk, or relevant information, we found huge improvements in recall and accuracy.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Workflow:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/8aso1i7y05ad1.png?width=1109&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e115ee50b70fbf790e78af365ce96f7d2e16a020&quot;&gt;https://preview.redd.it/8aso1i7y05ad1.png?width=1109&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e115ee50b70fbf790e78af365ce96f7d2e16a020&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Step by step with example on above workflow&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Query:&lt;/strong&gt; What are the recent advancements in self-supervised object detection technique&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Reconstruct document.&lt;/strong&gt; (highlighted would be the vector that came back) Then we&amp;#39;d reconstruct the doc until we get to a header.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/ztj8q1f415ad1.png?width=1122&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ab3805273bf3320d7478c3619c3c0df8dcdc98d9&quot;&gt;https://preview.redd.it/ztj8q1f415ad1.png?width=1122&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ab3805273bf3320d7478c3619c3c0df8dcdc98d9&lt;/a&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Input the reconstructed document chunk into the LLM. &lt;strong&gt;(Parallel Quotes)&lt;/strong&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;Prompt #1:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;_______&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;You are an expert research assistant. Here is a document you will find relevant quotes to the question asked:&lt;/p&gt; &lt;p&gt; &amp;lt;doc&amp;gt;&lt;/p&gt; &lt;p&gt; ${chunk}&lt;/p&gt; &lt;p&gt; &amp;lt;/doc&amp;gt;&lt;/p&gt; &lt;p&gt;Find the quotes from the document that are most relevant to answering the question, and then print them in numbered order. Quotes should be relatively short.&lt;/p&gt; &lt;p&gt;The format of your overall response should look like what&amp;#39;s shown below. Make sure to follow the formatting and spacing exactly.&lt;/p&gt; &lt;p&gt; Example:&lt;/p&gt; &lt;p&gt; [1] &amp;quot;Company X reported revenue of $12 million in 2021.&amp;quot;&lt;/p&gt; &lt;p&gt; [2] &amp;quot;Almost 90% of revenue came from widget sales, with gadget sales making up the remaining 10%.&amp;quot;&lt;/p&gt; &lt;p&gt; Do not write anything that&amp;#39;s not a quote direct quote.&lt;/p&gt; &lt;p&gt; If there are no quotes, please only print, &amp;quot;N/a&amp;quot;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;_______&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Response&lt;/strong&gt; from the LLM:&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;[1.0]&amp;quot;Recent advancements have seen the development of end-to-end self-supervised object detection models like UP-DETR and DETReg, as well as backbone pre-training strategies such as Self-EMD and Odin .&amp;quot;&lt;/p&gt; &lt;p&gt;[1.1] &amp;quot;Despite the remarkable success of supervised object detection techniques such as Mask RCNN , Yolo , Retinanet , and DETR , their self-supervised alternatives have been somewhat limited in scope until recently.&lt;/p&gt; &lt;p&gt;Notes:&lt;/p&gt; &lt;p&gt;I deleted the internal references to make it less confusing&lt;/p&gt; &lt;p&gt;If there&amp;#39;s more than 1 doc/chunk we start each new one with a new number i.e. [2.0] which makes it easier to find which quote relates to which doc.&lt;/p&gt; &lt;p&gt;We put the query in the user prompt and the above in the system prompt&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Give the LLM that will be generating the answer the &lt;strong&gt;document name &amp;amp; quotes&lt;/strong&gt;.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;Prompt #2:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;_______&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;All quotes are relevant to the question, please use them to answer the question:&lt;/p&gt; &lt;p&gt;When answering questions:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Make references to quotes relevant to each section of the answer solely by adding their bracketed numbers at the end of relevant sentences.&lt;/li&gt; &lt;li&gt; Feel free to shorten quotes or merge quotes together as long as you reference them.&lt;/li&gt; &lt;li&gt; Focus on making short, readable answers.&lt;/li&gt; &lt;li&gt; Bold headers, bold general topics, bullet point, list, etc. if needed to make it easier to read.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;DocName: UnSupDLA: Towards Unsupervised Document Layout Analysis&lt;/p&gt; &lt;p&gt;Quotes:&lt;/p&gt; &lt;p&gt;[1.0]&amp;quot;Recent advancements have seen the development of end-to-end self-supervised object detection models like UP-DETR and DETReg, as well as backbone pre-training strategies such as Self-EMD and Odin .&amp;quot;&lt;/p&gt; &lt;p&gt;[1.1] &amp;quot;Despite the remarkable success of supervised object detection techniques such as Mask RCNN , Yolo , Retinanet , and DETR , their self-supervised alternatives have been somewhat limited in scope until recently.&amp;quot;&lt;/p&gt; &lt;p&gt;DocName: &amp;#39;doc2&amp;#39;&lt;/p&gt; &lt;p&gt;Quotes:&lt;/p&gt; &lt;p&gt;[2.0]&amp;quot;example quote2&amp;quot;&lt;/p&gt; &lt;p&gt;[2.1]&amp;quot;example&amp;quot;&lt;/p&gt; &lt;p&gt;If there isn&amp;#39;t information to answer this question, please write:&lt;/p&gt; &lt;p&gt;I&amp;#39;m sorry, but there is no documentation available.&lt;/p&gt; &lt;p&gt;________&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Final LLM &lt;strong&gt;Response&lt;/strong&gt;:&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;End-to-End Self-Supervised Models&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;UP-DETR&lt;/strong&gt;: This model is a recent development in self-supervised object detection, aiming to enhance detection capabilities without relying on labeled data [1.0].&lt;/li&gt; &lt;li&gt;&lt;strong&gt;DETReg&lt;/strong&gt;: Another advanced model, DETReg, also focuses on self-supervised object detection, providing improvements in training and detection accuracy without requiring extensive labeled datasets [1.0].&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Backbone Pre-Training Strategies&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Self-EMD&lt;/strong&gt;: This strategy is utilized to pre-train the backbone of detection models, helping in learning robust representations from unlabeled data [1.0].&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Odin&lt;/strong&gt;: Similar to Self-EMD, Odin is a pre-training strategy that enhances the performance of detection models through self-supervised learning techniques [1.0].&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Comparison with Supervised Techniques&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Scope and Limitations&lt;/strong&gt;: Despite the success of supervised techniques like Mask RCNN, Yolo, Retinanet, and DETR, their self-supervised alternatives have only recently started to expand in scope and effectiveness [1.1].&lt;/p&gt; &lt;p&gt;_________________________________&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Real world examples of where this comes into use:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;A lot of internal company documents are made with human workflows in mind only. For example, often see a document named &amp;quot;integrations&amp;quot; or &amp;quot;partners&amp;quot; and then just a list of 500 companies they integrate/partner with. If a vector came back from within that document, the LLM would not be able to know it was regarding integrations or partnership because it&amp;#39;s only the document name.&lt;/li&gt; &lt;li&gt;Some documents will talk about the product, idea, or topic in the header. Then not discuss it by that name again. Meaning if you only get the relevant chunk back, you will not know which product it&amp;#39;s referencing.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Based on our experience with internal documents, about 15% of queries fall into one of the above scenarios.&lt;/p&gt; &lt;p&gt;Notes - Yes, we plan on open sourcing this at some point but don&amp;#39;t currently have the bandwidth (we built it as a production product first so we have to rip out some things before doing so)&lt;/p&gt; &lt;p&gt;Happy to answer any questions!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Video:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://reddit.com/link/1dtr49t/video/o196uuch15ad1/player&quot;&gt;https://reddit.com/link/1dtr49t/video/o196uuch15ad1/player&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/coolcloud&quot;&gt; /u/coolcloud &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dtr49t/agent_rag_parallel_quotes_how_we_built_rag_on/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dtr49t/agent_rag_parallel_quotes_how_we_built_rag_on/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1dtr49t</id><media:thumbnail url="https://b.thumbs.redditmedia.com/bMd79q8bCS6eIl2j-osvJEQzKD5Q0MMW8-q6edsGYHc.jpg" /><link href="https://www.reddit.com/r/LangChain/comments/1dtr49t/agent_rag_parallel_quotes_how_we_built_rag_on/" /><updated>2024-07-02T17:11:30+00:00</updated><published>2024-07-02T17:11:30+00:00</published><title>Agent RAG (Parallel Quotes) - How we built RAG on 10,000's of docs with extremely high accuracy</title></entry><entry><author><name>/u/hackermud</name><uri>https://www.reddit.com/user/hackermud</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hello everyone,&lt;/p&gt; &lt;p&gt;I&amp;#39;ve been working on developing an enterprise platform for the past 2 months. This platform allows our clients to connect multiple data sources and create customized features powered by LLM for their products. The main issue I&amp;#39;m encountering is creating an appropriate query to retrieve relevant information from a large database with over 200 tables, each containing approximately 30-50 columns. I&amp;#39;ve experimented with various approaches such as Langgraph, custom LCEL, retriever with Langgraph, and different LLMs, but I&amp;#39;m still not getting the desired response. If anyone has experience with this type of problem, I would greatly appreciate it if you could share your knowledge. Thank you in advance.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/hackermud&quot;&gt; /u/hackermud &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dudch2/is_there_a_better_approach_for_generating_proper/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dudch2/is_there_a_better_approach_for_generating_proper/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dudch2</id><link href="https://www.reddit.com/r/LangChain/comments/1dudch2/is_there_a_better_approach_for_generating_proper/" /><updated>2024-07-03T12:39:37+00:00</updated><published>2024-07-03T12:39:37+00:00</published><title>Is there a better approach for generating proper SQL queries for large databases to retrieve relevant information?</title></entry></feed>