<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/"><category term="LangChain" label="r/LangChain"/><updated>2024-07-21T04:54:57+00:00</updated><icon>https://www.redditstatic.com/icon.png/</icon><id>/r/LangChain.rss</id><link rel="self" href="https://www.reddit.com/r/LangChain.rss" type="application/atom+xml" /><link rel="alternate" href="https://www.reddit.com/r/LangChain" type="text/html" /><subtitle>LangChain is an open-source framework and developer toolkit that helps developers get LLM applications from prototype to production. It is available for Python and Javascript at https://www.langchain.com/.</subtitle><title>LangChain</title><entry><author><name>/u/zchaarm</name><uri>https://www.reddit.com/user/zchaarm</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;A place for members of &lt;a href=&quot;/r/LangChain&quot;&gt;r/LangChain&lt;/a&gt; to chat with each other&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/zchaarm&quot;&gt; /u/zchaarm &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_10ljho9</id><link href="https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/" /><updated>2023-01-26T04:33:05+00:00</updated><published>2023-01-26T04:33:05+00:00</published><title>r/LangChain Lounge</title></entry><entry><author><name>/u/empirical-sadboy</name><uri>https://www.reddit.com/user/empirical-sadboy</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/empirical-sadboy&quot;&gt; /u/empirical-sadboy &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;/r/LocalLLaMA/comments/1e84de2/top_enhancements_to_try_once_you_have_a_vanilla/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e84w2n/top_enhancements_to_try_once_you_have_a_vanilla/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1e84w2n</id><link href="https://www.reddit.com/r/LangChain/comments/1e84w2n/top_enhancements_to_try_once_you_have_a_vanilla/" /><updated>2024-07-20T20:31:16+00:00</updated><published>2024-07-20T20:31:16+00:00</published><title>Top enhancements to try once you have a vanilla RAG set-up with a text vector database?</title></entry><entry><author><name>/u/Phoenix_20_23</name><uri>https://www.reddit.com/user/Phoenix_20_23</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hello everyone, I hope you&amp;#39;re all doing well! I‚Äôm currently on the lookout for a library that can extract text in paragraph chunks from PDFs. For instance, I need it to pull out the Introduction with all its paragraphs separately, the Conclusion with all its paragraphs separately, and so on, essentially chunking the text by paragraphs. Do you have any suggestions? Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Phoenix_20_23&quot;&gt; /u/Phoenix_20_23 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e7cntq/whats_the_best_python_library_for_extracting_text/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e7cntq/whats_the_best_python_library_for_extracting_text/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1e7cntq</id><link href="https://www.reddit.com/r/LangChain/comments/1e7cntq/whats_the_best_python_library_for_extracting_text/" /><updated>2024-07-19T19:48:44+00:00</updated><published>2024-07-19T19:48:44+00:00</published><title>What‚Äôs the Best Python Library for Extracting Text from PDFs?</title></entry><entry><author><name>/u/pvbang</name><uri>https://www.reddit.com/user/pvbang</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;m having problems building my system.&lt;/p&gt; &lt;p&gt;Let&amp;#39;s say I have one (or more pdf files), I load, splitters, chunking, clean data,... and then save it to a vector database (qdrant). I can query its data quite well with knowledge questions located somewhere in the files.&lt;/p&gt; &lt;p&gt;But suppose in my data file is a list of about 1000 products distributed on many different pages, is there any way I can solve the question: &amp;quot;How many products are there?&amp;quot; Are not?&lt;/p&gt; &lt;p&gt;Or ask &amp;quot;List all the major and minor headings in the file&amp;quot; and it can answer correctly if there is no table of contents available.&lt;/p&gt; &lt;p&gt;My problem is that I can&amp;#39;t read the whole document when putting it in the context part of LLM, because it&amp;#39;s too long if k is increased in the retrievers part, and I also don&amp;#39;t think it can completely satisfy the context content because Maybe it is still left somewhere in other segments if k is fixed?&lt;/p&gt; &lt;p&gt;If anyone has any ideas or solutions, please help me.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/pvbang&quot;&gt; /u/pvbang &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e7pcxp/search_for_data_across_entire_text_files/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e7pcxp/search_for_data_across_entire_text_files/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1e7pcxp</id><link href="https://www.reddit.com/r/LangChain/comments/1e7pcxp/search_for_data_across_entire_text_files/" /><updated>2024-07-20T06:40:13+00:00</updated><published>2024-07-20T06:40:13+00:00</published><title>Search for data across entire text files</title></entry><entry><author><name>/u/AGI-is-coming</name><uri>https://www.reddit.com/user/AGI-is-coming</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;You&amp;#39;ve probably heard about Mistral&amp;#39;s groundbreaking release of Codestral Mamba, a 7B parameter model. But why all the hype over a 7B model when we have giants like GPT-4? Well, it&amp;#39;s not just about size this time ‚Äì it&amp;#39;s about architecture. üîç&lt;/p&gt; &lt;p&gt;The &lt;strong&gt;Transformer Dilemma:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Transformers have been the cornerstone architecture for language models, powering everything from open-source LLMs to chatGPT and Claude.&lt;/p&gt; &lt;p&gt;However, they come with a significant drawback: as context expands, so does processing time (hello, quadratic bottleneck!).&lt;/p&gt; &lt;p&gt;Transformers are undeniably effective, storing every detail from the past for theoretically perfect recall.&lt;/p&gt; &lt;p&gt;On the other hand, traditional RNN (Recurring Neural Networks)‚Äì forget a lot, retaining only a small portion in their hidden state and discarding the rest. This makes them highly efficient but less effective since discarded information cannot be retrieved.&lt;/p&gt; &lt;p&gt;Finding the Sweet Spot - &lt;strong&gt;Enter Mamba&lt;/strong&gt; üêç.&lt;/p&gt; &lt;p&gt;Mamba belongs to a class of models known as State Space Models (SSMs). SSMs excel in understanding and predicting how systems (like cars) evolve based on measurable data.&lt;/p&gt; &lt;p&gt;Notably, Mamba offers comparable performance and scalability to Transformers but crucially eliminates the quadratic bottleneck in the Attention Mechanism.&lt;/p&gt; &lt;p&gt;Language models are good at summarizing text, though some details may be lost. However, summarizing other forms of content, like a two-hour movie, is trickier.&lt;/p&gt; &lt;p&gt;This is where Mamba&amp;#39;s long-term memory comes into play, enabling the model to retain important information.&lt;/p&gt; &lt;p&gt;Why should you care? Mamba could be a game-changer for tasks requiring extensive context, like:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;DNA processing üß¨&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Report writing üìö&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Agents with long-term memory and goals ü§ñ(Mistral - Codestral Mamba)&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/AGI-is-coming&quot;&gt; /u/AGI-is-coming &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e72ayj/attention_isnt_all_you_need/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e72ayj/attention_isnt_all_you_need/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1e72ayj</id><link href="https://www.reddit.com/r/LangChain/comments/1e72ayj/attention_isnt_all_you_need/" /><updated>2024-07-19T12:16:45+00:00</updated><published>2024-07-19T12:16:45+00:00</published><title>&quot;Attention Isn‚Äôt All You Need&quot;</title></entry><entry><author><name>/u/Either-Ambassador738</name><uri>https://www.reddit.com/user/Either-Ambassador738</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Is there any way to persists the parent documents in the ParentDocumentRetrieval?&lt;br/&gt; All the tutorials I see use the InMemoryStore, but I&amp;#39;d like to persist the parent documents in a redis database or a mysql database. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Either-Ambassador738&quot;&gt; /u/Either-Ambassador738 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e7h5ea/persists_documents_on_parentdocumentretrieval/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e7h5ea/persists_documents_on_parentdocumentretrieval/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1e7h5ea</id><link href="https://www.reddit.com/r/LangChain/comments/1e7h5ea/persists_documents_on_parentdocumentretrieval/" /><updated>2024-07-19T23:04:23+00:00</updated><published>2024-07-19T23:04:23+00:00</published><title>Persists documents on ParentDocumentRetrieval</title></entry><entry><author><name>/u/Either-Ambassador738</name><uri>https://www.reddit.com/user/Either-Ambassador738</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I want to bind some functions into a llm, I&amp;#39;m using the ChatOpenAI wrapper to connect to a Ollama llama3 model locally, I have this code:&lt;/p&gt; &lt;pre&gt;&lt;code&gt; options = [&amp;quot;FINISH&amp;quot;] + members function_def = { &amp;quot;name&amp;quot;: &amp;quot;route&amp;quot;, &amp;quot;description&amp;quot;: &amp;quot;Select the next role&amp;quot;, &amp;quot;parameters&amp;quot;: { &amp;quot;title&amp;quot;: &amp;quot;routeSchema&amp;quot;, &amp;quot;type&amp;quot;: &amp;quot;object&amp;quot;, &amp;quot;properties&amp;quot;: { &amp;quot;next&amp;quot;: { &amp;quot;title&amp;quot;: &amp;quot;Next&amp;quot;, &amp;quot;anyOf&amp;quot;: [ {&amp;quot;enum&amp;quot;: options}, ], }, }, &amp;quot;required&amp;quot;: [&amp;quot;next&amp;quot;], }, } prompt = ChatPromptTemplate.from_messages( [ (&amp;quot;system&amp;quot;, system_prompt), MessagesPlaceholder(variable_name=&amp;quot;messages&amp;quot;), ( &amp;quot;system&amp;quot;, &amp;quot;Given the conversation above, who should act next?&amp;quot; &amp;quot;Or should we FINISH?: select one of {options} ) ] ).partial(options=str(options), team_members=&amp;#39;,&amp;#39;.join(members)) # Bind the function to the LLM, specify the function call, and parse the output as JSON return ( prompt | llm.bind_functions(functions=[function_def], function_call=&amp;quot;route&amp;quot;) | JsonOutputFunctionsParser() ) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This is the LLM I&amp;#39;m using:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;llm = ChatOpenAI(base_url=&amp;quot;http://localhost:11434/v1&amp;quot;, model=&amp;quot;llama3&amp;quot;, api_key=&amp;quot;ollama&amp;quot;) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;However, I encounter the following issue:&lt;/p&gt; &lt;p&gt;langchain_core.exceptions.OutputParserException: Could not parse function call: &amp;#39;function_call&amp;#39;&lt;/p&gt; &lt;p&gt;Could anyone help me here? Thanks :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Either-Ambassador738&quot;&gt; /u/Either-Ambassador738 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e7cvzl/bind_functions_with_ollama_model_from_chatopenai/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e7cvzl/bind_functions_with_ollama_model_from_chatopenai/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1e7cvzl</id><link href="https://www.reddit.com/r/LangChain/comments/1e7cvzl/bind_functions_with_ollama_model_from_chatopenai/" /><updated>2024-07-19T19:58:32+00:00</updated><published>2024-07-19T19:58:32+00:00</published><title>Bind functions with Ollama model from ChatOpenAI.</title></entry><entry><author><name>/u/emersoftware</name><uri>https://www.reddit.com/user/emersoftware</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hello everyone. Has anyone deployed Langgraph in Google Cloud services? Currently, I&amp;#39;ve created my own method to do it using the Reasoning Engine, but I am a newbie in cloud services. I want to know if there is a better way to do it&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/emersoftware&quot;&gt; /u/emersoftware &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e7em5x/deploy_langgraph_in_google_cloud/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e7em5x/deploy_langgraph_in_google_cloud/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1e7em5x</id><link href="https://www.reddit.com/r/LangChain/comments/1e7em5x/deploy_langgraph_in_google_cloud/" /><updated>2024-07-19T21:12:04+00:00</updated><published>2024-07-19T21:12:04+00:00</published><title>Deploy Langgraph in Google Cloud?</title></entry><entry><author><name>/u/Danidre</name><uri>https://www.reddit.com/user/Danidre</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Is LangGraph production-ready?&lt;/p&gt; &lt;p&gt;I am finally seeing more documentation on checkpoint implementations, such as persistence using PostgreSQL, MongoDB, and Redis. Thanks a lot to the LangChain devs for the continued development of this open source tool.&lt;/p&gt; &lt;p&gt;However, I notice that these implementations are mainly phrased as &amp;quot;example&amp;quot; implementations. Does this mean they are not production ready?&lt;/p&gt; &lt;p&gt;Are checkpoints in a stable condition? I have been wanting to add an implementation myself, but chalked it up to be something I&amp;#39;d have to spend considerable time implementing as the specifications is lengthy. However, now I see the code for the core checkpoint usage has been updated recently, and even the implementations have new things like &lt;code&gt;write&lt;/code&gt; and &lt;code&gt;channel&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;There are also other areas (comment sections under the notebooks) where someone states that &lt;code&gt;thread_ts&lt;/code&gt; has been deprecated, and &lt;code&gt;checkpoint_id&lt;/code&gt; is now being used. Yet, the notebook example implementations themselves still use &lt;code&gt;thread_ts&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;Finally, the behind the scenes of what is stored is a bit complicated to understand as well, without much explanations nor documentations. And even these base abstractions seem to be changing recently. For example, the checkpointer implementations have some code &amp;quot;for backward compatibility&amp;quot;.&lt;/p&gt; &lt;p&gt;If I were to maintain an implementation for another dialect (MariaDB, SQL Server, etc), changing it at such a dynamic pace would take more away from using LangGraph itself on my projects. Especially when the LangGraph changes are discovered when browsing the git history, rather than the LangGraph blogs or documentations.&lt;/p&gt; &lt;p&gt;Can these be documented? It&amp;#39;s a bit of a magic right now with what is being stored unless one attempts to actually reverse engineer it. Again, I do not have an issue doing that; after all, it is an open source tool. However, with the ever-changing seemingly silent changes, it will make it difficult to keep up.&lt;/p&gt; &lt;p&gt;Is LangGraph stable? Or still in heavy development?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Danidre&quot;&gt; /u/Danidre &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e73gzj/langgraph_stability/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e73gzj/langgraph_stability/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1e73gzj</id><link href="https://www.reddit.com/r/LangChain/comments/1e73gzj/langgraph_stability/" /><updated>2024-07-19T13:15:05+00:00</updated><published>2024-07-19T13:15:05+00:00</published><title>LangGraph Stability</title></entry><entry><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hello everyone. Has anyone deployed Langgraph in Google Cloud services? Currently, I&amp;#39;ve created my own method to do it using the Reasoning Engine, but I am a newbie in cloud services. I want to know if there is a better way to do it&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e7dzzf/deploy_langgraph_on_google_cloud/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e7dzzf/deploy_langgraph_on_google_cloud/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1e7dzzf</id><link href="https://www.reddit.com/r/LangChain/comments/1e7dzzf/deploy_langgraph_on_google_cloud/" /><updated>2024-07-19T20:45:39+00:00</updated><published>2024-07-19T20:45:39+00:00</published><title>Deploy Langgraph on Google Cloud?</title></entry><entry><author><name>/u/SnooOranges529</name><uri>https://www.reddit.com/user/SnooOranges529</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e77ye2/using_langchain_runnables_and_running_into/&quot;&gt; &lt;img src=&quot;https://b.thumbs.redditmedia.com/muWoC4zg6G4f_3ceYgsRd4fQyXABZEXjrjJTS7EIVeE.jpg&quot; alt=&quot;Using Langchain runnables and running into pydantic errors&quot; title=&quot;Using Langchain runnables and running into pydantic errors&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi Y&amp;#39;all,&lt;br/&gt; I encounter this issue with running langchain runnables and it can&amp;#39;t get input_schema. To replicate this using a basic chain. This happens with all the chain, when I look for input_schema it yells with this error for those chain&amp;#39;s inputs.&lt;br/&gt; Any leads on how to resolve this issue?&lt;br/&gt; TIA&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/rwbz1r8b5idd1.png?width=1468&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=dcfb12069db3cf22268725f921c36a3c5a09ab27&quot;&gt;https://preview.redd.it/rwbz1r8b5idd1.png?width=1468&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=dcfb12069db3cf22268725f921c36a3c5a09ab27&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/SnooOranges529&quot;&gt; /u/SnooOranges529 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e77ye2/using_langchain_runnables_and_running_into/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e77ye2/using_langchain_runnables_and_running_into/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1e77ye2</id><media:thumbnail url="https://b.thumbs.redditmedia.com/muWoC4zg6G4f_3ceYgsRd4fQyXABZEXjrjJTS7EIVeE.jpg" /><link href="https://www.reddit.com/r/LangChain/comments/1e77ye2/using_langchain_runnables_and_running_into/" /><updated>2024-07-19T16:29:07+00:00</updated><published>2024-07-19T16:29:07+00:00</published><title>Using Langchain runnables and running into pydantic errors</title></entry><entry><author><name>/u/phan_ngt</name><uri>https://www.reddit.com/user/phan_ngt</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e6yz2y/how_to_trace_cost_of_ragas_i_am_using_langfuse/&quot;&gt; &lt;img src=&quot;https://a.thumbs.redditmedia.com/Mb-jWXlU_2XOlcFtNYrP9PSDlPgW56VysAlhtOZNW68.jpg&quot; alt=&quot;How to trace cost of RAGAS? ( I am using LANGFUSE)&quot; title=&quot;How to trace cost of RAGAS? ( I am using LANGFUSE)&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I use this code and decorator &lt;strong&gt;&lt;em&gt;observe()&lt;/em&gt;&lt;/strong&gt; to trace ragas cost. However, as you can see the result below. Total cost is 0$ for function &lt;strong&gt;&lt;em&gt;score_with_ragas()&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Any simple ideas to help? Thank you. &lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/lvjrbdgeufdd1.png?width=2594&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=948f921caf2cb56f5592ec39f161f4527438d286&quot;&gt;https://preview.redd.it/lvjrbdgeufdd1.png?width=2594&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=948f921caf2cb56f5592ec39f161f4527438d286&lt;/a&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;from app.core.services.openai import llm from ragas.embeddings import LangchainEmbeddingsWrapper from ragas.llms import LangchainLLMWrapper from ragas.metrics import answer_relevancy, faithfulness, context_utilization from ragas.metrics.critique import harmfulness from app.biz.performance.base import init_ragas_metrics from app.core.services.store import embeddings # metrics you chose metrics = [faithfulness, answer_relevancy, context_utilization, harmfulness] init_ragas_metrics( metrics, llm=LangchainLLMWrapper(llm), embedding=LangchainEmbeddingsWrapper(embeddings), ) import asyncio from langfuse.decorators import observe from app.core.datasets.main import dataset, langfuse from app.core.services.performance import metrics @observe(as_type=&amp;quot;generation&amp;quot;) async def score_with_ragas(query, chunks, answer, ground_truths): scores = {} for m in metrics: print(f&amp;quot;calculating {m.name}&amp;quot;) scores[m.name] = await m.ascore( row={&amp;quot;question&amp;quot;: query, &amp;quot;contexts&amp;quot;: chunks, &amp;quot;answer&amp;quot;: answer, &amp;quot;ground_truths&amp;quot;: ground_truths} ) return scores # Function to handle the full process including scoring async def main(): for row in dataset: question, contexts, answer, ground_truths = (row[&amp;quot;question&amp;quot;], row[&amp;quot;contexts&amp;quot;], row[&amp;quot;answer&amp;quot;], row[&amp;#39;ground_truths&amp;#39;]) trace = langfuse.trace(name=&amp;quot;rag&amp;quot;, input=question, output={ &amp;quot;answer&amp;quot;: answer, &amp;quot;contexts&amp;quot;: contexts }) # pass it as span trace.span( name=&amp;quot;retrieval&amp;quot;, input={&amp;#39;question&amp;#39;: question, &amp;#39;ground_truths&amp;#39;: ground_truths}, output={&amp;#39;contexts&amp;#39;: contexts} ) # use llm to generate a answer with the chunks # answer = get_response_from_llm(question, chunks) answer = row[&amp;#39;answer&amp;#39;] trace.span( name=&amp;quot;generation&amp;quot;, input={&amp;#39;question&amp;#39;: question, &amp;#39;contexts&amp;#39;: contexts, &amp;#39;ground_truths&amp;#39;: ground_truths}, output={&amp;#39;answer&amp;#39;: answer} ) ragas_scores = await score_with_ragas(question, contexts, answer, ground_truths) for m in metrics: trace.score(name=m.name, value=ragas_scores[m.name]) # Run the main function asyncio.run(main()) &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/phan_ngt&quot;&gt; /u/phan_ngt &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e6yz2y/how_to_trace_cost_of_ragas_i_am_using_langfuse/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e6yz2y/how_to_trace_cost_of_ragas_i_am_using_langfuse/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1e6yz2y</id><media:thumbnail url="https://a.thumbs.redditmedia.com/Mb-jWXlU_2XOlcFtNYrP9PSDlPgW56VysAlhtOZNW68.jpg" /><link href="https://www.reddit.com/r/LangChain/comments/1e6yz2y/how_to_trace_cost_of_ragas_i_am_using_langfuse/" /><updated>2024-07-19T08:44:03+00:00</updated><published>2024-07-19T08:44:03+00:00</published><title>How to trace cost of RAGAS? ( I am using LANGFUSE)</title></entry><entry><author><name>/u/quentinL52</name><uri>https://www.reddit.com/user/quentinL52</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;hi. im actually working on a rag projet, that suppose to take a CV in entry and recommand a list of job according to the cv skills. at this stage i have the full rag system who take the cv and index it on pinecone, for the jobs offer it work the same. for the model i use a rag chain with 2 different retriever. but im stuck on how to get the response as a json.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/quentinL52&quot;&gt; /u/quentinL52 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e73y4n/rag_response_json_parsed/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e73y4n/rag_response_json_parsed/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1e73y4n</id><link href="https://www.reddit.com/r/LangChain/comments/1e73y4n/rag_response_json_parsed/" /><updated>2024-07-19T13:37:46+00:00</updated><published>2024-07-19T13:37:46+00:00</published><title>Rag, response json parsed</title></entry><entry><author><name>/u/HomunMage</name><uri>https://www.reddit.com/user/HomunMage</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e72vpe/langgraphgui_selfhosted_visual_editor_for/&quot;&gt; &lt;img src=&quot;https://external-preview.redd.it/TgfqNQEkkgppHRjoC_zrlvYZxWQ5Y_8O1MF0kdVopkY.jpg?width=216&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0b3f2f7f401b06194c1e5a10eb39c2a6193c2f56&quot; alt=&quot;LangGraph-GUI: Self-hosted Visual Editor for Node-Edge Graphs with Reactflow &amp;amp; Ollama&quot; title=&quot;LangGraph-GUI: Self-hosted Visual Editor for Node-Edge Graphs with Reactflow &amp;amp; Ollama&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;I&amp;#39;m excited to share my latest project: LangGraph-GUI ( &lt;a href=&quot;https://github.com/LangGraph-GUI/&quot;&gt;https://github.com/LangGraph-GUI/&lt;/a&gt; ) It&amp;#39;s a powerful, self-hosted visual editor for node-edge graphs that combines:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Reactflow frontend for intuitive graph manipulation&lt;/li&gt; &lt;li&gt;Ollama backend for AI capabilities on GPU-enabled PCs&lt;/li&gt; &lt;li&gt;Docker Compose for easy setup&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/12jviwao1hdd1.jpg?width=1541&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=68717af32acc7acdd8ee2dd09d294f83f6cbed31&quot;&gt;https://preview.redd.it/12jviwao1hdd1.jpg?width=1541&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=68717af32acc7acdd8ee2dd09d294f83f6cbed31&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Key Features:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;low code or no code&lt;/li&gt; &lt;li&gt;Local LLM such gemma2&lt;/li&gt; &lt;li&gt;Simple self-hosting with Docker Compose&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;See more on &lt;a href=&quot;https://langgraph-gui.github.io/&quot;&gt;Documentation&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This project builds on my previous work with &lt;a href=&quot;https://github.com/LangGraph-GUI/LangGraph-GUI-Qt&quot;&gt;LangGraph-GUI-Qt&lt;/a&gt; and &lt;a href=&quot;https://github.com/LangGraph-GUI/CrewAI-GUI&quot;&gt;CrewAI-GUI&lt;/a&gt;, now leveraging Reactflow for an improved frontend experience.&lt;/p&gt; &lt;p&gt;I&amp;#39;d love to hear your thoughts, questions, or feedback on LangGraph-GUI. How might you use this tool in your projects?&lt;/p&gt; &lt;p&gt;Moreover, if you want to learn langgraph, we have &lt;a href=&quot;https://github.com/LangGraph-GUI/LangGraph-learn&quot;&gt;LangGraph Learning for dummy&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/HomunMage&quot;&gt; /u/HomunMage &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e72vpe/langgraphgui_selfhosted_visual_editor_for/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e72vpe/langgraphgui_selfhosted_visual_editor_for/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1e72vpe</id><media:thumbnail url="https://external-preview.redd.it/TgfqNQEkkgppHRjoC_zrlvYZxWQ5Y_8O1MF0kdVopkY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=0b3f2f7f401b06194c1e5a10eb39c2a6193c2f56" /><link href="https://www.reddit.com/r/LangChain/comments/1e72vpe/langgraphgui_selfhosted_visual_editor_for/" /><updated>2024-07-19T12:46:14+00:00</updated><published>2024-07-19T12:46:14+00:00</published><title>LangGraph-GUI: Self-hosted Visual Editor for Node-Edge Graphs with Reactflow &amp; Ollama</title></entry><entry><author><name>/u/Money_Mycologist4939</name><uri>https://www.reddit.com/user/Money_Mycologist4939</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi everybody. I am trying to streaming the response of a langgraph. With astream_events is it possible to choose in the metadata the node we want to stream the response, and in my case is of course the last node. The problem is that the chain used in the last node has been built using runnable Parallel, because I needed to invoke multiple chains in parallel. But what I am interested in is just the output of one of the chains. However the astream_event applied on the last node streams out the output of the chains mixed. So I get the streaming of all the chain at the same time and it get messed up. Has anyone already tried to build a graph with parallel indipendent chains and used astream_events on it??&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Money_Mycologist4939&quot;&gt; /u/Money_Mycologist4939 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e77fkx/using_astram_events_on_langgraph_with_parallel/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e77fkx/using_astram_events_on_langgraph_with_parallel/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1e77fkx</id><link href="https://www.reddit.com/r/LangChain/comments/1e77fkx/using_astram_events_on_langgraph_with_parallel/" /><updated>2024-07-19T16:07:03+00:00</updated><published>2024-07-19T16:07:03+00:00</published><title>Using astram_events on langgraph with parallel chains</title></entry><entry><author><name>/u/Big_Barracuda_6753</name><uri>https://www.reddit.com/user/Big_Barracuda_6753</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;pre&gt;&lt;code&gt;import streamlit as st import os from langchain_openai import ChatOpenAI from langchain_core.messages import HumanMessage, AIMessage, SystemMessage from dotenv import load_dotenv from llama_parse import LlamaParse from langchain_community.document_loaders import UnstructuredMarkdownLoader from langchain.text_splitter import RecursiveCharacterTextSplitter from langchain_openai import OpenAIEmbeddings from pinecone import Pinecone, ServerlessSpec from langchain_pinecone import PineconeVectorStore from langchain.retrievers import ContextualCompressionRetriever from cohere.client import Client as CohereClient from langchain_cohere import CohereRerank from langchain.chains import create_retrieval_chain from langchain.memory import ConversationSummaryMemory from langchain_core.prompts import ChatPromptTemplate from langchain.chains.combine_documents import create_stuff_documents_chain from langchain.chains import create_history_aware_retriever from langchain_core.prompts import MessagesPlaceholder from langchain_core.chat_history import BaseChatMessageHistory from langchain_community.chat_message_histories import ChatMessageHistory from langchain_core.runnables.history import RunnableWithMessageHistory import asyncio import joblib import nest_asyncio load_dotenv() nest_asyncio.apply() # clear console function def cls(): os.system(&amp;#39;cls&amp;#39; if os.name==&amp;#39;nt&amp;#39; else &amp;#39;clear&amp;#39;) # OpenAI setup openai_api_key = os.getenv(&amp;quot;OPENAI_API_KEY&amp;quot;) llm = ChatOpenAI( api_key=openai_api_key, temperature=0, model=&amp;quot;gpt-3.5-turbo-0125&amp;quot;, streaming=True, ) # Pinecone setup (for vector storage) api_key_pinecone = os.getenv(&amp;quot;PINECONE_API_KEY&amp;quot;) pc = Pinecone(api_key=api_key_pinecone) # Cohere setup (for reranking) cohere_api_key = os.getenv(&amp;quot;COHERE_API_KEY&amp;quot;) cohere_client = CohereClient(api_key=cohere_api_key) # Cohere model API key and configuration embeddings = OpenAIEmbeddings(model=&amp;quot;text-embedding-3-large&amp;quot;) # Initialize Pinecone Vector Store vectorStore = PineconeVectorStore(index_name=&amp;quot;rag-newchatmodel&amp;quot;, embedding=embeddings) # Creating pkl string (required for llamaParser to work efficiently) def create_pkl_string(filename): file_name, extension = os.path.splitext(filename) new_string = file_name + &amp;quot;.pkl&amp;quot; return new_string # Loading and Parsing Data with the help of LlamaParse def load_or_parse_data(file_name): # LlamaParse creates a pkl file # PDF -&amp;gt; pkl -&amp;gt; md -&amp;gt; vector changed_file_ext = create_pkl_string(file_name) data_file = f&amp;quot;data/{changed_file_ext}&amp;quot; if os.path.exists(data_file): # Load the parsed data from the file parsed_data = joblib.load(data_file) else: # Perform the parsing step and store the result in llama_parse_documents parsingInstructionUber10k = &amp;quot;&amp;quot;&amp;quot;The provided document is unstructured It contains many tables, text, image and list. Try to be precise while answering the questions&amp;quot;&amp;quot;&amp;quot; parser = LlamaParse( api_key=&amp;quot;llamaparse-api-key&amp;quot;, result_type=&amp;quot;markdown&amp;quot;, # we want md file back parsing_instruction=parsingInstructionUber10k, max_timeout=5000, ) llama_parse_documents = parser.load_data(f&amp;quot;PDF_PATH/{file_name}&amp;quot;) # Save the parsed data to a file print(&amp;quot;Saving the parse results in .pkl format ..........&amp;quot;) joblib.dump(llama_parse_documents, f&amp;quot;data/{file_name}&amp;quot;) # Set the parsed data to the variable parsed_data = llama_parse_documents return parsed_data # Create vector database def create_vector_database(file_name): &amp;quot;&amp;quot;&amp;quot; Creates a vector database using document loaders and embeddings. This function loads urls, splits the loaded documents into chunks, transforms them into embeddings using OllamaEmbeddings, and finally persists the embeddings into a Chroma vector database. &amp;quot;&amp;quot;&amp;quot; print(file_name) # Call the function to either load or parse the data llama_parse_documents = load_or_parse_data(file_name) with open(&amp;quot;data/output.md&amp;quot;, &amp;quot;w&amp;quot;, encoding=&amp;quot;utf-8&amp;quot;) as f: for doc in llama_parse_documents: f.write(doc.text + &amp;quot;\n&amp;quot;) markdown_path = &amp;quot;data/output.md&amp;quot; print(&amp;quot;markdown_path&amp;quot;, markdown_path) loader = UnstructuredMarkdownLoader(markdown_path, encoding=&amp;quot;utf-8&amp;quot;) documents = loader.load() # Split loaded documents into chunks text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100) docs = text_splitter.split_documents(documents) # Prepare texts and metadatas texts = [d.page_content for d in docs] metadatas = [d.metadata for d in docs] # Inserting to index PineconeVectorStore.from_texts( texts, embeddings, index_name=&amp;quot;rag-newchatmodel&amp;quot;, metadatas=metadatas ) print(&amp;quot;Vector DB created successfully !&amp;quot;) return async def convert_to_vector(file_path, file_name): index_name = &amp;quot;rag-newchatmodel&amp;quot; # Check if the index exists existing_indexes = pc.list_indexes() if existing_indexes and existing_indexes[0].name == index_name: # Delete the old index pc.delete_index(index_name) existing_indexes = pc.list_indexes() # Create a new data pc.create_index( name=index_name, dimension=3072, metric=&amp;quot;cosine&amp;quot;, spec=ServerlessSpec(cloud=&amp;quot;aws&amp;quot;, region=&amp;quot;us-east-1&amp;quot;), ) create_vector_database(file_name) # PDF files directory (to save PDF files to local db) save_folder = &amp;quot;PDF_PATH&amp;quot; if &amp;quot;file_uploader_key&amp;quot; not in st.session_state: st.session_state[&amp;quot;file_uploader_key&amp;quot;] = 0 if &amp;quot;uploaded_files&amp;quot; not in st.session_state: st.session_state[&amp;quot;uploaded_files&amp;quot;] = [] # File uploader in the sidebar (Streamlit&amp;#39;s PDF uploader widget) files = st.sidebar.file_uploader( &amp;quot;Upload File&amp;quot;, type=[&amp;quot;pdf&amp;quot;], accept_multiple_files=True,key=st.session_state[&amp;quot;file_uploader_key&amp;quot;], ) if files: for uploaded_file in files: if not os.path.exists(save_folder): os.makedirs(save_folder) file_path = os.path.join(save_folder, uploaded_file.name) with open(file_path, mode=&amp;#39;wb&amp;#39;) as w: w.write(uploaded_file.getvalue()) file_name = uploaded_file.name asyncio.run(convert_to_vector(file_path, file_name)) st.sidebar.success(f&amp;quot;File {uploaded_file.name} uploaded successfully!&amp;quot;) st.session_state[&amp;quot;file_uploader_key&amp;quot;] += 1 st.rerun() # Function to list files in a directory def list_files_in_directory(directory): if os.path.exists(directory): return os.listdir(directory) return [] # Display the list of uploaded files with delete buttons st.sidebar.write(&amp;quot;### Uploaded Files:&amp;quot;) uploaded_files_list = list_files_in_directory(save_folder) # Function to delete a file def delete_file(file_path): if os.path.exists(file_path): os.remove(file_path) for file in uploaded_files_list: file_path = os.path.join(save_folder, file) col1, col2 = st.sidebar.columns([3, 1]) col1.write(file) if col2.button(&amp;quot;‚ùå&amp;quot;, key=file): delete_file(file_path) st.rerun() # Refresh the app to update the file list # Reranker def reRanker(): compressor = CohereRerank(client=cohere_client) compression_retriever = ContextualCompressionRetriever( base_compressor=compressor, base_retriever=vectorStore.as_retriever( search_kwargs={&amp;quot;k&amp;quot;: 5}, ), ) return compression_retriever # Initialize store if not in session state if &amp;quot;store&amp;quot; not in st.session_state: st.session_state.store = {} ### Statefully manage chat history ### store = {} def get_session_history(session_id: str) -&amp;gt; BaseChatMessageHistory: if session_id not in st.session_state.store: st.session_state.store[session_id] = ChatMessageHistory() return st.session_state.store[session_id] contextualize_q_system_prompt = ( &amp;quot;Given a chat history and the latest user question &amp;quot; &amp;quot;which might reference context in the chat history, &amp;quot; &amp;quot;formulate a standalone question which can be understood &amp;quot; &amp;quot;without the chat history. Do NOT answer the question, &amp;quot; &amp;quot;just reformulate it if needed and otherwise return it as is.&amp;quot; ) contextualize_q_prompt = ChatPromptTemplate.from_messages( [ (&amp;quot;system&amp;quot;, contextualize_q_system_prompt), MessagesPlaceholder(&amp;quot;chat_history&amp;quot;), (&amp;quot;human&amp;quot;, &amp;quot;{input}&amp;quot;), ] ) compression_retriever = reRanker() history_aware_retriever = create_history_aware_retriever( llm, compression_retriever, contextualize_q_prompt ) # After 19 July system_prompt = ( &amp;quot;You are an assistant for question-answering tasks specifically about the provided PDF documents.&amp;quot; &amp;quot;Follow the prompt STRICTLY but DO ensure that you don&amp;#39;t answer any question out of context.&amp;quot; &amp;quot;Use ONLY the following pieces of retrieved context to answer the question.&amp;quot; &amp;quot;Provide answers exactly as they are written in the PDF, quoting or paraphrasing text directly from the provided context.&amp;quot; &amp;quot;If you can&amp;#39;t find the answer in the given context, say &amp;#39;I&amp;#39;m sorry, but I couldn&amp;#39;t find information about that in the provided PDF documents.&amp;#39; &amp;quot; &amp;quot; Do not use any external knowledge.&amp;quot; &amp;quot;\n\n&amp;quot; &amp;quot;{context}&amp;quot; ) chatPrompt = ChatPromptTemplate.from_messages( [ (&amp;quot;system&amp;quot;, system_prompt), MessagesPlaceholder(&amp;quot;chat_history&amp;quot;), (&amp;quot;human&amp;quot;, &amp;quot;{input}&amp;quot;), ] ) question_answer_chain = create_stuff_documents_chain(llm, chatPrompt) rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain) conversational_rag_chain = RunnableWithMessageHistory( rag_chain, get_session_history, input_messages_key=&amp;quot;input&amp;quot;, output_messages_key=&amp;quot;answer&amp;quot;, history_messages_key=&amp;quot;chat_history&amp;quot;, ) # generate response def generate_response(prompt: str) : for chunk in conversational_rag_chain.stream(input={&amp;quot;input&amp;quot;: prompt},config={&amp;#39;configurable&amp;#39;: {&amp;#39;session_id&amp;#39;: &amp;quot;gaurav&amp;quot;}}): answer_chunk = chunk.get(&amp;quot;answer&amp;quot;) if answer_chunk: yield answer_chunk # Render chat history session_id = &amp;quot;gaurav&amp;quot; # Define your session ID if &amp;quot;chat_history&amp;quot; not in st.session_state: st.session_state.chat_history = [] # Conversation History for message in st.session_state.chat_history: if isinstance(message,HumanMessage): with st.chat_message(&amp;quot;Human&amp;quot;): st.markdown(message.content) else: with st.chat_message(&amp;quot;AI&amp;quot;): st.markdown(message.content) prompt = st.chat_input(&amp;quot;Hey, What&amp;#39;s up?&amp;quot;) if prompt is not None and prompt !=&amp;quot;&amp;quot; : st.session_state.chat_history.append(HumanMessage(prompt)) with st.chat_message(&amp;quot;Human&amp;quot;): st.markdown(prompt) if len(pc.list_indexes()) == 0: st.error(&amp;quot;Please upload some files first!&amp;quot;) else: with st.chat_message(&amp;quot;AI&amp;quot;): ai_response = st.write_stream(generate_response(prompt)) st.session_state.chat_history.append(AIMessage(ai_response)) &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Big_Barracuda_6753&quot;&gt; /u/Big_Barracuda_6753 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e6yskw/why_my_rag_is_a_bad_rag/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e6yskw/why_my_rag_is_a_bad_rag/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1e6yskw</id><link href="https://www.reddit.com/r/LangChain/comments/1e6yskw/why_my_rag_is_a_bad_rag/" /><updated>2024-07-19T08:30:15+00:00</updated><published>2024-07-19T08:30:15+00:00</published><title>Why my RAG is a bad RAG ?</title></entry><entry><author><name>/u/Fireche</name><uri>https://www.reddit.com/user/Fireche</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;how do you do it? I would like the reponse to be properly formatted in markdown for example. This does work somewhat but it still uses some non-unicode formatting for example: &amp;#39;\( LAF,max \geq 45 \)&amp;#39; instead of using &amp;quot;‚â•&amp;quot;. Even with one-shot prompting it does not follow the instructions properly. &lt;/p&gt; &lt;p&gt;Maybe there is another model that is trained on this task? Any suggestions? How do you solve this?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Fireche&quot;&gt; /u/Fireche &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e71886/how_to_instruct_gpt4_to_format_text/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e71886/how_to_instruct_gpt4_to_format_text/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1e71886</id><link href="https://www.reddit.com/r/LangChain/comments/1e71886/how_to_instruct_gpt4_to_format_text/" /><updated>2024-07-19T11:15:57+00:00</updated><published>2024-07-19T11:15:57+00:00</published><title>how to instruct GPT4 to format text?</title></entry><entry><author><name>/u/kingai404</name><uri>https://www.reddit.com/user/kingai404</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;&lt;strong&gt;Introduction&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Hey everyone! üöÄ I‚Äôm excited to share a new project: a Retrieval-Augmented Generation (RAG) Agent leveraging CrewAI, Composio, and ChatGPT to perform web searches and compile research reports.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Objectives&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;This project aims to create an intelligent agent that can enhance research capabilities by combining powerful AI tools to search the web and generate comprehensive reports.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Implementation Details&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Tools Used&lt;/strong&gt;: Composio, CrewAI, ChatGPT, Python&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Setup&lt;/strong&gt;: &lt;ol&gt; &lt;li&gt;Navigate to the project directory.&lt;/li&gt; &lt;li&gt;Run the setup file.&lt;/li&gt; &lt;li&gt;Fill in the &lt;code&gt;.env&lt;/code&gt; file with your secrets.&lt;/li&gt; &lt;li&gt;Run the Python script.&lt;/li&gt; &lt;/ol&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Results&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The RAG agent streamlines the process of conducting web searches and generating research reports, making it a valuable tool for researchers, students, and professionals.&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://git.new/RAGagent&quot;&gt;REPO LINK&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/kingai404&quot;&gt; /u/kingai404 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e6umwn/guide_to_create_a_rag_agent/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e6umwn/guide_to_create_a_rag_agent/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1e6umwn</id><link href="https://www.reddit.com/r/LangChain/comments/1e6umwn/guide_to_create_a_rag_agent/" /><updated>2024-07-19T04:00:02+00:00</updated><published>2024-07-19T04:00:02+00:00</published><title>Guide to create a RAG Agent</title></entry><entry><author><name>/u/Complete-Pie5760</name><uri>https://www.reddit.com/user/Complete-Pie5760</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hello,&lt;/p&gt; &lt;p&gt;I am currently looking into using LLMs for a project and figured Portkey might be a good start to test out different models. With a bit of research I saw that langchain is basically the number one library for doing this kind of things, because of RAG support etc.&lt;/p&gt; &lt;p&gt;What I do not quite understand is how do I integrate Portkey-AI with Langchain? I am quite confused here, would appreciate any input on this topic. Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Complete-Pie5760&quot;&gt; /u/Complete-Pie5760 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e7093e/how_to_use_langchain_with_portkey_ai_im_beginner/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e7093e/how_to_use_langchain_with_portkey_ai_im_beginner/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1e7093e</id><link href="https://www.reddit.com/r/LangChain/comments/1e7093e/how_to_use_langchain_with_portkey_ai_im_beginner/" /><updated>2024-07-19T10:13:55+00:00</updated><published>2024-07-19T10:13:55+00:00</published><title>How to use Langchain with Portkey AI? (I'm beginner with LLMs)</title></entry><entry><author><name>/u/gentleseahorse</name><uri>https://www.reddit.com/user/gentleseahorse</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2309.07864&quot;&gt;There‚Äôs been a huge rise in papers on LLM-based agents in the two years,&lt;/a&gt; showing obvious benefits in output quality and complexity of the task that can be handled. There are 2 obvious problems:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Latency: because agents are talking to each other ‚Äî usually sequentially ‚Äî output generation will take longer.&lt;/li&gt; &lt;li&gt;Cost: much more tokens are being spent on feeding one output to another input. Over and over.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;But these are exactly the things GPT 4o Mini concerns itself with. I‚Äôm actually incorporating it right now into some preprocessing workflows we have at &lt;a href=&quot;https://adorno.ai/&quot;&gt;adorno.ai&lt;/a&gt;. On principle, I dislike OpenAI, but it seems they&amp;#39;ve hit the ball out of the park? Again. I&amp;#39;m looking for criticism against 4o Mini? Right now it&amp;#39;s just rainbows and unicorns, but why is it overhyped?&lt;/p&gt; &lt;p&gt;(I&amp;#39;ve got a full blog post on the subject here: &lt;a href=&quot;https://chrisjanwust.medium.com/at-15c-million-tokens-will-gpt-4o-mini-be-the-foundation-of-agentic-workflows-7fd189138da4&quot;&gt;https://chrisjanwust.medium.com/at-15c-million-tokens-will-gpt-4o-mini-be-the-foundation-of-agentic-workflows-7fd189138da4&lt;/a&gt; )&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/gentleseahorse&quot;&gt; /u/gentleseahorse &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e6pizc/why_gpt_4o_mini_not_be_the_foundation_of_agentic/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e6pizc/why_gpt_4o_mini_not_be_the_foundation_of_agentic/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1e6pizc</id><link href="https://www.reddit.com/r/LangChain/comments/1e6pizc/why_gpt_4o_mini_not_be_the_foundation_of_agentic/" /><updated>2024-07-18T23:36:19+00:00</updated><published>2024-07-18T23:36:19+00:00</published><title>Why GPT 4o Mini not be the foundation of Agentic Workflows?</title></entry><entry><author><name>/u/ksaimohan2k</name><uri>https://www.reddit.com/user/ksaimohan2k</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi, I am able to upload the pdf file using the unstrucutred loader and query the PDf file, but I ask something like, Can you mention the source file to LLM? It is not answering.&lt;/p&gt; &lt;p&gt;The workflow is&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Uploading files using an unstructured loader, Text Splitter&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Creating Embeddings&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Storing in Vector DB&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;creating a retriever (as_retriever)&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Creating a Tool&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;creating a conversational agent.&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Is there any way to do it?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/ksaimohan2k&quot;&gt; /u/ksaimohan2k &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e6vqw0/retreiving_metadata_from_documents/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e6vqw0/retreiving_metadata_from_documents/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1e6vqw0</id><link href="https://www.reddit.com/r/LangChain/comments/1e6vqw0/retreiving_metadata_from_documents/" /><updated>2024-07-19T05:04:57+00:00</updated><published>2024-07-19T05:04:57+00:00</published><title>Retreiving Metadata from Documents</title></entry><entry><author><name>/u/Odd_Research_6995</name><uri>https://www.reddit.com/user/Odd_Research_6995</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Odd_Research_6995&quot;&gt; /u/Odd_Research_6995 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e6yc72/i_want_to_build_a_ecommerce_assistanti_have_tried/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e6yc72/i_want_to_build_a_ecommerce_assistanti_have_tried/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1e6yc72</id><link href="https://www.reddit.com/r/LangChain/comments/1e6yc72/i_want_to_build_a_ecommerce_assistanti_have_tried/" /><updated>2024-07-19T07:56:56+00:00</updated><published>2024-07-19T07:56:56+00:00</published><title>I want to build a ecommerce assistant.i have tried self queryand all.its not fetching as expected.can you suggest some way to fix the issues with rag?or some better rag approach?</title></entry><entry><author><name>/u/blogger786amd</name><uri>https://www.reddit.com/user/blogger786amd</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;HI&lt;/p&gt; &lt;p&gt;I am learning langchain these days and what I observe in youtube tutorials that they create chat applications mostly in which you get different responses like changing the tone of customer language, get replies to queries from documents etc..&lt;/p&gt; &lt;p&gt;This is what we can do with chatgpt, co-pilot as well. Then how we use langchain in pratical life? Also is there any tutorial on youtube which really create something which we actually use for businesses?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/blogger786amd&quot;&gt; /u/blogger786amd &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e6x8mb/routing_use_of_langchain_application/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e6x8mb/routing_use_of_langchain_application/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1e6x8mb</id><link href="https://www.reddit.com/r/LangChain/comments/1e6x8mb/routing_use_of_langchain_application/" /><updated>2024-07-19T06:40:53+00:00</updated><published>2024-07-19T06:40:53+00:00</published><title>Routing Use of Langchain Application</title></entry><entry><author><name>/u/AdAway2620</name><uri>https://www.reddit.com/user/AdAway2620</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Helllo guys, i am trying to create a PDF chatbot using Huggingface models. Open source embeddings and Open source LLM. Have anyone does this before or have similar kind of project ? I would be grateful if you help me . &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/AdAway2620&quot;&gt; /u/AdAway2620 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e6zj3b/arabic_pdf_rag/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e6zj3b/arabic_pdf_rag/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1e6zj3b</id><link href="https://www.reddit.com/r/LangChain/comments/1e6zj3b/arabic_pdf_rag/" /><updated>2024-07-19T09:23:38+00:00</updated><published>2024-07-19T09:23:38+00:00</published><title>Arabic PDF RAG</title></entry><entry><author><name>/u/FigureClassic6675</name><uri>https://www.reddit.com/user/FigureClassic6675</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hello!&lt;/p&gt; &lt;p&gt;I&amp;#39;m currently planning to develop a medical RAG chatbot.&lt;/p&gt; &lt;p&gt;The user will start by filling out a comprehensive questionnaire that includes details such as their name, age, blood group, blood test results, medical history, and other relevant medical records.&lt;/p&gt; &lt;p&gt;After the user submits this form, their responses will be processed by an LLM to generate personalized treatment plans, exercise routines, meal recommendations, and sleep schedules, among other tailored advice.&lt;/p&gt; &lt;p&gt;Any assistance or suggestions for relevant open-source repositories would be greatly appreciated.&lt;/p&gt; &lt;p&gt;Thank you!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/FigureClassic6675&quot;&gt; /u/FigureClassic6675 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e6bic0/how_can_i_create_medical_rag_chatbot/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e6bic0/how_can_i_create_medical_rag_chatbot/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1e6bic0</id><link href="https://www.reddit.com/r/LangChain/comments/1e6bic0/how_can_i_create_medical_rag_chatbot/" /><updated>2024-07-18T13:31:23+00:00</updated><published>2024-07-18T13:31:23+00:00</published><title>How can i create Medical RAG chatbot.</title></entry><entry><author><name>/u/Yeddine</name><uri>https://www.reddit.com/user/Yeddine</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e5z34y/should_i_open_source_this_tool/&quot;&gt; &lt;img src=&quot;https://b.thumbs.redditmedia.com/N0-_rhm56nZrU4qPFiBJlPcihF5IfMJX86dQjUd8vVM.jpg&quot; alt=&quot;Should I open source this tool?&quot; title=&quot;Should I open source this tool?&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey guys - I started building an AI tool for myself to talk to my data with SQL and RAG and need your feedback to know if it&amp;#39;s worth turning into an open-source project and/or SaaS.&lt;/p&gt; &lt;p&gt;The way it works is that you can connect a lot of data sources, structured or unstructured such as PostgreSQL, Snowflake, Notion, Facebook Ads, Shopify, PDFs... and you can chat with it, visualize it with tables and charts.&lt;/p&gt; &lt;p&gt;Do you see value in this, should I keep going?&lt;/p&gt; &lt;p&gt;Would love to hear your feedback and if you&amp;#39;d be interested in contributing or trying it for free&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://i.redd.it/14o76c00i6dd1.gif&quot;&gt;Text-to-SQL + RAG + Visualization&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Yeddine&quot;&gt; /u/Yeddine &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e5z34y/should_i_open_source_this_tool/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e5z34y/should_i_open_source_this_tool/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1e5z34y</id><media:thumbnail url="https://b.thumbs.redditmedia.com/N0-_rhm56nZrU4qPFiBJlPcihF5IfMJX86dQjUd8vVM.jpg" /><link href="https://www.reddit.com/r/LangChain/comments/1e5z34y/should_i_open_source_this_tool/" /><updated>2024-07-18T01:18:49+00:00</updated><published>2024-07-18T01:18:49+00:00</published><title>Should I open source this tool?</title></entry></feed>