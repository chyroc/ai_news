<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/"><category term="LangChain" label="r/LangChain"/><updated>2024-05-29T11:30:39+00:00</updated><icon>https://www.redditstatic.com/icon.png/</icon><id>/r/LangChain.rss</id><link rel="self" href="https://www.reddit.com/r/LangChain.rss" type="application/atom+xml" /><link rel="alternate" href="https://www.reddit.com/r/LangChain" type="text/html" /><subtitle>LangChain is an open-source framework and developer toolkit that helps developers get LLM applications from prototype to production. It is available for Python and Javascript at https://www.langchain.com/.</subtitle><title>LangChain</title><entry><author><name>/u/zchaarm</name><uri>https://www.reddit.com/user/zchaarm</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;A place for members of &lt;a href=&quot;/r/LangChain&quot;&gt;r/LangChain&lt;/a&gt; to chat with each other&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/zchaarm&quot;&gt; /u/zchaarm &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_10ljho9</id><link href="https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/" /><updated>2023-01-26T04:33:05+00:00</updated><published>2023-01-26T04:33:05+00:00</published><title>r/LangChain Lounge</title></entry><entry><author><name>/u/areweliving1</name><uri>https://www.reddit.com/user/areweliving1</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;I&amp;#39;m currently working on a LangChain agent and need some help with making a DataFrame (df) accessible to the agent. Here‚Äôs a quick overview of what I‚Äôm trying to achieve:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The agent has access to several function tools.&lt;/li&gt; &lt;li&gt;These tools require a DataFrame (df) as a parameter.&lt;/li&gt; &lt;li&gt;The agent‚Äôs task is to call these tools, passing the data stored in a df variable.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;My Question:&lt;/strong&gt; Is there a way to ensure that the df variable is accessible to the agent so it can successfully pass the data to the function tools?&lt;/p&gt; &lt;p&gt;Any guidance or examples would be greatly appreciated!&lt;/p&gt; &lt;p&gt;Thanks in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/areweliving1&quot;&gt; /u/areweliving1 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d368lr/how_to_make_dataframe_accessible_to_langchain/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d368lr/how_to_make_dataframe_accessible_to_langchain/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1d368lr</id><link href="https://www.reddit.com/r/LangChain/comments/1d368lr/how_to_make_dataframe_accessible_to_langchain/" /><updated>2024-05-29T07:17:10+00:00</updated><published>2024-05-29T07:17:10+00:00</published><title>How to Make DataFrame Accessible to LangChain Agent</title></entry><entry><author><name>/u/Individual-Deer-3984</name><uri>https://www.reddit.com/user/Individual-Deer-3984</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;üöÄ Exciting News! I just published a blog, Ragatouille‚Äì A guide to get started with Retrieval-Augmented Generation (RAG) with &lt;a href=&quot;https://www.linkedin.com/company/langchain/&quot;&gt;LangChain&lt;/a&gt; !&lt;/p&gt; &lt;p&gt;Based on Langchain&amp;#39;s RAG from scratch series, in this comprehensive guide, we break down the RAG process and take you through each step to enhance and optimize your system. Here‚Äôs a sneak peek of what you&amp;#39;ll learn:&lt;/p&gt; &lt;p&gt;üìå Introduction to RAG: Understand the basics of the RAG pipeline and how it combines retrieval-based systems with generative models.&lt;/p&gt; &lt;p&gt;üîç Query Transformation: Learn how to refine user queries for better comprehension and accurate responses.&lt;/p&gt; &lt;p&gt;üìö Hypothetical Document Embeddings: Discover techniques for generating vector representations of potential documents to assess relevance.&lt;/p&gt; &lt;p&gt;üõ§Ô∏è Routing Mechanisms: Implement intelligent routing to dynamically select the best data sources for your queries.&lt;/p&gt; &lt;p&gt;üìù Executable Queries: Master the art of translating user questions into executable queries.&lt;/p&gt; &lt;p&gt;üìÇ Effective Indexing: Explore indexing strategies to enhance retrieval efficiency.&lt;/p&gt; &lt;p&gt;üîÑ Advanced Retrieval Techniques: Dive into Self-RAG, Adaptive RAG, and CRAG for tailored retrieval approaches.&lt;/p&gt; &lt;p&gt;üí° Generation Phase: Ensure coherent and accurate responses using the retrieved information.&lt;/p&gt; &lt;p&gt;üè• Practical Application: Apply everything you&amp;#39;ve learned to build a sophisticated hospital management system using LangChain, LangGraph, and Neo4j.&lt;/p&gt; &lt;p&gt;üëâ Check out: &lt;a href=&quot;https://www.sakunaharinda.xyz/ragatouille-book&quot;&gt;https://www.sakunaharinda.xyz/ragatouille-book&lt;/a&gt;&lt;/p&gt; &lt;p&gt;‚¨ÜÔ∏è I will be updating the blog soon with RAG evaluation using RAGAS and Langsmith !!! Please let me know what you guys want to appear in this blog, which will help fellow RAG enthusiasts üí≠ I appreciate your constructive feedback and contributions !!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Individual-Deer-3984&quot;&gt; /u/Individual-Deer-3984 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d2xedu/ragatouille_a_guide_to_get_started_with/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d2xedu/ragatouille_a_guide_to_get_started_with/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1d2xedu</id><link href="https://www.reddit.com/r/LangChain/comments/1d2xedu/ragatouille_a_guide_to_get_started_with/" /><updated>2024-05-28T23:05:51+00:00</updated><published>2024-05-28T23:05:51+00:00</published><title>Ragatouille: A guide to get started with Retrieval-Augmented Generation (RAG) with LangChain !</title></entry><entry><author><name>/u/Intelligent-Fill-876</name><uri>https://www.reddit.com/user/Intelligent-Fill-876</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hello, how are you?&lt;/p&gt; &lt;p&gt;I am deploying a Kernel Memory service in production and wanted to get your opinion on my decision. Is it more cost-effective? The idea is to make it an async REST API.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Service host: EC2 - AWS.&lt;/li&gt; &lt;li&gt;Queue service: RabbitMQ on the EC2 machine hosting the Kernel Memory web service.&lt;/li&gt; &lt;li&gt;Storage &amp;amp; Vector Search: MongoDB Atlas.&lt;/li&gt; &lt;li&gt;The embedding and LLM models used will be from OpenAI.&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Intelligent-Fill-876&quot;&gt; /u/Intelligent-Fill-876 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d32vkw/kernel_memory_deploy_with_a_cheap_infrastructure/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d32vkw/kernel_memory_deploy_with_a_cheap_infrastructure/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1d32vkw</id><link href="https://www.reddit.com/r/LangChain/comments/1d32vkw/kernel_memory_deploy_with_a_cheap_infrastructure/" /><updated>2024-05-29T03:39:43+00:00</updated><published>2024-05-29T03:39:43+00:00</published><title>Kernel Memory | Deploy with a cheap infrastructure</title></entry><entry><author><name>/u/mahadevbhakti</name><uri>https://www.reddit.com/user/mahadevbhakti</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;m currently working on a project where I need to integrate GPT-3.5 or GPT-4 into a product carousel, instead of the typical markdown text display. Specifically, I&amp;#39;m looking to intercept the model&amp;#39;s function calls and then present the results in a carousel format on my site.&lt;/p&gt; &lt;p&gt;Has anyone here tackled a similar challenge? I&amp;#39;m interested in any insights or approaches you might have used to modify the output format of these models.&lt;/p&gt; &lt;p&gt;I&amp;#39;d still want the gpt model to have context of what was fetched from the API i.e. what products, specially their names and product Ids?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/mahadevbhakti&quot;&gt; /u/mahadevbhakti &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d3852u/intercepting_function_calling_and_showing_the/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d3852u/intercepting_function_calling_and_showing_the/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1d3852u</id><link href="https://www.reddit.com/r/LangChain/comments/1d3852u/intercepting_function_calling_and_showing_the/" /><updated>2024-05-29T09:35:40+00:00</updated><published>2024-05-29T09:35:40+00:00</published><title>Intercepting Function Calling and showing the data in UI</title></entry><entry><author><name>/u/BellaHi</name><uri>https://www.reddit.com/user/BellaHi</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;In a world where LLM applications are pushing the boundaries of what&amp;#39;s possible, observability is not just a nice-to-have‚Äîit&amp;#39;s essential for production-grade applications! Ensuring robust performance and reliability is a must, and that&amp;#39;s exactly what MyScale Telemetry delivers. &lt;/p&gt; &lt;p&gt;Say hello to the open-source alternative to LangSmith! &lt;a href=&quot;https://myscale.com/blog/myscale-telemetry-llm-app-observability/&quot;&gt;MyScale Telemetry&lt;/a&gt; is here to revolutionize how you trace and evaluate your LLM applications. With seamless integration with LangChain Callbacks, it&amp;#39;s the perfect tool to diagnose issues, optimize performance, and understand model behavior‚Äîall with the power of open-source! &lt;/p&gt; &lt;p&gt;MyScale Team&amp;#39;s Commitment to Open Source: Our passion for contributing to the community is unwavering, and with MyScale Telemetry, we&amp;#39;re excited to empower developers with the tools they need to innovate and excel. &lt;/p&gt; &lt;p&gt;Join us on this journey to enhance your LLM applications with MyScale Telemetry. Let&amp;#39;s shape the future of AI together, one trace at a time! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/BellaHi&quot;&gt; /u/BellaHi &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d2yjqe/introducing_myscale_telemetry_your_opensource/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d2yjqe/introducing_myscale_telemetry_your_opensource/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1d2yjqe</id><link href="https://www.reddit.com/r/LangChain/comments/1d2yjqe/introducing_myscale_telemetry_your_opensource/" /><updated>2024-05-28T23:58:32+00:00</updated><published>2024-05-28T23:58:32+00:00</published><title>Introducing MyScale Telemetry - Your Open-Source Alternative to LangSmith!</title></entry><entry><author><name>/u/Even-Constant-169</name><uri>https://www.reddit.com/user/Even-Constant-169</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d33nx3/gpt4o_langchain_rag_i_built_a_companion_robot/&quot;&gt; &lt;img src=&quot;https://external-preview.redd.it/PPPA2lfO59E4DAdlSvPLm8Et0y5yutcPR-22Y-hpYCo.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bc1b71d2d1ff2e986e192d59adbafdb21ef2ab72&quot; alt=&quot;[GPT-4o + LangChain + RAG] I built a companion robot with &amp;quot;memories&amp;quot; and &amp;quot;emotions&amp;quot;, his name is EVA [v1.0.0]&quot; title=&quot;[GPT-4o + LangChain + RAG] I built a companion robot with &amp;quot;memories&amp;quot; and &amp;quot;emotions&amp;quot;, his name is EVA [v1.0.0]&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Even-Constant-169&quot;&gt; /u/Even-Constant-169 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://youtu.be/riliV2PGKWQ&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d33nx3/gpt4o_langchain_rag_i_built_a_companion_robot/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1d33nx3</id><media:thumbnail url="https://external-preview.redd.it/PPPA2lfO59E4DAdlSvPLm8Et0y5yutcPR-22Y-hpYCo.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=bc1b71d2d1ff2e986e192d59adbafdb21ef2ab72" /><link href="https://www.reddit.com/r/LangChain/comments/1d33nx3/gpt4o_langchain_rag_i_built_a_companion_robot/" /><updated>2024-05-29T04:24:47+00:00</updated><published>2024-05-29T04:24:47+00:00</published><title>[GPT-4o + LangChain + RAG] I built a companion robot with &quot;memories&quot; and &quot;emotions&quot;, his name is EVA [v1.0.0]</title></entry><entry><author><name>/u/HappyDataGuy</name><uri>https://www.reddit.com/user/HappyDataGuy</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Like I want to show, executing agent chain, executing sql genery, generating response like it prints in termial. Is ther any way to do this?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/HappyDataGuy&quot;&gt; /u/HappyDataGuy &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d34xfq/how_to_show_what_agent_is_currently_doing_to_user/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d34xfq/how_to_show_what_agent_is_currently_doing_to_user/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1d34xfq</id><link href="https://www.reddit.com/r/LangChain/comments/1d34xfq/how_to_show_what_agent_is_currently_doing_to_user/" /><updated>2024-05-29T05:45:11+00:00</updated><published>2024-05-29T05:45:11+00:00</published><title>How to show what agent is currently doing to user in streamlit?</title></entry><entry><author><name>/u/GeorgiaWitness1</name><uri>https://www.reddit.com/user/GeorgiaWitness1</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d2iubg/a_library_just_for_document_extraction_with_llms/&quot;&gt; &lt;img src=&quot;https://b.thumbs.redditmedia.com/Nfj1Xt2q-wi-SUE2TM66lJeoEkGv1In07CH5M7vbu_o.jpg&quot; alt=&quot;A library just for Document Extraction with LLMs, connector to Langchain | ExtractThinker&quot; title=&quot;A library just for Document Extraction with LLMs, connector to Langchain | ExtractThinker&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;A month back I did a &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c6afp1/creating_a_framework_like_langchain_but_just_for/&quot;&gt;post&lt;/a&gt; about creating a library just focused on Extraction for documents. Was well received here and in other places, including by some companies. So I gave it a try.&lt;/p&gt; &lt;p&gt;After a month and 2k+ lines of code, I created this repo, based on the previous one, that will contain a full-open source code. Contains already close to 200+ (as the writing of this post).&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/enoch3712/ExtractThinker&quot;&gt;https://github.com/enoch3712/ExtractThinker&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;The motivation&lt;/h1&gt; &lt;p&gt;Langchain works great integrating the several pieces &lt;strong&gt;but tends to be a pain to extract data from documents and other sources&lt;/strong&gt;. ExtractThinker falls into the class of tools like instructor (pydantic outputs) and litellm (agnostic call between LLM models), which solves a specific problem. A bit more high level yes, but the focus is the same. &lt;strong&gt;Extraction for documents like ORM with LLM.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;You can read in detail here:&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://medium.com/towards-artificial-intelligence/extractthinker-ai-document-intelligence-with-llms-72cbce1890ef&quot;&gt;https://medium.com/towards-artificial-intelligence/extractthinker-ai-document-intelligence-with-llms-72cbce1890ef&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Basic use case and idea&lt;/h1&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/6qqufnee063d1.png?width=904&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=55bbd1aad1606fe6b83f4f07f338efab4deb6023&quot;&gt;https://preview.redd.it/6qqufnee063d1.png?width=904&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=55bbd1aad1606fe6b83f4f07f338efab4deb6023&lt;/a&gt;&lt;/p&gt; &lt;p&gt;You can then use a middleware to inject the QR code content, and so on. I think you get the drill&lt;/p&gt; &lt;h1&gt;Why is this useful? Just do GPT-4o everything, use vision if needed&lt;/h1&gt; &lt;p&gt;The project will focus in two things:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Reducing the pain of leading with document extraction&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The project will handle tasks such as classifying and grouping documents. For example, it can be used to separate content within a collection of PDFs with random pages.&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/k10voqea063d1.png?width=1170&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a64eaf8a015668ef85e0126226dd6e942ca53e5a&quot;&gt;splitting in action&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This would give you a list already separated and extracted (e.g first 2 pages invoice, last page driver&amp;#39;s license). This classification will expand to multiple strategies and techniques.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Reducing costs for scalability&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Build your architecture that works as well as GPT-4 with scalability and low cost. More in this article:&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://medium.com/@enoch3712/how-companies-use-llms-to-process-4-000-cvs-for-1-extractthinker-3fa0815057c3?sk=7fe626701a203135370e95f68bcb59f1&quot;&gt;https://medium.com/@enoch3712/how-companies-use-llms-to-process-4-000-cvs-for-1-extractthinker-3fa0815057c3?sk=7fe626701a203135370e95f68bcb59f1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Just finished the final touches for the code, and it&amp;#39;s a real use case that worked out great using inexpensive quantized models from deepinfra.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The code still not production-ready and missing most of the features, but will make more sense with templates once i build the documentation.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I intend to eventually integrate this into langchain to be used as &lt;strong&gt;pypdf&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;If you can assist me with features as issues, use cases, or if anyone is interested in giving it a try, I would greatly appreciate it.&lt;/p&gt; &lt;p&gt;Thank you for your time.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/GeorgiaWitness1&quot;&gt; /u/GeorgiaWitness1 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d2iubg/a_library_just_for_document_extraction_with_llms/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d2iubg/a_library_just_for_document_extraction_with_llms/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1d2iubg</id><media:thumbnail url="https://b.thumbs.redditmedia.com/Nfj1Xt2q-wi-SUE2TM66lJeoEkGv1In07CH5M7vbu_o.jpg" /><link href="https://www.reddit.com/r/LangChain/comments/1d2iubg/a_library_just_for_document_extraction_with_llms/" /><updated>2024-05-28T12:58:12+00:00</updated><published>2024-05-28T12:58:12+00:00</published><title>A library just for Document Extraction with LLMs, connector to Langchain | ExtractThinker</title></entry><entry><author><name>/u/EfficientSurvival</name><uri>https://www.reddit.com/user/EfficientSurvival</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I want to use AI to expand my 10,000 word story into a 60k word novel. Obviously Chat GPT doesn&amp;#39;t offer enough tokens to do this, so I&amp;#39;m wondering, can Lang Chain combine with Chat GPT to accomplish this? I haven&amp;#39;t used Lang Chain yet to understand its capabilities yet. &lt;/p&gt; &lt;p&gt;If it is possible for this to work, how complicated would it be to do?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/EfficientSurvival&quot;&gt; /u/EfficientSurvival &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d34r3h/10k_story_to_60k_novel/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d34r3h/10k_story_to_60k_novel/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1d34r3h</id><link href="https://www.reddit.com/r/LangChain/comments/1d34r3h/10k_story_to_60k_novel/" /><updated>2024-05-29T05:33:26+00:00</updated><published>2024-05-29T05:33:26+00:00</published><title>10k Story to 60k Novel?</title></entry><entry><author><name>/u/Dense_Technology_638</name><uri>https://www.reddit.com/user/Dense_Technology_638</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;GPT4AllEmbeddings modify model path&lt;/p&gt; &lt;p&gt;I&amp;#39;d like to modify the model path using GPT4AllEmbeddings and use a model I already downloading from the browser (the all-MiniLM-L6-v2-f16.gguf model, the same that GPT4AllEmbeddings downloads by default).&lt;/p&gt; &lt;p&gt;I need it to create RAG chatbot completely offline.&lt;/p&gt; &lt;p&gt;The langchain documentation chatbot suggests me to use:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;from langchain_community.embeddings &lt;strong&gt;import&lt;/strong&gt; GPT4AllEmbeddings&lt;br/&gt; model_path = &amp;quot;/path/to/your/model.bin&amp;quot;&lt;br/&gt; gpt4all_embd = GPT4AllEmbeddings(model=model_path)&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;I tried it but it does not work. It completely ignores the model path.&lt;/p&gt; &lt;p&gt;Is there something I&amp;#39;m missing?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Dense_Technology_638&quot;&gt; /u/Dense_Technology_638 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d2xqz3/gpt4allembeddings_doesnt_work_offline_no_way_to/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d2xqz3/gpt4allembeddings_doesnt_work_offline_no_way_to/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1d2xqz3</id><link href="https://www.reddit.com/r/LangChain/comments/1d2xqz3/gpt4allembeddings_doesnt_work_offline_no_way_to/" /><updated>2024-05-28T23:21:11+00:00</updated><published>2024-05-28T23:21:11+00:00</published><title>GPT4AllEmbeddings doesn‚Äôt work offline no way to pass a model path (any workarounds?)</title></entry><entry><author><name>/u/phicreative1997</name><uri>https://www.reddit.com/user/phicreative1997</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d2qqqy/building_an_agent_for_data_visualization_plotly/&quot;&gt; &lt;img src=&quot;https://external-preview.redd.it/vr9_wzciKe4iumRmBa3g5c3KTA0PkUeTfMxce8sgy8U.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e97032457cd9f6f0da1334fcf91b3a6e02ee9234&quot; alt=&quot;Building an Agent for Data Visualization (Plotly)&quot; title=&quot;Building an Agent for Data Visualization (Plotly)&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/phicreative1997&quot;&gt; /u/phicreative1997 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://medium.com/@arslanshahid-1997/building-an-agent-for-data-visualization-plotly-39310034c4e9&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d2qqqy/building_an_agent_for_data_visualization_plotly/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1d2qqqy</id><media:thumbnail url="https://external-preview.redd.it/vr9_wzciKe4iumRmBa3g5c3KTA0PkUeTfMxce8sgy8U.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=e97032457cd9f6f0da1334fcf91b3a6e02ee9234" /><link href="https://www.reddit.com/r/LangChain/comments/1d2qqqy/building_an_agent_for_data_visualization_plotly/" /><updated>2024-05-28T18:35:17+00:00</updated><published>2024-05-28T18:35:17+00:00</published><title>Building an Agent for Data Visualization (Plotly)</title></entry><entry><author><name>/u/Ok_Session_7304</name><uri>https://www.reddit.com/user/Ok_Session_7304</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hello everyone,&lt;/p&gt; &lt;p&gt;I&amp;#39;m exploring multi-agent systems and am curious about the role of an orchestrator in managing tasks among specialized agents. For instance, imagine a scenario where there are four agents, each designed to perform one of the basic mathematical operations: addition, subtraction, multiplication, and division.&lt;/p&gt; &lt;p&gt;If the orchestrator receives a question like &amp;quot;How much is 4 + 4?&amp;quot;, how does it determine which agent to send the query to? What logic or algorithms might it use to parse the question and delegate the task appropriately?&lt;/p&gt; &lt;p&gt;Additionally, if anyone could provide insights or resources into how such systems are generally designed or any examples of such orchestrators in action, it would be greatly appreciated.&lt;/p&gt; &lt;p&gt;Thanks in advance for your help and sharing your knowledge!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Ok_Session_7304&quot;&gt; /u/Ok_Session_7304 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d2omoe/how_does_an_llm_orchestrator_decide_which_agent/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d2omoe/how_does_an_llm_orchestrator_decide_which_agent/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1d2omoe</id><link href="https://www.reddit.com/r/LangChain/comments/1d2omoe/how_does_an_llm_orchestrator_decide_which_agent/" /><updated>2024-05-28T17:09:49+00:00</updated><published>2024-05-28T17:09:49+00:00</published><title>How does an LLM orchestrator decide which agent to use in a multi-agent system?</title></entry><entry><author><name>/u/cryptokaykay</name><uri>https://www.reddit.com/user/cryptokaykay</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d2nfuz/shopify_all_in_on_promptfoo/&quot;&gt; &lt;img src=&quot;https://preview.redd.it/qgg45w3d073d1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ab39be24a8d163f8797a5621ffba8c8ee470b5a6&quot; alt=&quot;Shopify all in on Promptfoo&quot; title=&quot;Shopify all in on Promptfoo&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I am a big fan of Promptfoo aswell. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/cryptokaykay&quot;&gt; /u/cryptokaykay &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://i.redd.it/qgg45w3d073d1.jpeg&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d2nfuz/shopify_all_in_on_promptfoo/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1d2nfuz</id><media:thumbnail url="https://preview.redd.it/qgg45w3d073d1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=ab39be24a8d163f8797a5621ffba8c8ee470b5a6" /><link href="https://www.reddit.com/r/LangChain/comments/1d2nfuz/shopify_all_in_on_promptfoo/" /><updated>2024-05-28T16:19:36+00:00</updated><published>2024-05-28T16:19:36+00:00</published><title>Shopify all in on Promptfoo</title></entry><entry><author><name>/u/nicoloboschi</name><uri>https://www.reddit.com/user/nicoloboschi</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;This is a poetry plugin to generate Dockerfile and images automatically.&lt;/p&gt; &lt;p&gt;This is a perfect choice if you built the Langchain application and you‚Äôre looking for to distribute is as microservice in the cloud. &lt;/p&gt; &lt;p&gt;This project lets you generate a docker image or just a Dockerfile for your poetry application without manual setup&lt;/p&gt; &lt;p&gt;It is meant for production images.&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/nicoloboschi/poetry-dockerize-plugin&quot;&gt;https://github.com/nicoloboschi/poetry-dockerize-plugin&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://pypi.org/project/poetry-dockerize-plugin/&quot;&gt;https://pypi.org/project/poetry-dockerize-plugin/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Get started with&lt;/p&gt; &lt;pre&gt;&lt;code&gt;poetry self add poetry-dockerize-plugin@latest &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This command generates a production-ready, optimized python image:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;poetry dockerize &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;or to generate a Dockerfile&lt;/p&gt; &lt;pre&gt;&lt;code&gt;poetry dockerize --generate &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/nicoloboschi&quot;&gt; /u/nicoloboschi &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d2rq1k/dockerize_langchainlangserve_applications/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d2rq1k/dockerize_langchainlangserve_applications/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1d2rq1k</id><link href="https://www.reddit.com/r/LangChain/comments/1d2rq1k/dockerize_langchainlangserve_applications/" /><updated>2024-05-28T19:14:29+00:00</updated><published>2024-05-28T19:14:29+00:00</published><title>Dockerize langchain/langserve applications</title></entry><entry><author><name>/u/rai_shi</name><uri>https://www.reddit.com/user/rai_shi</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d2eooc/how_to_make_rag_chain_faster_so_we_get_answer/&quot;&gt; &lt;img src=&quot;https://b.thumbs.redditmedia.com/mWWERioLyboV63rfWmqbccwffIRrWFxV91i_3-l0gFo.jpg&quot; alt=&quot;How to make RAG chain faster so we get answer faster&quot; title=&quot;How to make RAG chain faster so we get answer faster&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;We created a chatbot with Llama2-7b-chat, RAG architecture using LangChain, Qdrant VectorDB, and for web side Django. Everything works well. Even we hold the chat history. Now the only problem is the time we get the answer. Sometimes it takes 15 seconds to return the answer from the RAG chain and it&amp;#39;s too bad.&lt;/p&gt; &lt;p&gt;What should we do to make the system faster?&lt;/p&gt; &lt;p&gt;If it&amp;#39;s necessary I can share some of the codes.&lt;/p&gt; &lt;p&gt;Edit:&lt;/p&gt; &lt;p&gt;We are running all system in local. Because system have to be in local. Also we are running in GPU.&lt;/p&gt; &lt;p&gt;GPU: RTX A4000, 16GB&lt;/p&gt; &lt;p&gt;RAM: 32 GB&lt;/p&gt; &lt;p&gt;CPU: Intel¬Æ Xeon(R) W-2235 CPU @ 3.80GHz √ó 12&lt;/p&gt; &lt;p&gt;OS: Ubuntu 22.04.4 LTS&lt;/p&gt; &lt;p&gt;We realized that retrieving from the DB chain is taking the most time of the work. Actually, we configured it with some parameters like collection is now with Manhattan distance, 384 size (because of the embedded model), and binary quantization. We read that binary is x40 faster. But could we make more configuration?&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/30knup1kq63d1.png?width=1918&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8faf69a2a62d5e628fd46c4da9f909a3d5838c09&quot;&gt;https://preview.redd.it/30knup1kq63d1.png?width=1918&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8faf69a2a62d5e628fd46c4da9f909a3d5838c09&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/bedycqvkq63d1.png?width=1916&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8ab2aec755a0e58f155026d49d9c4442b408e90b&quot;&gt;https://preview.redd.it/bedycqvkq63d1.png?width=1916&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8ab2aec755a0e58f155026d49d9c4442b408e90b&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/rai_shi&quot;&gt; /u/rai_shi &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d2eooc/how_to_make_rag_chain_faster_so_we_get_answer/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d2eooc/how_to_make_rag_chain_faster_so_we_get_answer/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1d2eooc</id><media:thumbnail url="https://b.thumbs.redditmedia.com/mWWERioLyboV63rfWmqbccwffIRrWFxV91i_3-l0gFo.jpg" /><link href="https://www.reddit.com/r/LangChain/comments/1d2eooc/how_to_make_rag_chain_faster_so_we_get_answer/" /><updated>2024-05-28T08:39:52+00:00</updated><published>2024-05-28T08:39:52+00:00</published><title>How to make RAG chain faster so we get answer faster</title></entry><entry><author><name>/u/Gloomy-Traffic4964</name><uri>https://www.reddit.com/user/Gloomy-Traffic4964</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I find trying to do streaming makes everything else harder to do. Especially the checkpointer for message history.&lt;/p&gt; &lt;p&gt;This is my setup:&lt;/p&gt; &lt;pre&gt;&lt;code&gt; class GraphState(TypedDict): question: str messages: Annotated[list, add_messages] documents: List[str] workflow = StateGraph(GraphState) workflow.add_node(&amp;quot;retrieve&amp;quot;, retrieve) workflow.add_node(&amp;quot;generate&amp;quot;, generate) workflow.set_entry_point(&amp;quot;retrieve&amp;quot;) workflow.add_edge(&amp;quot;retrieve&amp;quot;, &amp;quot;generate&amp;quot;) workflow.add_edge(&amp;quot;generate&amp;quot;, END) pool = AsyncConnectionPool( conninfo=&amp;quot;postgresql://...&amp;quot;, max_size=20, ) # PostgresSaver.create_tables(pool) checkpoint = PostgresSaver( #package is langchain_postgres==0.0.3 serializer=PickleCheckpointSerializer(), async_connection=pool, ) app = workflow.compile(checkpointer=checkpoint) config = { &amp;quot;configurable&amp;quot;: { &amp;quot;thread_id&amp;quot;: &amp;quot;1223&amp;quot; } } async for event in app.astream_events({&amp;quot;messages&amp;quot;: [(&amp;quot;user&amp;quot;, user_content)], &amp;quot;question&amp;quot;: user_content}, config, version=&amp;quot;v1&amp;quot;): state = app.get_state(config) #this is the line that errors try: kind = event[&amp;quot;event&amp;quot;] except: kind = None if kind and kind == &amp;quot;on_chat_model_stream&amp;quot;: content = event[&amp;quot;data&amp;quot;][&amp;quot;chunk&amp;quot;].content if content: message = {&amp;quot;content&amp;quot;: content} await self.send(text_data=json.dumps(message)) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Specifically, I want to be able to use get_state() to get the document references from an answer (which is in the GraphState), as well as to do conditional interrupt like in &lt;a href=&quot;https://langchain-ai.github.io/langgraph/tutorials/customer-support/customer-support/#state-assistant&quot;&gt;this &lt;/a&gt;Customer Service LangGraph example (which requires get_state()).&lt;/p&gt; &lt;p&gt;In generral, is there a better way to handle the checkpointer while streaming the response?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Gloomy-Traffic4964&quot;&gt; /u/Gloomy-Traffic4964 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d2li6s/async_streaming_makes_everything_else_harder/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d2li6s/async_streaming_makes_everything_else_harder/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1d2li6s</id><link href="https://www.reddit.com/r/LangChain/comments/1d2li6s/async_streaming_makes_everything_else_harder/" /><updated>2024-05-28T14:58:09+00:00</updated><published>2024-05-28T14:58:09+00:00</published><title>Async streaming makes everything else harder</title></entry><entry><author><name>/u/rai_shi</name><uri>https://www.reddit.com/user/rai_shi</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;We created a chatbot with Llama2-7b-chat, RAG architecture using LangChain, Qdrant VectorDB, and for web side Django. Everything works well. Additionally, our system works locally. Now the only problem is the time we get the answer. Sometimes it takes 15 seconds to return the answer from the RAG chain and it&amp;#39;s too bad. After monitoring with LangSmith we saw that retrieve_documents takes most of the run time.&lt;/p&gt; &lt;p&gt;Our Qdrant VectorDB collections distance type Manhattan and binary quantization. The embedding model is sentence-transformers/all-MiniLM-L6-v2 so the size is 384. Also we clear the cache everytime. Here is the chains code and the invoke , (We write two system prompt for holding the chat history.)&lt;/p&gt; &lt;pre&gt;&lt;code&gt;class RAG(): def _init_(self): self.embed_model_id = &amp;#39;sentence-transformers/all-MiniLM-L6-v2&amp;#39; self.initializeEmbeddingModel(device) self.model_id = &amp;#39;meta-llama/Llama-2-13b-chat-hf&amp;#39; self.hf_auth = &amp;#39;token&amp;#39; self.initializeLLM() self.data_collection_name = &amp;quot;collection_name&amp;quot; self.db_url = &amp;quot;qdrant-docker-port&amp;quot; self.initializeDBclient() self.initializesearchDB() self.LLMpipeline() self.configSystemPrompt() self.RAGpipeline() def initializeEmbeddingModel(device): pass def initializeLLM(self): bnb_config = transformers.BitsAndBytesConfig( load_in_4bit=True, bnb_4bit_quant_type=&amp;#39;nf4&amp;#39;, bnb_4bit_use_double_quant=True, bnb_4bit_compute_dtype=bfloat16 ) model_config = transformers.AutoConfig.from_pretrained( self.model_id, token=self.hf_auth ) self.LLMmodel = transformers.AutoModelForCausalLM.from_pretrained( self.model_id, trust_remote_code=True, config=model_config, quantization_config=bnb_config, device_map=&amp;#39;auto&amp;#39;, token=self.hf_auth ) self.LLMmodel.eval() def initializeDBclient(self): pass def initializesearchDB(self): # self.searchDB creation pass def LLMpipeline(self): tokenizer = transformers.AutoTokenizer.from_pretrained( self.model_id, token=self.hf_auth ) pipeline = transformers.pipeline( model=self.LLMmodel, tokenizer=tokenizer, return_full_text=True, task=&amp;#39;text-generation&amp;#39;, temperature=0.0, max_new_tokens=512, repetition_penalty=1.1, do_sample=False ) self.llm = HuggingFacePipeline(pipeline=pipeline) def configSystemPrompt(self): # sohbet ge√ßmi≈üi i√ßin alt prompt ve genel system promptu tanƒ±mlƒ±yoruz contextualize_q_system_prompt = &amp;quot;&amp;quot;&amp;quot;prompt&amp;quot;&amp;quot;&amp;quot; self.contextualize_q_prompt = ChatPromptTemplate.from_messages( [ (&amp;quot;system&amp;quot;, contextualize_q_system_prompt), MessagesPlaceholder(&amp;quot;chat_history&amp;quot;), (&amp;quot;human&amp;quot;, &amp;quot;{input}&amp;quot;), ] ) qa_system_prompt = &amp;quot;&amp;quot;&amp;quot;prompt/ {context}&amp;quot;&amp;quot;&amp;quot; self.qa_prompt = ChatPromptTemplate.from_messages( [ (&amp;quot;system&amp;quot;, qa_system_prompt), MessagesPlaceholder(&amp;quot;chat_history&amp;quot;), (&amp;quot;human&amp;quot;, &amp;quot;{input}&amp;quot;), ] ) def RAGpipeline(self): retriever = self.searchDB.as_retriever(search_kwargs={&amp;quot;k&amp;quot;:6 }) history_aware_retriever = create_history_aware_retriever( self.llm, retriever, self.contextualize_q_prompt ) question_answer_chain = create_stuff_documents_chain(self.llm, self.qa_prompt) self.rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain) def ragQA(self, question, history): ai_msg = self.rag_chain.invoke({&amp;quot;input&amp;quot;: question, &amp;quot;chat_history&amp;quot;: history}) return ai_msg def clear_cache(self): torch.cuda.empty_cache() gc.collect() &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;LangSmith monitor result is here,&lt;/p&gt; &lt;p&gt;retrieval_chain 15.71s&lt;/p&gt; &lt;ul&gt; &lt;li&gt;retrieve_documents 11.66s &lt;ul&gt; &lt;li&gt;HuggingFacePipeline gpt2 11.61s&lt;/li&gt; &lt;li&gt;Retriever 0.04s&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;stuff_documents_chain 3.96s &lt;ul&gt; &lt;li&gt;format_inputs 0.01s &lt;ul&gt; &lt;li&gt;format_docs 0.00s&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;HuggingFacePipeline 3.94s&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;So the problem is mostly retrieve_documents. What should we do to make the system faster?&lt;/p&gt; &lt;p&gt;Edit:&lt;/p&gt; &lt;p&gt;GPU: RTX A4000, 16GB&lt;/p&gt; &lt;p&gt;RAM: 32 GB&lt;/p&gt; &lt;p&gt;CPU: Intel¬Æ Xeon(R) W-2235 CPU @ 3.80GHz √ó 12&lt;/p&gt; &lt;p&gt;OS: Ubuntu 22.04.4 LTS&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/rai_shi&quot;&gt; /u/rai_shi &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d2ooua/how_to_make_rag_retrieve_documents_chain_make/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d2ooua/how_to_make_rag_retrieve_documents_chain_make/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1d2ooua</id><link href="https://www.reddit.com/r/LangChain/comments/1d2ooua/how_to_make_rag_retrieve_documents_chain_make/" /><updated>2024-05-28T17:12:13+00:00</updated><published>2024-05-28T17:12:13+00:00</published><title>How to make RAG retrieve_documents chain make faster?</title></entry><entry><author><name>/u/JKSenior</name><uri>https://www.reddit.com/user/JKSenior</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi, I am working with langChain right now and created a FAISS vector store. Since today, my kernel crashes when running a similarity search on my vector store. Has anyone an idea why this is happening?&lt;/p&gt; &lt;pre&gt;&lt;code&gt;from langchain_community.document_loaders import PyPDFLoader from langchain_community.vectorstores import FAISS f = open(&amp;#39;credentials.txt&amp;#39;) OPENAI_API_KEY = f.read() embeddings_model = OpenAIEmbeddings(api_key=OPENAI_API_KEY) document_loader = PyPDFLoader(&amp;#39;filename.pdf&amp;#39;) text_splitter=RecursiveCharacterTextSplitter() documents = document_loader.load_and_split(text_splitter) vectorstore = FAISS.from_documents(documents, embeddings_model) vectorstore.similarity_search(&amp;#39;query&amp;#39;) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;MacBook Pro intel, python 3.9, jupyter notebook, langchain 0.2.0&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/JKSenior&quot;&gt; /u/JKSenior &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d2gxqi/kernel_crashes_for_faiss_similarity_search/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d2gxqi/kernel_crashes_for_faiss_similarity_search/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1d2gxqi</id><link href="https://www.reddit.com/r/LangChain/comments/1d2gxqi/kernel_crashes_for_faiss_similarity_search/" /><updated>2024-05-28T11:13:40+00:00</updated><published>2024-05-28T11:13:40+00:00</published><title>Kernel crashes for FAISS similarity search</title></entry><entry><author><name>/u/OtherAd3010</name><uri>https://www.reddit.com/user/OtherAd3010</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d2ebjg/i_just_made_a_openai_chatgpt_macos_clone_for/&quot;&gt; &lt;img src=&quot;https://external-preview.redd.it/yiLZNizOqeTGU311KWeYM6UbyTqjaCXj0ToJTGscvOo.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=86bea2b068f187fe638d3e41fddc1b81e06b399b&quot; alt=&quot;I just made a OpenAI ChatGPT MacOS Clone for Windows and Ubuntu and MIT licenced&quot; title=&quot;I just made a OpenAI ChatGPT MacOS Clone for Windows and Ubuntu and MIT licenced&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;h1&gt;Explanation and Reason&lt;/h1&gt; &lt;p&gt;Hi i am python developer and after OpenAI GPT-4o launch, i just i little bit angry because the app that they talked about just work in MacOS and a joke, it will come to windows november lol. Come on bro, if there is API, developers like us can make this app and release in just few days with langchain agent and tool infrastructure.&lt;/p&gt; &lt;p&gt;So i just release our GPT-4o clone and its totaly usable. You can use to take meeting notes, writing code and copying to your clipboard, or read and remember your calendar. There is unlimited possibilities.&lt;/p&gt; &lt;h1&gt;Current Features&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;**Screen Read**&lt;/li&gt; &lt;li&gt;Microphone&lt;/li&gt; &lt;li&gt;**System Audio**&lt;/li&gt; &lt;li&gt;Memory&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;**Clipboard**&lt;/li&gt; &lt;li&gt;Search Engines&lt;/li&gt; &lt;li&gt;**Python and SH Interpreters**&lt;/li&gt; &lt;li&gt;Writing and Running Scripts&lt;/li&gt; &lt;li&gt;Using your Telegram Account&lt;/li&gt; &lt;li&gt;Knowledge Management&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Open Call to devs&lt;/h1&gt; &lt;p&gt;If there is some people to interest and develop this and adding some features just like auto take screen shot each 5 second and say somethings if something wrong. We can open a &lt;strong&gt;GitHub Organization&lt;/strong&gt; and develop together. Just for open-source, just for competition.&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/vcbo2sw4l43d1.png?width=656&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=29c11615550af7eebdcaa7cdaf31d2c986536a44&quot;&gt;https://preview.redd.it/vcbo2sw4l43d1.png?width=656&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=29c11615550af7eebdcaa7cdaf31d2c986536a44&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/onuratakan/gpt-computer-assistant&quot;&gt;onuratakan/gpt-computer-assistant: gpt-4o Desktop Personel Asisstant (github.com)&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/OtherAd3010&quot;&gt; /u/OtherAd3010 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d2ebjg/i_just_made_a_openai_chatgpt_macos_clone_for/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d2ebjg/i_just_made_a_openai_chatgpt_macos_clone_for/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1d2ebjg</id><media:thumbnail url="https://external-preview.redd.it/yiLZNizOqeTGU311KWeYM6UbyTqjaCXj0ToJTGscvOo.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=86bea2b068f187fe638d3e41fddc1b81e06b399b" /><link href="https://www.reddit.com/r/LangChain/comments/1d2ebjg/i_just_made_a_openai_chatgpt_macos_clone_for/" /><updated>2024-05-28T08:11:18+00:00</updated><published>2024-05-28T08:11:18+00:00</published><title>I just made a OpenAI ChatGPT MacOS Clone for Windows and Ubuntu and MIT licenced</title></entry><entry><author><name>/u/Minute_Scientist8107</name><uri>https://www.reddit.com/user/Minute_Scientist8107</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d2i9fr/llm_to_get_the_relevant_table_from_db/&quot;&gt; &lt;img src=&quot;https://b.thumbs.redditmedia.com/mM2WSN5QqrKBeDXL-Bo76v--LrrG6-cOIltF3tsZb5w.jpg&quot; alt=&quot;LLM to get the relevant table from db &quot; title=&quot;LLM to get the relevant table from db &quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey guys ! I have Azure OpenAI GPT 4 LLM connected to Azure SQL DB. When I use this command db.get_usable_table_names() must return all the tables available, how every I&amp;#39;m getting the names of the ones that are dbo.table_name&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/fw1477bhu53d1.png?width=334&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=55b2f27dd632bf0c984eb3f71b22d7f8d052071c&quot;&gt;https://preview.redd.it/fw1477bhu53d1.png?width=334&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=55b2f27dd632bf0c984eb3f71b22d7f8d052071c&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/rwqeu6nku53d1.png?width=377&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ca4eaeffdebd441b75ad22122f3e4798ecc67ae3&quot;&gt;https://preview.redd.it/rwqeu6nku53d1.png?width=377&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ca4eaeffdebd441b75ad22122f3e4798ecc67ae3&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The tables in my db are as follows:&lt;/p&gt; &lt;p&gt;Question 1:How can I have the llm return all the available tables ?&lt;br/&gt; Question 2: Let&amp;#39;s say I prompt the llm like &amp;quot;Aggregate what age group customers prefer what products.&amp;quot; This prompt has two tables: Customer and Product. So I want the llm to join these two tables, and generate a response. So we want the llm to go through multiple tables and give a response.&lt;br/&gt; How can I achieve this?&lt;br/&gt; Thanks for your help !&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Minute_Scientist8107&quot;&gt; /u/Minute_Scientist8107 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d2i9fr/llm_to_get_the_relevant_table_from_db/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d2i9fr/llm_to_get_the_relevant_table_from_db/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1d2i9fr</id><media:thumbnail url="https://b.thumbs.redditmedia.com/mM2WSN5QqrKBeDXL-Bo76v--LrrG6-cOIltF3tsZb5w.jpg" /><link href="https://www.reddit.com/r/LangChain/comments/1d2i9fr/llm_to_get_the_relevant_table_from_db/" /><updated>2024-05-28T12:28:13+00:00</updated><published>2024-05-28T12:28:13+00:00</published><title>LLM to get the relevant table from db</title></entry><entry><author><name>/u/jy2k</name><uri>https://www.reddit.com/user/jy2k</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;What do you use to handle agents permissin to retrieve access certain data? Let&amp;#39;s say your building an agent in a large org. How do you make sure it can access finance question if you asked it a question about finance. It needs to validate if you are permitted to access that kind of data.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/jy2k&quot;&gt; /u/jy2k &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d2cezb/agent_permission/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d2cezb/agent_permission/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1d2cezb</id><link href="https://www.reddit.com/r/LangChain/comments/1d2cezb/agent_permission/" /><updated>2024-05-28T05:56:42+00:00</updated><published>2024-05-28T05:56:42+00:00</published><title>Agent permission</title></entry><entry><author><name>/u/glip-glop-evil</name><uri>https://www.reddit.com/user/glip-glop-evil</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;m working on a project that needs to create long term memory with a knowledge graph. Does anyone have any experience with any specific libraries to achieve this?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/glip-glop-evil&quot;&gt; /u/glip-glop-evil &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d2dwpo/knowledge_graphs_and_longterm_memory/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d2dwpo/knowledge_graphs_and_longterm_memory/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1d2dwpo</id><link href="https://www.reddit.com/r/LangChain/comments/1d2dwpo/knowledge_graphs_and_longterm_memory/" /><updated>2024-05-28T07:41:15+00:00</updated><published>2024-05-28T07:41:15+00:00</published><title>Knowledge Graphs and long-term memory</title></entry><entry><author><name>/u/SpaceXDragon17</name><uri>https://www.reddit.com/user/SpaceXDragon17</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hello everyone! I&amp;#39;m in the process of developing a custom LLM for my startup. I&amp;#39;m looking for help and would appreciate it if anyone on this forum has experience with this and would be willing to meet via Zoom to discuss it further.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/SpaceXDragon17&quot;&gt; /u/SpaceXDragon17 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d2g20b/need_to_build_custom_llm_expert_needed/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d2g20b/need_to_build_custom_llm_expert_needed/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1d2g20b</id><link href="https://www.reddit.com/r/LangChain/comments/1d2g20b/need_to_build_custom_llm_expert_needed/" /><updated>2024-05-28T10:16:58+00:00</updated><published>2024-05-28T10:16:58+00:00</published><title>Need to Build Custom LLM - Expert Needed</title></entry><entry><author><name>/u/Puzzleheaded_Bee5489</name><uri>https://www.reddit.com/user/Puzzleheaded_Bee5489</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;m using OpenAI GPT 3.5 turbo for summarising data from sensitive documents, which contains some of my personal information. Currently, I&amp;#39;m manually removing some of the sensitive data from the inputs. I want to know if LangChain or any other tool/library handles this automatically without me getting involved?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Puzzleheaded_Bee5489&quot;&gt; /u/Puzzleheaded_Bee5489 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d1v710/hashingmasking_sensitive_data_before_sending_out/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d1v710/hashingmasking_sensitive_data_before_sending_out/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1d1v710</id><link href="https://www.reddit.com/r/LangChain/comments/1d1v710/hashingmasking_sensitive_data_before_sending_out/" /><updated>2024-05-27T16:13:41+00:00</updated><published>2024-05-27T16:13:41+00:00</published><title>Hashing/Masking sensitive data before sending out to OpenAI</title></entry><entry><author><name>/u/AnEdgeLordWeeb</name><uri>https://www.reddit.com/user/AnEdgeLordWeeb</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey guys! I&amp;#39;m trying to build a chatbot that offer video games recommendations using LangGraph.&lt;/p&gt; &lt;p&gt;Right now, I was working on the agent called &amp;quot;Game Search Agent&amp;quot; which objective is to search the web for the best results to the user query.&lt;/p&gt; &lt;p&gt;Problem is, the execution never moves from this node to the __end__ one. It will always be stuck in a loop, where the tool function is called without providing a concrete final answer in the end.&lt;/p&gt; &lt;p&gt;Can somebody please take a look at this code snippet and tell me please what&amp;#39;s wrong? Thank you!&lt;/p&gt; &lt;pre&gt;&lt;code&gt;from typing import Annotated, List from langchain_openai import ChatOpenAI from langchain_community.tools.tavily_search import TavilySearchResults from langchain_core.prompts import PromptTemplate from langchain_core.messages import BaseMessage from typing_extensions import TypedDict from langgraph.graph import StateGraph from langgraph.graph.message import add_messages from langgraph.prebuilt import ToolNode, tools_condition os.environ[&amp;quot;TAVILY_API_KEY&amp;quot;] = &amp;quot;your_api_key&amp;quot; os.environ[&amp;quot;OPEN_AI_KEY&amp;quot;] = &amp;quot;your_api_key&amp;quot; class AgentState(TypedDict): messages: Annotated[List[BaseMessage], add_messages] query: str games: List[str] tool = TavilySearchResults(max_results=2) tools = [tool] llm = ChatOpenAI(model=&amp;quot;gpt-4o&amp;quot;, temperature=0) llm_with_tools = llm.bind_tools(tools) def game_title_search(state: AgentState): game_search_prompt = PromptTemplate( template=&amp;quot;&amp;quot;&amp;quot;You are part of a chatbot that provides personalized video game recommendations based on user preferences. \n Your task is to search for video games that match the user query, using the Tavily API. \n Only return the titles of the games. \n The number of games to return is limited to 5. \n\n The results provided will STRICTLY look as follows (Python list): \n [&amp;quot;game_title_1&amp;quot;, &amp;quot;game_title_2&amp;quot;, &amp;quot;game_title_3&amp;quot;, ...] \n\n User Query: {query}&amp;quot;&amp;quot;&amp;quot;, input_variables=[&amp;quot;query&amp;quot;], ) game_search = game_search_prompt | llm_with_tools return {&amp;quot;messages&amp;quot;: [game_search.invoke({&amp;quot;query&amp;quot;: state[&amp;quot;query&amp;quot;]})]} graph_builder = StateGraph(AgentState) graph_builder.add_node(&amp;quot;game_search&amp;quot;, game_title_search) tool_node = ToolNode(tools=[tool]) graph_builder.add_node(&amp;quot;tools&amp;quot;, tool_node) graph_builder.add_conditional_edges( &amp;quot;game_search&amp;quot;, tools_condition, ) graph_builder.add_edge(&amp;quot;tools&amp;quot;, &amp;quot;game_search&amp;quot;) graph_builder.set_entry_point(&amp;quot;game_search&amp;quot;) graph = graph_builder.compile() while True: user_input = input(&amp;quot;User: &amp;quot;) if user_input.lower() in [&amp;quot;quit&amp;quot;, &amp;quot;exit&amp;quot;, &amp;quot;q&amp;quot;]: print(&amp;quot;Goodbye!&amp;quot;) break for event in graph.stream({&amp;quot;messages&amp;quot;: [user_input], &amp;quot;query&amp;quot;: user_input}, {&amp;quot;recursion_limit&amp;quot;: 150}): for value in event.values(): if isinstance(value[&amp;quot;messages&amp;quot;][-1], BaseMessage): print(&amp;quot;Assistant:&amp;quot;, value[&amp;quot;messages&amp;quot;][-1].content) &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/AnEdgeLordWeeb&quot;&gt; /u/AnEdgeLordWeeb &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d24j6j/agent_enters_a_loop_of_continuous_tool_calling/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d24j6j/agent_enters_a_loop_of_continuous_tool_calling/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1d24j6j</id><link href="https://www.reddit.com/r/LangChain/comments/1d24j6j/agent_enters_a_loop_of_continuous_tool_calling/" /><updated>2024-05-27T22:45:47+00:00</updated><published>2024-05-27T22:45:47+00:00</published><title>Agent enters a loop of continuous tool calling without exiting and providing a final answer</title></entry></feed>