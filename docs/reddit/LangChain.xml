<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/"><category term="LangChain" label="r/LangChain"/><updated>2024-04-19T17:32:29+00:00</updated><icon>https://www.redditstatic.com/icon.png/</icon><id>/r/LangChain.rss</id><link rel="self" href="https://www.reddit.com/r/LangChain.rss" type="application/atom+xml" /><link rel="alternate" href="https://www.reddit.com/r/LangChain" type="text/html" /><subtitle>LangChain is an open-source framework and developer toolkit that helps developers get LLM applications from prototype to production. It is available for Python and Javascript at https://www.langchain.com/.</subtitle><title>LangChain</title><entry><author><name>/u/zchaarm</name><uri>https://www.reddit.com/user/zchaarm</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;A place for members of &lt;a href=&quot;/r/LangChain&quot;&gt;r/LangChain&lt;/a&gt; to chat with each other&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/zchaarm&quot;&gt; /u/zchaarm &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_10ljho9</id><link href="https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/" /><updated>2023-01-26T04:33:05+00:00</updated><published>2023-01-26T04:33:05+00:00</published><title>r/LangChain Lounge</title></entry><entry><author><name>/u/cryptokaykay</name><uri>https://www.reddit.com/user/cryptokaykay</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c7zp7y/langtrace_evaluations_breeze_through_with_hotkeys/&quot;&gt; &lt;img src=&quot;https://external-preview.redd.it/O9Z_56AIGA8RzHakPI49lAWCrCkAXsFQRc9Kf8-iHoQ.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e336e7bce43cfe2c02944103702fbfa275762561&quot; alt=&quot;Langtrace Evaluations - Breeze through with hotkeys&quot; title=&quot;Langtrace Evaluations - Breeze through with hotkeys&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;&lt;a href=&quot;https://reddit.com/link/1c7zp7y/video/v8ord1suegvc1/player&quot;&gt;https://reddit.com/link/1c7zp7y/video/v8ord1suegvc1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We have been busy shipping updates to &lt;strong&gt;Langtrace&lt;/strong&gt;, an &lt;strong&gt;Open Source LLM observability tool&lt;/strong&gt; and I am excited to show our new and improved Evaluations Dashboard. We learned from our early users that improving the RAG/model accuracy and gaining confidence with deploying their LLM based apps to production has been the number 1 priority.&lt;/p&gt; &lt;p&gt;To solve for this, we have built a couple of things:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;1. Create tests with different scoring scales and automatically capture LLM requests to these tests using Langtrace&amp;#39;s SDK.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;2. Evaluate the the requests by scoring against the response provided by the LLM to measure the overall average of each test.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Effectively, teams can come up with a release criteria like - &amp;quot;Factual Accuracy &amp;gt; 99%, Response Quality &amp;gt; 95%, Response Bias &amp;gt; 85%, Context Recall &amp;gt; 90%&amp;quot; and measure their product&amp;#39;s performance against this release metric with Langtrace.&lt;/p&gt; &lt;p&gt;Additionally, we also realized that the user experience is extremely important for effective and fast evaluations. As a result, the evaluations flow is fully optimized for hot keys and as an evaluator, you can breeze through a series of evaluations with just the arrow keys, enter and backspace without having to click through a bunch of times for each request.&lt;/p&gt; &lt;p&gt;Finally, all of this can be setup with just 2 lines of code and Langtrace&amp;#39;s Evaluation&amp;#39;s dashboard will start capturing the requests in the appropriate test automatically&lt;/p&gt; &lt;p&gt;Don&amp;#39;t forget to check out Langtrace and star the repository on Github.&lt;/p&gt; &lt;p&gt;Github - &lt;a href=&quot;https://github.com/Scale3-Labs/langtrace&quot;&gt;https://github.com/Scale3-Labs/langtrace&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/cryptokaykay&quot;&gt; /u/cryptokaykay &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c7zp7y/langtrace_evaluations_breeze_through_with_hotkeys/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c7zp7y/langtrace_evaluations_breeze_through_with_hotkeys/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1c7zp7y</id><media:thumbnail url="https://external-preview.redd.it/O9Z_56AIGA8RzHakPI49lAWCrCkAXsFQRc9Kf8-iHoQ.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=e336e7bce43cfe2c02944103702fbfa275762561" /><link href="https://www.reddit.com/r/LangChain/comments/1c7zp7y/langtrace_evaluations_breeze_through_with_hotkeys/" /><updated>2024-04-19T15:28:34+00:00</updated><published>2024-04-19T15:28:34+00:00</published><title>Langtrace Evaluations - Breeze through with hotkeys</title></entry><entry><author><name>/u/GPT-Claude-Gemini</name><uri>https://www.reddit.com/user/GPT-Claude-Gemini</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I have heard from many people that Langchain is good for prototyping but not for production, is it because it&amp;#39;s slower than using each LLM&amp;#39;s APIs directly? I did some testing comparing the response speed from calling OpenAI directly versus calling it via Langchain, and Langchain consistently generates output 10% - 30% slower, not sure if it&amp;#39;s my local problem or if it&amp;#39;s a universal observation. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/GPT-Claude-Gemini&quot;&gt; /u/GPT-Claude-Gemini &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c82clj/is_it_true_that_its_slower_to_use_langchain_than/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c82clj/is_it_true_that_its_slower_to_use_langchain_than/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1c82clj</id><link href="https://www.reddit.com/r/LangChain/comments/1c82clj/is_it_true_that_its_slower_to_use_langchain_than/" /><updated>2024-04-19T17:15:38+00:00</updated><published>2024-04-19T17:15:38+00:00</published><title>Is it true that it's slower to use Langchain than to call the model API's directly?</title></entry><entry><author><name>/u/Glittering-Bear5748</name><uri>https://www.reddit.com/user/Glittering-Bear5748</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;how prevent duplicate embedding instance in pgvector without delete collection.&lt;/p&gt; &lt;p&gt;update existing once and want new one &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Glittering-Bear5748&quot;&gt; /u/Glittering-Bear5748 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c80n7e/pgvector_duplicate_embedding/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c80n7e/pgvector_duplicate_embedding/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1c80n7e</id><link href="https://www.reddit.com/r/LangChain/comments/1c80n7e/pgvector_duplicate_embedding/" /><updated>2024-04-19T16:06:52+00:00</updated><published>2024-04-19T16:06:52+00:00</published><title>PGVector duplicate embedding</title></entry><entry><author><name>/u/mehul_gupta1997</name><uri>https://www.reddit.com/user/mehul_gupta1997</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/mehul_gupta1997&quot;&gt; /u/mehul_gupta1997 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;/r/learnmachinelearning/comments/1c7rksc/tried_llama3_by_meta_today/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c7rl12/tried_llama3_by_meta_today/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1c7rl12</id><link href="https://www.reddit.com/r/LangChain/comments/1c7rl12/tried_llama3_by_meta_today/" /><updated>2024-04-19T08:17:28+00:00</updated><published>2024-04-19T08:17:28+00:00</published><title>Tried Llama3 by Meta today</title></entry><entry><author><name>/u/Educational-String94</name><uri>https://www.reddit.com/user/Educational-String94</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c6zktz/llms_frameworks_langchain_llamaindex_griptape/&quot;&gt; &lt;img src=&quot;https://preview.redd.it/1iz91emwo7vc1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=370b047730989d9b9c4a6c0366e977a641be69db&quot; alt=&quot;LLMs frameworks (langchain, llamaindex, griptape, autogen, crewai etc.) are overengineered and makes easy tasks hard, correct me if im wrong&quot; title=&quot;LLMs frameworks (langchain, llamaindex, griptape, autogen, crewai etc.) are overengineered and makes easy tasks hard, correct me if im wrong&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Educational-String94&quot;&gt; /u/Educational-String94 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://i.redd.it/1iz91emwo7vc1.jpeg&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c6zktz/llms_frameworks_langchain_llamaindex_griptape/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1c6zktz</id><media:thumbnail url="https://preview.redd.it/1iz91emwo7vc1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=370b047730989d9b9c4a6c0366e977a641be69db" /><link href="https://www.reddit.com/r/LangChain/comments/1c6zktz/llms_frameworks_langchain_llamaindex_griptape/" /><updated>2024-04-18T10:04:19+00:00</updated><published>2024-04-18T10:04:19+00:00</published><title>LLMs frameworks (langchain, llamaindex, griptape, autogen, crewai etc.) are overengineered and makes easy tasks hard, correct me if im wrong</title></entry><entry><author><name>/u/boy_with_eng_tattoo</name><uri>https://www.reddit.com/user/boy_with_eng_tattoo</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey guys! Anyone here running RAG pipelines in production with user uploaded documents (semantic chunking, summaries, knowledge graphs etc)? I am working on a RAG application for PPC and want to chunks SOPs into chunks for vector DB. But I am confused on which chunking strategy is better to get the most optimal results from the LLM.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/boy_with_eng_tattoo&quot;&gt; /u/boy_with_eng_tattoo &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c7pnzg/need_help_in_understanding_chunking_strategy_for/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c7pnzg/need_help_in_understanding_chunking_strategy_for/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1c7pnzg</id><link href="https://www.reddit.com/r/LangChain/comments/1c7pnzg/need_help_in_understanding_chunking_strategy_for/" /><updated>2024-04-19T06:08:21+00:00</updated><published>2024-04-19T06:08:21+00:00</published><title>Need help in understanding chunking strategy for RAG application in production</title></entry><entry><author><name>/u/naotemfin</name><uri>https://www.reddit.com/user/naotemfin</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;m struggling trying to use a .txt and a .csv file to give more context to my Chatbot.&lt;/p&gt; &lt;p&gt;This is partly what I have at the moment:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;# For text files text_loader = DirectoryLoader(&amp;quot;&amp;quot;, glob=&amp;quot;./test.txt&amp;quot;, loader_cls=TextLoader) text_docs = text_loader.load() # For CSV files csv_loader = DirectoryLoader(&amp;quot;&amp;quot;, glob=&amp;quot;./stock.csv&amp;quot;, loader_cls=CSVLoader) csv_docs = csv_loader.load() # Combine the documents loader_all = MergedDataLoader(loaders=[text_loader, csv_loader]) docs = loader_all.load() db = Chroma.from_documents(docs, embedding_function) retriever = db.as_retriever() chain = ( {&amp;quot;context&amp;quot;: retriever, &amp;quot;question&amp;quot;: RunnablePassthrough()} | prompt | model | StrOutputParser() ) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;When I ask something related to the data within the .csv (related to some products&amp;#39; stock) it answers well. But, if it&amp;#39;s a question of something within the .txt file, it doesn&amp;#39;t know what to answer.&lt;/p&gt; &lt;p&gt;Have someone already dealt with this?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/naotemfin&quot;&gt; /u/naotemfin &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c7fcad/best_way_of_using_context_from_different_files/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c7fcad/best_way_of_using_context_from_different_files/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1c7fcad</id><link href="https://www.reddit.com/r/LangChain/comments/1c7fcad/best_way_of_using_context_from_different_files/" /><updated>2024-04-18T21:34:39+00:00</updated><published>2024-04-18T21:34:39+00:00</published><title>Best way of using context from different files?</title></entry><entry><author><name>/u/Diligent_Tonight3232</name><uri>https://www.reddit.com/user/Diligent_Tonight3232</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c7skfd/llama2_generating_empty_response/&quot;&gt; &lt;img src=&quot;https://preview.redd.it/85fat4n5nevc1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=27212a571ebd9bf1b7554f8c95b26285d8a138da&quot; alt=&quot;Llama2 generating empty response&quot; title=&quot;Llama2 generating empty response&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey everyone! I&amp;#39;m working on building a simple email replier, which uses the llama2-7b-chat-gguf model from Huggingface. I&amp;#39;m using the CTransformers function to create a llm instance and then using a custom prompt template to generate reply to an email body. I&amp;#39;m using the get_response() function from the image above.&lt;/p&gt; &lt;p&gt;However, for certain mails which are having a longer body, the response is just coming out blank. My streamlit app showing the output, just runs and stops without showing any error, just a blank response. Same in the terminal. However, it does generate okay replies for some other mails. Cant understand if this is random or some mails are causing some bug here.&lt;/p&gt; &lt;p&gt;Note: Previously, i had gotten an error of token limit exceeded, when i tried to generate response for a spam email having a body much longer than any of my ham emails, but here no such error is coming.&lt;/p&gt; &lt;p&gt;Can anyone give any hint/guidance/help? The project is to be submitted urgently.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Diligent_Tonight3232&quot;&gt; /u/Diligent_Tonight3232 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://i.redd.it/85fat4n5nevc1.jpeg&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c7skfd/llama2_generating_empty_response/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1c7skfd</id><media:thumbnail url="https://preview.redd.it/85fat4n5nevc1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=27212a571ebd9bf1b7554f8c95b26285d8a138da" /><link href="https://www.reddit.com/r/LangChain/comments/1c7skfd/llama2_generating_empty_response/" /><updated>2024-04-19T09:26:46+00:00</updated><published>2024-04-19T09:26:46+00:00</published><title>Llama2 generating empty response</title></entry><entry><author><name>/u/Glittering-Bear5748</name><uri>https://www.reddit.com/user/Glittering-Bear5748</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;let&amp;#39;s understand what scenario i want implement&lt;/p&gt; &lt;ol&gt; &lt;li&gt;i already have some information about user in database.&lt;/li&gt; &lt;li&gt;now if user ask question and if that question&amp;#39;s answer is not available in database then chatbot should ask user to provide me that information and it should store in database now if user ask same question in future then my chatbot should to ans that que&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;ex: lets user &amp;#39;A&amp;#39; whose first name and last name i have already in my database but i don&amp;#39;t have his birth date and if user A ask query &amp;quot;when is my birthday &amp;quot; then my chat bot is unable to ans that que as my db has no information.&lt;br/&gt; so my chatbot should reply &amp;quot;i don&amp;#39;t have that information can you please provide me your birth date&amp;quot;&lt;/p&gt; &lt;p&gt;now if user A provide his birth date then it should be store in Database . now whenever user A ask in future when is my birthday then now my chatbot is able to ans that que.&lt;/p&gt; &lt;p&gt;i want implement this feature using langchain&lt;/p&gt; &lt;p&gt;guy&amp;#39;s please help and thanks in advance&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Glittering-Bear5748&quot;&gt; /u/Glittering-Bear5748 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c7s200/i_need_help_i_want_create_user_input_chat/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c7s200/i_need_help_i_want_create_user_input_chat/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1c7s200</id><link href="https://www.reddit.com/r/LangChain/comments/1c7s200/i_need_help_i_want_create_user_input_chat/" /><updated>2024-04-19T08:51:00+00:00</updated><published>2024-04-19T08:51:00+00:00</published><title>I need help i want create user input chat</title></entry><entry><author><name>/u/Just_Guide7361</name><uri>https://www.reddit.com/user/Just_Guide7361</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I have a fairly simple idea, which surprisingly difficult to execute. I am using LangChain in Python and I am trying to do the following: &lt;/p&gt; &lt;ol&gt; &lt;li&gt;Sent gpt-4-vision an image&lt;/li&gt; &lt;li&gt;Make it extract some items in the image&lt;/li&gt; &lt;li&gt;Parse the response using the Pydantic parser (as I have a set structure in which i want the items) &lt;/li&gt; &lt;/ol&gt; &lt;p&gt;However, the uploading of an image seems to only be possible with the &amp;quot;HumanMessage&amp;quot; object, which does not work together with the PromptTemplate. Has anyone managed to get this to work? &lt;/p&gt; &lt;p&gt;Below is a very simplified version of my code:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;model = ChatOpenAI(model=&amp;quot;gpt-4-vision-preview&amp;quot;, max_tokens=1024) parser = PydanticOutputParser(pydantic_object=ImageDetails) prompt = PromptTemplate( template=&amp;quot;&amp;quot;&amp;quot; You are an expert on OCR. &amp;quot;&amp;quot;&amp;quot;, input_variables=[&amp;quot;query&amp;quot;], partial_variables={&amp;quot;format_instructions&amp;quot;: parser.get_format_instructions()}, validate_template=True, ) chain = prompt | model | parser chain.invoke({&amp;quot;query&amp;quot;: &amp;quot;Identify all items, quantities and amounts on this picture.&amp;quot;}) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Uploading an image with this code works, but ideally I&amp;#39;d like to combine them both into 1.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;image = encode_image(&amp;quot;./image.jpeg&amp;quot;) msg = model.invoke( [ AIMessage( content=&amp;quot;You are a useful bot that is especially good at OCR from images&amp;quot; ), HumanMessage( content=[ {&amp;quot;type&amp;quot;: &amp;quot;text&amp;quot;, &amp;quot;text&amp;quot;: &amp;quot;Identify all items, quantities and amounts on this picture&amp;quot;}, { &amp;quot;type&amp;quot;: &amp;quot;image_url&amp;quot;, &amp;quot;image_url&amp;quot;: { &amp;quot;url&amp;quot;: f&amp;quot;data:image/jpeg;base64,{image}&amp;quot; }, }, ] ) ] ) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Just_Guide7361&quot;&gt; /u/Just_Guide7361 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c7a141/langchain_gpt4visionpreview/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c7a141/langchain_gpt4visionpreview/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1c7a141</id><link href="https://www.reddit.com/r/LangChain/comments/1c7a141/langchain_gpt4visionpreview/" /><updated>2024-04-18T18:03:46+00:00</updated><published>2024-04-18T18:03:46+00:00</published><title>LangChain + gpt-4-vision-preview?</title></entry><entry><author><name>/u/Practical-Win5009</name><uri>https://www.reddit.com/user/Practical-Win5009</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;im trying to do a bot that answer questions from a chromadb , i have stored multiple pdf files with metadata like the filename and candidate name , my problem is when i use conversational retrieval chain the LLM model just receive page_content without the metadata , i want the LLM model to be aware of the page_content with its metadata like filename and candidate name here is my code &lt;/p&gt; &lt;pre&gt;&lt;code&gt;conversation_chain=ConversationalRetrievalChain.from_llm( llm=llm, retriever=SelfQueryRetriever.from_llm(llm,vectorstore,document_content_description,metadata_field_info), memory=memory, verbose=True, ) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;and here is my attribute info&lt;/p&gt; &lt;pre&gt;&lt;code&gt;metadata_field_info = [ AttributeInfo( name=&amp;quot;filename&amp;quot;, description=&amp;quot;The name of the resumee&amp;quot;, type=&amp;quot;string&amp;quot;, ), AttributeInfo( name=&amp;quot;candidatename&amp;quot;, description=&amp;quot;the name of the candidate&amp;quot;, type=&amp;quot;string&amp;quot; ) ] &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;and here is how i use the chain &lt;/p&gt; &lt;p&gt;conversation_chain({&amp;#39;query&amp;#39;:user_question})&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Practical-Win5009&quot;&gt; /u/Practical-Win5009 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c7ra1w/how_to_make_conversationalretrievalchain_to/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c7ra1w/how_to_make_conversationalretrievalchain_to/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1c7ra1w</id><link href="https://www.reddit.com/r/LangChain/comments/1c7ra1w/how_to_make_conversationalretrievalchain_to/" /><updated>2024-04-19T07:56:15+00:00</updated><published>2024-04-19T07:56:15+00:00</published><title>how to make conversationalretrievalchain to include metadata in the prompt using langchain with chromadb to make the LLM aware of metadata?</title></entry><entry><author><name>/u/mehul_gupta1997</name><uri>https://www.reddit.com/user/mehul_gupta1997</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c71thi/packt_publishing_my_book_on_langchain/&quot;&gt; &lt;img src=&quot;https://preview.redd.it/8ciwy6z4c8vc1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ddb071f640e0f4b0dfceb7b8e017453f7e2ad1a7&quot; alt=&quot;Packt publishing my book on LangChain &quot; title=&quot;Packt publishing my book on LangChain &quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;m glad to share with the community that my debut book, &amp;quot;LangChain in your Pocket Beginners guide to building Generative AI applications using LLMs&amp;quot; is now getting published by Packt publications (one of the leading tech publishers). A big thanks to the community for supporting my self-published book and making it a blockbuster. &lt;/p&gt; &lt;p&gt;The book can be checked out here : &lt;a href=&quot;https://www.amazon.com/gp/aw/d/B0CTHQHT25/ref=tmm_kin_swatch_0?ie=UTF8&amp;amp;qid=&amp;amp;sr=&quot;&gt;https://www.amazon.com/gp/aw/d/B0CTHQHT25/ref=tmm_kin_swatch_0?ie=UTF8&amp;amp;qid=&amp;amp;sr=&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/mehul_gupta1997&quot;&gt; /u/mehul_gupta1997 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://i.redd.it/8ciwy6z4c8vc1.png&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c71thi/packt_publishing_my_book_on_langchain/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1c71thi</id><media:thumbnail url="https://preview.redd.it/8ciwy6z4c8vc1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=ddb071f640e0f4b0dfceb7b8e017453f7e2ad1a7" /><link href="https://www.reddit.com/r/LangChain/comments/1c71thi/packt_publishing_my_book_on_langchain/" /><updated>2024-04-18T12:14:18+00:00</updated><published>2024-04-18T12:14:18+00:00</published><title>Packt publishing my book on LangChain</title></entry><entry><author><name>/u/ssreeramj</name><uri>https://www.reddit.com/user/ssreeramj</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I have a doubt in the RAG quickstart example provided in Langchain docs. &lt;/p&gt; &lt;pre&gt;&lt;code&gt;rag_chain = ( { &amp;quot;context&amp;quot;: retriever | format_docs, &amp;quot;question&amp;quot;: RunnablePassthrough(), } | prompt | llm | StrOutputParser() ) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Here, how does the retriever in the &amp;quot;context&amp;quot; get access to the &amp;quot;question&amp;quot;?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/ssreeramj&quot;&gt; /u/ssreeramj &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c7qwsw/need_help_in_understanding_runnablepassthrough/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c7qwsw/need_help_in_understanding_runnablepassthrough/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1c7qwsw</id><link href="https://www.reddit.com/r/LangChain/comments/1c7qwsw/need_help_in_understanding_runnablepassthrough/" /><updated>2024-04-19T07:30:30+00:00</updated><published>2024-04-19T07:30:30+00:00</published><title>Need help in understanding RunnablePassthrough</title></entry><entry><author><name>/u/Glittering-Bear5748</name><uri>https://www.reddit.com/user/Glittering-Bear5748</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hello i am creating chatbot where i have store my data in database and create embedding for vector search but now i want add feature if anybody ask query to my chatbot and chatbot does not have specific information then chatbot should ask user provide me additional information and if user provide information that it should be stored in my db so if again that query asked then my chatbot able provide to response of same query ( using langchain ) . Thanks in advance &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Glittering-Bear5748&quot;&gt; /u/Glittering-Bear5748 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c7q8ke/create_chatbot_with_user_input/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c7q8ke/create_chatbot_with_user_input/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1c7q8ke</id><link href="https://www.reddit.com/r/LangChain/comments/1c7q8ke/create_chatbot_with_user_input/" /><updated>2024-04-19T06:45:43+00:00</updated><published>2024-04-19T06:45:43+00:00</published><title>Create chatbot with user input</title></entry><entry><author><name>/u/OrganicTowel_</name><uri>https://www.reddit.com/user/OrganicTowel_</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;m trying to build something like &lt;a href=&quot;http://Perplexity.AI&quot;&gt;Perplexity.AI&lt;/a&gt; and looking for an API that does the retrieval from the net, parses the topk links, passes this through an LLM and produces text along with citations? &lt;/p&gt; &lt;p&gt;LangChain has some functionality to cite sources from a text file but looking for an end-to-end solution here.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/OrganicTowel_&quot;&gt; /u/OrganicTowel_ &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c7pgho/is_there_an_api_that_generates_text_and_cites_the/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c7pgho/is_there_an_api_that_generates_text_and_cites_the/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1c7pgho</id><link href="https://www.reddit.com/r/LangChain/comments/1c7pgho/is_there_an_api_that_generates_text_and_cites_the/" /><updated>2024-04-19T05:55:16+00:00</updated><published>2024-04-19T05:55:16+00:00</published><title>Is there an API that generates text and cites the text with sources from the internet?</title></entry><entry><author><name>/u/godsmasterpiece1187</name><uri>https://www.reddit.com/user/godsmasterpiece1187</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I‚Äôm building an agent with custom tools with Langchain and wanna know how to use different llms within it.&lt;/p&gt; &lt;p&gt;The issue I ran into with assistant API from OpenAI is that it‚Äôs super slow. So I thought since Groq is ultra fast and rolled out the new tool calling feature, I‚Äôd give it a shot. But the thing is Groq‚Äôs tool calling isn‚Äôt really working, meaning it doesn‚Äôt recognize when to call a tool. So I thought maybe I could use GPT-turbo to decide whether it needs to call a tool and run the tool, and have Groq to output the final result. However, I don‚Äôt seem to find the way to do it. Does anyone know how to work it out? Or do I just need to use Router Chain or something for this?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/godsmasterpiece1187&quot;&gt; /u/godsmasterpiece1187 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c7ilq3/2_llms_within_langchain_agent/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c7ilq3/2_llms_within_langchain_agent/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1c7ilq3</id><link href="https://www.reddit.com/r/LangChain/comments/1c7ilq3/2_llms_within_langchain_agent/" /><updated>2024-04-18T23:53:46+00:00</updated><published>2024-04-18T23:53:46+00:00</published><title>2 LLMs within Langchain Agent</title></entry><entry><author><name>/u/Gerald00</name><uri>https://www.reddit.com/user/Gerald00</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;This is my code, It needs improvement. The objective of the app is to retrieve simillar costumer support tickets from a .csv, have the llm rerank the best one, and try t√¥ respond the query, while showing the source. I figured the map_rerank chain would be perfect for the case, but It lacks good examples in documentation.&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;from langchain.prompts import ChatPromptTemplate&lt;/p&gt; &lt;p&gt;from langchain.prompts.prompt import PromptTemplate&lt;/p&gt; &lt;p&gt;from operator import itemgetter&lt;/p&gt; &lt;p&gt;from typing import List, Tuple&lt;/p&gt; &lt;p&gt;from langchain.schema.runnable import RunnableMap, RunnablePassthrough&lt;/p&gt; &lt;p&gt;from langchain.schema import format_document&lt;/p&gt; &lt;p&gt;from langchain.chains import RetrievalQA&lt;/p&gt; &lt;p&gt;from langchain.callbacks.manager import CallbackManager&lt;/p&gt; &lt;p&gt;from langchain_community.callbacks import StreamlitCallbackHandler&lt;/p&gt; &lt;p&gt;from langchain.callbacks.streaming_stdout_final_only import FinalStreamingStdOutCallbackHandler&lt;/p&gt; &lt;p&gt;# %%&lt;/p&gt; &lt;p&gt;callback_manager=CallbackManager([FinalStreamingStdOutCallbackHandler()])&lt;/p&gt; &lt;p&gt;llm = Ollama(model=&amp;quot;Mistral&amp;quot;, callback_manager=CallbackManager([FinalStreamingStdOutCallbackHandler()])) # üëà stef default&lt;/p&gt; &lt;p&gt;# %%&lt;/p&gt; &lt;p&gt;# load document&lt;/p&gt; &lt;p&gt;loader = DirectoryLoader(&amp;#39;../buscaRAG/source_documents/&amp;#39;, glob=&amp;quot;./*.csv&amp;quot;, loader_cls=CSVLoader, &lt;/p&gt; &lt;p&gt;loader_kwargs={&amp;quot;encoding&amp;quot;: &amp;quot;utf-8&amp;quot;, &amp;quot;csv_args&amp;quot;: {&amp;quot;delimiter&amp;quot;: &amp;quot;;&amp;quot;,&amp;quot;fieldnames&amp;quot;: [&amp;#39;Id&amp;#39;, &amp;#39;T√≠tulo&amp;#39;, &amp;#39;Descri√ß√£o do chamado&amp;#39;, &amp;#39;√öltima a√ß√£o de acompanhamento&amp;#39;]}})&lt;/p&gt; &lt;p&gt;documents = loader.load()&lt;/p&gt; &lt;p&gt;# split the documents into chunks&lt;/p&gt; &lt;p&gt;text_splitter = CharacterTextSplitter( &lt;/p&gt; &lt;p&gt;separator = &amp;quot;,&amp;quot;,&lt;/p&gt; &lt;p&gt;chunk_size = 1000,&lt;/p&gt; &lt;p&gt;chunk_overlap = 0,&lt;/p&gt; &lt;p&gt;length_function = len,&lt;/p&gt; &lt;p&gt;)&lt;/p&gt; &lt;p&gt;texts = text_splitter.split_documents(documents) &lt;/p&gt; &lt;p&gt;# iniciar modelo de embedd&lt;/p&gt; &lt;p&gt;model_name = &amp;quot;intfloat/multilingual-e5-large&amp;quot;&lt;/p&gt; &lt;p&gt;encode_kwargs = {&amp;#39;normalize_embeddings&amp;#39;: True} # set True to compute cosine similarity&lt;/p&gt; &lt;p&gt;model_norm = HuggingFaceEmbeddings(model_name=model_name, model_kwargs={&amp;#39;device&amp;#39;: &amp;#39;cuda&amp;#39;}, encode_kwargs=encode_kwargs)&lt;/p&gt; &lt;p&gt;embeddings = model_norm&lt;/p&gt; &lt;p&gt;# expose this index in a retriever interface&lt;/p&gt; &lt;p&gt;db = Chroma(persist_directory=&amp;quot;../buscaRAG/chroma_db&amp;quot;, embedding_function=embeddings)&lt;/p&gt; &lt;p&gt;retriever = db.as_retriever(search_type=&amp;quot;similarity_score_threshold&amp;quot;, search_kwargs={&amp;quot;score_threshold&amp;quot;: 0.5, &amp;quot;k&amp;quot;: 6})&lt;/p&gt; &lt;p&gt;#TFIDF retriever&lt;/p&gt; &lt;p&gt;TFIDF_retriever = TFIDFRetriever.from_documents(documents)&lt;/p&gt; &lt;p&gt;#ensemble retriever&lt;/p&gt; &lt;p&gt;ensemble_retriever = EnsembleRetriever( retrievers=[TFIDF_retriever, retriever], weights=[0.5, 0.5], c= 80)&lt;/p&gt; &lt;p&gt;# %%&lt;/p&gt; &lt;p&gt;# create a chain to answer questions&lt;/p&gt; &lt;p&gt;qa = RetrievalQA.from_chain_type(&lt;/p&gt; &lt;p&gt;llm=llm,&lt;/p&gt; &lt;p&gt;chain_type=&amp;quot;map_rerank&amp;quot;,&lt;/p&gt; &lt;p&gt;retriever=ensemble_retriever,&lt;/p&gt; &lt;p&gt;return_source_documents=True,&lt;/p&gt; &lt;p&gt;verbose=True,&lt;/p&gt; &lt;p&gt;)&lt;/p&gt; &lt;p&gt;# %%&lt;/p&gt; &lt;p&gt;def embedd():&lt;/p&gt; &lt;p&gt;db = Chroma.from_documents(texts, embeddings, persist_directory=&amp;quot;../buscaRAG/chroma_db&amp;quot;)&lt;/p&gt; &lt;p&gt;return db&lt;/p&gt; &lt;p&gt;# %%&lt;/p&gt; &lt;p&gt;colA, colB = st.columns([.90, .10])&lt;/p&gt; &lt;p&gt;with colA:&lt;/p&gt; &lt;p&gt;prompt = st.text_input(&amp;quot;prompt&amp;quot;, value=&amp;quot;&amp;quot;, key=&amp;quot;prompt&amp;quot;)&lt;/p&gt; &lt;p&gt;response = &amp;quot;&amp;quot;&lt;/p&gt; &lt;p&gt;with colB:&lt;/p&gt; &lt;p&gt;st.markdown(&amp;quot;&amp;quot;)&lt;/p&gt; &lt;p&gt;st.markdown(&amp;quot;&amp;quot;)&lt;/p&gt; &lt;p&gt;if st.button(&amp;quot;üôã‚Äç‚ôÄÔ∏è&amp;quot;, key=&amp;quot;button&amp;quot;):&lt;/p&gt; &lt;p&gt;response = qa.invoke(prompt, callbacks=[StreamlitCallbackHandler(st.container())])&lt;/p&gt; &lt;p&gt;st.markdown(response)&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;The problems:&lt;/p&gt; &lt;p&gt;Mistral 7b Works Fine in ollama, but when a call It through langchain It slows down and hallucinates. Is It something with the system prompt?&lt;/p&gt; &lt;p&gt;g3r2ld0: Langchain complains that the chain doesnt have an output parser, I dont know How to put a simple string one in the chain. The warn in question?&lt;/p&gt; &lt;p&gt;AppData\Roaming\Python\Python311\site-packages\langchain\chains\llm.py:344: UserWarning: The apply_and_parse method is deprecated, instead pass an output parser directly to LLMChain&lt;/p&gt; &lt;p&gt;g3r2ld0: In streamlit, the widget showing the callbacks is too thin&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Gerald00&quot;&gt; /u/Gerald00 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c7kw5k/cant_get_map_rerank_to_work/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c7kw5k/cant_get_map_rerank_to_work/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1c7kw5k</id><link href="https://www.reddit.com/r/LangChain/comments/1c7kw5k/cant_get_map_rerank_to_work/" /><updated>2024-04-19T01:44:43+00:00</updated><published>2024-04-19T01:44:43+00:00</published><title>cant get map_rerank to work</title></entry><entry><author><name>/u/OfficeSalamander</name><uri>https://www.reddit.com/user/OfficeSalamander</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;First off - sorry if this is a question borne of ignorance, I&amp;#39;ve only been researching Langchain and RAG for about 2 weeks now and have mostly been working on toy projects with it in that time.&lt;/p&gt; &lt;p&gt;I am interested in making a modular agent bot that could help me with a few tasks.&lt;/p&gt; &lt;p&gt;The goal would be for the bot to be on 24/7ish, able to respond to user requests, my own admin requests, and to interact with APIs. Basically a chat and email bot with memory, that stays persistently up. The bot &lt;strong&gt;must&lt;/strong&gt; be RAG enabled - I need it to have memory and &amp;quot;learn&amp;quot; new things.&lt;/p&gt; &lt;p&gt;I originally started building all of this with Langchain and more or less had the event loop created, RAG enabled, etc. However from what I can tell AutoGPT seems to do very similar stuff, which is great, but also doesn&amp;#39;t allow the granularity of bot creation that Langchain does, and there&amp;#39;s no real way to integrate them (and from what I can tell, doesn&amp;#39;t really allow for any advanced form of RAG implementation?)&lt;/p&gt; &lt;p&gt;Are there ways to build bots like AutoGPT in Langchain? Would I essentially be re-inventing the wheel? Are there projects that are Langchain-based that work similarly, or allow greater modularity? I want a bot that can do a multi-step thinking progress and essentially work autonomously, but teams of people are obviously going to be better at developing a framework like that than just me alone, which is why I&amp;#39;m so concerned about trying to build a bot like that.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/OfficeSalamander&quot;&gt; /u/OfficeSalamander &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c731tr/i_need_some_help_in_understanding_the_difference/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c731tr/i_need_some_help_in_understanding_the_difference/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1c731tr</id><link href="https://www.reddit.com/r/LangChain/comments/1c731tr/i_need_some_help_in_understanding_the_difference/" /><updated>2024-04-18T13:15:04+00:00</updated><published>2024-04-18T13:15:04+00:00</published><title>I need some help in understanding the difference in utility between Langchain and AutoGPT</title></entry><entry><author><name>/u/ExplorerTechnical808</name><uri>https://www.reddit.com/user/ExplorerTechnical808</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;m working on a web chat to interact with LLMs, and I&amp;#39;m trying to add the possibility for the user to upload a PDF and use RAG to interact with it via a LLM (pretty standard so far). However, I was wondering if I can achieve that without managing a cloud vector database (e.g. such as Pinecone). Currently, the user&amp;#39;s data is stored locally in their browser with indexedDB (i.e. almost same as localstorage), so I would have liked to &lt;strong&gt;run a retriever in the browser as well&lt;/strong&gt; (in order to extract the relevant chunks to pass to the LLM). Is it possible? Basically I would need a vector database that can run in the browser where I load the embeddings (from indexedDB) and run the retriever.&lt;/p&gt; &lt;p&gt;Is there a better way? (My only preference would be not having to manage a cloud vector db, which would involve a server and authentication.)&lt;/p&gt; &lt;p&gt;Thanks in advance! (Please let me know if I&amp;#39;m not making sense - I&amp;#39;m quite new to RAG and Langchain.)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/ExplorerTechnical808&quot;&gt; /u/ExplorerTechnical808 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c72f3s/use_a_local_vector_store_in_the_browser/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c72f3s/use_a_local_vector_store_in_the_browser/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1c72f3s</id><link href="https://www.reddit.com/r/LangChain/comments/1c72f3s/use_a_local_vector_store_in_the_browser/" /><updated>2024-04-18T12:45:06+00:00</updated><published>2024-04-18T12:45:06+00:00</published><title>Use a local vector store in the browser?</title></entry><entry><author><name>/u/svenjacobs3</name><uri>https://www.reddit.com/user/svenjacobs3</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Does the Pandas Dataframe Agent allow for streaming, or rather - is it even beneficial to attempt to stream it or does it output all at once? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/svenjacobs3&quot;&gt; /u/svenjacobs3 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c7cp2r/streaming_pandas_dataframe/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c7cp2r/streaming_pandas_dataframe/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1c7cp2r</id><link href="https://www.reddit.com/r/LangChain/comments/1c7cp2r/streaming_pandas_dataframe/" /><updated>2024-04-18T19:50:30+00:00</updated><published>2024-04-18T19:50:30+00:00</published><title>Streaming Pandas Dataframe</title></entry><entry><author><name>/u/vvkuka</name><uri>https://www.reddit.com/user/vvkuka</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;The introduction of CoT prompting improved large language models‚Äô results in performing reasoning tasks.&lt;/p&gt; &lt;p&gt;I compiled the useful resources that could help you utilize CoT methods in your projects:&lt;/p&gt; &lt;p&gt;Methods that require you to write your prompt in a specific way:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Basic: zero-shot prompting, few-shot prompting&lt;/li&gt; &lt;li&gt;Chain-of-thought: Original method, self-consistency, zero-shot chain-of-thought -&amp;gt; Read our &lt;a href=&quot;https://www.turingpost.com/p/cot&quot;&gt;article&lt;/a&gt; and use these &lt;a href=&quot;https://www.turingpost.com/p/prompttomaster&quot;&gt;7 resources to master prompt engineering&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Other variations of Chain-of-Thought methods:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Automatic-Chain-of-Thought (Auto-CoT) proposes replacing the entire CoT framework with a single phrase: &amp;quot;Let&amp;#39;s think step by step.&amp;quot; ‚Üí &lt;a href=&quot;https://github.com/amazon-science/auto-cot&quot;&gt;Original code from AWS&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Program-of-Thoughts Prompting (PoT) suggested expressing the reasoning steps as Python programs by the LLM and delegating the computation to a Python interpreter instead of computing the result by the LLM itself ‚Üí &lt;a href=&quot;https://github.com/wenhuchen/Program-of-Thoughts&quot;&gt;Original code&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Multimodal Chain-of-Thought Reasoning (Multimodal-CoT) suggested incorporating language (text) and vision (images) modalities instead of working with just text ‚Üí &lt;a href=&quot;https://github.com/amazon-science/mm-cot&quot;&gt;Original code from AWS&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Tree-of-Thoughts (ToT) adopts a more human-like approach to problem-solving by framing each task as a search across a tree of possibilities where each node in this tree represents a partial solution. ‚Üí &lt;a href=&quot;https://github.com/princeton-nlp/tree-of-thought-llm&quot;&gt;Original code from the Princeton NLP team&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Graph-of-Thoughts (GoT) leverages graph theory to represent the reasoning process ‚Üí &lt;a href=&quot;https://github.com/spcl/graph-of-thoughts&quot;&gt;Original code&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Algorithm-of-Thoughts (AoT) embeds algorithmic processes within prompts, enabling efficient problem-solving with fewer queries ‚Üí &lt;a href=&quot;https://github.com/kyegomez/Algorithm-Of-Thoughts&quot;&gt;Code for implementing AoT from Agora AI lab&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Skeleton-of-Thought (SoT) is based on the idea of guiding the LLM itself to give a skeleton of the answer first and then write the overall answer parallelly instead of sequentially. ‚Üí &lt;a href=&quot;https://github.com/imagination-research/sot&quot;&gt;Original code&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Do you use any of these methods? Which one is your favorite?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/vvkuka&quot;&gt; /u/vvkuka &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c700pk/how_to_use_chainofthoughts_methods_in_your_project/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c700pk/how_to_use_chainofthoughts_methods_in_your_project/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1c700pk</id><link href="https://www.reddit.com/r/LangChain/comments/1c700pk/how_to_use_chainofthoughts_methods_in_your_project/" /><updated>2024-04-18T10:32:00+00:00</updated><published>2024-04-18T10:32:00+00:00</published><title>How to use Chain-of-Thoughts methods in your project?</title></entry><entry><author><name>/u/BoiElroy</name><uri>https://www.reddit.com/user/BoiElroy</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;See post title&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/BoiElroy&quot;&gt; /u/BoiElroy &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c6wkza/what_are_the_most_promising_ways_you_guys_have/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c6wkza/what_are_the_most_promising_ways_you_guys_have/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1c6wkza</id><link href="https://www.reddit.com/r/LangChain/comments/1c6wkza/what_are_the_most_promising_ways_you_guys_have/" /><updated>2024-04-18T06:34:08+00:00</updated><published>2024-04-18T06:34:08+00:00</published><title>What are the most promising ways you guys have done RAG evaluation during dev and monitoring?</title></entry><entry><author><name>/u/Putrid_Spinach3961</name><uri>https://www.reddit.com/user/Putrid_Spinach3961</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey, can we create a question answer over documents System without the vectorization using langchain? If yes plis, differentiate between the system created using vectorization and without vectorization.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Putrid_Spinach3961&quot;&gt; /u/Putrid_Spinach3961 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c70t2h/with_or_without_vectorization/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c70t2h/with_or_without_vectorization/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1c70t2h</id><link href="https://www.reddit.com/r/LangChain/comments/1c70t2h/with_or_without_vectorization/" /><updated>2024-04-18T11:19:44+00:00</updated><published>2024-04-18T11:19:44+00:00</published><title>With or without vectorization</title></entry><entry><author><name>/u/Spirited_Analysis863</name><uri>https://www.reddit.com/user/Spirited_Analysis863</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;i have been using the multi vector retriever from langchain for storing images and their description to my embeddings db. i am embedding the descriptions and storing the images as it is for retrieval. the documents are labelled by doc_id which i am storing in a InMemoryStore but i would want this to be in a persistent directory or variable (which i can store in a directory).&lt;br/&gt; i have tried using the LocalFileStore but it stores byte-like object and the docstore needs to be in str format (&lt;a href=&quot;https://api.python.langchain.com/en/latest/retrievers/langchain.retrievers.multi_vector.MultiVectorRetriever.html#langchain-retrievers-multi-vector-multivectorretriever&quot;&gt;documentation&lt;/a&gt;) so this approach threw TypeError. &lt;/p&gt; &lt;p&gt;is there anyway to implement this functionality? please help me, i am just a beginner with langchain and llms.&lt;/p&gt; &lt;p&gt;this is my code for the retriever and the llm chain- &lt;/p&gt; &lt;pre&gt;&lt;code&gt;import uuid from langchain.retrievers.multi_vector import MultiVectorRetriever from langchain.storage import InMemoryStore, LocalFileStore from langchain_community.vectorstores import Chroma from langchain_core.documents import Document def create_multi_vector_retriever(store, vectorstore, id_key, text_summaries=None, texts=None): &amp;quot;&amp;quot;&amp;quot; Create a multi-vector retriever that indexes summaries and stores embeddings in a folder. Args: folder_path: Path to the folder for storing embeddings. text_summaries: List of text summaries for new documents (optional). texts: List of corresponding texts/images/etc. for new documents ek(optional). Returns: The multi-vector retriever instance. &amp;quot;&amp;quot;&amp;quot; # Create the multi-vector retriever retriever = MultiVectorRetriever( vectorstore=vectorstore, docstore=store, id_key=id_key, search_kwargs={&amp;quot;k&amp;quot;: 5}, ) def add_documents(retriever, doc_summaries, doc_contents): doc_ids = [str(uuid.uuid4()) for _ in doc_contents] summary_docs = [ Document(page_content=s, metadata={id_key: doc_ids[i]}) for i,s, in enumerate(doc_summaries) ] retriever.vectorstore.add_documents(summary_docs) retriever.docstore.mset(list(zip(doc_ids, doc_contents))) print(&amp;quot;added&amp;quot;) # Filter out empty text_summaries non_empty_text_summaries = [summary for summary in text_summaries if summary.strip()] # Add texts, tables, and images if summaries are not empty if non_empty_text_summaries: add_documents(retriever, non_empty_text_summaries, texts) return retriever vectorstore = Chroma( collection_name = &amp;quot;second&amp;quot;, persist_directory = &amp;quot;/content/embeddings12&amp;quot;, embedding_function=VertexAIEmbeddings(model_name=&amp;quot;textembedding-gecko@latest&amp;quot;), ) docstore = InMemoryStore() id_key=&amp;quot;doc_id&amp;quot; # Create Retriever retriever_multi_vector_img = create_multi_vector_retriever( docstore, vectorstore, id_key, doc_summaries, #description of the images doc_img_base64_list #the actual images ) def multi_modal_rag_chain(retriever): &amp;quot;&amp;quot;&amp;quot; Multi-modal RAG chain &amp;quot;&amp;quot;&amp;quot; # Multi-modal LLM model = ChatVertexAI( temperature = 0, model_name = &amp;quot;gemini-pro-vision&amp;quot;, max_output_tokens=2048, safety_settings = safety_settings ) # RAG Pipeline chain = ( { &amp;quot;context&amp;quot;: retriever | RunnableLambda(split_image_text_types), &amp;quot;question&amp;quot;: RunnablePassthrough() } | RunnableLambda(img_prompt_func) | model | StrOutputParser() ) return chain # Create RAG chain chain_multimodal_rag = multi_modal_rag_chain(retriever_multi_vector_img) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;thanks a lot!!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Spirited_Analysis863&quot;&gt; /u/Spirited_Analysis863 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c6y7oo/persistent_directory_storage_for_docstore_in/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c6y7oo/persistent_directory_storage_for_docstore_in/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1c6y7oo</id><link href="https://www.reddit.com/r/LangChain/comments/1c6y7oo/persistent_directory_storage_for_docstore_in/" /><updated>2024-04-18T08:26:12+00:00</updated><published>2024-04-18T08:26:12+00:00</published><title>Persistent directory storage for docstore in MultiVectorRetriever</title></entry><entry><author><name>/u/LaAlice</name><uri>https://www.reddit.com/user/LaAlice</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Edit: I found out how to do this with (lambda x: intermediate_results.update({&amp;quot;resultA&amp;quot;:x}) or x)&lt;/p&gt; &lt;p&gt;I have a chain in langchain that calls on LLMs several times and then processes the output. It looks something like prompt1 | llm | OutputParser | prompt2 | llm | OutlutParser etc. I would like to capture the results that come out of the Output Parser and have the chain return those together with the actual result of the chain. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/LaAlice&quot;&gt; /u/LaAlice &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c6zl4g/how_to_output_specific_intermediary_results_at/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c6zl4g/how_to_output_specific_intermediary_results_at/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1c6zl4g</id><link href="https://www.reddit.com/r/LangChain/comments/1c6zl4g/how_to_output_specific_intermediary_results_at/" /><updated>2024-04-18T10:04:54+00:00</updated><published>2024-04-18T10:04:54+00:00</published><title>How to output specific intermediary results at the end of the chain</title></entry></feed>