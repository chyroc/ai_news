<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/"><category term="LangChain" label="r/LangChain"/><updated>2024-07-18T13:10:59+00:00</updated><icon>https://www.redditstatic.com/icon.png/</icon><id>/r/LangChain.rss</id><link rel="self" href="https://www.reddit.com/r/LangChain.rss" type="application/atom+xml" /><link rel="alternate" href="https://www.reddit.com/r/LangChain" type="text/html" /><subtitle>LangChain is an open-source framework and developer toolkit that helps developers get LLM applications from prototype to production. It is available for Python and Javascript at https://www.langchain.com/.</subtitle><title>LangChain</title><entry><author><name>/u/zchaarm</name><uri>https://www.reddit.com/user/zchaarm</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;A place for members of &lt;a href=&quot;/r/LangChain&quot;&gt;r/LangChain&lt;/a&gt; to chat with each other&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/zchaarm&quot;&gt; /u/zchaarm &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_10ljho9</id><link href="https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/" /><updated>2023-01-26T04:33:05+00:00</updated><published>2023-01-26T04:33:05+00:00</published><title>r/LangChain Lounge</title></entry><entry><author><name>/u/Yeddine</name><uri>https://www.reddit.com/user/Yeddine</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e5z34y/should_i_open_source_this_tool/&quot;&gt; &lt;img src=&quot;https://b.thumbs.redditmedia.com/N0-_rhm56nZrU4qPFiBJlPcihF5IfMJX86dQjUd8vVM.jpg&quot; alt=&quot;Should I open source this tool?&quot; title=&quot;Should I open source this tool?&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey guys - I started building an AI tool for myself to talk to my data with SQL and RAG and need your feedback to know if it&amp;#39;s worth turning into an open-source project and/or SaaS.&lt;/p&gt; &lt;p&gt;The way it works is that you can connect a lot of data sources, structured or unstructured such as PostgreSQL, Snowflake, Notion, Facebook Ads, Shopify, PDFs... and you can chat with it, visualize it with tables and charts.&lt;/p&gt; &lt;p&gt;Do you see value in this, should I keep going?&lt;/p&gt; &lt;p&gt;Would love to hear your feedback and if you&amp;#39;d be interested in contributing or trying it for free&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://i.redd.it/14o76c00i6dd1.gif&quot;&gt;Text-to-SQL + RAG + Visualization&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Yeddine&quot;&gt; /u/Yeddine &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e5z34y/should_i_open_source_this_tool/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e5z34y/should_i_open_source_this_tool/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1e5z34y</id><media:thumbnail url="https://b.thumbs.redditmedia.com/N0-_rhm56nZrU4qPFiBJlPcihF5IfMJX86dQjUd8vVM.jpg" /><link href="https://www.reddit.com/r/LangChain/comments/1e5z34y/should_i_open_source_this_tool/" /><updated>2024-07-18T01:18:49+00:00</updated><published>2024-07-18T01:18:49+00:00</published><title>Should I open source this tool?</title></entry><entry><author><name>/u/Standard-Society-568</name><uri>https://www.reddit.com/user/Standard-Society-568</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;ve been noticing more and more people are using GraphRAG instead of embedding and vector databases...is it really helpful or just the hype?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Standard-Society-568&quot;&gt; /u/Standard-Society-568 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e66e9r/graphrag/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e66e9r/graphrag/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1e66e9r</id><link href="https://www.reddit.com/r/LangChain/comments/1e66e9r/graphrag/" /><updated>2024-07-18T08:30:22+00:00</updated><published>2024-07-18T08:30:22+00:00</published><title>GraphRAG</title></entry><entry><author><name>/u/mehul_gupta1997</name><uri>https://www.reddit.com/user/mehul_gupta1997</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;This video demonstrates how GraphRAG (using LangChain) can be implemented for CSV files with example and code explanation using LLMGraphTransformer : &lt;a href=&quot;https://youtu.be/3B6VjDtbsbw?si=ubuyOD-_bAmP-IAg&quot;&gt;https://youtu.be/3B6VjDtbsbw?si=ubuyOD-_bAmP-IAg&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/mehul_gupta1997&quot;&gt; /u/mehul_gupta1997 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e627bz/graphrag_using_csv_langchain/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e627bz/graphrag_using_csv_langchain/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1e627bz</id><link href="https://www.reddit.com/r/LangChain/comments/1e627bz/graphrag_using_csv_langchain/" /><updated>2024-07-18T03:58:46+00:00</updated><published>2024-07-18T03:58:46+00:00</published><title>GraphRAG using CSV, LangChain</title></entry><entry><author><name>/u/vuongagiflow</name><uri>https://www.reddit.com/user/vuongagiflow</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;If you’re a developer working 9-5 job and have young kids, finding time to upskill with LLMs can be challenging. With only 15-30 minutes a day, what approaches would you recommend?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/vuongagiflow&quot;&gt; /u/vuongagiflow &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e633xx/practical_approach_to_llm/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e633xx/practical_approach_to_llm/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1e633xx</id><link href="https://www.reddit.com/r/LangChain/comments/1e633xx/practical_approach_to_llm/" /><updated>2024-07-18T04:49:51+00:00</updated><published>2024-07-18T04:49:51+00:00</published><title>Practical approach to LLM?</title></entry><entry><author><name>/u/Kind-Worry3072</name><uri>https://www.reddit.com/user/Kind-Worry3072</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;m looking for examples of Python code that demonstrate parsing Excel files into SQLite databases using eparser. I&amp;#39;ve come across the Excelparser package, but I&amp;#39;m unsure how to use it as the documentation primarily provides shell prompts rather than Python code examples.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Kind-Worry3072&quot;&gt; /u/Kind-Worry3072 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e67x37/eparser_parsing_to_sqllite/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e67x37/eparser_parsing_to_sqllite/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1e67x37</id><link href="https://www.reddit.com/r/LangChain/comments/1e67x37/eparser_parsing_to_sqllite/" /><updated>2024-07-18T10:16:25+00:00</updated><published>2024-07-18T10:16:25+00:00</published><title>eparser parsing to sqllite</title></entry><entry><author><name>/u/thanghaimeow</name><uri>https://www.reddit.com/user/thanghaimeow</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I gave a workshop recently and managed to record it&lt;/p&gt; &lt;p&gt;You can do hybrid search (e.g combining similarity search with keywords search - your traditional search) in many ways and not just in Pinecone or Weaviate.&lt;/p&gt; &lt;p&gt;You can even do your own setup of hybrid search in Postgres too (with Supabase, for example)&lt;/p&gt; &lt;p&gt;Here&amp;#39;s the video: &lt;a href=&quot;https://www.youtube.com/watch?v=d_WwEdxyuGs&quot;&gt;https://www.youtube.com/watch?v=d_WwEdxyuGs&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Here&amp;#39;s the Jupyter notebook: &lt;a href=&quot;https://github.com/trancethehuman/ai-workshop-code/blob/main/Hybrid_Search_Workshop.ipynb&quot;&gt;https://github.com/trancethehuman/ai-workshop-code/blob/main/Hybrid_Search_Workshop.ipynb&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Enjoy.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/thanghaimeow&quot;&gt; /u/thanghaimeow &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e65b82/code_included_if_youre_doing_rag_and_your_search/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e65b82/code_included_if_youre_doing_rag_and_your_search/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1e65b82</id><link href="https://www.reddit.com/r/LangChain/comments/1e65b82/code_included_if_youre_doing_rag_and_your_search/" /><updated>2024-07-18T07:13:33+00:00</updated><published>2024-07-18T07:13:33+00:00</published><title>(Code included) If you're doing RAG and your search queries are sometimes very specific, then you want to do hybrid search (here're are a few setups that makes sense)</title></entry><entry><author><name>/u/Present_Owl742</name><uri>https://www.reddit.com/user/Present_Owl742</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;hi&lt;/p&gt; &lt;p&gt;I&amp;#39;m building a RAG system and while I have some documentation in the vector database which is more specific, I have the product documentation on a public URL. When a user asks a question, I&amp;#39;m thinking it should retrieve the context from the vector db and then access the URL documentation as well. Has anyone tried this and what tools have they used?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Present_Owl742&quot;&gt; /u/Present_Owl742 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e6auhx/can_i_access_a_documentation_url_using_langchain/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e6auhx/can_i_access_a_documentation_url_using_langchain/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1e6auhx</id><link href="https://www.reddit.com/r/LangChain/comments/1e6auhx/can_i_access_a_documentation_url_using_langchain/" /><updated>2024-07-18T13:00:36+00:00</updated><published>2024-07-18T13:00:36+00:00</published><title>Can I access a documentation URL using Langchain/ Anthropic API?</title></entry><entry><author><name>/u/Typical-Scene-5794</name><uri>https://www.reddit.com/user/Typical-Scene-5794</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi &lt;a href=&quot;/r/langchain&quot;&gt;r/langchain&lt;/a&gt;,&lt;/p&gt; &lt;p&gt;Microsoft SharePoint is to enterprises what Google Drive is to consumers. Happy to share my work on an app template that makes it easy to build applications that deliver up-to-date answers using your RAG pipeline with SharePoint data. &lt;/p&gt; &lt;ul&gt; &lt;li&gt;Repo Link: &lt;a href=&quot;https://pathway.com/developers/templates/enterprise_rag_sharepoint&quot;&gt;~https://pathway.com/developers/templates/enterprise_rag_sharepoint~&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Thousands of employees at large corporations collaborate and make changes in the documents stored in Microsoft SharePoint folders – making it a valuable data source for dynamic RAG/Gen AI applications to boost productivity. &lt;/p&gt; &lt;p&gt;However, existing connectors for SharePoint lack necessary security features. My template covers:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Real-Time Sync with changes in your SharePoint files, with the help of Pathway (link: &lt;a href=&quot;https://python.langchain.com/v0.2/docs/integrations/vectorstores/pathway/&quot;&gt;~Pathway Vector Store on LangChain~&lt;/a&gt;).&lt;/li&gt; &lt;li&gt;Step by step process to setup Entra ID and SSL authentication. &lt;/li&gt; &lt;li&gt;Security and Scalability, given the choice of frameworks and minimalistic architecture.&lt;/li&gt; &lt;li&gt;Ease of Setup to help you run the app template in Docker within minutes.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I plan to further refine this by using:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://pathway.com/developers/templates/adaptive-rag&quot;&gt;~Adaptive RAG~&lt;/a&gt;: Implementing cost-effective strategies without sacrificing accuracy.&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://pathway.com/developers/user-guide/llm-xpack/overview/#rerankers&quot;&gt;~Pathway Rerankers~&lt;/a&gt;: Integrating advanced reranking techniques for improved results.&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://github.com/pathwaycom/llm-app/tree/main/examples/pipelines/slides_ai_search&quot;&gt;~Multimodal Pipelines with Hybrid Indexes~&lt;/a&gt;: Using advanced parsing capabilities and indexing techniques&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;🤝 Let&amp;#39;s Discuss! I&amp;#39;m open to your questions and feedback!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Typical-Scene-5794&quot;&gt; /u/Typical-Scene-5794 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e6aby5/template_to_use_microsoft_sharepoint_as_a_data/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e6aby5/template_to_use_microsoft_sharepoint_as_a_data/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1e6aby5</id><link href="https://www.reddit.com/r/LangChain/comments/1e6aby5/template_to_use_microsoft_sharepoint_as_a_data/" /><updated>2024-07-18T12:35:22+00:00</updated><published>2024-07-18T12:35:22+00:00</published><title>Template to use Microsoft SharePoint as a data source for Enterprise RAG pipelines</title></entry><entry><author><name>/u/Big_Barracuda_6753</name><uri>https://www.reddit.com/user/Big_Barracuda_6753</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e635xo/pdf_rag_app_is_giving_answer_of_general_questions/&quot;&gt; &lt;img src=&quot;https://b.thumbs.redditmedia.com/l5wpPHiN_9U6AohTdPGi5S6sMgML7Vt95YSdnJhjx4U.jpg&quot; alt=&quot;PDF RAG app is giving answer of general questions , I don't want it to . What is wrong with my code ?&quot; title=&quot;PDF RAG app is giving answer of general questions , I don't want it to . What is wrong with my code ?&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;m developing a PDF RAG app . You can see the code below .&lt;/p&gt; &lt;pre&gt;&lt;code&gt;def reRanker(): compressor = CohereRerank(client=cohere_client) compression_retriever = ContextualCompressionRetriever( base_compressor=compressor, base_retriever=vectorStore.as_retriever( search_kwargs={&amp;quot;k&amp;quot;: 5}, ), ) return compression_retriever if &amp;quot;store&amp;quot; not in st.session_state: st.session_state.store = {} store = {} def get_session_history(session_id: str) -&amp;gt; BaseChatMessageHistory: if session_id not in st.session_state.store: st.session_state.store[session_id] = ChatMessageHistory() return st.session_state.store[session_id] contextualize_q_system_prompt = ( &amp;quot;Given a chat history and the latest user question &amp;quot; &amp;quot;which might reference context in the chat history, &amp;quot; &amp;quot;formulate a standalone question which can be understood &amp;quot; &amp;quot;without the chat history. Do NOT answer the question, &amp;quot; &amp;quot;just reformulate it if needed and otherwise return it as is.&amp;quot; ) contextualize_q_prompt = ChatPromptTemplate.from_messages( [ (&amp;quot;system&amp;quot;, contextualize_q_system_prompt), MessagesPlaceholder(&amp;quot;chat_history&amp;quot;), (&amp;quot;human&amp;quot;, &amp;quot;{input}&amp;quot;), ] ) compression_retriever = reRanker() history_aware_retriever = create_history_aware_retriever( llm, compression_retriever, contextualize_q_prompt ) system_prompt = ( &amp;quot;You are an assistant for question-answering tasks. &amp;quot; &amp;quot;Use the following pieces of retrieved context to answer &amp;quot; &amp;quot;the question. If you don&amp;#39;t know the answer, say that you &amp;quot; &amp;quot;don&amp;#39;t know. Use three sentences maximum and keep the &amp;quot; &amp;quot;answer concise.&amp;quot; &amp;quot;\n\n&amp;quot; &amp;quot;{context}&amp;quot; ) chatPrompt = ChatPromptTemplate.from_messages( [ (&amp;quot;system&amp;quot;, system_prompt), MessagesPlaceholder(&amp;quot;chat_history&amp;quot;), (&amp;quot;human&amp;quot;, &amp;quot;{input}&amp;quot;), ] ) question_answer_chain = create_stuff_documents_chain(llm, chatPrompt) rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain) conversational_rag_chain = RunnableWithMessageHistory( rag_chain, get_session_history, input_messages_key=&amp;quot;input&amp;quot;, output_messages_key=&amp;quot;answer&amp;quot;, history_messages_key=&amp;quot;chat_history&amp;quot;, ) def generate_response(prompt: str) : for chunk in conversational_rag_chain.stream(input={&amp;quot;input&amp;quot;: prompt},config={&amp;#39;configurable&amp;#39;: {&amp;#39;session_id&amp;#39;: &amp;quot;uniqueValue1234&amp;quot;}}): answer_chunk = chunk.get(&amp;quot;answer&amp;quot;) if answer_chunk: yield answer_chunk session_id = &amp;quot;uniqueValue1234&amp;quot; if &amp;quot;chat_history&amp;quot; not in st.session_state: st.session_state.chat_history = [] for message in st.session_state.chat_history: if isinstance(message,HumanMessage): with st.chat_message(&amp;quot;Human&amp;quot;): st.markdown(message.content) else: with st.chat_message(&amp;quot;AI&amp;quot;): st.markdown(message.content) prompt = st.chat_input(&amp;quot;Hey, What&amp;#39;s up?&amp;quot;) if prompt is not None and prompt !=&amp;quot;&amp;quot; : st.session_state.chat_history.append(HumanMessage(prompt)) with st.chat_message(&amp;quot;Human&amp;quot;): st.markdown(prompt) if len(pc.list_indexes()) == 0: st.error(&amp;quot;Please upload some files first!&amp;quot;) else: with st.chat_message(&amp;quot;AI&amp;quot;): ai_response = st.write_stream(generate_response(prompt)) st.session_state.chat_history.append(AIMessage(ai_response)) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Now , I ask my RAG app questions related to the pdf . It gives answer as I expect , chat history is also working . LLM is able to refer to previous conversation in the session .&lt;/p&gt; &lt;p&gt;Now , I ask a general question in my prompt . Sometimes , I get the response &amp;#39; I don&amp;#39;t know &amp;#39; but other times it is returning the answer like ChatGPT . I want my app to limit response to PDF only . &lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/j6nvl2fek7dd1.png?width=1497&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a8de42944ac67dbb14ad61675edd3765e4833896&quot;&gt;https://preview.redd.it/j6nvl2fek7dd1.png?width=1497&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a8de42944ac67dbb14ad61675edd3765e4833896&lt;/a&gt;&lt;/p&gt; &lt;p&gt;A screenshot of my problem : &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Big_Barracuda_6753&quot;&gt; /u/Big_Barracuda_6753 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e635xo/pdf_rag_app_is_giving_answer_of_general_questions/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e635xo/pdf_rag_app_is_giving_answer_of_general_questions/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1e635xo</id><media:thumbnail url="https://b.thumbs.redditmedia.com/l5wpPHiN_9U6AohTdPGi5S6sMgML7Vt95YSdnJhjx4U.jpg" /><link href="https://www.reddit.com/r/LangChain/comments/1e635xo/pdf_rag_app_is_giving_answer_of_general_questions/" /><updated>2024-07-18T04:53:16+00:00</updated><published>2024-07-18T04:53:16+00:00</published><title>PDF RAG app is giving answer of general questions , I don't want it to . What is wrong with my code ?</title></entry><entry><author><name>/u/EngineeringFree313</name><uri>https://www.reddit.com/user/EngineeringFree313</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hello,&lt;/p&gt; &lt;p&gt;I have a document that explains finance policies and processes of some company. The goal is to build a chatbot using RAG framework upon that document to serve employees who have queries related to financial issues. &lt;/p&gt; &lt;p&gt;I got sample questions from stack-holders and I made them as a criteria to measure the chatbot performance. What I noticed is that sample questions are slightly complex and do not have direct answers in the document. The retrieved context via semantic similarity search does not get the desired part of the document so the answers are incorrect or misleading. &lt;/p&gt; &lt;p&gt;So, I am wondering if the document is not suitable to answer user questions or not. How to measure the relevance of the document content to user questions?! what kind of preprocessing I may do on the document to fix that issue. &lt;/p&gt; &lt;p&gt;What made me more confused is that when I uploaded the document on chatgpt and tried to ask the same questions, it was able to answer questions correctly. This made me think of the chatgpt method to process documents and answer questions based on them? I think it is not using embedding and similarity search as I do!&lt;/p&gt; &lt;p&gt;I have tried different techniques like parent document retrieval and multi-representation indexing but still have the problem of capturing wrong parts of the document!&lt;/p&gt; &lt;p&gt;I am using OpenAi Embedding and gpt-3.5-turbo.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/EngineeringFree313&quot;&gt; /u/EngineeringFree313 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e67ods/measuring_relevance_of_the_knowledge_base_to_user/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e67ods/measuring_relevance_of_the_knowledge_base_to_user/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1e67ods</id><link href="https://www.reddit.com/r/LangChain/comments/1e67ods/measuring_relevance_of_the_knowledge_base_to_user/" /><updated>2024-07-18T10:00:25+00:00</updated><published>2024-07-18T10:00:25+00:00</published><title>Measuring relevance of the knowledge base to user questions</title></entry><entry><author><name>/u/pekkamama</name><uri>https://www.reddit.com/user/pekkamama</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;m building a simple text-2-sql application leveraging Llama-3-7B model on bedrock. My db is on postgresql . The schema is fairly complex with 25 tables with an average of 10 columns per table. The tables have primary key, foreign key and unique key setup. &lt;/p&gt; &lt;p&gt;At the moment, I have exported the schema for each table as a document and have pushed it into vector db as a collection. Similarly, I&amp;#39;ve summarised each table&amp;#39;s schema with added context and pushed the same into vector db as another collection too. The vectorDB I am using is ChromaDB. the n_results I&amp;#39;ve set is 4 and the similarity search algo is cosine similarity. &lt;/p&gt; &lt;p&gt;My Prompt instructs the model to consider the tables and the summarises along with a few sanitisation techniques and then generate a sql query. &lt;/p&gt; &lt;p&gt;The challenge and the difficulty I am facing is that when I type in a query that is simple user english, the RAG fetches 4 most common tables. And sometime it often assume Join with keys/column name that are diverse and do not match. And sometimes, it just hallucinaties. &lt;/p&gt; &lt;p&gt;To keep my context window in check, I simply can&amp;#39;t feed the 8-10 relevant schemas from the vectorDB. Is there a way I can mitigate the challenges and modify my RAG to suit my usecase. &lt;/p&gt; &lt;p&gt;Thanks in advance&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/pekkamama&quot;&gt; /u/pekkamama &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e5pe1a/optimal_rag_for_text2sql/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e5pe1a/optimal_rag_for_text2sql/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1e5pe1a</id><link href="https://www.reddit.com/r/LangChain/comments/1e5pe1a/optimal_rag_for_text2sql/" /><updated>2024-07-17T18:23:14+00:00</updated><published>2024-07-17T18:23:14+00:00</published><title>Optimal RAG for text-2-sql</title></entry><entry><author><name>/u/zmccormick7</name><uri>https://www.reddit.com/user/zmccormick7</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e5le9h/solving_the_outofcontext_chunk_problem_for_rag/&quot;&gt; &lt;img src=&quot;https://external-preview.redd.it/_q1gwtUaSI8ZGrE9UVTyea73HvmYDrIc89DmMaJ-fY0.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c1e6a2e33500ef62a7ef074552a238d2d3d82cc9&quot; alt=&quot;Solving the out-of-context chunk problem for RAG&quot; title=&quot;Solving the out-of-context chunk problem for RAG&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Many of the problems developers face with RAG come down to this: Individual chunks don’t contain sufficient context to be properly used by the retrieval system or the LLM. This leads to the inability to answer seemingly simple questions and, more worryingly, hallucinations.&lt;/p&gt; &lt;p&gt;Examples of this problem&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Chunks oftentimes refer to their subject via implicit references and pronouns. This causes them to not be retrieved when they should be, or to not be properly understood by the LLM.&lt;/li&gt; &lt;li&gt;Individual chunks oftentimes don’t contain the complete answer to a question. The answer may be scattered across a few adjacent chunks.&lt;/li&gt; &lt;li&gt;Adjacent chunks presented to the LLM out of order cause confusion and can lead to hallucinations.&lt;/li&gt; &lt;li&gt;Naive chunking can lead to text being split “mid-thought” leaving neither chunk with useful context.&lt;/li&gt; &lt;li&gt;Individual chunks oftentimes only make sense in the context of the entire section or document, and can be misleading when read on their own.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;What would a solution look like?&lt;/h1&gt; &lt;p&gt;We’ve found that there are two methods that together solve the bulk of these problems.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Contextual chunk headers&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The idea here is to add in higher-level context to the chunk by prepending a chunk header. This chunk header could be as simple as just the document title, or it could use a combination of document title, a concise document summary, and the full hierarchy of section and sub-section titles.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Chunks -&amp;gt; segments&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Large chunks provide better context to the LLM than small chunks, but they also make it harder to precisely retrieve specific pieces of information. Some queries (like simple factoid questions) are best handled by small chunks, while other queries (like higher-level questions) require very large chunks. What we really need is a more dynamic system that can retrieve short chunks when that&amp;#39;s all that&amp;#39;s needed, but can also retrieve very large chunks when required. How do we do that?&lt;/p&gt; &lt;h1&gt;Break the document into sections&lt;/h1&gt; &lt;p&gt;Information about the section a chunk comes from can provide important context, so our first step will be to break the document into semantically cohesive sections. There are many ways to do this, but we’ll use a semantic sectioning approach. This works by annotating the document with line numbers and then prompting an LLM to identify the starting and ending lines for each “semantically cohesive section.” These sections should be anywhere from a few paragraphs to a few pages long. These sections will then get broken into smaller chunks if needed.&lt;/p&gt; &lt;p&gt;We’ll use Nike’s 2023 10-K to illustrate this. Here are the first 10 sections we identified:&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/8ux5h0drl3dd1.png?width=1260&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=be590f246f7e06d387e1f0a6952b19b0222c209d&quot;&gt;https://preview.redd.it/8ux5h0drl3dd1.png?width=1260&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=be590f246f7e06d387e1f0a6952b19b0222c209d&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Add contextual chunk headers&lt;/h1&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/ow83jnzsl3dd1.png?width=1200&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f59e39f143ee8510559ec105fdd0f585ac395786&quot;&gt;https://preview.redd.it/ow83jnzsl3dd1.png?width=1200&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f59e39f143ee8510559ec105fdd0f585ac395786&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The purpose of the chunk header is to add context to the chunk text. Rather than using the chunk text by itself when embedding and reranking the chunk, we use the concatenation of the chunk header and the chunk text, as shown in the image above. This helps the ranking models (embeddings and rerankers) retrieve the correct chunks, even when the chunk text itself has implicit references and pronouns that make it unclear what it’s about. For this example, we just use the document title and the section title as context. But there are many ways to do this. We’ve also seen great results with using a concise document summary as the chunk header, for example.&lt;/p&gt; &lt;p&gt;Let’s see how much of an impact the chunk header has for the chunk shown above.&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/y1xux1ful3dd1.png?width=1352&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=27397c923761da40c91fee4e9406d3a40ba15219&quot;&gt;https://preview.redd.it/y1xux1ful3dd1.png?width=1352&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=27397c923761da40c91fee4e9406d3a40ba15219&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Chunks -&amp;gt; segments&lt;/h1&gt; &lt;p&gt;Now let’s run a query and visualize chunk relevance across the entire document. We’ll use the query “Nike stock-based compensation expenses.”&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/6df9gflwl3dd1.png?width=1001&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1e3a2e9c8fc360d98e2fbb3a6934f8320b89317e&quot;&gt;https://preview.redd.it/6df9gflwl3dd1.png?width=1001&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1e3a2e9c8fc360d98e2fbb3a6934f8320b89317e&lt;/a&gt;&lt;/p&gt; &lt;p&gt;In the plot above, the x-axis represents the chunk index. The first chunk in the document has index 0, the next chunk has index 1, etc. There are 483 chunks in total for this document. The y-axis represents the relevance of each chunk to the query. Viewing it this way lets us see how relevant chunks tend to be clustered in one or more sections of a document. For this query we can see that there’s a cluster of relevant chunks around index 400, which likely indicates there’s a multi-page section of the document that covers the topic we’re interested in. Not all queries will have clusters of relevant chunks like this. Queries for specific pieces of information where the answer is likely to be contained in a single chunk may just have one or two isolated chunks that are relevant.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What can we do with these clusters of relevant chunks?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The core idea is that clusters of relevant chunks, in their original contiguous form, provide much better context to the LLM than individual chunks can. Now for the hard part: how do we actually identify these clusters?&lt;/p&gt; &lt;p&gt;If we can calculate chunk values in such a way that the value of a segment is just the sum of the values of its constituent chunks, then finding the optimal segment is a version of the maximum subarray problem, for which a solution can be found relatively easily. How do we define chunk values in such a way? We&amp;#39;ll start with the idea that highly relevant chunks are good, and irrelevant chunks are bad. We already have a good measure of chunk relevance (shown in the plot above), on a scale of 0-1, so all we need to do is subtract a constant threshold value from it. This will turn the chunk value of irrelevant chunks to a negative number, while keeping the values of relevant chunks positive. We call this the &lt;code&gt;irrelevant_chunk_penalty&lt;/code&gt;. A value around 0.2 seems to work well empirically. Lower values will bias the results towards longer segments, and higher values will bias them towards shorter segments.&lt;/p&gt; &lt;p&gt;For this query, the algorithm identifies chunks 397-410 as the most relevant segment of text from the document. It also identifies chunk 362 as sufficiently relevant to include in the results. Here is what the first segment looks like:&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/2irxe9nyl3dd1.png?width=2684&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=395a06fdad66e57fab10fd67c8c44786c663d4ad&quot;&gt;https://preview.redd.it/2irxe9nyl3dd1.png?width=2684&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=395a06fdad66e57fab10fd67c8c44786c663d4ad&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This looks like a great result. Let’s zoom in on the chunk relevance plot for this segment.&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/2fguxao0m3dd1.png?width=1001&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a5a25959a000b7013869a17215c1b762f5a42f7e&quot;&gt;https://preview.redd.it/2fguxao0m3dd1.png?width=1001&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a5a25959a000b7013869a17215c1b762f5a42f7e&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Looking at the content of each of these chunks, it&amp;#39;s clear that chunks 397-401 are highly relevant, as expected. But looking closely at chunks 402-404 (this is the section about stock options), we can see they&amp;#39;re actually also relevant, despite being marked as irrelevant by our ranking model. This is a common theme: chunks that are marked as not relevant, but are sandwiched between highly relevant chunks, are oftentimes quite relevant. In this case, the chunks were about stock option valuation, so while they weren&amp;#39;t explicitly discussing stock-based compensation expenses (which is what we were searching for), in the context of the surrounding chunks it&amp;#39;s clear that they are actually relevant. So in addition to providing more complete context to the LLM, this method of dynamically constructing segments of relevant text also makes our retrieval system less sensitive to mistakes made by the ranking model.&lt;/p&gt; &lt;h1&gt;Try it for yourself&lt;/h1&gt; &lt;p&gt;If you want to give these methods a try, we’ve open-sourced a retrieval engine that implements these methods, called &lt;a href=&quot;https://github.com/D-Star-AI/dsRAG&quot;&gt;dsRAG&lt;/a&gt;. You can also play around with the &lt;a href=&quot;https://github.com/D-Star-AI/dsRAG/blob/main/examples/dsRAG_motivation.ipynb&quot;&gt;iPython notebook&lt;/a&gt; we used to run these examples and generate the plots. And if you want to use this with LangChain, we have a &lt;a href=&quot;https://github.com/D-Star-AI/dsRAG/blob/main/integrations/langchain_retriever.py&quot;&gt;LangChain custom retriever&lt;/a&gt; implementation as well.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/zmccormick7&quot;&gt; /u/zmccormick7 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e5le9h/solving_the_outofcontext_chunk_problem_for_rag/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e5le9h/solving_the_outofcontext_chunk_problem_for_rag/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1e5le9h</id><media:thumbnail url="https://external-preview.redd.it/_q1gwtUaSI8ZGrE9UVTyea73HvmYDrIc89DmMaJ-fY0.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=c1e6a2e33500ef62a7ef074552a238d2d3d82cc9" /><link href="https://www.reddit.com/r/LangChain/comments/1e5le9h/solving_the_outofcontext_chunk_problem_for_rag/" /><updated>2024-07-17T15:42:23+00:00</updated><published>2024-07-17T15:42:23+00:00</published><title>Solving the out-of-context chunk problem for RAG</title></entry><entry><author><name>/u/BellaHi</name><uri>https://www.reddit.com/user/BellaHi</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e61er2/dify_vs_langchain_a_comprehensive_analysis_for_ai/&quot;&gt; &lt;img src=&quot;https://external-preview.redd.it/OcERBh67stxAp_qKMZTPfYwDM5B-0ATFf_k7yYHG5qs.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5ced81747b2affb02ebca2d242093543aeaa1c5b&quot; alt=&quot;Dify vs Langchain: A Comprehensive Analysis for AI App Development&quot; title=&quot;Dify vs Langchain: A Comprehensive Analysis for AI App Development&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/BellaHi&quot;&gt; /u/BellaHi &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://myscale.com/blog/dify-vs-langchain-comprehensive-analysis-ai-app-development/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e61er2/dify_vs_langchain_a_comprehensive_analysis_for_ai/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1e61er2</id><media:thumbnail url="https://external-preview.redd.it/OcERBh67stxAp_qKMZTPfYwDM5B-0ATFf_k7yYHG5qs.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=5ced81747b2affb02ebca2d242093543aeaa1c5b" /><link href="https://www.reddit.com/r/LangChain/comments/1e61er2/dify_vs_langchain_a_comprehensive_analysis_for_ai/" /><updated>2024-07-18T03:15:34+00:00</updated><published>2024-07-18T03:15:34+00:00</published><title>Dify vs Langchain: A Comprehensive Analysis for AI App Development</title></entry><entry><author><name>/u/Confident-Honeydew66</name><uri>https://www.reddit.com/user/Confident-Honeydew66</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Confident-Honeydew66&quot;&gt; /u/Confident-Honeydew66 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://thepi.pe/evals&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e5wpad/i_spent_a_few_days_aggregating_llm_performance/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1e5wpad</id><link href="https://www.reddit.com/r/LangChain/comments/1e5wpad/i_spent_a_few_days_aggregating_llm_performance/" /><updated>2024-07-17T23:26:22+00:00</updated><published>2024-07-17T23:26:22+00:00</published><title>I spent a few days aggregating LLM performance metrics, token cost, and context size with all sources cited. Here it is for free.</title></entry><entry><author><name>/u/jy2k</name><uri>https://www.reddit.com/user/jy2k</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;What additional supporting frameworks do you use in order to structure the conversation ? Is the only alternative Langraph ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/jy2k&quot;&gt; /u/jy2k &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e5s323/how_do_you_structure_the_conversation/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e5s323/how_do_you_structure_the_conversation/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1e5s323</id><link href="https://www.reddit.com/r/LangChain/comments/1e5s323/how_do_you_structure_the_conversation/" /><updated>2024-07-17T20:12:42+00:00</updated><published>2024-07-17T20:12:42+00:00</published><title>How do you structure the conversation?</title></entry><entry><author><name>/u/SmellyCatJon</name><uri>https://www.reddit.com/user/SmellyCatJon</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I am trying to use llama3 8b on groq. I want to use langchain to make my prompts better. I am implementing this on server side js. I keep running into one issue or another. Do you guys have recommendation to an API documentation or a good similar implementation I can learn from. I have done google search on this but the ones out there do not seem to do the trick so far - maybe outdated? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/SmellyCatJon&quot;&gt; /u/SmellyCatJon &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e60xf5/langchain_integration_with_groq/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e60xf5/langchain_integration_with_groq/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1e60xf5</id><link href="https://www.reddit.com/r/LangChain/comments/1e60xf5/langchain_integration_with_groq/" /><updated>2024-07-18T02:50:56+00:00</updated><published>2024-07-18T02:50:56+00:00</published><title>Langchain integration with GROQ</title></entry><entry><author><name>/u/PotentialNo826</name><uri>https://www.reddit.com/user/PotentialNo826</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey Guys,&lt;/p&gt; &lt;p&gt;Tried looking for this online for ages, but didn&amp;#39;t find a solution. I have a use case where I retrieve 100s of papers online for every user query and index them in a collection specific to the query. This data also persists in an SQL database. &lt;/p&gt; &lt;p&gt;I want to run a retriever on this data only once, and the documents in the collection are also only stored once. &lt;/p&gt; &lt;p&gt;But, as there are a lot of user queries, my vector store, which in this case is qdrant usually fills up and then refuses to connect.&lt;/p&gt; &lt;p&gt;I&amp;#39;ve tried deleting the collection but that still causes the database to fill up. Are there any better vector stores/strategies I can use? Really lost here.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/PotentialNo826&quot;&gt; /u/PotentialNo826 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e60u0v/which_vectore_store_to_use_single_use_collection/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e60u0v/which_vectore_store_to_use_single_use_collection/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1e60u0v</id><link href="https://www.reddit.com/r/LangChain/comments/1e60u0v/which_vectore_store_to_use_single_use_collection/" /><updated>2024-07-18T02:46:06+00:00</updated><published>2024-07-18T02:46:06+00:00</published><title>Which vectore store to use? Single use collection.</title></entry><entry><author><name>/u/Jl_btdipsbro</name><uri>https://www.reddit.com/user/Jl_btdipsbro</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;New tool for evaluating RAG pipelines with local models&lt;/p&gt; &lt;p&gt;I&amp;#39;ve released a RagRelevanceEvaluator that works with open-source models. Key features:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Test chunk sizes and top K retrieval settings&lt;/li&gt; &lt;li&gt;Get relevance scores for retrieved passages&lt;/li&gt; &lt;li&gt;No external API needed - uses fine tuned local models&lt;/li&gt; &lt;li&gt;Integrates with LangChain or other orchestrators&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Great for optimizing RAG performance locally. Helps tune parameters and compare configurations objectively.&lt;/p&gt; &lt;p&gt;Runs completely locally for quick iteration without API costs or privacy concerns.&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href=&quot;https://github.com/grounded-ai/grounded_ai&quot;&gt;https://github.com/grounded-ai/grounded_ai&lt;/a&gt; HuggingFace: &lt;a href=&quot;https://huggingface.co/grounded-ai&quot;&gt;https://huggingface.co/grounded-ai&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Jl_btdipsbro&quot;&gt; /u/Jl_btdipsbro &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e5jw84/evaluate_rag_pipelines/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e5jw84/evaluate_rag_pipelines/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1e5jw84</id><link href="https://www.reddit.com/r/LangChain/comments/1e5jw84/evaluate_rag_pipelines/" /><updated>2024-07-17T14:41:39+00:00</updated><published>2024-07-17T14:41:39+00:00</published><title>Evaluate RAG pipelines</title></entry><entry><author><name>/u/HotRepresentative325</name><uri>https://www.reddit.com/user/HotRepresentative325</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Ideally as opensource as possible unless there isn&amp;#39;t a good one. Ideally the best one for RAG retrieval.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/HotRepresentative325&quot;&gt; /u/HotRepresentative325 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e5zb2n/what_vector_store_are_you_using_for_hybrid_search/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e5zb2n/what_vector_store_are_you_using_for_hybrid_search/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1e5zb2n</id><link href="https://www.reddit.com/r/LangChain/comments/1e5zb2n/what_vector_store_are_you_using_for_hybrid_search/" /><updated>2024-07-18T01:29:10+00:00</updated><published>2024-07-18T01:29:10+00:00</published><title>What vector store are you using for hybrid search?</title></entry><entry><author><name>/u/Puzzleheaded_Exit426</name><uri>https://www.reddit.com/user/Puzzleheaded_Exit426</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I am currently building a streamlit app from &amp;#39;scratch&amp;#39; without any llm libraries, just using api calls to openrouter and pinecone, along with some custom classes to handle the app&amp;#39;s state management, conversation history, context truncation etc.&lt;/p&gt; &lt;p&gt;The basic idea of the app is to chat with an LLM to develop a marketing targeting campaign. &lt;/p&gt; &lt;p&gt;It starts with an api call to perplexity to research a company and receive back a target audience description. This is passed to a multi-turn chain of thought refining and eventually outputting valid json that includes targeted and excluded &amp;#39;hypothetical data segments&amp;#39; for a marketing campaign. These are hypothetical descriptions of data segments that might be available on an advertising data marketplace like &amp;#39;coffee enthusiasts&amp;#39; or &amp;#39;parents with children under 18&amp;#39; etc.&lt;/p&gt; &lt;p&gt;Once the initial audience plan is generated, there is an opportunity for a user to &amp;#39;chat&amp;#39; with the plan. It is similar to the claude artifact system where your comments lead to changes in a &amp;#39;document&amp;#39;. In this case the document is stored as json, and it updates a user interface with the set of included and excluded segments displayed as clickable boxes. There also a few buttons which basically construct predefined prompts that can be sent to the llm, asking it to change the document in particular ways. &lt;/p&gt; &lt;p&gt;Once the hypothetical plan is developed, there is an api call to pinecone where I have a vector store of ~500k actual audience segments so that each hypothetical segment returns an actual segment with a reranking function to pick the best one.&lt;/p&gt; &lt;p&gt;The part that is getting difficult to manage in this &amp;#39;from scratch&amp;#39; way is the user feedback on the json audience plan. Often it works great, but there are certain moments where the LLM returns JSON in a format not expected by the UI-renderer. I am presently patching all these as they come up via both prompt engineering and output parsing stuff, but it feels a bit hard to maintain, especially if we want to add features. If anyone has made it this far into the post I would love to hear advice on how to make this app more maintainable... a few possibilities I might pursue are the following:&lt;br/&gt; 1. Any time the expected json is not met, manually send the llm a message indicating the needed format, along with conversation history, and then parse its response for the correct json. The needed format could be stored as part of the app&amp;#39;s state so it can be returned easily to the llm at any time.&lt;br/&gt; 2. Implement instructor to more easily manage the data validation and retries&lt;br/&gt; 3. rebuild using langchain&lt;br/&gt; 4. any other ideas?&lt;/p&gt; &lt;p&gt;Thanks so much for reading.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Puzzleheaded_Exit426&quot;&gt; /u/Puzzleheaded_Exit426 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e5wf7u/langchain_instructor_or_something_else_for_my_use/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e5wf7u/langchain_instructor_or_something_else_for_my_use/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1e5wf7u</id><link href="https://www.reddit.com/r/LangChain/comments/1e5wf7u/langchain_instructor_or_something_else_for_my_use/" /><updated>2024-07-17T23:13:57+00:00</updated><published>2024-07-17T23:13:57+00:00</published><title>langchain, instructor, or something else for my use case?</title></entry><entry><author><name>/u/northwolf56</name><uri>https://www.reddit.com/user/northwolf56</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;&lt;a href=&quot;https://youtu.be/MHuvSuK2dnY?si=qtHWZlmNom7E-RtS&quot;&gt;https://youtu.be/MHuvSuK2dnY?si=qtHWZlmNom7E-RtS&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/northwolf56&quot;&gt; /u/northwolf56 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e5sp47/100_in_browser_llm_using_langchain_and_webllm/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e5sp47/100_in_browser_llm_using_langchain_and_webllm/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1e5sp47</id><link href="https://www.reddit.com/r/LangChain/comments/1e5sp47/100_in_browser_llm_using_langchain_and_webllm/" /><updated>2024-07-17T20:38:10+00:00</updated><published>2024-07-17T20:38:10+00:00</published><title>100% in browser LLM using langchain and WebLLM</title></entry><entry><author><name>/u/Least_Suspect_7256</name><uri>https://www.reddit.com/user/Least_Suspect_7256</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I am using langchain chain class for summarization. When I invoke() the chain for summarization, where do the computations occur? In my deployment compute or where the LLM I am using is hosted? Pretty confused about it since I need to allocate enough memory for doing some summarization task and I’m unsure of it. I know invoke() is I/O bound but that being said does the resource allocation for my API deployment that calls langchain chain() matter? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Least_Suspect_7256&quot;&gt; /u/Least_Suspect_7256 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e5ricy/question_about_langchain_api_calls/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e5ricy/question_about_langchain_api_calls/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1e5ricy</id><link href="https://www.reddit.com/r/LangChain/comments/1e5ricy/question_about_langchain_api_calls/" /><updated>2024-07-17T19:49:32+00:00</updated><published>2024-07-17T19:49:32+00:00</published><title>Question about langchain api calls</title></entry><entry><author><name>/u/Ecto-1A</name><uri>https://www.reddit.com/user/Ecto-1A</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I figured it&amp;#39;d be interesting to get input from different people and industries on this. There are probably a million reasons out there, from lack of executive buy-in to inconsistent RAG results. What are the current roadblocks everyones facing?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Ecto-1A&quot;&gt; /u/Ecto-1A &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e54hjr/whats_your_biggest_holdup_in_taking_ai_to/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e54hjr/whats_your_biggest_holdup_in_taking_ai_to/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1e54hjr</id><link href="https://www.reddit.com/r/LangChain/comments/1e54hjr/whats_your_biggest_holdup_in_taking_ai_to/" /><updated>2024-07-17T00:26:52+00:00</updated><published>2024-07-17T00:26:52+00:00</published><title>What's your biggest holdup in taking AI to production?</title></entry><entry><author><name>/u/gwen_from_nile</name><uri>https://www.reddit.com/user/gwen_from_nile</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;When you are starting a new project, how do you choose which model you are going to use? &lt;/p&gt; &lt;p&gt;Even when we look at only text generation models, there are so many and things change every day. What do you go with? and how do you decide?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/gwen_from_nile&quot;&gt; /u/gwen_from_nile &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e5b9f7/how_do_you_choose_a_model/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e5b9f7/how_do_you_choose_a_model/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1e5b9f7</id><link href="https://www.reddit.com/r/LangChain/comments/1e5b9f7/how_do_you_choose_a_model/" /><updated>2024-07-17T06:28:53+00:00</updated><published>2024-07-17T06:28:53+00:00</published><title>How do you choose a model?</title></entry><entry><author><name>/u/andrecorumba</name><uri>https://www.reddit.com/user/andrecorumba</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;&amp;quot;The Prompt Report: A Systematic Survey of Prompting Techniques&amp;quot; offers a detailed review of prompting techniques in AI. It introduces a taxonomy of 33 terms, 58 textual and 40 multimodal techniques. The study covers terminology, safety, evaluation, and practical applications across various languages and modalities. It also discusses the evolution and challenges of prompting in AI, emphasizing its importance in the development of generative models. The report aims to standardize prompting practices and address existing gaps in the literature.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/andrecorumba&quot;&gt; /u/andrecorumba &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e5fyq9/the_prompt_report_a_systematic_survey_of/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e5fyq9/the_prompt_report_a_systematic_survey_of/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1e5fyq9</id><link href="https://www.reddit.com/r/LangChain/comments/1e5fyq9/the_prompt_report_a_systematic_survey_of/" /><updated>2024-07-17T11:35:59+00:00</updated><published>2024-07-17T11:35:59+00:00</published><title>The Prompt Report: A Systematic Survey of Prompting Techniques</title></entry></feed>