<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/"><category term="LangChain" label="r/LangChain"/><updated>2024-06-26T10:24:52+00:00</updated><icon>https://www.redditstatic.com/icon.png/</icon><id>/r/LangChain.rss</id><link rel="self" href="https://www.reddit.com/r/LangChain.rss" type="application/atom+xml" /><link rel="alternate" href="https://www.reddit.com/r/LangChain" type="text/html" /><subtitle>LangChain is an open-source framework and developer toolkit that helps developers get LLM applications from prototype to production. It is available for Python and Javascript at https://www.langchain.com/.</subtitle><title>LangChain</title><entry><author><name>/u/zchaarm</name><uri>https://www.reddit.com/user/zchaarm</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;A place for members of &lt;a href=&quot;/r/LangChain&quot;&gt;r/LangChain&lt;/a&gt; to chat with each other&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/zchaarm&quot;&gt; /u/zchaarm &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_10ljho9</id><link href="https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/" /><updated>2023-01-26T04:33:05+00:00</updated><published>2023-01-26T04:33:05+00:00</published><title>r/LangChain Lounge</title></entry><entry><author><name>/u/alex6011</name><uri>https://www.reddit.com/user/alex6011</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hello, I&amp;#39;m building a RAG app with Langchain.js. It&amp;#39;s my first time using LLM framework, thus first time using Langchain too. I&amp;#39;m currently using the Recursive URL Loader integration to recursively fetch data from websites. And, behave as I wanted, but I have some issue with websites using modern frontend frameworks. So, I tried the Playwright Langchain integration (I&amp;#39;m a bit familiar with Playwright), and it&amp;#39;s work on the desired websites, but as you guess, it only works on one page. So my question is there is an integration that do both? Handling JS and recursively browse the website, or how can I combine the two to achieve my goal? I&amp;#39;m open to alternative solution using Puppeteer or even Python example.&lt;/p&gt; &lt;p&gt;This is how I use the Recursive URL Loader:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;import { RecursiveUrlLoader } from &amp;#39;@langchain/community/document_loaders/web/recursive_url&amp;#39;; import { compile } from &amp;#39;html-to-text&amp;#39;; export async function loadWebsite(url: string, excludeDirs?: string[]) { const compiledConvert = compile({ wordwrap: 130 }); // returns (text: string) =&amp;gt; string; const loader = new RecursiveUrlLoader(url, { extractor: compiledConvert, maxDepth: 1, excludeDirs, preventOutside: true, }); return loader.load(); } &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;And, this is my Playwright attempt:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;import { PlaywrightWebBaseLoader } from &amp;#39;@langchain/community/document_loaders/web/playwright&amp;#39;; import { MozillaReadabilityTransformer } from &amp;#39;@langchain/community/document_transformers/mozilla_readability&amp;#39;; import { RunnableSequence } from &amp;#39;@langchain/core/runnables&amp;#39;; import { RecursiveCharacterTextSplitter } from &amp;#39;@langchain/textsplitters&amp;#39;; export async function loadWebsite(url: string) { const loader = new PlaywrightWebBaseLoader(url, { launchOptions: { chromiumSandbox: false, }, }); const documents = await loader.load(); const transformChain = RunnableSequence.from([ RecursiveCharacterTextSplitter.fromLanguage(&amp;#39;html&amp;#39;), new MozillaReadabilityTransformer(), ]); return transformChain.invoke(documents); } &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Thanks for reading! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/alex6011&quot;&gt; /u/alex6011 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1doudja/how_to_use_recursive_url_loader_with/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1doudja/how_to_use_recursive_url_loader_with/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1doudja</id><link href="https://www.reddit.com/r/LangChain/comments/1doudja/how_to_use_recursive_url_loader_with/" /><updated>2024-06-26T09:24:56+00:00</updated><published>2024-06-26T09:24:56+00:00</published><title>How to use Recursive URL Loader with Playwright/Puppeteer?</title></entry><entry><author><name>/u/Danidre</name><uri>https://www.reddit.com/user/Danidre</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;h1&gt;Preamble&lt;/h1&gt; &lt;p&gt;What I&amp;#39;ve realized through blogs and experience, is that it is best to have different agents for different purposes. E.G.: one agent for docs RAG, one agent for API calls, one agent for SQL queries.&lt;/p&gt; &lt;p&gt;These agents, by themselves, work quite fine when used in a conversational sense. You can prompt the agent for API calls to reply with follow-up questions to obtain the remaining required parameters for the specific request to be made, based on the user request, and then execute the tool call (fetch request).&lt;/p&gt; &lt;p&gt;Similarly, the agent for docs RAG can send a response, and the user can follow up with a vague question. The LLM will have the context to know what they&amp;#39;re referring to.&lt;/p&gt; &lt;h1&gt;Problem&lt;/h1&gt; &lt;p&gt;But how can we merge these three together? I know there are different design patterns such as Hierarchy, and Supervisor. Supervisor sounds like the better approach for this use case: creating a 3th supervisor agent that takes the user request and delegates it to one of the 3 specialized agents. However, these only seem to work when each request perform the action and respond completely in one invocation.&lt;/p&gt; &lt;p&gt;If the supervisor agent delegates to the API calling agent, and that agent responds with a follow-up question for more information, it goes back up the hierarchy to the supervisor agent and the follow-up question is returned as the response to the user. So if the user then sends more information, of course the invocation starts back at the supervisor agent.&lt;/p&gt; &lt;p&gt;How does it keep track of the last sub-agent invoked, whether a user response is to answer a follow-up question, re-invoke the previous agent, whether the user response deviated and required a new agent to be invoked, etc? I have a few ideas, let me know which ones you guys have experienced?&lt;/p&gt; &lt;h1&gt;Ideas&lt;/h1&gt; &lt;h1&gt;Manual Tracking&lt;/h1&gt; &lt;p&gt;Rather than a 4th agent, the user message is first passed to an LLM with definitions of the types of agents. It&amp;#39;s job is to respond with the name of the agent most likely to handle this request. That agent is then invoked. The last agent called, as well as it&amp;#39;s last response is stored. Follow up user messages call this LLM again with definitions of the type of agents, the message, the last agent invoked, and the last message it replied. The LLM will use this context to determine if it should call that same agent again with the new user message, or another agent instead.&lt;/p&gt; &lt;h1&gt;Supervisor Agent with Agent Named as Messages State&lt;/h1&gt; &lt;p&gt;Each sub-agent will have its own isolated messages list, however the supervisor agent will track messages by the name of the agent, to determine who best to delegate the request to. However, it will only track the last response from each invoked agent.&lt;/p&gt; &lt;h1&gt;Example Conversation:&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;User: Hi Agent: Hi, how can I help you today? User: What is the purpose of this company? Agent: *delegates to RAG agent User: What is the purpose of this company? RAG Agent: *tool calls RAG search Tool: ...company purpose...categories... RAG Agent: This company manages categories.... Agent: This company manages categories.... User: I want to create another category Agent: *delegates to API agent User: I want to create another category API Agent: What is the category name and how many stars? Agent: What is the category name and how many stars? User: Name it Category 5 Agent: *delegates to API agent User: Name it Category 5 API Agent: How many stars (1-5)? Agent: How many stars (1-5)? User: 5 Agent: *delegates to API agent User: 5 API Agent: *tool call endpoint with required params Tool: success API Agent: You have successfully created Category 5. Agent: You have successfully created Category 5. User: How many categories have been created today Agent: *delegates to SQL Agent User: How many categories have been created today SQL Agent: *tool calls sql query generation Tool: select count(1) from categories... SQL Agent: *tool calls sql query execution Tool: (8) SQL Agent: 8 categories have been created today. Agent: 8 categories have been created today. &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The history for each agent may be as follows:&lt;/p&gt; &lt;p&gt;RAG Agent:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;User: What is the purpose of this company? Agent: *tool calls RAG search Tool: ...company purpose...categories... Agent: This company manages categories.... &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;API Agent:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;User: I want to create another category Agent: What is the category name and how many stars? User: Name it Category 5 Agent: How many stars (1-5)? User: 5 Agent: *tool call endpoint with required params Tool: success Agent: You have successfully created Category 5. &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;SQL Agent:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;User: How many categories have been created today SQL Agent: *tool calls sql query generation Tool: select count(1) from categories... SQL Agent: *tool calls sql query execution Tool: (8) SQL Agent: 8 categories have been created today. &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Supervisor Agent:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;System: You are a supervisor Agent with the following assistants: RAG Agent helps when.... API Agent helps when.... SQL Agent helps when.... At different times during the conversation, your assistants may interject to respond to the user based on their specialty. Whenever the user responds, based on the history, determine which one of your assistants should respond next. User: Hi Agent: Hi, how can I help you today? User: What is the purpose of this company? RAG Agent: This company manages categories.... User: I want to create another category API Agent: What is the category name and how many stars? User: Name it Category 5 API Agent: How many stars (1-5)? User: 5 API Agent: You have successfully created Category 5. User: How many categories have been created today SQL Agent: 8 categories have been created today. &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Perhaps like this, it can better determine who to delegate future responses to. This by itself already seems a bit more complex than seen developed so far. However, there are still things to consider, such as when the user changes their mind, how would delegation work?&lt;/p&gt; &lt;h1&gt;Example Conversation:&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;User: Hi Agent: Hi, how can I help you today? User: What is the purpose of this company? Agent: *delegates to RAG agent User: What is the purpose of this company? RAG Agent: *tool calls RAG search Tool: ...company purpose...categories... RAG Agent: This company manages categories.... Agent: This company manages categories.... User: I want to create another category Agent: *delegates to API agent User: I want to create another category API Agent: What is the category name and how many stars? Agent: What is the category name and how many stars? User: How many categories have been created today? &amp;lt;-- new request, not meant to be the category name Agent: *delegates to SQL Agent User: How many categories have been created today? SQL Agent: *tool calls sql query generation Tool: select count(1) from categories... SQL Agent: *tool calls sql query execution Tool: (9) SQL Agent: 9 categories have been created today. Agent: 9 categories have been created today. User: Okay. I want to create a sub-category. Agent: *delegates to API agent User: Okay. I want to create a sub-category. API Agent: I&amp;#39;m sorry, you cannot create sub-categories. Agent: I&amp;#39;m sorry, you cannot create sub-categories. &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The history for each agent may be as follows:&lt;/p&gt; &lt;p&gt;RAG Agent:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;User: What is the purpose of this company? Agent: *tool calls RAG search Tool: ...company purpose...categories... Agent: This company manages categories.... &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;API Agent:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;User: I want to create another category Agent: What is the category name and how many stars? User: Okay. I want to create a sub-category. &amp;lt;-- somehow it knows this is meant as a new request, and not part of the category name as above Agent: I&amp;#39;m sorry, you cannot create sub-categories. &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;SQL Agent:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;User: How many categories have been created today? Agent: *tool calls sql query generation Tool: select count(1) from categories... Agent: *tool calls sql query execution Tool: (9) Agent: 9 categories have been created today. &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Supervisor Agent:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;System: You are a supervisor Agent with the following assistants: RAG Agent helps when.... API Agent helps when.... SQL Agent helps when.... At different times during the conversation, your assistants may interject to respond to the user based on their specialty. Whenever the user responds, based on the history, determine which one of your assistants should respond next. User: Hi Agent: Hi, how can I help you today? User: What is the purpose of this company? RAG Agent: This company manages categories.... User: I want to create another category API Agent: What is the category name and how many stars? User: How many categories have been created today? &amp;lt;-- new request, not meant to be the category name. somehow it knows to delegate to SQL Agent instead SQL Agent: 9 categories have been created today. User: Okay. I want to create a sub-category. API Agent: I&amp;#39;m sorry, you cannot create sub-categories. &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;To solve this, maybe there should be an additional step that re-crafts the user prompt before delegating it to each sub-agent?&lt;/p&gt; &lt;p&gt;Does anyone have experiences with these in LangGraph?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Danidre&quot;&gt; /u/Danidre &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dogdy8/multiagent_conversational_graph_designs/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dogdy8/multiagent_conversational_graph_designs/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dogdy8</id><link href="https://www.reddit.com/r/LangChain/comments/1dogdy8/multiagent_conversational_graph_designs/" /><updated>2024-06-25T20:47:49+00:00</updated><published>2024-06-25T20:47:49+00:00</published><title>Multi-Agent Conversational Graph Designs</title></entry><entry><author><name>/u/Lost-Season-4196</name><uri>https://www.reddit.com/user/Lost-Season-4196</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I want to host my app on AWS with multiple GPUs. I tried Llama-Index, but they do not support multi-GPU setups as far as I know. How can I run Hugging Face models on multiple GPUs?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Lost-Season-4196&quot;&gt; /u/Lost-Season-4196 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1doszle/multi_gpu_support/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1doszle/multi_gpu_support/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1doszle</id><link href="https://www.reddit.com/r/LangChain/comments/1doszle/multi_gpu_support/" /><updated>2024-06-26T07:45:04+00:00</updated><published>2024-06-26T07:45:04+00:00</published><title>Multi GPU support</title></entry><entry><author><name>/u/Mission_Rain7133</name><uri>https://www.reddit.com/user/Mission_Rain7133</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Anyone knows if LangGraph integrates with Bedrock and what are the capabilties. I am quite new to this and I was following the langChain youtube series on LangGraph and they used a lot of OpenAI Functions so I wanted to know if it was possible to do the same with Bedrock models?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Mission_Rain7133&quot;&gt; /u/Mission_Rain7133 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dosz4u/langgraph_integration_with_bedrock/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dosz4u/langgraph_integration_with_bedrock/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dosz4u</id><link href="https://www.reddit.com/r/LangChain/comments/1dosz4u/langgraph_integration_with_bedrock/" /><updated>2024-06-26T07:44:13+00:00</updated><published>2024-06-26T07:44:13+00:00</published><title>LangGraph integration with bedrock</title></entry><entry><author><name>/u/Electrical_Art_1518</name><uri>https://www.reddit.com/user/Electrical_Art_1518</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;m creating a chatbot using the OpenAI Assistant API and considering LangChain. Should I use it?&lt;/p&gt; &lt;p&gt;What are the pros and cons?&lt;/p&gt; &lt;p&gt;Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Electrical_Art_1518&quot;&gt; /u/Electrical_Art_1518 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dosb9q/should_i_use_langchain_for_building_a_chatbot/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dosb9q/should_i_use_langchain_for_building_a_chatbot/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dosb9q</id><link href="https://www.reddit.com/r/LangChain/comments/1dosb9q/should_i_use_langchain_for_building_a_chatbot/" /><updated>2024-06-26T06:57:51+00:00</updated><published>2024-06-26T06:57:51+00:00</published><title>Should I Use LangChain for Building a Chatbot with OpenAI Assistant API?</title></entry><entry><author><name>/u/Smooth-Loquat-4954</name><uri>https://www.reddit.com/user/Smooth-Loquat-4954</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dogr36/legal_semantic_search_w_langchain_pineccone_and/&quot;&gt; &lt;img src=&quot;https://external-preview.redd.it/XwBFGcZE3ASay1j0fNTmfW6lJmBX9Z__MYtad6ExHAg.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=85a994ab1b17700deba8a70a35ab9b6ea5f350b8&quot; alt=&quot;Legal Semantic Search w LangChain, Pineccone and Next.js - full source code &quot; title=&quot;Legal Semantic Search w LangChain, Pineccone and Next.js - full source code &quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Smooth-Loquat-4954&quot;&gt; /u/Smooth-Loquat-4954 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://docs.pinecone.io/examples/sample-apps/legal-semantic-search&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dogr36/legal_semantic_search_w_langchain_pineccone_and/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1dogr36</id><media:thumbnail url="https://external-preview.redd.it/XwBFGcZE3ASay1j0fNTmfW6lJmBX9Z__MYtad6ExHAg.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=85a994ab1b17700deba8a70a35ab9b6ea5f350b8" /><link href="https://www.reddit.com/r/LangChain/comments/1dogr36/legal_semantic_search_w_langchain_pineccone_and/" /><updated>2024-06-25T21:03:03+00:00</updated><published>2024-06-25T21:03:03+00:00</published><title>Legal Semantic Search w LangChain, Pineccone and Next.js - full source code</title></entry><entry><author><name>/u/singhinsiliconvalley</name><uri>https://www.reddit.com/user/singhinsiliconvalley</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I am working on a project where user injects a JSON/YAML/XML file as structured desired for output formatting and able to generate output by giving that structure to LLM.&lt;/p&gt; &lt;p&gt;So far I was coding all the data models as per user&amp;#39;s requirements into a Pydantic Class and using &lt;code&gt;PydanticOutputParser()&lt;/code&gt; to pass the structure information into prompt. I want to upgrade this feature and let user provide structure in JSON/YAML/XML (for testing I am thinking of doing with JSON) instead of my manually adding different data models. &lt;/p&gt; &lt;p&gt;Langchain&amp;#39;s abstractions makes it very complicated to implement changes, I wanted to test my hypothesis before investing time to code something of this kind. &lt;/p&gt; &lt;p&gt;Any help would be much appreciated, Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/singhinsiliconvalley&quot;&gt; /u/singhinsiliconvalley &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dolptc/alternatives_to_pydantic_data_model_for_output/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dolptc/alternatives_to_pydantic_data_model_for_output/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dolptc</id><link href="https://www.reddit.com/r/LangChain/comments/1dolptc/alternatives_to_pydantic_data_model_for_output/" /><updated>2024-06-26T00:45:42+00:00</updated><published>2024-06-26T00:45:42+00:00</published><title>Alternatives to Pydantic Data Model for Output Parsers.</title></entry><entry><author><name>/u/ANil1729</name><uri>https://www.reddit.com/user/ANil1729</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I have built an open-source AI agent which can handle voice calls and respond back in real-time. Can be used for many use-cases such as sales calls, customer support etc.&lt;/p&gt; &lt;p&gt;Link to project :- &lt;a href=&quot;https://github.com/Anil-matcha/AI-Voice-Agent&quot;&gt;https://github.com/Anil-matcha/AI-Voice-Agent&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/ANil1729&quot;&gt; /u/ANil1729 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1do81l5/opensource_ai_voice_agent_with_openai/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1do81l5/opensource_ai_voice_agent_with_openai/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1do81l5</id><link href="https://www.reddit.com/r/LangChain/comments/1do81l5/opensource_ai_voice_agent_with_openai/" /><updated>2024-06-25T14:58:08+00:00</updated><published>2024-06-25T14:58:08+00:00</published><title>Open-source AI Voice Agent with OpenAI</title></entry><entry><author><name>/u/Txflip</name><uri>https://www.reddit.com/user/Txflip</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I have a document, such as this &lt;a href=&quot;https://www.xpo.com/cdn/download_files/s1/p2831/CNWY_199-AI.2.pdf&quot;&gt;one&lt;/a&gt;, that describes the costs one can expect with a shipment, and throughout describe other fees that may come up depending on a specific shipment. I&amp;#39;m trying parse the doc and save it in a neo4j DB for RAG, using the LLMGraphTransformer class.&lt;/p&gt; &lt;p&gt;Any ideas on how I should go about parsing the document to have relevant chunks, and what I should set allowed nodes and relationships to? Everything I&amp;#39;ve tried so far, I haven&amp;#39;t gotten any nodes or relationships from the model. &lt;/p&gt; &lt;p&gt;Or, do you recommend maybe abandoning using a graph DB and go back to a traditional vectorDB, with a RAPTOR system? Open to suggestions!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Txflip&quot;&gt; /u/Txflip &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1doiikb/graphdb_rag_w_langchain_to_find_fees/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1doiikb/graphdb_rag_w_langchain_to_find_fees/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1doiikb</id><link href="https://www.reddit.com/r/LangChain/comments/1doiikb/graphdb_rag_w_langchain_to_find_fees/" /><updated>2024-06-25T22:17:30+00:00</updated><published>2024-06-25T22:17:30+00:00</published><title>GraphDB RAG w/ LangChain to find fees</title></entry><entry><author><name>/u/The404Dude</name><uri>https://www.reddit.com/user/The404Dude</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I have limited experience with LangChain and LLMs, primarily building simple chatbots with Retrieval-Augmented Generation (RAG). Currently, I&amp;#39;m helping a friend build a WhatsApp chatbot that retrieves its answers from a SQL database. I&amp;#39;ve been experimenting with the SQL tutorials in LangChain, but I haven&amp;#39;t yet achieved satisfactory results for a v1. I&amp;#39;ve tried using &lt;code&gt;create_sql_query_chain&lt;/code&gt; and &lt;code&gt;create_sql_agent&lt;/code&gt;. &lt;code&gt;create_sql_query_chain&lt;/code&gt; has been very inconsistent, while &lt;code&gt;create_sql_agent&lt;/code&gt; has produced better results but still struggles with the following issues:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Ensuring the LLM restricts queries to a specific &lt;code&gt;customer_id&lt;/code&gt; obtained from the context.&lt;/li&gt; &lt;li&gt;Preventing the LLM from answering questions that are too open, generic, or related to restricted device types (the user should only be allowed to ask about device type A).&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Given these challenges, I&amp;#39;m looking for advice, ideas, and any relevant experiences you might have. Thank you&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/The404Dude&quot;&gt; /u/The404Dude &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1do5f9p/chatbot_that_talks_to_sql/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1do5f9p/chatbot_that_talks_to_sql/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1do5f9p</id><link href="https://www.reddit.com/r/LangChain/comments/1do5f9p/chatbot_that_talks_to_sql/" /><updated>2024-06-25T13:00:22+00:00</updated><published>2024-06-25T13:00:22+00:00</published><title>Chatbot that talks to SQL</title></entry><entry><author><name>/u/upandfastLFGG</name><uri>https://www.reddit.com/user/upandfastLFGG</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dohi7o/need_help_with_streaming_agent_output_and_am_i/&quot;&gt; &lt;img src=&quot;https://a.thumbs.redditmedia.com/hAfDoMI0jhayISnECBe-g0JOcM2uH86Xf2BHB5zdX-8.jpg&quot; alt=&quot;Need help with streaming agent output and am I going crazy or did langchain documentation just change?&quot; title=&quot;Need help with streaming agent output and am I going crazy or did langchain documentation just change?&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Right now, my chat app involves users making a post request to an endpoint from my fastapi server.&lt;/p&gt; &lt;p&gt;The endpoint is responsible for processing user input and calling the agent. Everything is setup but I&amp;#39;m running into an issue with streaming. For some reason, sometimes I&amp;#39;ll find the event object apart of my streamed response (see image). &lt;strong&gt;For reference I&amp;#39;m using the astream_events API&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Here is the overall flow:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;1. My endpoint will return:&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt; return StreamingResponse(call_agent(agent_payload, redis_key), media_type=&amp;quot;text/event-stream&amp;quot;) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;2. call_agent is defined as follows:&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;async def call_agent( payload, version=&amp;quot;v1&amp;quot;): agent_response = &amp;quot;&amp;quot; async for chunk in agent_ex.astream_events(payload, version=version): if chunk[&amp;quot;event&amp;quot;] == &amp;quot;on_chat_model_stream&amp;quot; and chunk[&amp;quot;name&amp;quot;] == &amp;quot;ChatOpenAI&amp;quot; and chunk[&amp;quot;data&amp;quot;][&amp;quot;chunk&amp;quot;].content: content = chunk[&amp;quot;data&amp;quot;][&amp;quot;chunk&amp;quot;].content yield f&amp;quot;{content}&amp;quot; agent_response += content elif chunk[&amp;quot;event&amp;quot;] == &amp;quot;on_tool_end&amp;quot; and chunk[&amp;quot;name&amp;quot;] == &amp;quot;product_info_search&amp;quot;: yield f&amp;quot;{json.dumps(serialize_chunk(chunk))}&amp;quot; elif chunk[&amp;quot;event&amp;quot;] == &amp;quot;on_chat_model_end&amp;quot; and chunk[&amp;quot;name&amp;quot;] == &amp;quot;ChatOpenAI&amp;quot;: yield f&amp;quot;{json.dumps(serialize_chunk(chunk))}&amp;quot; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;3. My utility function serialize_chunk is defined as follows:&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;def serialize_chunk(chunk): if isinstance(chunk, dict): return {k: serialize_chunk(v) for k, v in chunk.items()} elif isinstance(chunk, list): return [serialize_chunk(item) for item in chunk] elif ( isinstance(chunk, AIMessageChunk) or isinstance(chunk, SystemMessage) or isinstance(chunk, HumanMessage) or isinstance(chunk, ToolMessage) or isinstance(chunk, ChatPromptValue) or isinstance(chunk, AgentFinish) or isinstance(chunk, AIMessage) ): return chunk.dict() else: return chunk &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;I know my serialize_chunk is a little unique. It&amp;#39;s what made sense to me when handling the non-serializeable encoding error while sending the agent response to the front end and having it treated as JSON. Thing is, I also don&amp;#39;t even see the .dict() method being mentioned anywhere in the documentation.&lt;/p&gt; &lt;p&gt;For some reason, sometimes the streamed response will include the event JSON object. I&amp;#39;m pretty lost as to why this is happening. 70% of the time things are rendering and streaming as expected for the most part.&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/c9i1oe3qds8d1.jpg?width=658&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=1335ac4c0c835dd8ce5e71c44d5ce50a5de6bcaa&quot;&gt;https://preview.redd.it/c9i1oe3qds8d1.jpg?width=658&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=1335ac4c0c835dd8ce5e71c44d5ce50a5de6bcaa&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;/u/hwchase17&quot;&gt;u/hwchase17&lt;/a&gt; would greattttly appreciate any feedback if you ever see this!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/upandfastLFGG&quot;&gt; /u/upandfastLFGG &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dohi7o/need_help_with_streaming_agent_output_and_am_i/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dohi7o/need_help_with_streaming_agent_output_and_am_i/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1dohi7o</id><media:thumbnail url="https://a.thumbs.redditmedia.com/hAfDoMI0jhayISnECBe-g0JOcM2uH86Xf2BHB5zdX-8.jpg" /><link href="https://www.reddit.com/r/LangChain/comments/1dohi7o/need_help_with_streaming_agent_output_and_am_i/" /><updated>2024-06-25T21:34:25+00:00</updated><published>2024-06-25T21:34:25+00:00</published><title>Need help with streaming agent output and am I going crazy or did langchain documentation just change?</title></entry><entry><author><name>/u/thatsusernameistaken</name><uri>https://www.reddit.com/user/thatsusernameistaken</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi&lt;/p&gt; &lt;p&gt;Iâ€™ve tried to make some agent using crewai and autogen. And now Iâ€™m trying to make something similar in langchain. &lt;/p&gt; &lt;p&gt;What Iâ€™m trying to do is to ask the chatbot a question about a topic, this topic needs to be researched (googled), the sites scraped and embedded for content, and as a result Iâ€™d like the AI to output a structured data which I can then format as markdown. &lt;/p&gt; &lt;p&gt;Iâ€™ve made this work fairly ok with crewai, but would like to do this with langchain. I am able to - search for topics (DuckDuckGo, serper) - scrape and embed a site (bs4, web scraper) - output structured data (json)&lt;/p&gt; &lt;p&gt;But Iâ€™m unable to do this in an agent chain. &lt;/p&gt; &lt;p&gt;I also have a second use case which is to make an AI access APIs for searching (self hosted content), and use that result as a context for the answer. &lt;/p&gt; &lt;p&gt;Using local models only with ollama, preferably the openapi spec api. &lt;/p&gt; &lt;p&gt;Where should I start? Trying to chain these tools together went for easy to expert way too fast! ðŸ˜‚&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/thatsusernameistaken&quot;&gt; /u/thatsusernameistaken &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1doefau/how_to_get_started/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1doefau/how_to_get_started/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1doefau</id><link href="https://www.reddit.com/r/LangChain/comments/1doefau/how_to_get_started/" /><updated>2024-06-25T19:26:09+00:00</updated><published>2024-06-25T19:26:09+00:00</published><title>How to get started?</title></entry><entry><author><name>/u/kamishugaze</name><uri>https://www.reddit.com/user/kamishugaze</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Using the following prompt template in building a RAG based QnA application:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;template = &amp;quot;&amp;quot;&amp;quot;Use the following pieces of context to answer the question about the story at the end. If the context doesn&amp;#39;t provide enough information, just say that you don&amp;#39;t know, don&amp;#39;t try to make up an answer. Pay attention to the context of the question rather than just looking for similar keywords in the corpus. Use three sentences maximum and keep the answer as concise as possible. Always say &amp;quot;thanks for asking!&amp;quot; at the end of the answer. {context} Question: {question} Helpful Answer: &amp;quot;&amp;quot;&amp;quot; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;After a frequent testing, this turns out to be a very primitive prompt to the llm as the responses went on to hallucinate and sometimes produced manipulated responses on asking for a misleading query i.e. to test if it corrects the user. &lt;/p&gt; &lt;p&gt;If anyone working on the same or knows how to craft complex prompt templates for an optimized response generation, please care to enlighten.&lt;/p&gt; &lt;p&gt;Besides, are there any frameworks for such templates to ease up the task of prompt engineering?&lt;/p&gt; &lt;p&gt;Thank You &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/kamishugaze&quot;&gt; /u/kamishugaze &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1doecqa/creating_a_prompt_template_for_rag_chain/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1doecqa/creating_a_prompt_template_for_rag_chain/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1doecqa</id><link href="https://www.reddit.com/r/LangChain/comments/1doecqa/creating_a_prompt_template_for_rag_chain/" /><updated>2024-06-25T19:22:20+00:00</updated><published>2024-06-25T19:22:20+00:00</published><title>Creating a prompt template for RAG chain</title></entry><entry><author><name>/u/Jean_dta</name><uri>https://www.reddit.com/user/Jean_dta</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi community, I have some problems with my model; I used GPT-4 for do a health model with RAG; I require that my model doesn&amp;#39;t speak about: financial, techonology... I want my model only can speak about health topics.&lt;/p&gt; &lt;p&gt;I used Fine-tuning for this issue, but my model got overfitting in some cases, for example when I wrote &amp;quot;Hi, how ar you&amp;quot; their answer was &amp;quot;I can&amp;#39;t speak about that...&amp;quot;, when I passed some examples in the traning data some examples that in which model respond with &amp;quot;Hi, my name in CemGPT....&amp;quot;.&lt;/p&gt; &lt;p&gt;How could I solve this problem?&lt;/p&gt; &lt;p&gt;help me pls!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Jean_dta&quot;&gt; /u/Jean_dta &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1do8d7m/custom_moderation_gpt_4_model/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1do8d7m/custom_moderation_gpt_4_model/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1do8d7m</id><link href="https://www.reddit.com/r/LangChain/comments/1do8d7m/custom_moderation_gpt_4_model/" /><updated>2024-06-25T15:11:24+00:00</updated><published>2024-06-25T15:11:24+00:00</published><title>Custom Moderation GPT 4 Model</title></entry><entry><author><name>/u/italian_giga_chad</name><uri>https://www.reddit.com/user/italian_giga_chad</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi everyone!&lt;/p&gt; &lt;p&gt;I have three CSV files for a RAG project. Two of the files are interconnected by a field that acts like a relational database key. The third file contains information related to the others, but there is no clear relational ID or similar field to connect them.&lt;/p&gt; &lt;p&gt;My idea was to unify the first two files into a JSON format and then use an LLM to classify natural language queries to extract a JSON for searching and generating a response based on the results. However, I have two problems with this solution:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;How can I integrate the information from the third CSV file?&lt;/li&gt; &lt;li&gt;The customer requested using a vector database like Chroma or Pinecone.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;What do you suggest I do?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/italian_giga_chad&quot;&gt; /u/italian_giga_chad &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dod2o3/rag_system_on_3_different_csv_any_suggestions/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dod2o3/rag_system_on_3_different_csv_any_suggestions/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dod2o3</id><link href="https://www.reddit.com/r/LangChain/comments/1dod2o3/rag_system_on_3_different_csv_any_suggestions/" /><updated>2024-06-25T18:29:40+00:00</updated><published>2024-06-25T18:29:40+00:00</published><title>RAG system on 3 different CSV. Any suggestions?</title></entry><entry><author><name>/u/Present_Owl742</name><uri>https://www.reddit.com/user/Present_Owl742</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;hi&lt;/p&gt; &lt;p&gt;so, I&amp;#39;ve just started this week a mini project using AWS Bedrock, Claude Haiku and Titan embedding model. It&amp;#39;s more for me to understand how things work together so no production expectations - I want to add some documentation in the knowledge base and then ask questions and be provided with a full list of steps. The steps are split in different documents and use different components of the software, somebody with 5+ years of experience would be aware of this and this is what the system prompt that I use explains. &lt;/p&gt; &lt;p&gt;I&amp;#39;ve reached the step where I ask the question in the Test Knowledge Base area and it just outputs the like for like extract from 1 single document. I tried to play with the temperature and top-p of the model but it&amp;#39;s the same answer. &lt;/p&gt; &lt;p&gt;My question is: has anyone else experience something similar and how did they improve the answers? Is it through a lot of prompting/ changing the chunking strategy or moving to using APIs and langchain directly? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Present_Owl742&quot;&gt; /u/Present_Owl742 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1do2odx/aws_bedrock_building_llm_using_a_knowledge_base/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1do2odx/aws_bedrock_building_llm_using_a_knowledge_base/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1do2odx</id><link href="https://www.reddit.com/r/LangChain/comments/1do2odx/aws_bedrock_building_llm_using_a_knowledge_base/" /><updated>2024-06-25T10:23:39+00:00</updated><published>2024-06-25T10:23:39+00:00</published><title>AWS Bedrock - Building LLM using a knowledge base</title></entry><entry><author><name>/u/oldyoungin</name><uri>https://www.reddit.com/user/oldyoungin</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I am trying to implement a Sentence Window Retrieval method in Langchain. The best way i can figure out how to do it is to perform semantic similarity and return a chunk_id along with the page_content. I then want to retrieve documents from the vector database that correspond to chunk_id - 1 and chunk_id + 1. how can i query the vector database based on a specific value, rather than a similarity search? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/oldyoungin&quot;&gt; /u/oldyoungin &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dob8va/how_can_i_search_for_a_specific_value_in_a_column/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dob8va/how_can_i_search_for_a_specific_value_in_a_column/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dob8va</id><link href="https://www.reddit.com/r/LangChain/comments/1dob8va/how_can_i_search_for_a_specific_value_in_a_column/" /><updated>2024-06-25T17:12:49+00:00</updated><published>2024-06-25T17:12:49+00:00</published><title>How can I search for a specific value in a column in a vector database without performing a semantic similarity search?</title></entry><entry><author><name>/u/The404Dude</name><uri>https://www.reddit.com/user/The404Dude</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I have limited experience with LangChain and LLMs, primarily building simple chatbots with Retrieval-Augmented Generation (RAG). Currently, I&amp;#39;m helping a friend build a WhatsApp chatbot that retrieves its answers from a SQL database. I&amp;#39;ve been experimenting with the SQL tutorials in LangChain, but I haven&amp;#39;t yet achieved satisfactory results for a v1. I&amp;#39;ve tried using &lt;code&gt;create_sql_query_chain&lt;/code&gt; and &lt;code&gt;create_sql_agent&lt;/code&gt;. &lt;code&gt;create_sql_query_chain&lt;/code&gt; has been very inconsistent, while &lt;code&gt;create_sql_agent&lt;/code&gt; has produced better results but still struggles with the following issues:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Ensuring the LLM restricts queries to a specific &lt;code&gt;customer_id&lt;/code&gt; obtained from the context.&lt;/li&gt; &lt;li&gt;Preventing the LLM from answering questions that are too open, generic, or related to restricted device types (the user should only be allowed to ask about device type A).&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Given these challenges, I&amp;#39;m looking for advice, ideas, and any relevant experiences you might have. Thank you&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/The404Dude&quot;&gt; /u/The404Dude &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1do5emr/chatbot_with_sql/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1do5emr/chatbot_with_sql/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1do5emr</id><link href="https://www.reddit.com/r/LangChain/comments/1do5emr/chatbot_with_sql/" /><updated>2024-06-25T12:59:35+00:00</updated><published>2024-06-25T12:59:35+00:00</published><title>Chatbot with SQL</title></entry><entry><author><name>/u/WillowHefty</name><uri>https://www.reddit.com/user/WillowHefty</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Title says it all. PostgresSaver was deprecated and removed from langchain-postgres. Are there any plans on updating this or should we just stick to sqlite?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/WillowHefty&quot;&gt; /u/WillowHefty &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1do889i/any_news_on_when_langgraph_checkpointing_returns/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1do889i/any_news_on_when_langgraph_checkpointing_returns/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1do889i</id><link href="https://www.reddit.com/r/LangChain/comments/1do889i/any_news_on_when_langgraph_checkpointing_returns/" /><updated>2024-06-25T15:05:39+00:00</updated><published>2024-06-25T15:05:39+00:00</published><title>Any news on when langgraph checkpointing returns to langchain-postgres?</title></entry><entry><author><name>/u/Strange-Ant-4194</name><uri>https://www.reddit.com/user/Strange-Ant-4194</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Basically I&amp;#39;ve built one use case on linux desktop to simulate an AI personal assistant on the mobile phone which can automate various actions. My use case is basically that when an incoming call comes, the personal assistant determines the identity of the caller using the contact list and whether or not the user is in a meeting by checking the calendar. (Since i havent done anything on the phone, calendar contact list etc. are implemented as simple text files). Then depending on the above two factors, it either performs receive_call, send_message or mute. &lt;/p&gt; &lt;p&gt;So the final output is just one line stating the name of the action to be taken(ie send message, receive call or mute) &lt;/p&gt; &lt;p&gt;Is there any way i can display this on a mobile simulator?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Strange-Ant-4194&quot;&gt; /u/Strange-Ant-4194 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1do4qmi/is_there_any_way_i_can_integrate_ollama_langchain/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1do4qmi/is_there_any_way_i_can_integrate_ollama_langchain/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1do4qmi</id><link href="https://www.reddit.com/r/LangChain/comments/1do4qmi/is_there_any_way_i_can_integrate_ollama_langchain/" /><updated>2024-06-25T12:26:05+00:00</updated><published>2024-06-25T12:26:05+00:00</published><title>Is there any way i can integrate ollama, langchain and llama3 into a mobile simulator?</title></entry><entry><author><name>/u/harshit_nariya</name><uri>https://www.reddit.com/user/harshit_nariya</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/harshit_nariya&quot;&gt; /u/harshit_nariya &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;/r/AnyBodyCanAI/comments/1do4eon/zero_shot_prompting_vs_few_shot_prompting/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1do4g5l/zero_shot_prompting_vs_few_shot_prompting/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1do4g5l</id><link href="https://www.reddit.com/r/LangChain/comments/1do4g5l/zero_shot_prompting_vs_few_shot_prompting/" /><updated>2024-06-25T12:10:23+00:00</updated><published>2024-06-25T12:10:23+00:00</published><title>Zero shot prompting vs few shot prompting</title></entry><entry><author><name>/u/cosmic_predator</name><uri>https://www.reddit.com/user/cosmic_predator</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hello,&lt;/p&gt; &lt;p&gt;I&amp;#39;m using WebBaseLoader to extract web contents from a list of 70k urls. Everytime I run it, it runs initially well and after some point, the site refuses to connect. I&amp;#39;m currently setting requests_per_second to 50.&lt;/p&gt; &lt;p&gt;Is there any way I can make it faster without hitting limits?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/cosmic_predator&quot;&gt; /u/cosmic_predator &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1do409j/effieicent_way_to_do_webbaseloader_for_a_list_of/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1do409j/effieicent_way_to_do_webbaseloader_for_a_list_of/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1do409j</id><link href="https://www.reddit.com/r/LangChain/comments/1do409j/effieicent_way_to_do_webbaseloader_for_a_list_of/" /><updated>2024-06-25T11:45:48+00:00</updated><published>2024-06-25T11:45:48+00:00</published><title>Effieicent way to do WebBaseLoader for a list of 70k urls</title></entry><entry><author><name>/u/Informal-Victory8655</name><uri>https://www.reddit.com/user/Informal-Victory8655</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1do0d7y/unable_to_delete_vectors_from_serverless_pinecone/&quot;&gt; &lt;img src=&quot;https://external-preview.redd.it/XwBFGcZE3ASay1j0fNTmfW6lJmBX9Z__MYtad6ExHAg.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=85a994ab1b17700deba8a70a35ab9b6ea5f350b8&quot; alt=&quot;Unable to delete vectors from Serverless Pinecone for specific source files (like a.pdf)&quot; title=&quot;Unable to delete vectors from Serverless Pinecone for specific source files (like a.pdf)&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi, I&amp;#39;m unable to delete vectors for a specific source using metadata filtering with serverless pinecone as this feature is not available with serverless pinecone. And the other architecture of pinecone is not free, so I&amp;#39;m using serverless for development purposes. &lt;/p&gt; &lt;p&gt;A solution is recommended which is &lt;a href=&quot;https://docs.pinecone.io/guides/data/manage-rag-documents#delete-all-records-for-a-parent-document&quot;&gt;https://docs.pinecone.io/guides/data/manage-rag-documents#delete-all-records-for-a-parent-document&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;But this solution is not useful as I cannot pass specific metadata for deletion of vectors, instead it asks for a prefix in ids of vector. Now I don&amp;#39;t know how to make it work for my use case. I&amp;#39;ve also tried adding custom ids for vectors using name of source file + str(uuid) for each chunk and created a list which I&amp;#39;m trying to pass to pinecone.from_documents but it doesn&amp;#39;t support setting custom ids for vectors.&lt;/p&gt; &lt;p&gt;Now I also don&amp;#39;t know how they set the ids as shown in example like &amp;#39;doc1#chunk1&amp;#39; etc.... &lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/34yirw1t8o8d1.png?width=1844&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c4ecf51d149179459ed2042f1694d5fee7ec462b&quot;&gt;https://preview.redd.it/34yirw1t8o8d1.png?width=1844&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c4ecf51d149179459ed2042f1694d5fee7ec462b&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Informal-Victory8655&quot;&gt; /u/Informal-Victory8655 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1do0d7y/unable_to_delete_vectors_from_serverless_pinecone/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1do0d7y/unable_to_delete_vectors_from_serverless_pinecone/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1do0d7y</id><media:thumbnail url="https://external-preview.redd.it/XwBFGcZE3ASay1j0fNTmfW6lJmBX9Z__MYtad6ExHAg.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=85a994ab1b17700deba8a70a35ab9b6ea5f350b8" /><link href="https://www.reddit.com/r/LangChain/comments/1do0d7y/unable_to_delete_vectors_from_serverless_pinecone/" /><updated>2024-06-25T07:40:43+00:00</updated><published>2024-06-25T07:40:43+00:00</published><title>Unable to delete vectors from Serverless Pinecone for specific source files (like a.pdf)</title></entry><entry><author><name>/u/emersounds</name><uri>https://www.reddit.com/user/emersounds</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I&amp;#39;m currently working on a project involving LangGraph and Gemini Pro 1.5 (Vertex AI).&lt;/p&gt; &lt;p&gt;Gemini is not returning multiple function calls for parallel execution. I need to make several tool calls (e.g., for different months). However, it processes these calls sequentially, one at a time, which significantly increases the response time and costs due to token usage. For instance, if I need data for four months, it will:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Make the first tool call&lt;/li&gt; &lt;li&gt;Wait for the response&lt;/li&gt; &lt;li&gt;Make the second tool call&lt;/li&gt; &lt;li&gt;Wait for the response&lt;/li&gt; &lt;li&gt;And so on...&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Previously, I had implemented parallel function calling with LangChain&amp;#39;s &lt;code&gt;AgentExecutor&lt;/code&gt; class, where the agent would make all the necessary calls simultaneously, wait for all responses, and then process them together. This parallel calling mechanism has stopped working with LangGraph. Maybe because now the prompt is much larger than before?&lt;/p&gt; &lt;p&gt;Any tips or advice?&lt;/p&gt; &lt;p&gt;Thanks in advance for your help!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/emersounds&quot;&gt; /u/emersounds &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dnmeuy/help_parallel_function_calling_with_langgraph_and/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dnmeuy/help_parallel_function_calling_with_langgraph_and/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dnmeuy</id><link href="https://www.reddit.com/r/LangChain/comments/1dnmeuy/help_parallel_function_calling_with_langgraph_and/" /><updated>2024-06-24T19:43:01+00:00</updated><published>2024-06-24T19:43:01+00:00</published><title>[HELP] Parallel function calling with Langgraph and Gemini 1.5 Pro</title></entry><entry><author><name>/u/fantasyleaguelottery</name><uri>https://www.reddit.com/user/fantasyleaguelottery</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Has anyone implemented a working streamlit app with LangGraph? I am having issues with state management between the two. If anyone has implemented the LangGraph tutorial chatbot in streamlit, please let me know&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/fantasyleaguelottery&quot;&gt; /u/fantasyleaguelottery &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dngwkn/langgraph_streamlit_state_management/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dngwkn/langgraph_streamlit_state_management/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dngwkn</id><link href="https://www.reddit.com/r/LangChain/comments/1dngwkn/langgraph_streamlit_state_management/" /><updated>2024-06-24T15:55:45+00:00</updated><published>2024-06-24T15:55:45+00:00</published><title>LangGraph + Streamlit State Management</title></entry></feed>