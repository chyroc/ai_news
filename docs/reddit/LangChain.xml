<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/"><category term="LangChain" label="r/LangChain"/><updated>2024-05-12T07:38:53+00:00</updated><icon>https://www.redditstatic.com/icon.png/</icon><id>/r/LangChain.rss</id><link rel="self" href="https://www.reddit.com/r/LangChain.rss" type="application/atom+xml" /><link rel="alternate" href="https://www.reddit.com/r/LangChain" type="text/html" /><subtitle>LangChain is an open-source framework and developer toolkit that helps developers get LLM applications from prototype to production. It is available for Python and Javascript at https://www.langchain.com/.</subtitle><title>LangChain</title><entry><author><name>/u/zchaarm</name><uri>https://www.reddit.com/user/zchaarm</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;A place for members of &lt;a href=&quot;/r/LangChain&quot;&gt;r/LangChain&lt;/a&gt; to chat with each other&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/zchaarm&quot;&gt; /u/zchaarm &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_10ljho9</id><link href="https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/" /><updated>2023-01-26T04:33:05+00:00</updated><published>2023-01-26T04:33:05+00:00</published><title>r/LangChain Lounge</title></entry><entry><author><name>/u/Honest-Worth3677</name><uri>https://www.reddit.com/user/Honest-Worth3677</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cq0lim/hugging_face_langchain_upwork_how_to_solve_real/&quot;&gt; &lt;img src=&quot;https://external-preview.redd.it/KB4ehp_t6BKwbj7Wv4cBOf_o6gTw9Y8Qo_eyjFZ_EIU.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d487ddc85293aceb7a5da849fae6075c0fd444bc&quot; alt=&quot;Hugging Face + Langchain+ Upwork | How to Solve Real World AI Job.&quot; title=&quot;Hugging Face + Langchain+ Upwork | How to Solve Real World AI Job.&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Honest-Worth3677&quot;&gt; /u/Honest-Worth3677 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=0G3rqJhcMwA&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cq0lim/hugging_face_langchain_upwork_how_to_solve_real/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1cq0lim</id><media:thumbnail url="https://external-preview.redd.it/KB4ehp_t6BKwbj7Wv4cBOf_o6gTw9Y8Qo_eyjFZ_EIU.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=d487ddc85293aceb7a5da849fae6075c0fd444bc" /><link href="https://www.reddit.com/r/LangChain/comments/1cq0lim/hugging_face_langchain_upwork_how_to_solve_real/" /><updated>2024-05-12T05:30:06+00:00</updated><published>2024-05-12T05:30:06+00:00</published><title>Hugging Face + Langchain+ Upwork | How to Solve Real World AI Job.</title></entry><entry><author><name>/u/dipta10</name><uri>https://www.reddit.com/user/dipta10</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I was following the &lt;a href=&quot;https://python.langchain.com/v0.1/docs/use_cases/sql/agents/&quot;&gt;LangChain SQL Documentation&lt;/a&gt;. So far I&amp;#39;m using Few shot prompt template as the prompt for the agent.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;agent = create_sql_agent( llm=self.llm, db=self.db, prompt=self.few_shot_prompt, verbose=True, agent_type=&amp;quot;openai-tools&amp;quot;, top_k=10, ) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Now what I want to do is&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Create a chain that creates a custom prompt based on user question&lt;/li&gt; &lt;li&gt;Then pass that custom prompt created by the chain to the agent to use when I invoke it&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;I&amp;#39;ve created the chain like this&lt;/p&gt; &lt;pre&gt;&lt;code&gt;rg = RunnableParallel( { &amp;quot;input&amp;quot;: lambda x: x[&amp;quot;input&amp;quot;], &amp;quot;dialect&amp;quot;: lambda x: x[&amp;quot;dialect&amp;quot;], &amp;quot;top_k&amp;quot;: lambda x: x[&amp;quot;top_k&amp;quot;], &amp;quot;tables&amp;quot;: self.table_extraction_chain, } ).assign(prompt=self.few_shot_prompt) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;can I pipe in the Agent here somehow to use the custom prompt?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/dipta10&quot;&gt; /u/dipta10 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cpsg8f/is_it_possible_to_change_agent_prompt_based_on/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cpsg8f/is_it_possible_to_change_agent_prompt_based_on/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cpsg8f</id><link href="https://www.reddit.com/r/LangChain/comments/1cpsg8f/is_it_possible_to_change_agent_prompt_based_on/" /><updated>2024-05-11T22:04:46+00:00</updated><published>2024-05-11T22:04:46+00:00</published><title>Is it possible to change agent prompt based on user question after the agent is already created?</title></entry><entry><author><name>/u/Monkey_D_Uzumaki7</name><uri>https://www.reddit.com/user/Monkey_D_Uzumaki7</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hello, I am a beginner in LLM and LangChain, and I am developing a small application that allows me to query PDF documents with my OpenAI API. Everything works well so far with the PDFs. I am able to query the PDFs. If the question is out of context, it shows that the information is not in the PDF, otherwise, it displays the information. So, everything is going well at the moment, but the problem is that with documents that are a bit longer, 100 pages or more, I really have problems because I can&amp;#39;t even load them to query them. So, what should I do?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Monkey_D_Uzumaki7&quot;&gt; /u/Monkey_D_Uzumaki7 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cps3s5/problem_with_heavy_documents/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cps3s5/problem_with_heavy_documents/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cps3s5</id><link href="https://www.reddit.com/r/LangChain/comments/1cps3s5/problem_with_heavy_documents/" /><updated>2024-05-11T21:48:17+00:00</updated><published>2024-05-11T21:48:17+00:00</published><title>Problem with heavy documents</title></entry><entry><author><name>/u/urfath3r</name><uri>https://www.reddit.com/user/urfath3r</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Using the same prompt, GPT 3.5 seems more likely to use tools correctly all the time whereas Claude haiku needs better prompting to achieve tool calling? Even so, it misses out a good number of times. &lt;/p&gt; &lt;p&gt;For eg. I don’t even have to mention any tools in the gpt prompt but I have to mention in for haiku to even work. Putting the tool description and query works enough for gpt.&lt;/p&gt; &lt;p&gt;E.g a tool parameter for “rag vector query” correctly abstracts the query to keywords but haiku dumps the entire question in. GPT is also able to infer from history and use tools (eg, tell me more) while haiku just plain responds saying it has no more data. &lt;/p&gt; &lt;p&gt;Am I using anything wrong? Everyone is saying haiku is smarter and all.. main reason I am trying it out is that it is cheaper, potentially allowing for more documents to stuff in. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/urfath3r&quot;&gt; /u/urfath3r &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cpjgn1/gpt35_tool_calling_more_accurately_than_claude/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cpjgn1/gpt35_tool_calling_more_accurately_than_claude/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cpjgn1</id><link href="https://www.reddit.com/r/LangChain/comments/1cpjgn1/gpt35_tool_calling_more_accurately_than_claude/" /><updated>2024-05-11T15:03:08+00:00</updated><published>2024-05-11T15:03:08+00:00</published><title>GPT3.5 tool calling more accurately than Claude haiku.</title></entry><entry><author><name>/u/thanghaimeow</name><uri>https://www.reddit.com/user/thanghaimeow</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Good to be back making content for y’all.&lt;/p&gt; &lt;p&gt;Video: &lt;a href=&quot;https://www.youtube.com/watch?v=RkWor1BZOn0&quot;&gt;https://www.youtube.com/watch?v=RkWor1BZOn0&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Code: &lt;a href=&quot;https://colab.research.google.com/drive/18_L_OX4YPlAHdQCSMfRgjTE9aSOEl-6l?usp=sharing&quot;&gt;https://colab.research.google.com/drive/18_L_OX4YPlAHdQCSMfRgjTE9aSOEl-6l?usp=sharing&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Let me know in the comments what you liked / didn’t like about this and I’ll make it better next time.&lt;/p&gt; &lt;p&gt;or if you need clarifications, I‘m happy to answer to the best of my ability.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/thanghaimeow&quot;&gt; /u/thanghaimeow &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cp5q9p/code_included_i_did_a_workshop_on_longterm/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cp5q9p/code_included_i_did_a_workshop_on_longterm/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cp5q9p</id><link href="https://www.reddit.com/r/LangChain/comments/1cp5q9p/code_included_i_did_a_workshop_on_longterm/" /><updated>2024-05-11T01:22:37+00:00</updated><published>2024-05-11T01:22:37+00:00</published><title>(Code included) I did a workshop on long-term selective memory for LLM applications recently</title></entry><entry><author><name>/u/hwchase17</name><uri>https://www.reddit.com/user/hwchase17</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;hi all! we&amp;#39;re gearing up for a release of langchain 0.2. The main change is no longer depending on langchain-community (this will increase modularity, decrease size of package, make more secure). We&amp;#39;re also adding in a new docs structure and highlighting a bunch of the changes we made as part of 0.1&lt;/p&gt; &lt;p&gt;We posted more about this on GitHub (&lt;a href=&quot;https://github.com/langchain-ai/langchain/discussions/21437&quot;&gt;https://github.com/langchain-ai/langchain/discussions/21437&lt;/a&gt;) but happy to answer any questions here! Would obviously love and really appreciate any feedback :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/hwchase17&quot;&gt; /u/hwchase17 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1coyy48/langchain_02_prerelease/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1coyy48/langchain_02_prerelease/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1coyy48</id><link href="https://www.reddit.com/r/LangChain/comments/1coyy48/langchain_02_prerelease/" /><updated>2024-05-10T20:13:19+00:00</updated><published>2024-05-10T20:13:19+00:00</published><title>LangChain 0.2 prerelease</title></entry><entry><author><name>/u/mmkostov</name><uri>https://www.reddit.com/user/mmkostov</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I have a Next.js application that requires RAG (Specifically web search) with multiple open source models. Is there a way to get them to work with function/tool calling?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/mmkostov&quot;&gt; /u/mmkostov &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cpagwd/functiontool_calling_on_nonopenai_models/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cpagwd/functiontool_calling_on_nonopenai_models/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cpagwd</id><link href="https://www.reddit.com/r/LangChain/comments/1cpagwd/functiontool_calling_on_nonopenai_models/" /><updated>2024-05-11T05:56:25+00:00</updated><published>2024-05-11T05:56:25+00:00</published><title>Function/tool calling on non-OpenAI models?</title></entry><entry><author><name>/u/Exciting_Maximum_335</name><uri>https://www.reddit.com/user/Exciting_Maximum_335</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey there, Is there any way to generate a voice?&lt;br/&gt; I found some impressive tools out there, but wish they were integrated with langchain like this one: &lt;a href=&quot;https://github.com/jasonppy/VoiceCraft&quot;&gt;https://github.com/jasonppy/VoiceCraft&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Exciting_Maximum_335&quot;&gt; /u/Exciting_Maximum_335 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cpcwcg/voice_generation/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cpcwcg/voice_generation/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cpcwcg</id><link href="https://www.reddit.com/r/LangChain/comments/1cpcwcg/voice_generation/" /><updated>2024-05-11T08:37:38+00:00</updated><published>2024-05-11T08:37:38+00:00</published><title>Voice generation</title></entry><entry><author><name>/u/moonbunR</name><uri>https://www.reddit.com/user/moonbunR</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;&lt;a href=&quot;https://docs.cycls.com/getting-started&quot;&gt;Cycls&lt;/a&gt; Python library significantly simplifies the development of AI chatbots for developers, particularly those who specialize in backend development and may find creating user interfaces tiring. This library eliminates the need to develop a UI from scratch by providing a prebuilt, ChatGPTlike user interface that developers can use immediately. By simply importing Cycls into your project, you can integrate it with major LLM Python libraries and APIs, enabling the use of various LLM models easily.&lt;/p&gt; &lt;p&gt;Upon importing and running your application, it is automatically deployed with the user interface and a public URL, enhancing the efficiency of both development and testing processes. This feature is especially valuable for speeding up deployment and reducing the workload on developers focusing on backend functionalities.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/moonbunR&quot;&gt; /u/moonbunR &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cot1we/library_that_automatically_creates_a_ui_for_your/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cot1we/library_that_automatically_creates_a_ui_for_your/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cot1we</id><link href="https://www.reddit.com/r/LangChain/comments/1cot1we/library_that_automatically_creates_a_ui_for_your/" /><updated>2024-05-10T16:01:39+00:00</updated><published>2024-05-10T16:01:39+00:00</published><title>Library that automatically creates a UI for your chatbot?</title></entry><entry><author><name>/u/Minute_Scientist8107</name><uri>https://www.reddit.com/user/Minute_Scientist8107</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cp9gqj/agent_to_generate_charts_based_on_the_users_prompt/&quot;&gt; &lt;img src=&quot;https://b.thumbs.redditmedia.com/w6YCT1Y6VWdm7hpM7h3n1m598hhj19Pr3FJPvQJXIqU.jpg&quot; alt=&quot;Agent to generate Charts based on the user's prompt&quot; title=&quot;Agent to generate Charts based on the user's prompt&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;&lt;a href=&quot;https://preview.redd.it/4738d3i5aqzc1.png?width=723&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=89b493a6fc2271d489b0df2e6faddd3bf4a19f43&quot;&gt;https://preview.redd.it/4738d3i5aqzc1.png?width=723&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=89b493a6fc2271d489b0df2e6faddd3bf4a19f43&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hi Guys, I have a RAG for csv file, the data is about temperature sensors and apps usage percentage in a phone. I was able to use azure gpt 4 and generate a textual response. Since we have a csv file, I want the llm to also generate a graph along with the textual response. Please tell me how to code this up. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Minute_Scientist8107&quot;&gt; /u/Minute_Scientist8107 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cp9gqj/agent_to_generate_charts_based_on_the_users_prompt/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cp9gqj/agent_to_generate_charts_based_on_the_users_prompt/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1cp9gqj</id><media:thumbnail url="https://b.thumbs.redditmedia.com/w6YCT1Y6VWdm7hpM7h3n1m598hhj19Pr3FJPvQJXIqU.jpg" /><link href="https://www.reddit.com/r/LangChain/comments/1cp9gqj/agent_to_generate_charts_based_on_the_users_prompt/" /><updated>2024-05-11T04:51:54+00:00</updated><published>2024-05-11T04:51:54+00:00</published><title>Agent to generate Charts based on the user's prompt</title></entry><entry><author><name>/u/Flying_Doge123</name><uri>https://www.reddit.com/user/Flying_Doge123</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Flying_Doge123&quot;&gt; /u/Flying_Doge123 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;/r/OpenAI/comments/1cp0tzk/getting_answers_from_gpt_based_on_document/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cp4l08/getting_answers_from_gpt_based_on_document/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cp4l08</id><link href="https://www.reddit.com/r/LangChain/comments/1cp4l08/getting_answers_from_gpt_based_on_document/" /><updated>2024-05-11T00:23:23+00:00</updated><published>2024-05-11T00:23:23+00:00</published><title>Getting Answers from GPT based on document</title></entry><entry><author><name>/u/theferalmonkey</name><uri>https://www.reddit.com/user/theferalmonkey</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cp3y1l/whats_the_scoop_on_serialization_its_all_over_the/&quot;&gt; &lt;img src=&quot;https://a.thumbs.redditmedia.com/SK3wRBz6PKwgSOdCQ0nsJhIrSkiavJY9Ee44pLez-64.jpg&quot; alt=&quot;What's the scoop on serialization? It's all over the show with LangChain&quot; title=&quot;What's the scoop on serialization? It's all over the show with LangChain&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi am I doing something wrong?&lt;/p&gt; &lt;p&gt;I&amp;#39;m using Documents and I want to save/load them. I&amp;#39;ve tried, `.json()` the new `load` module&amp;#39;s `.dumpd()` and `.load()` -- but some documents can&amp;#39;t be deserialized.&lt;/p&gt; &lt;p&gt;e.g. I am trying to build something that uses LangChain documents. But not all of them are deserializable -- instead you get goop like this when using the output of `ContextualCompressionRetriever` (which returns some documents) and use one of the above functions to serialize:&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/74vlfp1wsozc1.png?width=417&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0d3d238376da4f9305eed45ee212ce873602526e&quot;&gt;https://preview.redd.it/74vlfp1wsozc1.png?width=417&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0d3d238376da4f9305eed45ee212ce873602526e&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Basically I have to parse the repr to get the document? Any pointers? Or write my own custom serialization/deserialization?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/theferalmonkey&quot;&gt; /u/theferalmonkey &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cp3y1l/whats_the_scoop_on_serialization_its_all_over_the/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cp3y1l/whats_the_scoop_on_serialization_its_all_over_the/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1cp3y1l</id><media:thumbnail url="https://a.thumbs.redditmedia.com/SK3wRBz6PKwgSOdCQ0nsJhIrSkiavJY9Ee44pLez-64.jpg" /><link href="https://www.reddit.com/r/LangChain/comments/1cp3y1l/whats_the_scoop_on_serialization_its_all_over_the/" /><updated>2024-05-10T23:52:13+00:00</updated><published>2024-05-10T23:52:13+00:00</published><title>What's the scoop on serialization? It's all over the show with LangChain</title></entry><entry><author><name>/u/Choice_Engineering73</name><uri>https://www.reddit.com/user/Choice_Engineering73</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1coz2io/how_to_get_llm_to_infer_dates_from_a_json_file_or/&quot;&gt; &lt;img src=&quot;https://b.thumbs.redditmedia.com/N_ewQPz50EtO5bC2hySBwspZZfVxNl3hUQqU_JHhggk.jpg&quot; alt=&quot;how to get LLM to infer dates from a json file or text file?&quot; title=&quot;how to get LLM to infer dates from a json file or text file?&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;im currently using mistral-small in my project, i want to create a chatbot that can give answers about the weather in the future, so i have a huge json file that i want to feed to the model as a vector embedding and then let the user ask the chatbot questions about future weather such as what should i wear tomorrow etc.&lt;/p&gt; &lt;p&gt;the problem is that the LLM seems to not read the files, i even tried to programatically rebuild the file as a text file.&lt;/p&gt; &lt;p&gt;this is my code:&lt;/p&gt; &lt;p&gt;```&lt;/p&gt; &lt;pre&gt;&lt;code&gt;json_loader = JSONLoader(&amp;quot;C:\\xxxx\\data\\all_weather_by_date.json&amp;quot;) # Split text into chunks text_splitter = RecursiveCharacterTextSplitter() documents = text_splitter.split_documents(docs) # Define the embedding model embeddings = MistralAIEmbeddings(model=&amp;quot;mistral-embed&amp;quot;, mistral_api_key=&amp;quot;XXXXXXXXXXXXXXXXXXXXXXX&amp;quot;, ) # Create the vector store vector = FAISS.from_documents(documents, embeddings) vector.save_local(&amp;#39;cache&amp;#39;) # Define a retriever interface retriever = vector.as_retriever() # Define LLM model = ChatMistralAI(mistral_api_key=&amp;quot;XXXXXXXXXXXXXXXXXXXXX&amp;quot;, model=&amp;#39;mistral-small&amp;#39;, temperature=1) # Define prompt template prompt = ChatPromptTemplate.from_template(&amp;quot;&amp;quot;&amp;quot; &amp;lt;context&amp;gt; {context} &amp;lt;/context&amp;gt; Question: {input}&amp;quot;&amp;quot;&amp;quot;) # Create a retrieval chain to answer questions document_chain = create_stuff_documents_chain(model, prompt) retrieval_chain = create_retrieval_chain(retriever, document_chain) response = retrieval_chain.invoke({&amp;#39;context&amp;#39;: instructions,&amp;quot;input&amp;quot;: f&amp;quot;answer max 60 words, end with follow-up question, no clauses: what should i wear tomorrow? today is 10.5.2024&amp;quot;}) print(response[&amp;quot;answer&amp;quot;]) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;```&lt;/p&gt; &lt;p&gt;my json is built like this:&lt;/p&gt; &lt;p&gt;{&lt;/p&gt; &lt;p&gt;&amp;quot;current_year_dates&amp;quot; : {&lt;/p&gt; &lt;p&gt;&amp;quot;2024.05.11&amp;quot; : {&lt;/p&gt; &lt;p&gt;&amp;quot;humidity&amp;quot;: &amp;quot;50%&amp;quot;,&lt;/p&gt; &lt;p&gt;&amp;quot;degrees&amp;quot;: &amp;quot;34&amp;quot;&lt;/p&gt; &lt;p&gt;}&lt;/p&gt; &lt;p&gt;.... all the other dates throught the year&lt;/p&gt; &lt;p&gt;}&lt;/p&gt; &lt;p&gt;when i attempted to change the json file to text file, i changed it to this format:&lt;/p&gt; &lt;p&gt;&amp;quot;on 2024.10.5 there will be 50% humidity and 30 degress.&amp;quot;&lt;/p&gt; &lt;p&gt;and still it didnt work&lt;/p&gt; &lt;p&gt;also i have always get a warning when running my code about HF_TOKEN not being set and using len splitter.&lt;/p&gt; &lt;p&gt;please help me!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Choice_Engineering73&quot;&gt; /u/Choice_Engineering73 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1coz2io/how_to_get_llm_to_infer_dates_from_a_json_file_or/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1coz2io/how_to_get_llm_to_infer_dates_from_a_json_file_or/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1coz2io</id><media:thumbnail url="https://b.thumbs.redditmedia.com/N_ewQPz50EtO5bC2hySBwspZZfVxNl3hUQqU_JHhggk.jpg" /><link href="https://www.reddit.com/r/LangChain/comments/1coz2io/how_to_get_llm_to_infer_dates_from_a_json_file_or/" /><updated>2024-05-10T20:18:45+00:00</updated><published>2024-05-10T20:18:45+00:00</published><title>how to get LLM to infer dates from a json file or text file?</title></entry><entry><author><name>/u/echopurpose</name><uri>https://www.reddit.com/user/echopurpose</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Would LC be a good &amp;lt;framework&amp;gt; for building a LLM-based chat which builds user profiles?&lt;/p&gt; &lt;p&gt;For example, over the course of the conversation it could fill slots such as name, location, etc. &lt;/p&gt; &lt;p&gt;or there an off the shelf tool which is already doing this? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/echopurpose&quot;&gt; /u/echopurpose &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cooqdx/would_lc_be_a_good_platform_for_building_a/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cooqdx/would_lc_be_a_good_platform_for_building_a/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cooqdx</id><link href="https://www.reddit.com/r/LangChain/comments/1cooqdx/would_lc_be_a_good_platform_for_building_a/" /><updated>2024-05-10T12:47:06+00:00</updated><published>2024-05-10T12:47:06+00:00</published><title>Would LC be a good platform for building a LLM-based chat which builds user profiles?</title></entry><entry><author><name>/u/mahadevbhakti</name><uri>https://www.reddit.com/user/mahadevbhakti</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Tell me if this idea is feasible and how I can pull this off&lt;/p&gt; &lt;p&gt;I have a langchain agent that does function calling, but one shortcoming is that it fails to answer queries from the pulled data many times, &lt;/p&gt; &lt;p&gt;Can I store this pulled data into a knowledge graph vector database as this data is for holiday packages with key value relations such as duration, price, location, ref_id etc and sub categories like fare sets, cabin sub categories etc. &lt;/p&gt; &lt;p&gt;How can I make my langchain agent in python better using knowledge graphs and making it answer follow up questions using RAG on the last fetched data?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/mahadevbhakti&quot;&gt; /u/mahadevbhakti &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cokru8/function_calling_rag_langchain_tool_calling_agent/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cokru8/function_calling_rag_langchain_tool_calling_agent/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cokru8</id><link href="https://www.reddit.com/r/LangChain/comments/1cokru8/function_calling_rag_langchain_tool_calling_agent/" /><updated>2024-05-10T08:41:18+00:00</updated><published>2024-05-10T08:41:18+00:00</published><title>Function Calling + RAG + Langchain Tool Calling Agent + REDIS Memory</title></entry><entry><author><name>/u/Sensitive-Pen-1229</name><uri>https://www.reddit.com/user/Sensitive-Pen-1229</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;hey guys&lt;/p&gt; &lt;p&gt;I use correctness, hallucination &amp;amp; relevance score to evaluate a RAG chain on langsmith. &lt;/p&gt; &lt;p&gt;Generally, do you think 3.5 is good enough to evaluate the outcomes?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Sensitive-Pen-1229&quot;&gt; /u/Sensitive-Pen-1229 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1coj65b/evaluation_models_gpt35_vs_gpt_4_turbo/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1coj65b/evaluation_models_gpt35_vs_gpt_4_turbo/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1coj65b</id><link href="https://www.reddit.com/r/LangChain/comments/1coj65b/evaluation_models_gpt35_vs_gpt_4_turbo/" /><updated>2024-05-10T06:47:09+00:00</updated><published>2024-05-10T06:47:09+00:00</published><title>Evaluation Models - GPT-3.5 vs. GPT 4 Turbo</title></entry><entry><author><name>/u/kedu16</name><uri>https://www.reddit.com/user/kedu16</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi all, in the previous version of code, I had something like this:&lt;/p&gt; &lt;p&gt;qa = RetrievalQA.from_chain_type(&lt;br/&gt; llm=llm,&lt;br/&gt; chain_type_kwargs={&amp;quot;prompt&amp;quot;: prompt},&lt;br/&gt; retriever=retriever,&lt;br/&gt; return_source_documents=True, &lt;br/&gt; tags = tags,&lt;br/&gt; metadata = metadata &lt;br/&gt; )&lt;br/&gt; basically its a Multivector retriever with RetrievalQA. In LCEL chain it is like below:&lt;/p&gt; &lt;p&gt;chain = (&lt;br/&gt; {&amp;quot;context&amp;quot;: retriever, &amp;quot;question&amp;quot;: RunnablePassthrough()}&lt;br/&gt; | prompt&lt;br/&gt; | model&lt;br/&gt; | StrOutputParser()&lt;br/&gt; )&lt;br/&gt; The question I have is, how to add additional parameters in the above LCEL chain?&lt;br/&gt; additional parameters:&lt;br/&gt; return_source_documents=True, &lt;br/&gt; tags = tags,&lt;br/&gt; metadata = metadata &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/kedu16&quot;&gt; /u/kedu16 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1colp4k/just_a_question_on_lcel_vs_retrievalqa/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1colp4k/just_a_question_on_lcel_vs_retrievalqa/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1colp4k</id><link href="https://www.reddit.com/r/LangChain/comments/1colp4k/just_a_question_on_lcel_vs_retrievalqa/" /><updated>2024-05-10T09:48:22+00:00</updated><published>2024-05-10T09:48:22+00:00</published><title>Just a question on LCEL vs RetrievalQA</title></entry><entry><author><name>/u/Far_Possibility_6278</name><uri>https://www.reddit.com/user/Far_Possibility_6278</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Has anyone tried phi3 for text2sql in postgres? I am trying and can&amp;#39;t generate a correct query&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Far_Possibility_6278&quot;&gt; /u/Far_Possibility_6278 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1colhlx/phi3_text2sql/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1colhlx/phi3_text2sql/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1colhlx</id><link href="https://www.reddit.com/r/LangChain/comments/1colhlx/phi3_text2sql/" /><updated>2024-05-10T09:33:12+00:00</updated><published>2024-05-10T09:33:12+00:00</published><title>Phi3 text2sql</title></entry><entry><author><name>/u/Just_Guide7361</name><uri>https://www.reddit.com/user/Just_Guide7361</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey,&lt;/p&gt; &lt;p&gt;Like many others I am working on a RAG Q&amp;amp;A Chatbot. I have many pdfs related to a certain topic. My goal is to help the end-user answer questions based on the information in the pdfs. From the perspective of GIGO I am first evaluation how well I can retrieve information related to certain questions. However, I am not certain about my approach.&lt;/p&gt; &lt;p&gt;I am using precision at k, recall at k and ndcg at k as my evaluation metrics; next I simply create various pipelines starting from the raw pdfs, to embedding to retrieval. Here I vary the following element in the pipeline:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Chunking:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;CharacterTextSplitter vs RecursiveTextSplitter&lt;/li&gt; &lt;li&gt;Chuck size (400, 800, 1200)&lt;/li&gt; &lt;li&gt;Chuck overlap (200, 400, 600)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Embedding:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Model (ie. OpenAI vs Cohore)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Retrieval:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Search method: mmr vs similarity&lt;/li&gt; &lt;li&gt;Number of Documents to return [k] (4, 7)&lt;/li&gt; &lt;li&gt;Number of Documents to fetch to pass to MMR algorithm [fetch-k] (20, 40)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Retrieval is tested as follows; at the chunking stage I sample ~10% of the chucks and pass that to an LLM to generate questions. These are then used to test how well the pipeline can retrieve the chucks related to the questions.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Now my problems:&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;I consider a single pdf, one document. This document is then chucked, but the document ID is the same for each chuck. Meaning that if a question related to chuck A returns chuck Z; this would be counted as correct. While this would obviously not be correct; I&amp;#39;ve considered changing the relation from a single page in a pdf being considered one document as this is likely more accurate.&lt;/li&gt; &lt;li&gt;The evaluation questions generated vary with the chuck size and method; as the given input to the LLM will be different. So while testing the pipeline, this is strongly affected by the quality of questions the LLM came up with.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Are there people here with experience in testing, or does anyone know any good resources?&lt;/p&gt; &lt;p&gt;Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Just_Guide7361&quot;&gt; /u/Just_Guide7361 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1co5cl1/rag_retrieval_evaluation/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1co5cl1/rag_retrieval_evaluation/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1co5cl1</id><link href="https://www.reddit.com/r/LangChain/comments/1co5cl1/rag_retrieval_evaluation/" /><updated>2024-05-09T19:14:56+00:00</updated><published>2024-05-09T19:14:56+00:00</published><title>RAG Retrieval Evaluation</title></entry><entry><author><name>/u/Informal-Victory8655</name><uri>https://www.reddit.com/user/Informal-Victory8655</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi, guys. I&amp;#39;ve made a langchain chatbot agent and I want to deploy it as a simple flask app.&lt;br/&gt; I&amp;#39;m not sure, how the unique conversation sessions would be managed like a single endpoint would be used to invoke the agent but how the flask would ensure that this request belong to a specific user interacting with chatbot and we know there could be multiple users interacting with chatbot at same time. &lt;/p&gt; &lt;p&gt;So I wanna learn how to manage these kind of sessions and using credentials is not an option.&lt;/p&gt; &lt;p&gt;one more thing, how the agent memory would be specified per user session in deployment?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Informal-Victory8655&quot;&gt; /u/Informal-Victory8655 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cokd2v/how_to_deploy_langchain_chatbot_agent_using_flask/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cokd2v/how_to_deploy_langchain_chatbot_agent_using_flask/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cokd2v</id><link href="https://www.reddit.com/r/LangChain/comments/1cokd2v/how_to_deploy_langchain_chatbot_agent_using_flask/" /><updated>2024-05-10T08:11:40+00:00</updated><published>2024-05-10T08:11:40+00:00</published><title>How to deploy langchain chatbot (agent) using flask api and identify and manage unique user conversation sessions?</title></entry><entry><author><name>/u/deixhah</name><uri>https://www.reddit.com/user/deixhah</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi guys, I&amp;#39;m working on a chatbot for our company and we use ChatGPT 4 as an LLM. To keep the costs per request low, I implemented a chat history with only the last 6 messages (3 from user, 3 from bot)&lt;/p&gt; &lt;p&gt;The problem now is, that sometimes people have inputs that the bot can&amp;#39;t answer well, as the messages are already gone in his memory.&lt;/p&gt; &lt;p&gt;Is there any good way to fix this? Is it possible to use vectorsearch for memory? Or does this heavily increase the latency?&lt;/p&gt; &lt;p&gt;What is the best way to go, what are you guys using?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/deixhah&quot;&gt; /u/deixhah &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1coaa4f/vectorsearch_for_chat_history/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1coaa4f/vectorsearch_for_chat_history/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1coaa4f</id><link href="https://www.reddit.com/r/LangChain/comments/1coaa4f/vectorsearch_for_chat_history/" /><updated>2024-05-09T22:43:56+00:00</updated><published>2024-05-09T22:43:56+00:00</published><title>Vectorsearch for chat history?</title></entry><entry><author><name>/u/natheeshkumar</name><uri>https://www.reddit.com/user/natheeshkumar</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;This my code&lt;/p&gt; &lt;pre&gt;&lt;code&gt;conversational_rag_chain = RunnableWithMessageHistory( rag_chain, get_session_history, input_messages_key=&amp;quot;input&amp;quot;, history_messages_key=&amp;quot;chat_history&amp;quot;, output_messages_key=&amp;quot;answer&amp;quot;, ) conversational_rag_chain.invoke( {&amp;quot;input&amp;quot;: &amp;quot;I am interested in Screwdrivers, Sockets, Ratchets. Now generate new questions based on past behaviour&amp;quot;}, config={ &amp;quot;configurable&amp;quot;: {&amp;quot;session_id&amp;quot;: &amp;quot;abc123&amp;quot;} }, # constructs a key &amp;quot;abc123&amp;quot; in `store`. )[&amp;quot;answer&amp;quot;] &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;I want to add this `JsonOutputParser` to above `conversational_rag_chain`&lt;/p&gt; &lt;pre&gt;&lt;code&gt;class ProbingQuestion(BaseModel): Question: str = Field(description=&amp;quot;probing question&amp;quot;) Options: List[str] = Field(description=&amp;quot;list of options for the probing question&amp;quot;) output_parser = JsonOutputParser(pydantic_object=ProbingQuestion) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;So can anyone help this?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/natheeshkumar&quot;&gt; /u/natheeshkumar &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cofbkc/how_to_add_jsonoutputparser_with/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cofbkc/how_to_add_jsonoutputparser_with/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cofbkc</id><link href="https://www.reddit.com/r/LangChain/comments/1cofbkc/how_to_add_jsonoutputparser_with/" /><updated>2024-05-10T02:52:33+00:00</updated><published>2024-05-10T02:52:33+00:00</published><title>How to add JsonOutputParser with RunnableWithMessageHistory?</title></entry><entry><author><name>/u/easy_breeze5634</name><uri>https://www.reddit.com/user/easy_breeze5634</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I have a Neo4j graph that&amp;#39;s populated with common category nodes, and sub-nodes. These all have labels and names, other than that there is no data stored inside the properties. I want to find a way to identify related headers for that particular category node, for example &amp;quot;economic&amp;quot;, if a column in one of the CSV files is related to economics then it should be either be added as a new node to that category node or the data inside that cell from the csv be added as a property to an existing node. I&amp;#39;m wondering what everyone also thinks is the best way to set all this up as I am trying to make this Graph as efficient as possible because the plan is to add an LLM on top of it with a chatbot to query to graph for context and return answers. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/easy_breeze5634&quot;&gt; /u/easy_breeze5634 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1code3k/iterating_hundreds_of_csv_file_headers_trying_to/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1code3k/iterating_hundreds_of_csv_file_headers_trying_to/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1code3k</id><link href="https://www.reddit.com/r/LangChain/comments/1code3k/iterating_hundreds_of_csv_file_headers_trying_to/" /><updated>2024-05-10T01:11:26+00:00</updated><published>2024-05-10T01:11:26+00:00</published><title>Iterating hundreds of csv file headers, trying to find the best way to identify related headers using NLP or other technique to add data from columns as properties in Graph nodes.</title></entry><entry><author><name>/u/SensitiveStudy520</name><uri>https://www.reddit.com/user/SensitiveStudy520</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cod9zr/langchain_sql_mistral/&quot;&gt; &lt;img src=&quot;https://external-preview.redd.it/8ABZDWd9r3Ixo9grEHVIAHOHyBW0tLcEXujBylBi4Js.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d969b12e1cfdb941b967d24fef27d9ea4d81da9d&quot; alt=&quot;LangChain SQL &amp;amp; Mistral&quot; title=&quot;LangChain SQL &amp;amp; Mistral&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi guys,&lt;/p&gt; &lt;p&gt;Recently I am experimenting on using mistral 7b instruct-v0.1 with sql database, and I have try 3 approach, but all of these give me several errors and issues, and I really cannot fix it.&lt;br/&gt; Approach 1 (source=&lt;a href=&quot;https://medium.com/@shivansh.kaushik/talk-to-your-database-using-rag-and-llms-42eb852d2a3c&quot;&gt;Tal to your database using RAG and LLM&lt;/a&gt;)&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/hr1yz5w30izc1.png?width=767&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2de4ee09e1b184434f3c6baf0a7040000987dbed&quot;&gt;https://preview.redd.it/hr1yz5w30izc1.png?width=767&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2de4ee09e1b184434f3c6baf0a7040000987dbed&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/uyas7shgjdzc1.png?width=953&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=61b093e5253e193c1f02d69ba6890566a25858a9&quot;&gt;https://preview.redd.it/uyas7shgjdzc1.png?width=953&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=61b093e5253e193c1f02d69ba6890566a25858a9&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This give me the error as above, but i thought the question is already a string? And it&amp;#39;s so weird to give me 2 SQL Result and answer. &lt;/p&gt; &lt;p&gt;Approach 2 (source is from a youtube)&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/n4vj9z2e0izc1.png?width=930&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3d9ea5d0b76e5ba224f1a55e2abb7b5f130de8d0&quot;&gt;https://preview.redd.it/n4vj9z2e0izc1.png?width=930&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3d9ea5d0b76e5ba224f1a55e2abb7b5f130de8d0&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Approach 3&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/wdc8l03j0izc1.png?width=750&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0dcff653e8367b20ad6b4dcba5742bd4d7585bb9&quot;&gt;https://preview.redd.it/wdc8l03j0izc1.png?width=750&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0dcff653e8367b20ad6b4dcba5742bd4d7585bb9&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This same as approach 1, it gives me 2 different result and answer.&lt;br/&gt; How if i want just only the final answer? How can I get it? print(result[Answer])? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/SensitiveStudy520&quot;&gt; /u/SensitiveStudy520 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cod9zr/langchain_sql_mistral/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cod9zr/langchain_sql_mistral/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1cod9zr</id><media:thumbnail url="https://external-preview.redd.it/8ABZDWd9r3Ixo9grEHVIAHOHyBW0tLcEXujBylBi4Js.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=d969b12e1cfdb941b967d24fef27d9ea4d81da9d" /><link href="https://www.reddit.com/r/LangChain/comments/1cod9zr/langchain_sql_mistral/" /><updated>2024-05-10T01:05:30+00:00</updated><published>2024-05-10T01:05:30+00:00</published><title>LangChain SQL &amp; Mistral</title></entry><entry><author><name>/u/chaitu9701</name><uri>https://www.reddit.com/user/chaitu9701</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;My org has this usecase to build a rag to answer questions. In rag works great given that I given it a lot of instructions(prompts). One demand that rag isn&amp;#39;t able to fullfill is to never mention document reference in the response.&lt;/p&gt; &lt;p&gt;Eg. 1) The document does not mention how to ... 2) you can view the steps on page 60 in the document.&lt;/p&gt; &lt;p&gt;Any prompt suggestions to overcome this particular scenario. The rag should never share the source of its response &lt;/p&gt; &lt;p&gt;My pipeline&lt;/p&gt; &lt;p&gt;1) Pdf Document Langchain &lt;/p&gt; &lt;p&gt;2) Qdrant for retriever&lt;/p&gt; &lt;p&gt;3) Chat gpt3.5 turbo 16k for llm&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/chaitu9701&quot;&gt; /u/chaitu9701 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cnxfnz/rag_response_always_includes_reference_to_document/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cnxfnz/rag_response_always_includes_reference_to_document/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cnxfnz</id><link href="https://www.reddit.com/r/LangChain/comments/1cnxfnz/rag_response_always_includes_reference_to_document/" /><updated>2024-05-09T13:34:29+00:00</updated><published>2024-05-09T13:34:29+00:00</published><title>Rag response always includes reference to document</title></entry></feed>