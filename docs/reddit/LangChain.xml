<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/"><category term="LangChain" label="r/LangChain"/><updated>2024-07-07T13:43:58+00:00</updated><icon>https://www.redditstatic.com/icon.png/</icon><id>/r/LangChain.rss</id><link rel="self" href="https://www.reddit.com/r/LangChain.rss" type="application/atom+xml" /><link rel="alternate" href="https://www.reddit.com/r/LangChain" type="text/html" /><subtitle>LangChain is an open-source framework and developer toolkit that helps developers get LLM applications from prototype to production. It is available for Python and Javascript at https://www.langchain.com/.</subtitle><title>LangChain</title><entry><author><name>/u/zchaarm</name><uri>https://www.reddit.com/user/zchaarm</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;A place for members of &lt;a href=&quot;/r/LangChain&quot;&gt;r/LangChain&lt;/a&gt; to chat with each other&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/zchaarm&quot;&gt; /u/zchaarm &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_10ljho9</id><link href="https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/" /><updated>2023-01-26T04:33:05+00:00</updated><published>2023-01-26T04:33:05+00:00</published><title>r/LangChain Lounge</title></entry><entry><author><name>/u/oyavuzjr</name><uri>https://www.reddit.com/user/oyavuzjr</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi all.&lt;br/&gt; Like many software engineers, I have barely had an original thought since ChatGPT came out. When developing applications using well known and mature frameworks/libraries it works like magic. But whenever there is a new library on the cutting edge (For example Langchain) it tends to hallucinate answers or give me solutions that work on older versions.&lt;br/&gt; I was wondering if anyone else had this problem using it with Langchain? &lt;/p&gt; &lt;p&gt;Also I believe that we are at a phase where we haven&amp;#39;t found the most ergonomic and simple way to develop LLM applications. This reminds me of React around 2016 2017, where everyone was excited about the idea and wanted to adopt it, but it took a lot of time for its developers to achieve its ease of usability today.&lt;/p&gt; &lt;p&gt;What do you guys think about this?&lt;br/&gt; Do you think the API of langchain will get less complicated over time?&lt;br/&gt; Or is the nature of LLM development just so all encompassing that the API has to be vast to provide that flexibility?&lt;/p&gt; &lt;p&gt;Any thoughts appreciated&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/oyavuzjr&quot;&gt; /u/oyavuzjr &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dxbmty/the_maturity_of_langchain_api/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dxbmty/the_maturity_of_langchain_api/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dxbmty</id><link href="https://www.reddit.com/r/LangChain/comments/1dxbmty/the_maturity_of_langchain_api/" /><updated>2024-07-07T08:21:31+00:00</updated><published>2024-07-07T08:21:31+00:00</published><title>The maturity of Langchain API</title></entry><entry><author><name>/u/Big_Barracuda_6753</name><uri>https://www.reddit.com/user/Big_Barracuda_6753</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi , I&amp;#39;m a web dev ( MERN stack ) new to AI . I want to develop a RAG application . In this application , I plan to have support for atleast these types of files ( txt , pdf , csv , md ) for Q&amp;amp;A .&lt;/p&gt; &lt;p&gt;I don&amp;#39;t have much experience with Python language , I know only the basics . &lt;/p&gt; &lt;p&gt;Currently , I&amp;#39;m learning Langchain ( python version ) .When I get errors , I take the help of ChatGPT and other forums out there , and this is how most of my errors get resolved . &lt;/p&gt; &lt;p&gt;I&amp;#39;m on the learning phase currently and I want to know how much NLP coding ( the real python code ) will be required to develop such an application . &lt;/p&gt; &lt;p&gt;Or does Langchain has it all to develop such an application ?&lt;/p&gt; &lt;p&gt;NOTE : I want to build a production grade application .&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Big_Barracuda_6753&quot;&gt; /u/Big_Barracuda_6753 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dxb124/how_much_nlp_coding_will_be_required_for/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dxb124/how_much_nlp_coding_will_be_required_for/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dxb124</id><link href="https://www.reddit.com/r/LangChain/comments/1dxb124/how_much_nlp_coding_will_be_required_for/" /><updated>2024-07-07T07:39:25+00:00</updated><published>2024-07-07T07:39:25+00:00</published><title>How much NLP coding will be required for developing a RAG based application ?</title></entry><entry><author><name>/u/giorgiodidio</name><uri>https://www.reddit.com/user/giorgiodidio</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey there,&lt;/p&gt; &lt;p&gt;now with RAG technologies being accessible to anyone with some basic programming skills, people are scraping any source of content online. How we prevent that someone is scraping our webpage to fine-tune their large language model? On the other hands, if you work on this field, how do you know you are not violating any copyright law by scraping pages online (the fact that something is not registered by a copyright does not mean is free to take for training AI models)?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/giorgiodidio&quot;&gt; /u/giorgiodidio &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dwztph/regulation_about_llmai/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dwztph/regulation_about_llmai/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dwztph</id><link href="https://www.reddit.com/r/LangChain/comments/1dwztph/regulation_about_llmai/" /><updated>2024-07-06T21:15:00+00:00</updated><published>2024-07-06T21:15:00+00:00</published><title>regulation about LLM/AI</title></entry><entry><author><name>/u/sarthakai</name><uri>https://www.reddit.com/user/sarthakai</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;OpenAI, Microsoft, et al surveyed 58 prompting techniques in this paper:&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2406.06608&quot;&gt;https://arxiv.org/pdf/2406.06608&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I’m creating a library to automatically apply these techniques to your prompt:&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/sarthakrastogi/quality-prompts&quot;&gt;https://github.com/sarthakrastogi/quality-prompts&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Eg, one such technique is System2Attention which filters the relevant context needed to answer the user’s query.&lt;/p&gt; &lt;p&gt;Just call .system2attention() on your prompt and it’s done.&lt;/p&gt; &lt;p&gt;Similarly, in few shot prompting, suppose you have a large set of example inputs and labels.&lt;/p&gt; &lt;p&gt;All you have to do is call the .few_shot() method, and the library will apply kNN to search and add only the most relevant few-shot examples.&lt;/p&gt; &lt;p&gt;The prompt is dynamically customised at runtime according to the user’s message.&lt;/p&gt; &lt;p&gt;Let’s write quality prompts!&lt;/p&gt; &lt;p&gt;If you&amp;#39;d like to contribute to the library please raise a PR!&lt;/p&gt; &lt;p&gt;Colab notebook to get started:&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://colab.research.google.com/github/sarthakrastogi/quality-prompts/blob/main/examples/few_shot_prompt_usage.ipynb&quot;&gt;https://colab.research.google.com/github/sarthakrastogi/quality-prompts/blob/main/examples/few_shot_prompt_usage.ipynb&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/sarthakai&quot;&gt; /u/sarthakai &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dwqhwb/creating_library_to_apply_58_prompting_techniques/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dwqhwb/creating_library_to_apply_58_prompting_techniques/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dwqhwb</id><link href="https://www.reddit.com/r/LangChain/comments/1dwqhwb/creating_library_to_apply_58_prompting_techniques/" /><updated>2024-07-06T14:10:55+00:00</updated><published>2024-07-06T14:10:55+00:00</published><title>Creating library to apply 58 prompting techniques to your prompt. Join me?</title></entry><entry><author><name>/u/netsfan914</name><uri>https://www.reddit.com/user/netsfan914</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;What observability platforms are people using for their voice agents? Have found the current solutions to be not useful for audio use cases (running conversation level evals, detecting latency &amp;amp; interruptions, audio playback connected to traces, flagging call failures, etc). Have checked out LangSmith, Agentops, and a few others&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/netsfan914&quot;&gt; /u/netsfan914 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dwzugx/alternative_to_langsmith_for_voice_agents/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dwzugx/alternative_to_langsmith_for_voice_agents/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dwzugx</id><link href="https://www.reddit.com/r/LangChain/comments/1dwzugx/alternative_to_langsmith_for_voice_agents/" /><updated>2024-07-06T21:15:58+00:00</updated><published>2024-07-06T21:15:58+00:00</published><title>Alternative to LangSmith for voice agents</title></entry><entry><author><name>/u/Lethal_Protector_404</name><uri>https://www.reddit.com/user/Lethal_Protector_404</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi everyone, I’m exploring the use of LangChain OpenAPI Agent for a project and have encountered a challenge with handling large amounts of tokens efficiently. Does anyone have experience or tips on managing this effectively? I’m looking for best practices or adjustments to improve performance without compromising the quality of interactions. Any advice or insights would be greatly appreciated!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Lethal_Protector_404&quot;&gt; /u/Lethal_Protector_404 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dws16l/managing_large_token_volumes_with_langchain/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dws16l/managing_large_token_volumes_with_langchain/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dws16l</id><link href="https://www.reddit.com/r/LangChain/comments/1dws16l/managing_large_token_volumes_with_langchain/" /><updated>2024-07-06T15:21:35+00:00</updated><published>2024-07-06T15:21:35+00:00</published><title>Managing Large Token Volumes with LangChain OpenAPI Agent</title></entry><entry><author><name>/u/Albert_AG</name><uri>https://www.reddit.com/user/Albert_AG</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;m trying to develop an application that can perform statistical analysis of CSV files and generate plots. I&amp;#39;ve been trying to do this with rag, but I&amp;#39;ve no IDEA how to split/load/embed the CSV files, I&amp;#39;ve done this before with PDFs. PLEASE HELP!!! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Albert_AG&quot;&gt; /u/Albert_AG &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dwm3xh/help_with_csv_rag/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dwm3xh/help_with_csv_rag/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dwm3xh</id><link href="https://www.reddit.com/r/LangChain/comments/1dwm3xh/help_with_csv_rag/" /><updated>2024-07-06T09:54:04+00:00</updated><published>2024-07-06T09:54:04+00:00</published><title>Help with CSV RAG.</title></entry><entry><author><name>/u/jscraft</name><uri>https://www.reddit.com/user/jscraft</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Made this short LangChain.js example on how to improve AI math accuracy by asking the LLM to create and execute JavaScript code. &lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://www.js-craft.io/blog/langchain-javascript-execute-generated-code/&quot;&gt;https://www.js-craft.io/blog/langchain-javascript-execute-generated-code/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/jscraft&quot;&gt; /u/jscraft &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dwk40w/langchain_javascript_execute_generated_code/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dwk40w/langchain_javascript_execute_generated_code/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dwk40w</id><link href="https://www.reddit.com/r/LangChain/comments/1dwk40w/langchain_javascript_execute_generated_code/" /><updated>2024-07-06T07:27:17+00:00</updated><published>2024-07-06T07:27:17+00:00</published><title>LangChain JavaScript – execute generated code</title></entry><entry><author><name>/u/shanumas</name><uri>https://www.reddit.com/user/shanumas</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I think currenlty the langchain implementations like chat-langchain supports conversational memory. But the conversation can sometimes be too long.&lt;/p&gt; &lt;p&gt;I am lookin for memory-summarization like this. &lt;a href=&quot;https://www.youtube.com/watch?v=oPCKB9MUP6c&amp;amp;t=81s&quot;&gt;https://www.youtube.com/watch?v=oPCKB9MUP6c&amp;amp;t=81s&lt;/a&gt;&lt;/p&gt; &lt;p&gt;to reduce tokens. Is there any chatbot implementation like this on github ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/shanumas&quot;&gt; /u/shanumas &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dwkr14/langchain_with_personalized_memory_or_summarized/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dwkr14/langchain_with_personalized_memory_or_summarized/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dwkr14</id><link href="https://www.reddit.com/r/LangChain/comments/1dwkr14/langchain_with_personalized_memory_or_summarized/" /><updated>2024-07-06T08:12:46+00:00</updated><published>2024-07-06T08:12:46+00:00</published><title>Langchain with personalized memory (or summarized conversational memory)</title></entry><entry><author><name>/u/pjbacelar</name><uri>https://www.reddit.com/user/pjbacelar</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey folks, we’ve just launched an open-source library called Django AI Assistant, and we’d love your feedback!&lt;/p&gt; &lt;p&gt;What It Does:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Function/Tool Calling&lt;/strong&gt;: Simplifies complex AI implementations with easy-to-use Python classes&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Retrieval-Augmented Generation&lt;/strong&gt;: Enhance AI functionalities efficiently.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Full Django Integration&lt;/strong&gt;: AI can access databases, check permissions, send emails, manage media files, and call external APIs effortlessly.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;How You Can Help:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Try It: &lt;a href=&quot;https://github.com/vintasoftware/django-ai-assistant/&quot;&gt;https://github.com/vintasoftware/django-ai-assistant/&lt;/a&gt;&lt;/li&gt; &lt;li&gt;▶️ &lt;a href=&quot;https://www.youtube.com/watch?v=bSJv4OIKLog&amp;amp;ab_channel=VintaSoftware&quot;&gt;Watch the Demo&lt;/a&gt;&lt;/li&gt; &lt;li&gt;📖 &lt;a href=&quot;https://vintasoftware.github.io/django-ai-assistant/latest/get-started/&quot;&gt;Read the Docs&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Test It &amp;amp; Break Things: Integrate it, experiment, and see what works (and what doesn’t).&lt;/li&gt; &lt;li&gt;Give Feedback: Drop your thoughts here or on our GitHub issues page.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Your input will help us make this lib better for everyone. Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/pjbacelar&quot;&gt; /u/pjbacelar &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dw6dws/django_ai_assistant_opensource_lib_launch/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dw6dws/django_ai_assistant_opensource_lib_launch/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dw6dws</id><link href="https://www.reddit.com/r/LangChain/comments/1dw6dws/django_ai_assistant_opensource_lib_launch/" /><updated>2024-07-05T19:34:19+00:00</updated><published>2024-07-05T19:34:19+00:00</published><title>Django AI Assistant - Open-source Lib Launch</title></entry><entry><author><name>/u/AudibleDruid</name><uri>https://www.reddit.com/user/AudibleDruid</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dwlfju/what_is_suppose_to_go_into_here_langflow/&quot;&gt; &lt;img src=&quot;https://b.thumbs.redditmedia.com/KWGg6CZOUlYqCi8xD4i6Ay2AYX8NAL1h6WHCBUYVk5w.jpg&quot; alt=&quot;What is suppose to go into here? Langflow&quot; title=&quot;What is suppose to go into here? Langflow&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;&lt;a href=&quot;https://preview.redd.it/ty9hu9dz5vad1.png?width=900&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2627d1ad0ee0bb3f888b9c583e96060447d55b77&quot;&gt;https://preview.redd.it/ty9hu9dz5vad1.png?width=900&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2627d1ad0ee0bb3f888b9c583e96060447d55b77&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/AudibleDruid&quot;&gt; /u/AudibleDruid &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dwlfju/what_is_suppose_to_go_into_here_langflow/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dwlfju/what_is_suppose_to_go_into_here_langflow/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1dwlfju</id><media:thumbnail url="https://b.thumbs.redditmedia.com/KWGg6CZOUlYqCi8xD4i6Ay2AYX8NAL1h6WHCBUYVk5w.jpg" /><link href="https://www.reddit.com/r/LangChain/comments/1dwlfju/what_is_suppose_to_go_into_here_langflow/" /><updated>2024-07-06T09:03:05+00:00</updated><published>2024-07-06T09:03:05+00:00</published><title>What is suppose to go into here? Langflow</title></entry><entry><author><name>/u/business24_ai</name><uri>https://www.reddit.com/user/business24_ai</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;&lt;a href=&quot;https://youtu.be/DBXdE_5Jces&quot;&gt;https://youtu.be/DBXdE_5Jces&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/business24_ai&quot;&gt; /u/business24_ai &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dwkzj8/langgraph_state_create_a_cyclic_graph_and/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dwkzj8/langgraph_state_create_a_cyclic_graph_and/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dwkzj8</id><link href="https://www.reddit.com/r/LangChain/comments/1dwkzj8/langgraph_state_create_a_cyclic_graph_and/" /><updated>2024-07-06T08:30:17+00:00</updated><published>2024-07-06T08:30:17+00:00</published><title>LangGraph state - Create a cyclic graph and watchdog a directory</title></entry><entry><author><name>/u/Front-Show7358</name><uri>https://www.reddit.com/user/Front-Show7358</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Potentially dumb question lol. Basically when I run my RAG, it takes a long time to process all the documents that it will then retrieve. Is there a way to just save off the model after it is done reading the documents so that when you run it again, it can skip that step? Similar to how a fine-tuned model would work? It doesn&amp;#39;t really make sense in my head, but I haven&amp;#39;t been able to find a concrete answer to this so I want to be sure.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Front-Show7358&quot;&gt; /u/Front-Show7358 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dw1mk2/is_there_a_way_to_save_a_rag_after_it_has_read/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dw1mk2/is_there_a_way_to_save_a_rag_after_it_has_read/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dw1mk2</id><link href="https://www.reddit.com/r/LangChain/comments/1dw1mk2/is_there_a_way_to_save_a_rag_after_it_has_read/" /><updated>2024-07-05T16:10:29+00:00</updated><published>2024-07-05T16:10:29+00:00</published><title>Is there a way to save a RAG after it has read its documents?</title></entry><entry><author><name>/u/PuzzleheadedDay5615</name><uri>https://www.reddit.com/user/PuzzleheadedDay5615</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;&lt;a href=&quot;https://ai.google.dev/competition&quot;&gt;Join the Gemini API Developer Competition | Google for Developers&lt;/a&gt; Here is the link&lt;/p&gt; &lt;p&gt;I have been freelancing for 4+ years and have decent experience of python. need someone who is competitive, creative, and willing to sacrifice at least 4 hours a day&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/PuzzleheadedDay5615&quot;&gt; /u/PuzzleheadedDay5615 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dwd1f9/trying_to_find_teammate_for_google_gemini/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dwd1f9/trying_to_find_teammate_for_google_gemini/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dwd1f9</id><link href="https://www.reddit.com/r/LangChain/comments/1dwd1f9/trying_to_find_teammate_for_google_gemini/" /><updated>2024-07-06T00:34:32+00:00</updated><published>2024-07-06T00:34:32+00:00</published><title>trying to find teammate for google gemini developer competition</title></entry><entry><author><name>/u/macxgaming</name><uri>https://www.reddit.com/user/macxgaming</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi!&lt;/p&gt; &lt;p&gt;I&amp;#39;m working on a RAG system for my company where we can use it to search through our internal wiki page.&lt;br/&gt; My system is nearly in a releasable state and finds the correct information 90% of the times, and I&amp;#39;m happy about it, but I&amp;#39;m constantly thinking, can I make it better?&lt;/p&gt; &lt;p&gt;I&amp;#39;ve made a custom scraper for our wiki, we&amp;#39;re using an older version of MediaWiki.&lt;br/&gt; The scraper I&amp;#39;ve made is basically extracting all sections out into its own &amp;quot;document&amp;quot; and then sending it into qdrant vector database.&lt;br/&gt; That means that in the vector database, it doesn&amp;#39;t have a full wiki page but rather a cut up version to make it easier for the search query to hit something right. But I feel like this is kinda wrong?&lt;/p&gt; &lt;p&gt;Whenever you send in your query to the backend, it&amp;#39;ll then search for the 10 documents matching and then reranking with BAAI/bge-reranker-large. Then the context is being sent to Llama3:8b with your question in mind.&lt;br/&gt; This means that Llama3 will never get a fully contextual article, since the vectors are only smaller sections from the full page.&lt;/p&gt; &lt;p&gt;What could be done do make this better in the end? The one thing I see as an issue here, is that it will never know anything about the rest of the full page, but if it has the full page, it feels like Llama3 get overwhelmed by the data and then craps out.&lt;/p&gt; &lt;p&gt;We have ~258 articles and that&amp;#39;s resulting in about 1488 points in qdrant.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/macxgaming&quot;&gt; /u/macxgaming &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dvr774/what_is_the_best_approach_to_achieve_a_better/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dvr774/what_is_the_best_approach_to_achieve_a_better/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dvr774</id><link href="https://www.reddit.com/r/LangChain/comments/1dvr774/what_is_the_best_approach_to_achieve_a_better/" /><updated>2024-07-05T06:17:40+00:00</updated><published>2024-07-05T06:17:40+00:00</published><title>What is the best approach to achieve a better performant RAG?</title></entry><entry><author><name>/u/Party_Jellyfish5380</name><uri>https://www.reddit.com/user/Party_Jellyfish5380</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;YouTube has a new feature where it organizes comments by. It it possible to organize a list of chat by topic with langchain?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Party_Jellyfish5380&quot;&gt; /u/Party_Jellyfish5380 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dw5fr6/youtube_comments_feature/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dw5fr6/youtube_comments_feature/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dw5fr6</id><link href="https://www.reddit.com/r/LangChain/comments/1dw5fr6/youtube_comments_feature/" /><updated>2024-07-05T18:53:47+00:00</updated><published>2024-07-05T18:53:47+00:00</published><title>YouTube comments feature</title></entry><entry><author><name>/u/frellothings</name><uri>https://www.reddit.com/user/frellothings</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I want to deploy a Huggingface model in Sagemaker with a context size of around 25-32k. I am having trouble finding a suitable model that performs well with this context size. The model&amp;#39;s task will be to map raw data to a target framework. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/frellothings&quot;&gt; /u/frellothings &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dvs05a/deploy_hugging_face_model_in_sagemaker/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dvs05a/deploy_hugging_face_model_in_sagemaker/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dvs05a</id><link href="https://www.reddit.com/r/LangChain/comments/1dvs05a/deploy_hugging_face_model_in_sagemaker/" /><updated>2024-07-05T07:11:15+00:00</updated><published>2024-07-05T07:11:15+00:00</published><title>Deploy Hugging Face model in Sagemaker</title></entry><entry><author><name>/u/Volodymyr_steax</name><uri>https://www.reddit.com/user/Volodymyr_steax</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;My question might be a bit basic, but I’m new to all of this and eager to learn.&lt;/p&gt; &lt;p&gt;I have a basic setup where I initialize an LLM using vLLM with Langchain RAG and the Llama model (specifically, llama2-13b-chat-hf). Here’s what I do:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;I define a system prompt and an instruction f&lt;/li&gt; &lt;li&gt;I create an &lt;code&gt;llm_chain&lt;/code&gt;&lt;/li&gt; &lt;li&gt;I then run the chain with &lt;code&gt;llm_chain.run(text)&lt;/code&gt; , which works for a single input.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I have build an app with FastAPI. Previously I used asyncio method to handle multiple request to llm, but with each new request it become slower in response. So I decide to use vLLM method, but I got a problem now how to provide parallel or concurrent requests to vLLM when I have dealing with dozen or more users. Is there a way to call &lt;code&gt;run&lt;/code&gt; in parallel for several inputs and receive valid results for each input?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Volodymyr_steax&quot;&gt; /u/Volodymyr_steax &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dvrj1k/concurrentparallel_requests_with_vllm/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dvrj1k/concurrentparallel_requests_with_vllm/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dvrj1k</id><link href="https://www.reddit.com/r/LangChain/comments/1dvrj1k/concurrentparallel_requests_with_vllm/" /><updated>2024-07-05T06:39:27+00:00</updated><published>2024-07-05T06:39:27+00:00</published><title>Concurrent/parallel requests with vLLM</title></entry><entry><author><name>/u/gibriyagi</name><uri>https://www.reddit.com/user/gibriyagi</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I would like to use Postgres with pgvector but could not figure out a way to do hybrid search using bm25.&lt;/p&gt; &lt;p&gt;Anyone using Postgres only for RAG? Do you do hybrid search? If not do you combine it with something else?&lt;/p&gt; &lt;p&gt;Would love to hear your experiences.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/gibriyagi&quot;&gt; /u/gibriyagi &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dvdnzc/hybrid_search_with_postgres/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dvdnzc/hybrid_search_with_postgres/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dvdnzc</id><link href="https://www.reddit.com/r/LangChain/comments/1dvdnzc/hybrid_search_with_postgres/" /><updated>2024-07-04T18:23:03+00:00</updated><published>2024-07-04T18:23:03+00:00</published><title>Hybrid search with Postgres</title></entry><entry><author><name>/u/ExplorerTechnical808</name><uri>https://www.reddit.com/user/ExplorerTechnical808</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;like title. Thanks in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/ExplorerTechnical808&quot;&gt; /u/ExplorerTechnical808 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dvrv8g/any_good_resourceguide_about_how_to_do_rag_on_a/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dvrv8g/any_good_resourceguide_about_how_to_do_rag_on_a/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dvrv8g</id><link href="https://www.reddit.com/r/LangChain/comments/1dvrv8g/any_good_resourceguide_about_how_to_do_rag_on_a/" /><updated>2024-07-05T07:01:51+00:00</updated><published>2024-07-05T07:01:51+00:00</published><title>Any good resource/guide about how to do RAG on a codebase? (e.g. Github repo)</title></entry><entry><author><name>/u/muditjps</name><uri>https://www.reddit.com/user/muditjps</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dvhvam/hey_rlangchain_weve_created_an_app_template_for/&quot;&gt; &lt;img src=&quot;https://external-preview.redd.it/6vdYm5zqVtm_CFLlTGc1uWkGnd-hhEdr4D2wHuVz3iQ.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b991cdf7470fc68f086bd51039bd6575a8817deb&quot; alt=&quot;Hey r/langchain, we've created an app template for multimodal RAG (MM-RAG) using GPT4o and Pathway. The incremental indexing pipeline parses tables as images, explains them in detail, and saves the table content with the document chunk. This outperforms traditional RAG methods. More in the link.&quot; title=&quot;Hey r/langchain, we've created an app template for multimodal RAG (MM-RAG) using GPT4o and Pathway. The incremental indexing pipeline parses tables as images, explains them in detail, and saves the table content with the document chunk. This outperforms traditional RAG methods. More in the link.&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/muditjps&quot;&gt; /u/muditjps &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://pathway.com/developers/templates/multimodal-rag&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dvhvam/hey_rlangchain_weve_created_an_app_template_for/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1dvhvam</id><media:thumbnail url="https://external-preview.redd.it/6vdYm5zqVtm_CFLlTGc1uWkGnd-hhEdr4D2wHuVz3iQ.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=b991cdf7470fc68f086bd51039bd6575a8817deb" /><link href="https://www.reddit.com/r/LangChain/comments/1dvhvam/hey_rlangchain_weve_created_an_app_template_for/" /><updated>2024-07-04T21:30:21+00:00</updated><published>2024-07-04T21:30:21+00:00</published><title>Hey r/langchain, we've created an app template for multimodal RAG (MM-RAG) using GPT4o and Pathway. The incremental indexing pipeline parses tables as images, explains them in detail, and saves the table content with the document chunk. This outperforms traditional RAG methods. More in the link.</title></entry><entry><author><name>/u/meamysace</name><uri>https://www.reddit.com/user/meamysace</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;m searching for a tool that allows users to compare outputs generated by several LLMs using just one prompt. While I understand that LangChain could potentially enable building such a solution locally, I&amp;#39;m curious if any existing products offer this functionality.&lt;/p&gt; &lt;p&gt;I&amp;#39;m weary of manually inputting the same prompt across different models like GPT, Claude, Bard, and Perplexity to cross-reference answers and verify accuracy. Any recommendations or insights would be greatly appreciated! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/meamysace&quot;&gt; /u/meamysace &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dvaenf/tool_for_comparing_outputs_of_multiple_llms_from/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dvaenf/tool_for_comparing_outputs_of_multiple_llms_from/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dvaenf</id><link href="https://www.reddit.com/r/LangChain/comments/1dvaenf/tool_for_comparing_outputs_of_multiple_llms_from/" /><updated>2024-07-04T16:02:08+00:00</updated><published>2024-07-04T16:02:08+00:00</published><title>Tool for Comparing Outputs of Multiple LLMs from Single Prompts</title></entry><entry><author><name>/u/QuasiEvil</name><uri>https://www.reddit.com/user/QuasiEvil</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;ve been playing around with GPT4All and langchain, for which there is a minimal demo here:&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://python.langchain.com/v0.2/docs/integrations/llms/gpt4all/&quot;&gt;https://python.langchain.com/v0.2/docs/integrations/llms/gpt4all/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;In this demo, they invoke the following:&lt;/p&gt; &lt;p&gt;&lt;code&gt;from langchain_core.callbacks import StreamingStdOutCallbackHandler&lt;/code&gt;&lt;/p&gt; &lt;p&gt;From the API, it states that this only works with LLMs that support streaming. According to the integrations page:&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://python.langchain.com/v0.2/docs/integrations/llms/&quot;&gt;https://python.langchain.com/v0.2/docs/integrations/llms/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;gpt4all does NOT support streaming. So I&amp;#39;m confused - what gives with this demo?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/QuasiEvil&quot;&gt; /u/QuasiEvil &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dvfs2b/beginner_here_found_something_confusing/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dvfs2b/beginner_here_found_something_confusing/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dvfs2b</id><link href="https://www.reddit.com/r/LangChain/comments/1dvfs2b/beginner_here_found_something_confusing/" /><updated>2024-07-04T19:56:38+00:00</updated><published>2024-07-04T19:56:38+00:00</published><title>Beginner here: found something confusing</title></entry><entry><author><name>/u/liljuden</name><uri>https://www.reddit.com/user/liljuden</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi there,&lt;/p&gt; &lt;p&gt;I am currently working on building a chatbot for internal use in my company. I have developed a relatively classic RAG framework and now want to include a knowledge graph. I have read several papers on this topic, but I am confused about how to actually &amp;quot;activate&amp;quot; a graph in the RAG flow.&lt;/p&gt; &lt;p&gt;So far, I have found different approaches based on extracting entities and relationships from chunks to generate community summaries of the related entities. This process occurs in the offline stage. At least, that is what I have tried to do. I am wondering how to activate this correctly. Currently, I match entities from the input with summaries as the flow runs, but I have the impression that others use the graph aspect differently, possibly using a function to inject relevant context into the LLM.&lt;/p&gt; &lt;p&gt;Can you help me understand this better?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/liljuden&quot;&gt; /u/liljuden &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1duz8qc/how_to_incorporate_a_knowledge_graph_in_rag/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1duz8qc/how_to_incorporate_a_knowledge_graph_in_rag/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1duz8qc</id><link href="https://www.reddit.com/r/LangChain/comments/1duz8qc/how_to_incorporate_a_knowledge_graph_in_rag/" /><updated>2024-07-04T05:33:58+00:00</updated><published>2024-07-04T05:33:58+00:00</published><title>How to incorporate a knowledge graph in RAG</title></entry><entry><author><name>/u/imharesh20</name><uri>https://www.reddit.com/user/imharesh20</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;m working with Langchain to build a tool that calls an agent. Currently, I&amp;#39;m passing the chat history as an input variable to the agent. However, I&amp;#39;ve encountered an issue where the agent doesn&amp;#39;t always seem to utilize the history data to answer questions consistently. This is especially problematic when users have queries spaced out over 10–15 days.&lt;/p&gt; &lt;p&gt;Is there a more efficient way to ensure the agent consistently remembers all chat history and context over multiple sessions? What approach or best practices should I follow to address this issue?&lt;/p&gt; &lt;p&gt;Thanks in advance for your guidance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/imharesh20&quot;&gt; /u/imharesh20 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dv7e89/passing_chat_history_to_langchain_tool_calling/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dv7e89/passing_chat_history_to_langchain_tool_calling/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dv7e89</id><link href="https://www.reddit.com/r/LangChain/comments/1dv7e89/passing_chat_history_to_langchain_tool_calling/" /><updated>2024-07-04T13:49:43+00:00</updated><published>2024-07-04T13:49:43+00:00</published><title>Passing Chat History to Langchain Tool Calling Agent</title></entry></feed>