<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/"><category term="LangChain" label="r/LangChain"/><updated>2024-08-01T15:53:15+00:00</updated><icon>https://www.redditstatic.com/icon.png/</icon><id>/r/LangChain.rss</id><link rel="self" href="https://www.reddit.com/r/LangChain.rss" type="application/atom+xml" /><link rel="alternate" href="https://www.reddit.com/r/LangChain" type="text/html" /><subtitle>LangChain is an open-source framework and developer toolkit that helps developers get LLM applications from prototype to production. It is available for Python and Javascript at https://www.langchain.com/.</subtitle><title>LangChain</title><entry><author><name>/u/zchaarm</name><uri>https://www.reddit.com/user/zchaarm</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;A place for members of &lt;a href=&quot;/r/LangChain&quot;&gt;r/LangChain&lt;/a&gt; to chat with each other&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/zchaarm&quot;&gt; /u/zchaarm &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_10ljho9</id><link href="https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/" /><updated>2023-01-26T04:33:05+00:00</updated><published>2023-01-26T04:33:05+00:00</published><title>r/LangChain Lounge</title></entry><entry><author><name>/u/Complete-Pie5760</name><uri>https://www.reddit.com/user/Complete-Pie5760</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;ReAct agents are powerful, but I&amp;#39;ve found a way to make them even better for real-world use. Here&amp;#39;s how:&lt;/p&gt; &lt;p&gt;Multi-LLM Integration: I&amp;#39;ve set up my ReAct agents to work with over 200 different LLMs. This flexibility lets you choose the best model for each task. &lt;/p&gt; &lt;p&gt;Performance Tracking: By monitoring costs, token usage, and latency, I&amp;#39;ve optimized my agents&amp;#39; efficiency. This is crucial for large-scale applications.&lt;/p&gt; &lt;p&gt;Improved Reliability: I&amp;#39;ve implemented fallbacks between LLMs, load-balancing, and automatic retries. This makes the agents much more stable in production environments.&lt;/p&gt; &lt;p&gt;Smart Caching: By storing frequently accessed data, I&amp;#39;ve significantly reduced API calls, making the agents faster and more cost-effective.&lt;/p&gt; &lt;p&gt;Detailed Logging: Comprehensive action tracking has been a game-changer for debugging complex ReAct runs.&lt;/p&gt; &lt;p&gt;Easy Prompt Management- I can now update prompts without touching the code, which speeds up experimentation and optimization.&lt;/p&gt; &lt;p&gt;I&amp;#39;ve based my implementation on Simon Willison&amp;#39;s work. You can find the starting point here: &lt;a href=&quot;https://git.new/ReAct-framework&quot;&gt;https://git.new/ReAct-framework&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Has anyone else been working on improving ReAct agents? What challenges have you faced in real-world applications?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Complete-Pie5760&quot;&gt; /u/Complete-Pie5760 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ehcpct/how_to_build_production_grade_langchain_agents/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ehcpct/how_to_build_production_grade_langchain_agents/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1ehcpct</id><link href="https://www.reddit.com/r/LangChain/comments/1ehcpct/how_to_build_production_grade_langchain_agents/" /><updated>2024-08-01T09:50:53+00:00</updated><published>2024-08-01T09:50:53+00:00</published><title>How to build Production grade Langchain Agents using ReAct</title></entry><entry><author><name>/u/g_pal</name><uri>https://www.reddit.com/user/g_pal</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I recently had our AI interviewer speak with 22 developers who are building with LangGraph. The interviews covered various topics, including how they&amp;#39;re using LangGraph, what they like about it, and areas for improvement. I wanted to share the key findings because I thought you might find it interesting.&lt;/p&gt; &lt;h1&gt;Use Cases and Attractions&lt;/h1&gt; &lt;p&gt;LangGraph is attracting developers from a wide range of industries due to its versatility in managing complex AI workflows. Here are some interesting use cases:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Content Generation:&lt;/strong&gt; Teams are using LangGraph to create systems where multiple AI agents collaborate to draft, fact-check, and refine research papers in real-time.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Customer Service:&lt;/strong&gt; Developers are building dynamic response systems that analyze sentiment, retrieve relevant information, and generate personalized replies with built-in clarification mechanisms.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Financial Modeling:&lt;/strong&gt; Some are building valuation models in real estate that adapt in real-time based on market fluctuations and simulated scenarios.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Academic Research&lt;/strong&gt;: Institutions are developing adaptive research assistants capable of gathering data, synthesizing insights, and proposing new hypotheses within a single integrated system.&lt;/li&gt; &lt;/ol&gt; &lt;h1&gt;What Attracts Developers to LangGraph?&lt;/h1&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Multi-Agent System Orchestration&lt;/strong&gt;: LangGraph excels at managing multiple AI agents, allowing for a divide-and-conquer approach to complex problems.&amp;quot;We are working on a project that requires multiple AI agents to communicate and talk to one another. LangGraph helps with thinking through the problem using a divide-and-conquer approach with graphs, nodes, and edges.&amp;quot; - Founder, Property Technology Startup&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Workflow Visualization and Debugging&lt;/strong&gt;: The platform&amp;#39;s visualization capabilities are highly valued for development and debugging.&amp;quot;LangGraph can visualize all the requests and all the payloads instantly, and I can debug by taking LangGraph. It&amp;#39;s very convenient for the development experience.&amp;quot; - Cloud Solutions Architect, Microsoft&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Complex Problem-Solving&lt;/strong&gt;: Developers appreciate LangGraph&amp;#39;s ability to tackle intricate challenges that traditional programming struggles with.&amp;quot;Solving complex problems that are not, um, possible with traditional programming.&amp;quot; - AI Researcher, Nokia&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Abstraction of Flow Logic&lt;/strong&gt;: LangGraph simplifies the implementation of complex workflows by abstracting flow logic.&amp;quot;[LangGraph helped] abstract the flow logic and avoid having to write all of the boilerplate code to get started with the project.&amp;quot; - AI Researcher, Nokia&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Flexible Agentic Workflows&lt;/strong&gt;: The tool&amp;#39;s adaptability for various AI agent scenarios is a key attraction.&amp;quot;Being able to create an agentic workflow that is easy to visualize abstractly with graphs, nodes, and edges.&amp;quot; - Founder, Property Technology Startup&lt;/li&gt; &lt;/ol&gt; &lt;h1&gt;LangGraph vs Alternatives&lt;/h1&gt; &lt;p&gt;The most commonly considered alternatives were CrewAI and Microsoft&amp;#39;s Autogen. However, developers noted several areas where LangGraph stands out:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Handling Complex Workflows:&lt;/strong&gt; Unlike some competitors limited to simple, linear processes, LangGraph can handle complex graph flows, including cycles.&amp;quot;CrewAI can only handle DAGs and cannot handle cycles, whereas LangGraph can handle complex graph flows, including cycles.&amp;quot; - Developer&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Developer Control:&lt;/strong&gt; LangGraph offers a level of control that many find unmatched, especially for custom use cases.&amp;quot;We did tinker a bit with CrewAI and Meta GPT. But those could not come even near as powerful as LangGraph. And we did combine with LangChain because we have very custom use cases, and we need to have a lot of control. And the competitor frameworks just don&amp;#39;t offer that amount of, control over the code.&amp;quot; - Founder, GenAI Startup&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Mature Ecosystem:&lt;/strong&gt; LangGraph&amp;#39;s longer market presence has resulted in more resources, tools, and infrastructure.&amp;quot;LangGraph has the advantage of being in the market longer, offering more resources, tools, and infrastructure. The ability to use LangSmith in conjunction with LangGraph for debugging and performance analysis is a significant differentiator.&amp;quot; - Developer&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Market Leadership:&lt;/strong&gt; Despite a volatile market, LangGraph is currently seen as a leader in functionality and tooling for developing workflows.&amp;quot;Currently, LangGraph is one of the leaders in terms of functionality and tooling for developing workflows. The market is volatile, and I hope LangGraph continues to innovate and create more tools to facilitate developers&amp;#39; work.&amp;quot; - Developer&lt;/li&gt; &lt;/ol&gt; &lt;h1&gt;Areas for Improvement&lt;/h1&gt; &lt;p&gt;While LangGraph has garnered praise, developers also identified several areas for improvement:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Simplify Syntax and Reduce Complexity:&lt;/strong&gt; Some developers noted that the graph-based approach, while powerful, can be complex to maintain.&amp;quot;Some syntax can be made a lot simpler.&amp;quot; - Senior Engineering Director, BlackRock&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Enhance Documentation and Community Resources:&lt;/strong&gt; There&amp;#39;s a need for more in-depth, complex examples and community-driven documentation.&amp;quot;The lack of how-to articles and community-driven documentation... There&amp;#39;s a lot of entry-level stuff, but nothing really in-depth or complex.&amp;quot; - Research Assistant, BYU&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Improve Debugging Capabilities:&lt;/strong&gt; Developers expressed a need for more detailed debugging information, especially for tracking state within the graph.&amp;quot;There is a need for more debugging information. Sometimes, the bug information starts from the instantiation of the workflow, and it&amp;#39;s hard to track the state within the graph.&amp;quot; - Senior Software Engineer, Canadian Government Agency&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Better Human-in-the-Loop Integration:&lt;/strong&gt; Some users aren&amp;#39;t satisfied with the current implementation of human-in-the-loop concepts.&amp;quot;More options around the human-in-the-loop concept. I&amp;#39;m not a very big fan of their current implementation of that.&amp;quot; - AI Researcher, Nokia&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Enhanced Subgraph Integration:&lt;/strong&gt; Multiple developers mentioned issues with integrating and combining subgraphs.&amp;quot;The possibility to integrate subgraphs isn&amp;#39;t compatible with [graph drawing].&amp;quot; - Engineer, IT Consulting Company &amp;quot;I wish you could combine smaller graphs into bigger graphs more easily.&amp;quot; - Research Assistant, BYU&lt;/li&gt; &lt;li&gt;&lt;strong&gt;More Complex Examples:&lt;/strong&gt; There&amp;#39;s a desire for more complex examples that developers can use as starting points.&amp;quot;Creating more examples online that people can use as inspiration would be fantastic.&amp;quot; - Senior Engineering Director, BlackRock&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;____&lt;br/&gt; You can check out the interview transcripts here: &lt;a href=&quot;http://kgrid.ai/company/langgraph&quot;&gt;kgrid.ai/company/langgraph&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Curious to know whether this aligns with your experience? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/g_pal&quot;&gt; /u/g_pal &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1eh0ly3/spoke_to_22_langgraph_devs_and_heres_what_we_found/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1eh0ly3/spoke_to_22_langgraph_devs_and_heres_what_we_found/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1eh0ly3</id><link href="https://www.reddit.com/r/LangChain/comments/1eh0ly3/spoke_to_22_langgraph_devs_and_heres_what_we_found/" /><updated>2024-07-31T22:36:11+00:00</updated><published>2024-07-31T22:36:11+00:00</published><title>Spoke to 22 LangGraph devs and here's what we found</title></entry><entry><author><name>/u/moonbunR</name><uri>https://www.reddit.com/user/moonbunR</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/moonbunR&quot;&gt; /u/moonbunR &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;/r/SmythOS/comments/1efnjke/how_does_an_llm_orchestrator_decide_which_agent/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ehjtby/how_does_an_llm_orchestrator_decide_which_agent/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1ehjtby</id><link href="https://www.reddit.com/r/LangChain/comments/1ehjtby/how_does_an_llm_orchestrator_decide_which_agent/" /><updated>2024-08-01T15:40:52+00:00</updated><published>2024-08-01T15:40:52+00:00</published><title>How does an LLM orchestrator decide which agent to use in a multi-agent system?</title></entry><entry><author><name>/u/sharrajesh</name><uri>https://www.reddit.com/user/sharrajesh</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;m working on a production FastAPI application that uses LangChain with a cascade of tools for various AI tasks. I&amp;#39;m looking to add asynchronous streaming support to my API and would appreciate feedback on my proposed design:&lt;/p&gt; &lt;h2&gt;Current Setup:&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;FastAPI endpoints that use LangChain agents with multiple tools&lt;/li&gt; &lt;li&gt;Synchronous API calls that return complete responses, including main content and metadata (e.g., sources used)&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;Proposed Design:&lt;/h2&gt; &lt;ol&gt; &lt;li&gt;Keep existing synchronous API endpoints as-is for backward compatibility&lt;/li&gt; &lt;li&gt;Add new streaming endpoints for real-time token generation of the main response body&lt;/li&gt; &lt;li&gt;Use Redis as a message broker to collect and stream responses&lt;/li&gt; &lt;li&gt;Synchronous API continues to return full response with all fields (main content, sources, etc.)&lt;/li&gt; &lt;/ol&gt; &lt;h2&gt;Implementation Idea:&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;Modify existing endpoints to publish responses to Redis&lt;/li&gt; &lt;li&gt;Create new streaming endpoints that subscribe to Redis channels&lt;/li&gt; &lt;li&gt;Update LangChain agents to publish chunks and full responses to Redis&lt;/li&gt; &lt;li&gt;Client can use either sync API for full response or streaming API for real-time updates&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;Questions:&lt;/h2&gt; &lt;ol&gt; &lt;li&gt;Is this a sensible approach for adding streaming to an existing production API?&lt;/li&gt; &lt;li&gt;Are there better alternatives to using Redis for this purpose?&lt;/li&gt; &lt;li&gt;How can I ensure efficient resource usage and low latency with this design?&lt;/li&gt; &lt;li&gt;Any potential pitfalls or considerations I should be aware of?&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;I&amp;#39;d greatly appreciate any insights, alternative approaches, or best practices for implementing streaming in a FastAPI LangChain application. Thanks in advance for your help!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/sharrajesh&quot;&gt; /u/sharrajesh &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ehggqs/adding_streaming_support_to_fastapi_langchain/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ehggqs/adding_streaming_support_to_fastapi_langchain/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1ehggqs</id><link href="https://www.reddit.com/r/LangChain/comments/1ehggqs/adding_streaming_support_to_fastapi_langchain/" /><updated>2024-08-01T13:18:27+00:00</updated><published>2024-08-01T13:18:27+00:00</published><title>Adding Streaming Support to FastAPI LangChain Application with Agents</title></entry><entry><author><name>/u/man_rech</name><uri>https://www.reddit.com/user/man_rech</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;I’m developing a natural language to SQL to natural language agent using LangGraph. While my agent can generate good responses with nice visualizations for some questions, it’s often inconsistent. I get different answers for the same question even though I have top_p set to 1 and temperature set to 0.&lt;/p&gt; &lt;p&gt;Here are some details:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The process involves several steps before creating the query and composing the response like watching the db schema, executing tools and reflecting on the temporary response. &lt;/li&gt; &lt;li&gt;The median token utilization per run is around 40k tokens, except for very basic questions. Is this context too large?&lt;/li&gt; &lt;li&gt;Occasionally, the agent doesn’t follow my instructions. Sometimes changing the wording slightly improves performance, but I have to do so many attempts that I get crazy. Is it normal?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The database structure:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;2 tables with around 300 ish columns each&lt;/li&gt; &lt;li&gt;3 smaller tables with around 10 ish columns each&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Am I on the right track, or is there a fundamental issue, possibly related to the large context size? Any insights or suggestions would be greatly appreciated!&lt;/p&gt; &lt;p&gt;Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/man_rech&quot;&gt; /u/man_rech &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ehhzzq/need_help_with_nl_to_sql_agent_inconsistent/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ehhzzq/need_help_with_nl_to_sql_agent_inconsistent/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1ehhzzq</id><link href="https://www.reddit.com/r/LangChain/comments/1ehhzzq/need_help_with_nl_to_sql_agent_inconsistent/" /><updated>2024-08-01T14:25:12+00:00</updated><published>2024-08-01T14:25:12+00:00</published><title>Need Help with NL to SQL Agent: Inconsistent Responses and Large Context Issues</title></entry><entry><author><name>/u/Best_Sail5</name><uri>https://www.reddit.com/user/Best_Sail5</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hello guys,&lt;/p&gt; &lt;p&gt;I have a question concerning how to prompt a model.&lt;br/&gt; I&amp;#39;m currently using LLaMa 3.1 to interact with a tool. The model is given an objective and generate multiple rounds of tool input to achieve it.&lt;br/&gt; Currently i&amp;#39;m simply using the following format:&lt;/p&gt; &lt;p&gt;ChatPromptTemplate([(&amp;#39;system&amp;#39;,system_prompt),(&amp;#39;user&amp;#39;,user_prompt)])&lt;br/&gt; where user_prompt contains the previous rounds of him generating commands and tool output like this:&lt;br/&gt; user_prompt=&amp;quot;&amp;quot;&amp;quot;&lt;br/&gt; {prompt}&lt;br/&gt; previous commands executed:{previous_rounds_of_tool_call}&lt;br/&gt; &amp;quot;&amp;quot;&amp;quot;&lt;br/&gt; This is inspired of ReAct prompt formatting.&lt;/p&gt; &lt;p&gt;But I&amp;#39;m thinking about changing that to prompt him in the following format:&lt;br/&gt; ChatPromptTemplate([(&amp;#39;system&amp;#39;,system_prompt),(&amp;#39;user&amp;#39;,user_prompt)&lt;br/&gt; ,(&amp;#39;tool&amp;#39;,tool_message),(&amp;#39;user&amp;#39;,user_prompt).....])&lt;/p&gt; &lt;p&gt;adding each turns as separate message.&lt;br/&gt; I would like to know if someone already used that? Does it change something ? How to do the training with multi steps setup like that?simply train each step separately?&lt;/p&gt; &lt;p&gt;Thanks for your advices!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Best_Sail5&quot;&gt; /u/Best_Sail5 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1eheoc1/multiple_turns_prompting_strategy/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1eheoc1/multiple_turns_prompting_strategy/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1eheoc1</id><link href="https://www.reddit.com/r/LangChain/comments/1eheoc1/multiple_turns_prompting_strategy/" /><updated>2024-08-01T11:49:44+00:00</updated><published>2024-08-01T11:49:44+00:00</published><title>Multiple turns prompting strategy</title></entry><entry><author><name>/u/mehul_gupta1997</name><uri>https://www.reddit.com/user/mehul_gupta1997</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/mehul_gupta1997&quot;&gt; /u/mehul_gupta1997 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;/r/ArtificialInteligence/comments/1ehgr4f/graphrag_vs_rag_which_one_is_better/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ehgtik/graphrag_vs_rag_which_one_is_better/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1ehgtik</id><link href="https://www.reddit.com/r/LangChain/comments/1ehgtik/graphrag_vs_rag_which_one_is_better/" /><updated>2024-08-01T13:34:48+00:00</updated><published>2024-08-01T13:34:48+00:00</published><title>GraphRAG vs RAG: Which one is better?</title></entry><entry><author><name>/u/maniac_runner</name><uri>https://www.reddit.com/user/maniac_runner</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1eh73u7/github_pytorchtorchchat_run_pytorch_llms_locally/&quot;&gt; &lt;img src=&quot;https://external-preview.redd.it/KnyLvUQLSlhqwDrJ5al_7_sHY4CasKEA7RvCEIrbcO0.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=70859b752fa3814100646807363b0b2e1ac8d99c&quot; alt=&quot;GitHub - pytorch/torchchat: Run PyTorch LLMs locally on servers, desktop and mobile&quot; title=&quot;GitHub - pytorch/torchchat: Run PyTorch LLMs locally on servers, desktop and mobile&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/maniac_runner&quot;&gt; /u/maniac_runner &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://github.com/pytorch/torchchat&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1eh73u7/github_pytorchtorchchat_run_pytorch_llms_locally/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1eh73u7</id><media:thumbnail url="https://external-preview.redd.it/KnyLvUQLSlhqwDrJ5al_7_sHY4CasKEA7RvCEIrbcO0.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=70859b752fa3814100646807363b0b2e1ac8d99c" /><link href="https://www.reddit.com/r/LangChain/comments/1eh73u7/github_pytorchtorchchat_run_pytorch_llms_locally/" /><updated>2024-08-01T03:48:53+00:00</updated><published>2024-08-01T03:48:53+00:00</published><title>GitHub - pytorch/torchchat: Run PyTorch LLMs locally on servers, desktop and mobile</title></entry><entry><author><name>/u/Longjumping-Try1191</name><uri>https://www.reddit.com/user/Longjumping-Try1191</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hello, does anyone know of a model capable of understanding videos and answering my questions via an LLM? Something like Video-LLaMA but that allows commercial use? Thank you very much to those who take the time to respond.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Longjumping-Try1191&quot;&gt; /u/Longjumping-Try1191 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ehgnig/looking_for_a_video_understanding_model_with/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ehgnig/looking_for_a_video_understanding_model_with/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1ehgnig</id><link href="https://www.reddit.com/r/LangChain/comments/1ehgnig/looking_for_a_video_understanding_model_with/" /><updated>2024-08-01T13:27:03+00:00</updated><published>2024-08-01T13:27:03+00:00</published><title>Looking for a Video Understanding Model with Commercial Use License</title></entry><entry><author><name>/u/Jen1888Mik</name><uri>https://www.reddit.com/user/Jen1888Mik</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hello. I use convex and langchain to create my rag application. In my case i need add to context specific data from my db - logic my application is serching some embeddings by id from my vectoredb then i use its embeddings for generate answer - but if i just add to context from RunnableSequence my embeddings i need use some format or embeddings are enough for this? I dont find this example in langchain documentation&lt;/p&gt; &lt;p&gt;&lt;code&gt;const vectorStore = new ConvexVectorStore(new OpenAIEmbeddings(), { ctx });&lt;/code&gt;&lt;br/&gt; &lt;code&gt;const llm = new ChatOpenAI({&lt;/code&gt;&lt;br/&gt; &lt;code&gt;apiKey: process.env.OPENAI_API_KEY,&lt;/code&gt;&lt;br/&gt; &lt;code&gt;model: &amp;quot;gpt-3.5-turbo&amp;quot;,&lt;/code&gt;&lt;br/&gt; &lt;code&gt;temperature: 0,&lt;/code&gt;&lt;br/&gt; &lt;code&gt;});&lt;/code&gt;&lt;br/&gt; &lt;code&gt;const parcer = new StringOutputParser();&lt;/code&gt;&lt;br/&gt; &lt;code&gt;const retriever = vectorStore.asRetriever()&lt;/code&gt;&lt;br/&gt; &lt;code&gt;const prompt = await pull&amp;lt;ChatPromptTemplate&amp;gt;(&amp;quot;rlm/rag-prompt&amp;quot;);&lt;/code&gt;&lt;br/&gt; &lt;code&gt;const ragChain = RunnableSequence.from([&lt;/code&gt;&lt;br/&gt; &lt;code&gt;{&lt;/code&gt;&lt;br/&gt; &lt;code&gt;context: retriever.pipe(formatDocumentsAsString) ,&lt;/code&gt;&lt;br/&gt; &lt;code&gt;question: new RunnablePassthrough(),&lt;/code&gt;&lt;br/&gt; &lt;code&gt;},&lt;/code&gt;&lt;br/&gt; &lt;code&gt;prompt,&lt;/code&gt;&lt;br/&gt; &lt;code&gt;llm,&lt;/code&gt;&lt;br/&gt; &lt;code&gt;parcer,&lt;/code&gt;&lt;br/&gt; &lt;code&gt;]);&lt;/code&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Jen1888Mik&quot;&gt; /u/Jen1888Mik &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ehggmb/how_to_add_specific_source_from_db_to/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ehggmb/how_to_add_specific_source_from_db_to/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1ehggmb</id><link href="https://www.reddit.com/r/LangChain/comments/1ehggmb/how_to_add_specific_source_from_db_to/" /><updated>2024-08-01T13:18:17+00:00</updated><published>2024-08-01T13:18:17+00:00</published><title>How to add specific source from db to RunnableSequence using Langchain</title></entry><entry><author><name>/u/Traditional_Art_6943</name><uri>https://www.reddit.com/user/Traditional_Art_6943</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi guys I have created a PDF Chat/ Web Search RAG application deployed on Hugging Face Spaces &lt;a href=&quot;https://shreyas094-searchgpt.hf.space&quot;&gt;https://shreyas094-searchgpt.hf.space&lt;/a&gt;. Providing the model documentation below please feel free to contribute.&lt;/p&gt; &lt;h1&gt;AI-powered Web Search and PDF Chat Assistant&lt;/h1&gt; &lt;p&gt;This project combines the power of large language models with web search capabilities and PDF document analysis to create a versatile chat assistant. Users can interact with their uploaded PDF documents or leverage web search to get informative responses to their queries.&lt;/p&gt; &lt;h2&gt;Features&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;PDF Document Chat&lt;/strong&gt;: Upload and interact with multiple PDF documents.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Web Search Integration&lt;/strong&gt;: Option to use web search for answering queries.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Multiple AI Models&lt;/strong&gt;: Choose from a selection of powerful language models.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Customizable Responses&lt;/strong&gt;: Adjust temperature and API call settings for fine-tuned outputs.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;User-friendly Interface&lt;/strong&gt;: Built with Gradio for an intuitive chat experience.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Document Selection&lt;/strong&gt;: Choose which uploaded documents to include in your queries.&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;How It Works&lt;/h2&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;Document Processing&lt;/strong&gt;: &lt;/p&gt; &lt;ul&gt; &lt;li&gt;Upload PDF documents using either PyPDF or LlamaParse.&lt;/li&gt; &lt;li&gt;Documents are processed and stored in a FAISS vector database for efficient retrieval.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;Embedding&lt;/strong&gt;: &lt;/p&gt; &lt;ul&gt; &lt;li&gt;Utilizes HuggingFace embeddings (default: &amp;#39;sentence-transformers/all-mpnet-base-v2&amp;#39;) for document indexing and query matching.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;Query Processing&lt;/strong&gt;:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;For PDF queries, relevant document sections are retrieved from the FAISS database.&lt;/li&gt; &lt;li&gt;For web searches, results are fetched using the DuckDuckGo search API.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;Response Generation&lt;/strong&gt;:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Queries are processed using the selected AI model (options include Mistral, Mixtral, and others).&lt;/li&gt; &lt;li&gt;Responses are generated based on the retrieved context (from PDFs or web search).&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;User Interaction&lt;/strong&gt;:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Users can chat with the AI, asking questions about uploaded documents or general queries.&lt;/li&gt; &lt;li&gt;The interface allows for adjusting model parameters and switching between PDF and web search modes.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ol&gt; &lt;h2&gt;Setup and Usage&lt;/h2&gt; &lt;ol&gt; &lt;li&gt;Install the required dependencies (list of dependencies to be added).&lt;/li&gt; &lt;li&gt;Set up the necessary API keys and tokens in your environment variables.&lt;/li&gt; &lt;li&gt;Run the main script to launch the Gradio interface.&lt;/li&gt; &lt;li&gt;Upload PDF documents using the file input at the top of the interface.&lt;/li&gt; &lt;li&gt;Select documents to query using the checkboxes.&lt;/li&gt; &lt;li&gt;Toggle between PDF chat and web search modes as needed.&lt;/li&gt; &lt;li&gt;Adjust temperature and number of API calls to fine-tune responses.&lt;/li&gt; &lt;li&gt;Start chatting and asking questions!&lt;/li&gt; &lt;/ol&gt; &lt;h2&gt;Models&lt;/h2&gt; &lt;p&gt;The project supports multiple AI models, including: - mistralai/Mistral-7B-Instruct-v0.3 - mistralai/Mixtral-8x7B-Instruct-v0.1 - meta/llama-3.1-8b-instruct - mistralai/Mistral-Nemo-Instruct-2407&lt;/p&gt; &lt;h2&gt;Future Improvements&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;Integration of more embedding models for improved performance.&lt;/li&gt; &lt;li&gt;Enhanced PDF parsing capabilities.&lt;/li&gt; &lt;li&gt;Support for additional file formats beyond PDF.&lt;/li&gt; &lt;li&gt;Improved caching for faster response times.&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;Contribution&lt;/h2&gt; &lt;p&gt;Contributions to this project are welcome!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Traditional_Art_6943&quot;&gt; /u/Traditional_Art_6943 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1egyn3g/rag_pdf_chat_web_search/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1egyn3g/rag_pdf_chat_web_search/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1egyn3g</id><link href="https://www.reddit.com/r/LangChain/comments/1egyn3g/rag_pdf_chat_web_search/" /><updated>2024-07-31T21:14:26+00:00</updated><published>2024-07-31T21:14:26+00:00</published><title>RAG PDF Chat + Web Search</title></entry><entry><author><name>/u/o3omoomin</name><uri>https://www.reddit.com/user/o3omoomin</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I participated in the nvidia gen ai contest on June 18th in Korea time, or maybe June 17th in Pacific time.&lt;/p&gt; &lt;p&gt;And I&amp;#39;ve been waiting for today. But the result email and certificate haven&amp;#39;t arrived to me. What&amp;#39;s the reason?&lt;/p&gt; &lt;p&gt;Or is there anyone who is experiencing the same situation as me?&lt;/p&gt; &lt;p&gt;I also talked about this issue in the nvidia developer discord channel, but the people involved don&amp;#39;t input any chat.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/o3omoomin&quot;&gt; /u/o3omoomin &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ehajmd/i_did_not_receive_the_results_email_from_nvidia/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ehajmd/i_did_not_receive_the_results_email_from_nvidia/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1ehajmd</id><link href="https://www.reddit.com/r/LangChain/comments/1ehajmd/i_did_not_receive_the_results_email_from_nvidia/" /><updated>2024-08-01T07:21:44+00:00</updated><published>2024-08-01T07:21:44+00:00</published><title>I did not receive the results email from Nvidia generative AI contset. Also the certificate.</title></entry><entry><author><name>/u/No_Storm5504</name><uri>https://www.reddit.com/user/No_Storm5504</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I have been deloyed an langgraph app which running at LangServe. One question is that I have a feature which required human-in-the-loop that&amp;#39;s send a None Type to the langgraph, that&amp;#39;s the way to continue langgraph execution. I know how to do it in Python SDK but still no clues in LangServe client way，which I mimic request with Python &lt;code&gt;requests&lt;/code&gt; through payloads. &lt;/p&gt; &lt;p&gt;Any good ideas?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/No_Storm5504&quot;&gt; /u/No_Storm5504 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1eh9wwv/how_to_pass_none_type_as_an_input_to_langgraph/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1eh9wwv/how_to_pass_none_type_as_an_input_to_langgraph/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1eh9wwv</id><link href="https://www.reddit.com/r/LangChain/comments/1eh9wwv/how_to_pass_none_type_as_an_input_to_langgraph/" /><updated>2024-08-01T06:39:08+00:00</updated><published>2024-08-01T06:39:08+00:00</published><title>How to pass None Type as an input to LangGraph which deployed in LangServe</title></entry><entry><author><name>/u/jscraft</name><uri>https://www.reddit.com/user/jscraft</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi folks,&lt;/p&gt; &lt;p&gt;Exciting news! Two weeks ago, I had the pleasure of recording a podcast interview with Jacob Lee, the lead maintainer of LangChain.js. &lt;/p&gt; &lt;p&gt;Check it out here: &lt;a href=&quot;https://www.js-craft.io/blog/interview-jacob-lee-langchain-js/&quot;&gt;https://www.js-craft.io/blog/interview-jacob-lee-langchain-js/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;During this short talk, we go through topics such as:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The advantages of using LangChain&lt;/li&gt; &lt;li&gt;A good roadmap for learning LangChain&lt;/li&gt; &lt;li&gt;How all the Langs work together (LangGraph, LangSmith, LangServe, LangChain itself)&lt;/li&gt; &lt;li&gt;Using LangChain.js with or without TypeScript &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;... and much more.&lt;/p&gt; &lt;p&gt;Listen now:: &lt;a href=&quot;https://www.js-craft.io/blog/interview-jacob-lee-langchain-js/&quot;&gt;https://www.js-craft.io/blog/interview-jacob-lee-langchain-js/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hope you will like it, and happy to hear your opinions! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/jscraft&quot;&gt; /u/jscraft &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1egmw06/interview_with_jacob_lee_lead_maintainer_of/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1egmw06/interview_with_jacob_lee_lead_maintainer_of/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1egmw06</id><link href="https://www.reddit.com/r/LangChain/comments/1egmw06/interview_with_jacob_lee_lead_maintainer_of/" /><updated>2024-07-31T13:14:34+00:00</updated><published>2024-07-31T13:14:34+00:00</published><title>Interview with Jacob Lee, lead maintainer of LangChain.js</title></entry><entry><author><name>/u/Alternative_Spell981</name><uri>https://www.reddit.com/user/Alternative_Spell981</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;In building RAG systems, there&amp;#39;s a consensus that when documents have clear and accurate titles and hierarchies, it&amp;#39;s better to segment based on semantic understanding using subtitles and paragraphs rather than traditional chunking by length. &lt;/p&gt; &lt;p&gt;This enhances the system&amp;#39;s retrieval and overall performance in QA tasks.&lt;/p&gt; &lt;p&gt;Currently, accurately and consistently identifying primary, secondary, tertiary, and other subtitles is challenging in document parsing, due to varying title formats across different long document types and the semantic ambiguity of some titles.&lt;/p&gt; &lt;p&gt;For instance, TextIn&amp;#39;s achieved good results in title hierarchy recognition and directory tree construction for documents like annual reports, financial statements, and research reports. However, performance still needs optimization for documents with less consistent formats.&lt;/p&gt; &lt;p&gt;Accurately reconstructing title hierarchies is a difficult yet highly beneficial task for downstream processes. Therefore, recognizing document directory trees is a key focus in our parsing efforts. &lt;/p&gt; &lt;p&gt;We welcome users with high precision requirements to discuss application scenarios and try TextIn&amp;#39;s doc parser to experience the latest advancements firsthand!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Alternative_Spell981&quot;&gt; /u/Alternative_Spell981 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1eh66kc/how_to_better_recognize_hierarchical_titles_in/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1eh66kc/how_to_better_recognize_hierarchical_titles_in/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1eh66kc</id><link href="https://www.reddit.com/r/LangChain/comments/1eh66kc/how_to_better_recognize_hierarchical_titles_in/" /><updated>2024-08-01T02:59:24+00:00</updated><published>2024-08-01T02:59:24+00:00</published><title>How to Better Recognize Hierarchical Titles in Document Parsing？</title></entry><entry><author><name>/u/unreal_4567</name><uri>https://www.reddit.com/user/unreal_4567</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hello everyone. Im building an application using the langchain create_sql_agent constructor: langchain_community.agent_toolkits.sql.base.create_sql_agent, for a strange use case that requires finding out which table in an SQL database a particular dataset(which I feed in the prompt as key value pairs of column headers and corresponding 5 values as a list) most resembles. Ive written prompts asking the agent to use the column headers to guess which table the dataset resembles. This is happening with llama 3 8b running via ollama. &lt;/p&gt; &lt;p&gt;The problem is I keep getting an error message which im recalling from my memory like:&lt;/p&gt; &lt;p&gt;ValueError: An outputparsing error occured. In order to pass this back to the agent and have it try again, pass handle_parsing_errors = True to the Agent executor. &lt;/p&gt; &lt;p&gt;However, the create_sql_agent constructor does not even have handle_parsing_errors as a parameter! Anyone have any idea how to resolve this? Im sure i must be getting something wrong. &lt;/p&gt; &lt;p&gt;For context, I&amp;#39;ve worked with the AgentExecutor class before which has a parameter handle_parsing_errors, which worked well, but for this specific use case, I need the sql agent. Is there a way to call the sql agent using the AgentExecutor? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/unreal_4567&quot;&gt; /u/unreal_4567 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1egub1z/create_sql_agent_issue/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1egub1z/create_sql_agent_issue/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1egub1z</id><link href="https://www.reddit.com/r/LangChain/comments/1egub1z/create_sql_agent_issue/" /><updated>2024-07-31T18:19:22+00:00</updated><published>2024-07-31T18:19:22+00:00</published><title>Create_sql_agent issue</title></entry><entry><author><name>/u/bferreira85</name><uri>https://www.reddit.com/user/bferreira85</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi, right now my graph is not displaying to the user most of the messages that AI generate as it goes through the graph. This is specially bad in steps when I am getting the user confirmation for something, but it would be nice to display some messages as the llm moves from one node to another.&lt;br/&gt; What is the best practice for that? I&amp;#39;ve been doing console.print for displaying the last message in the message array in certain parts of the code, but I guess that&amp;#39;s not the best way to solve it. How do you usually do it? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/bferreira85&quot;&gt; /u/bferreira85 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1egjtsg/langgraph_what_is_the_best_practice_for/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1egjtsg/langgraph_what_is_the_best_practice_for/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1egjtsg</id><link href="https://www.reddit.com/r/LangChain/comments/1egjtsg/langgraph_what_is_the_best_practice_for/" /><updated>2024-07-31T10:32:36+00:00</updated><published>2024-07-31T10:32:36+00:00</published><title>Langgraph: What is the best practice for displaying messages to the user as the we move through the graph?</title></entry><entry><author><name>/u/sidtalesara01</name><uri>https://www.reddit.com/user/sidtalesara01</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;So I am creating a software it scrapes the website, cleans the unwanted html like img, meta tags, script, style etc. and now I have a clean html file. Now I have a user description on what he wants from that url for example he want to submit a form. then only the form and login form component is important and significant. now I don&amp;#39;t want to send the whole html code but in a way trim it down to more relevant code which in this case is only code related to login. maybe omit html code of header, footer and other unwanted stuff. Now want some guidance how I can achieve this?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/sidtalesara01&quot;&gt; /u/sidtalesara01 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1egt6tz/how_to_send_relevant_data_to_llm/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1egt6tz/how_to_send_relevant_data_to_llm/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1egt6tz</id><link href="https://www.reddit.com/r/LangChain/comments/1egt6tz/how_to_send_relevant_data_to_llm/" /><updated>2024-07-31T17:35:09+00:00</updated><published>2024-07-31T17:35:09+00:00</published><title>how to send relevant data to llm?</title></entry><entry><author><name>/u/Ignorance998</name><uri>https://www.reddit.com/user/Ignorance998</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;ps: This is a repost (2 days ago). Reddit decided to shadow-ban my previous new account simply because i have posted this. They mark it as &amp;quot;scam&amp;quot;. I hope they will not do so again this time, like this is using a open source license and i didn&amp;#39;t get any commercial benefit from it.&lt;/p&gt; &lt;h1&gt;Introduction (skip this if you like)&lt;/h1&gt; &lt;p&gt;I am an intermediate self-taught python coder with no formal CS experience. I have spent 5 months for this and learnt a lot when writing this project. I have never written anything this complicated before, and I have rewrite this project from scratch at least several times. There are many smaller-scale rewrite when i am not satisfied with the structure of anything. I hope it is useful for somebody. (Also warning, this might not be the most professional piece of code) Any feedback is appreciated!&lt;/p&gt; &lt;h1&gt;What My Project Does&lt;/h1&gt; &lt;p&gt;GPT Graph is a pipeline for llm data transfer. When I first studied LangChain, I don&amp;#39;t understand why we need a server(langsmith) to do debug, and things get so complicated. Therefore, i have spent time in order to write a pipeline structure targeting being flexible and easy to debug. While it&amp;#39;s still in early development and far less sophisticated as Langchain, I think my idea is better at least in some way in turns of how to abstract things (maybe i am wrong).&lt;/p&gt; &lt;p&gt;This library allows you to create more complex pipelines with features like dynamic caching, conditional execution, and easy debugging.&lt;/p&gt; &lt;p&gt;The main features of GPT Graph include:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Component-based pipelines&lt;/li&gt; &lt;li&gt;Allowing nested Pipeline&lt;/li&gt; &lt;li&gt;Dynamic caching according to defined keys&lt;/li&gt; &lt;li&gt;Conditional execution of components using bindings or linkings&lt;/li&gt; &lt;li&gt;Debugging and analysis methods&lt;/li&gt; &lt;li&gt;Priority Queue to run Steps in the Pipeline&lt;/li&gt; &lt;li&gt;Parameters can be updated with priority score. (e.g. if a Pipeline contains 4 Components, you can write config files for each of the Component and Pipeline, as Pipeline has higher priority than each component, if there are any conflict in parameters, the parent Pipeline&amp;#39;s parameters will be used)&lt;/li&gt; &lt;li&gt;One of the key advantages of GPT Graph is its debuggability. Every output is stored in a node (a dict with structure {&amp;quot;content&amp;quot;:xxx, “extra”:xxx})&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;The following features are lacking (They are all TODO in the future)&lt;/p&gt; &lt;ol&gt; &lt;li&gt;currently all are using sync mode&lt;/li&gt; &lt;li&gt;No database is used at this moment. All data stored in networkx graph&amp;#39;s wrapper.&lt;/li&gt; &lt;li&gt;No RAG at this moment. Although I have already written some prototype for it, basically calculate the vector and store in the nodes. They are not submitted yet.&lt;/li&gt; &lt;/ol&gt; &lt;h1&gt;Example&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;from gpt_graph.core.pipeline import Pipeline from gpt_graph.core.decorators.component import component @component() def greet(x): return x + &amp;quot; world!&amp;quot; pipeline = Pipeline() pipeline | greet() result = pipeline.run(input_data=&amp;quot;Hello&amp;quot;) print(result) # Output: [&amp;#39;Hello world!&amp;#39;] &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Comparison&lt;/h1&gt; &lt;p&gt;As for as I know and my understanding(which may be wrong)(e.g. Langgraph or Langchain), there is no framework that can do nested pipeline, or using priority queue.&lt;/p&gt; &lt;h1&gt;Target Audience&lt;/h1&gt; &lt;p&gt;Fast prototyping and small project related to llm data pipelines. It is because currently everything is stored as a wrapper of networkx graph (including outputs of each Step and step structure). Later I may write implementation for graph database, although I don&amp;#39;t have the skill now.&lt;/p&gt; &lt;h1&gt;Welcome Feedback and Contributions&lt;/h1&gt; &lt;p&gt;I welcome any comments, recommendations, or contributions from the community.&lt;br/&gt; I know that as someone that releases his first complicated project (at least for me), there may be a lot of things that i am not doing correctly, including documentations/ writing style/ testing or others. So any recommendation is encouraged! Your feedback will be invaluable for me.&lt;br/&gt; If you have any questions about the project, feel free to ask me as well. My documentation may not be the easiest to understand. I will soon take a long holiday for several months, and when I come back I will try to enhance this project to a better and usable level.&lt;br/&gt; The license now is GPL v3, if more people feel interested in or contribute to the project, i will consider change it to more permissive license.&lt;/p&gt; &lt;h1&gt;Link to Github&lt;/h1&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/Ignorance999/gpt_graph&quot;&gt;https://github.com/Ignorance999/gpt_graph&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Link to Documentation&lt;/h1&gt; &lt;p&gt;&lt;a href=&quot;https://gpt-graph.readthedocs.io/en/latest/hello_world.html&quot;&gt;https://gpt-graph.readthedocs.io/en/latest/hello_world.html&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;More Advanced Example (you can check documentation tutorial 1 Basics):&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;class z: def __init__(self): self.z = 0 def run(self): self.z += 1 return self.z @component( step_type=&amp;quot;node_to_list&amp;quot;, cache_schema={ &amp;quot;z&amp;quot;: { &amp;quot;key&amp;quot;: &amp;quot;[cp_or_pp.name]&amp;quot;, &amp;quot;initializer&amp;quot;: lambda: z(), } }, ) def f4(x, z, y=1): return x + y + z.run(), x - y + z.run() @component(step_type=&amp;quot;list_to_node&amp;quot;) def f5(x): return np.sum(x) @component( step_type=&amp;quot;node_to_list&amp;quot;, cache_schema={&amp;quot;z&amp;quot;: {&amp;quot;key&amp;quot;: &amp;quot;[base_name]&amp;quot;, &amp;quot;initializer&amp;quot;: lambda: z()}}, ) def f6(x, z): return [x, x - z.run(), x - z.run()] s = Session() s.f4 = f4() s.f6 = f6() s.f5 = f5() s.p6 = s.f4 | s.f6 | s.f5 result = s.p6.run(input_data=10) # output: 59 &amp;quot;&amp;quot;&amp;quot; output: Step: p6;InputInitializer:sp0 text = 10 (2 characters) Step: p6;f4.0:sp0 text = 12 (2 characters) text = 11 (2 characters) Step: p6;f6.0:sp0 text = 12 (2 characters) text = 11 (2 characters) text = 10 (2 characters) text = 11 (2 characters) text = 8 (1 characters) text = 7 (1 characters) Step: p6;f5.0:sp0 text = 59 (2 characters) &amp;quot;&amp;quot;&amp;quot; &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Ignorance998&quot;&gt; /u/Ignorance998 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1egh70y/gpt_graph_a_flexible_pipeline_library/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1egh70y/gpt_graph_a_flexible_pipeline_library/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1egh70y</id><link href="https://www.reddit.com/r/LangChain/comments/1egh70y/gpt_graph_a_flexible_pipeline_library/" /><updated>2024-07-31T07:32:46+00:00</updated><published>2024-07-31T07:32:46+00:00</published><title>GPT Graph: A Flexible Pipeline Library</title></entry><entry><author><name>/u/ravediamond000</name><uri>https://www.reddit.com/user/ravediamond000</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hello everyone,&lt;/p&gt; &lt;p&gt;Just wrote an article on how to use LangServe to create an API over LangChain chains.&lt;br/&gt; Here&amp;#39;s the &lt;a href=&quot;https://www.metadocs.co/2024/07/31/easily-create-production-ready-apis-over-your-langchain-chains-using-langserve/&quot;&gt;link&lt;/a&gt;.&lt;br/&gt; This is actually something that I use in production in my company :D.&lt;/p&gt; &lt;p&gt;Enjoy!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/ravediamond000&quot;&gt; /u/ravediamond000 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1egn31o/create_robust_api_over_langchain_chains_using/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1egn31o/create_robust_api_over_langchain_chains_using/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1egn31o</id><link href="https://www.reddit.com/r/LangChain/comments/1egn31o/create_robust_api_over_langchain_chains_using/" /><updated>2024-07-31T13:23:29+00:00</updated><published>2024-07-31T13:23:29+00:00</published><title>Create robust API over Langchain chains using Langserve</title></entry><entry><author><name>/u/lat23_longitude0</name><uri>https://www.reddit.com/user/lat23_longitude0</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi Folks,&lt;/p&gt; &lt;p&gt;Apologies if the above question does not belong here. But I am create a RAG application, Basically it is a RAG application for stackoverflow / stackexchange questions and answers.&lt;/p&gt; &lt;p&gt;But my first dilemma is - is it legal to scrape answers from stackoverflow / stackexchange?&lt;/p&gt; &lt;p&gt;I am planning to provide links back to the original answer in my app.&lt;/p&gt; &lt;p&gt;Any suggestions or advice would be great.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/lat23_longitude0&quot;&gt; /u/lat23_longitude0 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1egvbm4/want_to_create_rag_application_for_stackoverflow/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1egvbm4/want_to_create_rag_application_for_stackoverflow/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1egvbm4</id><link href="https://www.reddit.com/r/LangChain/comments/1egvbm4/want_to_create_rag_application_for_stackoverflow/" /><updated>2024-07-31T19:00:31+00:00</updated><published>2024-07-31T19:00:31+00:00</published><title>Want to create RAG application for stackoverflow / stackexchange questions and answers. Is it legal to scrape answers from stackoverflow / stackexchange?</title></entry><entry><author><name>/u/dhj9817</name><uri>https://www.reddit.com/user/dhj9817</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/dhj9817&quot;&gt; /u/dhj9817 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;/r/documentAutomation/comments/1egjm4g/a_call_to_individuals_who_want_document/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1egobx3/a_call_to_individuals_who_want_document/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1egobx3</id><link href="https://www.reddit.com/r/LangChain/comments/1egobx3/a_call_to_individuals_who_want_document/" /><updated>2024-07-31T14:17:40+00:00</updated><published>2024-07-31T14:17:40+00:00</published><title>A call to individuals who want Document Automation as the future</title></entry><entry><author><name>/u/HopeAway7784</name><uri>https://www.reddit.com/user/HopeAway7784</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I need it to process documents for government agency. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/HopeAway7784&quot;&gt; /u/HopeAway7784 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1egcvsy/is_anyone_aware_of_a_good_ocr_model_that_can_be/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1egcvsy/is_anyone_aware_of_a_good_ocr_model_that_can_be/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1egcvsy</id><link href="https://www.reddit.com/r/LangChain/comments/1egcvsy/is_anyone_aware_of_a_good_ocr_model_that_can_be/" /><updated>2024-07-31T03:12:11+00:00</updated><published>2024-07-31T03:12:11+00:00</published><title>Is anyone aware of a good OCR model that can be used for document processing (Multi-language support with Hindi/ Indian languages)?</title></entry><entry><author><name>/u/mehul_gupta1997</name><uri>https://www.reddit.com/user/mehul_gupta1997</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/mehul_gupta1997&quot;&gt; /u/mehul_gupta1997 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;/r/learnmachinelearning/comments/1egiiw2/llama_31_fine_tuning_codes_explained/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1egikdw/llama_31_fine_tuning_codes_explained_using_unsloth/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1egikdw</id><link href="https://www.reddit.com/r/LangChain/comments/1egikdw/llama_31_fine_tuning_codes_explained_using_unsloth/" /><updated>2024-07-31T09:09:39+00:00</updated><published>2024-07-31T09:09:39+00:00</published><title>Llama 3.1 Fine Tuning codes explained using unsloth</title></entry><entry><author><name>/u/kingai404</name><uri>https://www.reddit.com/user/kingai404</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey everyone! I’m excited to share a new project: SWEKit, a powerful framework for building software engineering agents using the Composio tooling ecosystem.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Objectives&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;SWEKit allows you to:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Scaffold agents that work out-of-the-box with frameworks like CrewAI and LlamaIndex.&lt;/li&gt; &lt;li&gt;Add or optimize your agent&amp;#39;s abilities.&lt;/li&gt; &lt;li&gt;Benchmark your agents against SWE-Bench.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Implementation Details&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Tools Used&lt;/strong&gt;: Composio, CrewAI, Python&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Setup&lt;/strong&gt;:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Install agentic framework of your choice and the Composio plugin&lt;/li&gt; &lt;li&gt;The agent requires a github access token to work with your repositories&lt;/li&gt; &lt;li&gt;You also need to setup API key for the LLM provider you&amp;#39;re planning to use &lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;Scaffold and Run Your Agent&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Workspace Environment:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;SWEKit supports different workspace environments:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Host&lt;/strong&gt;: Run on the host machine.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Docker&lt;/strong&gt;: Run inside a Docker container.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;E2B&lt;/strong&gt;: Run inside an E2B Sandbox.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;FlyIO&lt;/strong&gt;: Run inside a FlyIO machine.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Running the Benchmark:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;SWE-Bench&lt;/strong&gt; evaluates the performance of software engineering agents using real-world issues from popular Python open-source projects.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href=&quot;https://git.new/SWE&quot;&gt;GitHub&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Feel free to explore the project, give it a star if you find it useful, and let me know your thoughts or suggestions for improvements! 🌟&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/kingai404&quot;&gt; /u/kingai404 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ege590/i_was_working_on_this_for_a_long_time_a_swe_kit/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ege590/i_was_working_on_this_for_a_long_time_a_swe_kit/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1ege590</id><link href="https://www.reddit.com/r/LangChain/comments/1ege590/i_was_working_on_this_for_a_long_time_a_swe_kit/" /><updated>2024-07-31T04:20:02+00:00</updated><published>2024-07-31T04:20:02+00:00</published><title>I was working on this for a long time - a SWE Kit that simplifies SWE Agent Creation</title></entry></feed>