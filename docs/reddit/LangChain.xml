<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/"><category term="LangChain" label="r/LangChain"/><updated>2024-07-25T16:21:11+00:00</updated><icon>https://www.redditstatic.com/icon.png/</icon><id>/r/LangChain.rss</id><link rel="self" href="https://www.reddit.com/r/LangChain.rss" type="application/atom+xml" /><link rel="alternate" href="https://www.reddit.com/r/LangChain" type="text/html" /><subtitle>LangChain is an open-source framework and developer toolkit that helps developers get LLM applications from prototype to production. It is available for Python and Javascript at https://www.langchain.com/.</subtitle><title>LangChain</title><entry><author><name>/u/zchaarm</name><uri>https://www.reddit.com/user/zchaarm</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;A place for members of &lt;a href=&quot;/r/LangChain&quot;&gt;r/LangChain&lt;/a&gt; to chat with each other&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/zchaarm&quot;&gt; /u/zchaarm &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_10ljho9</id><link href="https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/" /><updated>2023-01-26T04:33:05+00:00</updated><published>2023-01-26T04:33:05+00:00</published><title>r/LangChain Lounge</title></entry><entry><author><name>/u/phan_ngt</name><uri>https://www.reddit.com/user/phan_ngt</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I use Semantic Chunker from this tutorial: &lt;a href=&quot;https://python.langchain.com/v0.2/docs/how_to/semantic-chunker/&quot;&gt;https://python.langchain.com/v0.2/docs/how_to/semantic-chunker/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;However, I met the error below. I think because my pdf has 64 pages. Too long for OpenAI to handle. What should I do? If I split page by page, I am afraid that I will lost the content between pages. Recursive Chunker seems better in this case. &lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;openai.InternalServerError: Error code: 503 - {&amp;#39;error&amp;#39;: {&amp;#39;code&amp;#39;: &amp;#39;InternalServerError&amp;#39;, &amp;#39;message&amp;#39;: &amp;#39;The service is temporarily unable to process your request. Please try again later.&amp;#39;}}&lt;/p&gt; &lt;p&gt;python-BaseException&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/phan_ngt&quot;&gt; /u/phan_ngt &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ebs25f/semanticchunker_for_very_large_text/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ebs25f/semanticchunker_for_very_large_text/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1ebs25f</id><link href="https://www.reddit.com/r/LangChain/comments/1ebs25f/semanticchunker_for_very_large_text/" /><updated>2024-07-25T11:02:03+00:00</updated><published>2024-07-25T11:02:03+00:00</published><title>SemanticChunker for very large text</title></entry><entry><author><name>/u/PretendVermicelli657</name><uri>https://www.reddit.com/user/PretendVermicelli657</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;m new to langchain and currently learning the official [tutorial](&lt;a href=&quot;https://python.langchain.com/v0.2/docs/tutorials/agents/&quot;&gt;https://python.langchain.com/v0.2/docs/tutorials/agents/&lt;/a&gt;). I have tried Ollama and llama.cpp, but none of them can finish the tutorial.&lt;/p&gt; &lt;p&gt;As known, Ollama doesn&amp;#39;t support bind_tools originally. With the help of OllamaFunctions in langchain_experiment package, it worked and outputed similar intermediate information but failed when generating text according to response from tools.&lt;/p&gt; &lt;p&gt;When it comes to llama.cpp, it does have bind_tools function. The problem is that it didn&amp;#39;t generate text according to response from tools.&lt;/p&gt; &lt;p&gt;So, is there a way to go through the tutorials with local llms or an example about finishing those tutorials with Ollama and llama.cpp? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/PretendVermicelli657&quot;&gt; /u/PretendVermicelli657 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ebundi/how_to_build_agent_with_local_llm/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ebundi/how_to_build_agent_with_local_llm/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1ebundi</id><link href="https://www.reddit.com/r/LangChain/comments/1ebundi/how_to_build_agent_with_local_llm/" /><updated>2024-07-25T13:17:22+00:00</updated><published>2024-07-25T13:17:22+00:00</published><title>How to build agent with local llm</title></entry><entry><author><name>/u/divinity27</name><uri>https://www.reddit.com/user/divinity27</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi I am trying to extract information from purchase orders PDFs with different formats , when conventional py libraries didn&amp;#39;t extract the data the way I wanted I resorted to Azure Gpt 4 vision model and converted the pages of my pdf as images and used the api to get back the response. The problem is in some documents it is deliberately missing clearly written information in the images , I tried tweaking the prompt as well. But not helping much. I am using pdf2image to convert to JPEGs and using 500 dpi as parameter in the convert_from_path function imported from library. Any recommendations or help would be much appreciated.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/divinity27&quot;&gt; /u/divinity27 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ebt2gl/improving_output_of_azure_gpt_4_vision_model/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ebt2gl/improving_output_of_azure_gpt_4_vision_model/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1ebt2gl</id><link href="https://www.reddit.com/r/LangChain/comments/1ebt2gl/improving_output_of_azure_gpt_4_vision_model/" /><updated>2024-07-25T11:59:16+00:00</updated><published>2024-07-25T11:59:16+00:00</published><title>Improving output of Azure Gpt 4 vision model , ignoring part of text present in image</title></entry><entry><author><name>/u/mallerius</name><uri>https://www.reddit.com/user/mallerius</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey I don&amp;#39;t know if this is the right sub. I rented a server that uses a Rtx 4000 with 20gb. I tried to get models like mistral or llamma to run on it but it fails to generate answers because it runs out of memory. Are there anyways to reduce the amount of memory needed? Or other ways to solve this problem? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/mallerius&quot;&gt; /u/mallerius &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ebsdbv/video_ram_problems_on_a_server_with_a_rtx_4000/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ebsdbv/video_ram_problems_on_a_server_with_a_rtx_4000/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1ebsdbv</id><link href="https://www.reddit.com/r/LangChain/comments/1ebsdbv/video_ram_problems_on_a_server_with_a_rtx_4000/" /><updated>2024-07-25T11:20:08+00:00</updated><published>2024-07-25T11:20:08+00:00</published><title>Video RAM Problems on a server with a Rtx 4000 20gb</title></entry><entry><author><name>/u/AdAway2620</name><uri>https://www.reddit.com/user/AdAway2620</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hello all, Have anyone among here done sales forecasting using LLMs ?&lt;br/&gt; For eg: I have monthly sales data of last 2 years and i want to predict the monthly sales of upcoming year.&lt;br/&gt; What would be the best way to do it ?&lt;/p&gt; &lt;p&gt;If anyone has code snippet, I would be happy to look at it.&lt;br/&gt; I welcome ML/DL approach as well but since my dataset is very low what would be the best idea ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/AdAway2620&quot;&gt; /u/AdAway2620 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ebm8ss/salesforecasting_using_ai_models_llm/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ebm8ss/salesforecasting_using_ai_models_llm/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1ebm8ss</id><link href="https://www.reddit.com/r/LangChain/comments/1ebm8ss/salesforecasting_using_ai_models_llm/" /><updated>2024-07-25T04:42:39+00:00</updated><published>2024-07-25T04:42:39+00:00</published><title>Salesforecasting using AI models / LLM</title></entry><entry><author><name>/u/BigYesterday2785</name><uri>https://www.reddit.com/user/BigYesterday2785</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Why does EmbeddingStoreContentRetriever Not output directly Score in Java Langchain ? &lt;/p&gt; &lt;p&gt;It tells us to output, based on minScore, but no possibility to get score directly? Why is it ?&lt;/p&gt; &lt;p&gt;how would I go about implementing it in java or am I missing something&lt;/p&gt; &lt;p&gt;How can I get this score ?&lt;/p&gt; &lt;p&gt;this is how it looks like&lt;/p&gt; &lt;p&gt;&lt;code&gt;public class EmbeddingStoreContentRetriever implements ContentRetriever&lt;/code&gt; &lt;/p&gt; &lt;p&gt;&lt;code&gt;{&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;private final EmbeddingStore&amp;lt;TextSegment&amp;gt; embeddingStore;&lt;/code&gt; &lt;/p&gt; &lt;p&gt;&lt;code&gt;private final EmbeddingModel embeddingModel;&lt;/code&gt; &lt;/p&gt; &lt;p&gt;&lt;code&gt;private final Function&amp;lt;Query, Integer&amp;gt; maxResultsProvider;&lt;/code&gt; &lt;/p&gt; &lt;p&gt;&lt;code&gt;private final Function&amp;lt;Query, Double&amp;gt; minScoreProvider;&lt;/code&gt; &lt;/p&gt; &lt;p&gt;&lt;code&gt;private final Function&amp;lt;Query, Filter&amp;gt; filterProvider;&lt;/code&gt; &lt;/p&gt; &lt;p&gt;&lt;code&gt;}&lt;/code&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/BigYesterday2785&quot;&gt; /u/BigYesterday2785 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ebo69l/why_does_embeddingstorecontentretriever_not/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ebo69l/why_does_embeddingstorecontentretriever_not/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1ebo69l</id><link href="https://www.reddit.com/r/LangChain/comments/1ebo69l/why_does_embeddingstorecontentretriever_not/" /><updated>2024-07-25T06:41:50+00:00</updated><published>2024-07-25T06:41:50+00:00</published><title>Why does EmbeddingStoreContentRetriever Not output directly Score in Java Langchain ?</title></entry><entry><author><name>/u/Key_Science159</name><uri>https://www.reddit.com/user/Key_Science159</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I want to explore how these website with ai interviewer works like do they only works on the audio or they process the video also realtime. It is very fascinating to me. If anyone have any idea in this field, would happy to know your thoughts. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Key_Science159&quot;&gt; /u/Key_Science159 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ebnkqo/ai_interviewer_technique/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ebnkqo/ai_interviewer_technique/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1ebnkqo</id><link href="https://www.reddit.com/r/LangChain/comments/1ebnkqo/ai_interviewer_technique/" /><updated>2024-07-25T06:03:16+00:00</updated><published>2024-07-25T06:03:16+00:00</published><title>Ai interviewer technique</title></entry><entry><author><name>/u/rayquaza_111</name><uri>https://www.reddit.com/user/rayquaza_111</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Here&amp;#39;s my chain, but looks something wrong. Earlier without the code of agents part, it was working well with chat history.&lt;/p&gt; &lt;p&gt;Any help is appreciated.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;def _prepare_chain(self): contextualize_q_system_prompt = ( &amp;quot;Given a chat history and the latest user question &amp;quot; &amp;quot;which might reference context in the chat history, &amp;quot; &amp;quot;formulate a standalone question which can be understood &amp;quot; &amp;quot;without the chat history. Do NOT answer the question, &amp;quot; &amp;quot;just reformulate it if needed and otherwise return it as is.&amp;quot; ) _llm = self.llm if self.tools: _llm = self.llm.bind_tools(self.tools) contextualize_q_prompt = ChatPromptTemplate.from_messages( [ (&amp;quot;system&amp;quot;, contextualize_q_system_prompt), MessagesPlaceholder(&amp;quot;chat_history&amp;quot;), (&amp;quot;human&amp;quot;, &amp;quot;{input}&amp;quot;), ] ) history_aware_retriever = create_history_aware_retriever( _llm, self.retriever, contextualize_q_prompt ) ### Answer question ### system_prompt = ( &amp;quot;{base_prompt}&amp;quot; &amp;quot;Act like a support person who loves helping customers. &amp;quot; &amp;quot;Use the following pieces of retrieved context to answer &amp;quot; &amp;quot;the question. If you don&amp;#39;t know the answer, say that you &amp;quot; &amp;quot;don&amp;#39;t know. Use three sentences maximum and keep the &amp;quot; &amp;quot;answer concise.&amp;quot; &amp;quot;\n\n&amp;quot; &amp;quot;{context}&amp;quot; ) ANSWER_PROMPT = PromptTemplate.from_template(system_prompt) qa_prompt = ChatPromptTemplate.from_messages( [ (&amp;quot;system&amp;quot;, system_prompt), MessagesPlaceholder(&amp;quot;chat_history&amp;quot;), (&amp;quot;human&amp;quot;, &amp;quot;{question}&amp;quot;), ] ) question_answer_chain = create_stuff_documents_chain(_llm, qa_prompt) retrieval_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain) _runnable = ( RunnablePassthrough.assign( agent_scratchpad=lambda x: format_to_openai_tool_messages(x[&amp;quot;intermediate_steps&amp;quot;]) ) | retrieval_chain | _llm | OpenAIToolsAgentOutputParser() ) _agent = RunnableAgent(runnable=_runnable) _output = RunnableParallel( answer=AgentExecutor(agent=_agent, tools=self.tools), sources=history_aware_retriever | self._extract_sources ) rag_chain = RunnablePassthrough.assign( input=lambda x: x[&amp;quot;question&amp;quot;]) | _output | RunnableLambda(self.log_chain) conversational_rag_chain = RunnableWithMessageHistory( rag_chain, self.get_session_history, input_messages_key=&amp;quot;question&amp;quot;, history_messages_key=&amp;quot;chat_history&amp;quot;, output_messages_key=&amp;quot;answer&amp;quot;, ) return conversational_rag_chain &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/rayquaza_111&quot;&gt; /u/rayquaza_111 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1eblbm7/not_able_to_figure_out_agents_with_chat_history/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1eblbm7/not_able_to_figure_out_agents_with_chat_history/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1eblbm7</id><link href="https://www.reddit.com/r/LangChain/comments/1eblbm7/not_able_to_figure_out_agents_with_chat_history/" /><updated>2024-07-25T03:50:18+00:00</updated><published>2024-07-25T03:50:18+00:00</published><title>Not able to figure out Agents with Chat History</title></entry><entry><author><name>/u/mehul_gupta1997</name><uri>https://www.reddit.com/user/mehul_gupta1997</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;This demo talks about how to use Llama 3.1 with LangChain to build Generative AI applications: &lt;a href=&quot;https://youtu.be/LW64o3YgbE8?si=1nCi7Htoc-gH2zJ6&quot;&gt;https://youtu.be/LW64o3YgbE8?si=1nCi7Htoc-gH2zJ6&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/mehul_gupta1997&quot;&gt; /u/mehul_gupta1997 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1eay7kz/llama_31_using_langchain/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1eay7kz/llama_31_using_langchain/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1eay7kz</id><link href="https://www.reddit.com/r/LangChain/comments/1eay7kz/llama_31_using_langchain/" /><updated>2024-07-24T10:37:35+00:00</updated><published>2024-07-24T10:37:35+00:00</published><title>Llama 3.1 using LangChain</title></entry><entry><author><name>/u/Repulsive-Bedroom883</name><uri>https://www.reddit.com/user/Repulsive-Bedroom883</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ebbuh5/your_free_personal_ai_companion_for_emotional/&quot;&gt; &lt;img src=&quot;https://external-preview.redd.it/bTI2dzFocG4xamVkMYa7p5lcXDU-LbZnAe0Q65tAbY338Q3KoRvi2UTB2pMK.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b1c8f58cce53724a03a5347f3c99314dfc8a1888&quot; alt=&quot;Your Free Personal AI Companion for Emotional Support&quot; title=&quot;Your Free Personal AI Companion for Emotional Support&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Excited to share my AI Companion project—a supportive, empathetic companion available via call, now enhanced with guided sleep sessions, meditation exercises, and mental health tools. It respects privacy, keeps no personal data, and aims to make mental health support accessible to all. Your feedback shapes its evolution—let&amp;#39;s make companionship and well-being tools more accessible!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Repulsive-Bedroom883&quot;&gt; /u/Repulsive-Bedroom883 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://v.redd.it/ka68j1xn1jed1&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ebbuh5/your_free_personal_ai_companion_for_emotional/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1ebbuh5</id><media:thumbnail url="https://external-preview.redd.it/bTI2dzFocG4xamVkMYa7p5lcXDU-LbZnAe0Q65tAbY338Q3KoRvi2UTB2pMK.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=b1c8f58cce53724a03a5347f3c99314dfc8a1888" /><link href="https://www.reddit.com/r/LangChain/comments/1ebbuh5/your_free_personal_ai_companion_for_emotional/" /><updated>2024-07-24T20:34:15+00:00</updated><published>2024-07-24T20:34:15+00:00</published><title>Your Free Personal AI Companion for Emotional Support</title></entry><entry><author><name>/u/Time-Artist-6900</name><uri>https://www.reddit.com/user/Time-Artist-6900</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi I want to do metadata filtering first and then retrieve the document&lt;br/&gt; Code:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;langchain_chroma = Chroma( client=self.persistent_client, collection_name=self.COLLECTION_NAME, embedding_function=self.embedding_function # Use the variable containing the collection name ) retriever = langchain_chroma.as_retriever(search_type=&amp;quot;similarity&amp;quot;,search_kwargs={&amp;#39;k&amp;#39;: 1, &amp;#39;filter&amp;#39;: cond}) query = &amp;quot;What is patient family Medical history in reverse cronological order?&amp;quot; res = retriever.get_relevant_documents(query) res &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This is not returning scores, Whereas If use , &lt;/p&gt; &lt;pre&gt;&lt;code&gt;res = langchain_chroma.similarity_search_with_score(query) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;then i am getting score as well but how to do metadata filtering here?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Time-Artist-6900&quot;&gt; /u/Time-Artist-6900 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1eb5r0v/how_to_return_similarity_scores_using/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1eb5r0v/how_to_return_similarity_scores_using/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1eb5r0v</id><link href="https://www.reddit.com/r/LangChain/comments/1eb5r0v/how_to_return_similarity_scores_using/" /><updated>2024-07-24T16:27:36+00:00</updated><published>2024-07-24T16:27:36+00:00</published><title>How to return similarity scores using retriever.get_relevant_documents(query)</title></entry><entry><author><name>/u/BigYesterday2785</name><uri>https://www.reddit.com/user/BigYesterday2785</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;i am using in memory vector database where I get scoring of responses. &lt;/p&gt; &lt;p&gt;Now i want to implement reranking to get the most accurate responses. &lt;/p&gt; &lt;p&gt;What would be the easiest way to implmement this in Java Langchain. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/BigYesterday2785&quot;&gt; /u/BigYesterday2785 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1eawsb6/easiest_way_to_implement_reranking_in_langchain/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1eawsb6/easiest_way_to_implement_reranking_in_langchain/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1eawsb6</id><link href="https://www.reddit.com/r/LangChain/comments/1eawsb6/easiest_way_to_implement_reranking_in_langchain/" /><updated>2024-07-24T09:06:28+00:00</updated><published>2024-07-24T09:06:28+00:00</published><title>Easiest way to implement reranking in Langchain and Java</title></entry><entry><author><name>/u/New-Contribution6302</name><uri>https://www.reddit.com/user/New-Contribution6302</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi all. I am an LLM enthusiast trying to use GGUF version of Llama 3.1 for summarisation task. &lt;/p&gt; &lt;p&gt;I am using Q4_K_M model from this repo: MaziyarPanahi/Meta-Llama-3.1-8B-Instruct-GGUF Link: &lt;a href=&quot;https://huggingface.co/MaziyarPanahi/Meta-Llama-3.1-8B-Instruct-GGUF&quot;&gt;https://huggingface.co/MaziyarPanahi/Meta-Llama-3.1-8B-Instruct-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I used the following code to load the model: ``` from langchain_community.llms import LlamaCpp from langchain_core.callbacks import CallbackManager, StreamingStdOutCallbackHandler&lt;/p&gt; &lt;p&gt;callback_manager = CallbackManager([StreamingStdOutCallbackHandler()]) n_gpu_layers = -1&lt;br/&gt; n_batch = 2048 &lt;/p&gt; &lt;h1&gt;Make sure the model path is correct for your system!&lt;/h1&gt; &lt;p&gt;llm = LlamaCpp( model_path=&amp;quot;./Meta-Llama-3.1-8B-Instruct.Q4_K_M.gguf&amp;quot;, n_gpu_layers=n_gpu_layers, n_ctx = 32768, rope_freq_scale=0.25, temperature = 0, n_batch=n_batch, callback_manager=callback_manager, verbose=True, # Verbose is required to pass to the callback manager ) ```&lt;/p&gt; &lt;p&gt;When I pass long inputs to this model and instruct it to summarise it, it just blabbers with random and repitive texts/numbers.&lt;/p&gt; &lt;p&gt;How do I resolve this. Requesting for guidance.&lt;/p&gt; &lt;p&gt;(PS: Tried Rope_freq_scale with values 0.125, 0.25, 1, 4, 8. But they were not so good, even comparing to the above results)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/New-Contribution6302&quot;&gt; /u/New-Contribution6302 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1eayrtc/request_for_guidance/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1eayrtc/request_for_guidance/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1eayrtc</id><link href="https://www.reddit.com/r/LangChain/comments/1eayrtc/request_for_guidance/" /><updated>2024-07-24T11:09:56+00:00</updated><published>2024-07-24T11:09:56+00:00</published><title>Request for Guidance</title></entry><entry><author><name>/u/thevaliantfox04</name><uri>https://www.reddit.com/user/thevaliantfox04</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hello, I am starting to use LangChain and have a question, for which I did not find a response in the documentation.&lt;/p&gt; &lt;p&gt;From my understanding, each LLM is trained with a different &lt;em&gt;chat format&lt;/em&gt; to separate AI and user messages. For instance, I am currently developing with Phi3 which uses the following format for AI messages: &lt;code&gt;&amp;lt;|assistant|&amp;gt;Assistant Message&amp;lt;|end|&amp;gt;&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;How can I pass some parameters to tell LangChain to use this format? Above all, is this handled by the &lt;code&gt;LLM&lt;/code&gt; class or by the &lt;code&gt;Message&lt;/code&gt; class?&lt;/p&gt; &lt;p&gt;I make an example to make my point clearer. When I use the following code&lt;/p&gt; &lt;pre&gt;&lt;code&gt;ChatPromptTemplate.from_messages( [ (&amp;quot;system&amp;quot;, &amp;quot;Behave like this...&amp;quot;), (&amp;quot;human&amp;quot;, &amp;quot;{input}&amp;quot;), ] ) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;How can I tell LangChain to insert &lt;code&gt;&amp;lt;|user|&amp;gt;&lt;/code&gt; at the beginning of the user message? I do not see any parameter to pass to the &lt;code&gt;HumanMessage&lt;/code&gt; object. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/thevaliantfox04&quot;&gt; /u/thevaliantfox04 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1eav643/how_to_customize_the_chat_format_langchain_uses/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1eav643/how_to_customize_the_chat_format_langchain_uses/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1eav643</id><link href="https://www.reddit.com/r/LangChain/comments/1eav643/how_to_customize_the_chat_format_langchain_uses/" /><updated>2024-07-24T07:15:45+00:00</updated><published>2024-07-24T07:15:45+00:00</published><title>How to customize the Chat Format LangChain uses for my specific LLM?</title></entry><entry><author><name>/u/SpaceKey6285</name><uri>https://www.reddit.com/user/SpaceKey6285</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Does anyone have best practices to share on implementing long term memory for agents? E.g., personalization based on chat history. Based on the memgpt paper it seems best practices would be to have a secondary agent that can read/write long term context into a database, like a Redis cache. Curious if anyone has tuned a model for this? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/SpaceKey6285&quot;&gt; /u/SpaceKey6285 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1eat8c4/long_term_memory_for_agents/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1eat8c4/long_term_memory_for_agents/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1eat8c4</id><link href="https://www.reddit.com/r/LangChain/comments/1eat8c4/long_term_memory_for_agents/" /><updated>2024-07-24T05:10:31+00:00</updated><published>2024-07-24T05:10:31+00:00</published><title>Long term memory for agents?</title></entry><entry><author><name>/u/MagentaSpark</name><uri>https://www.reddit.com/user/MagentaSpark</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;At the time of posting,&lt;/p&gt; &lt;p&gt;LangChain repository&amp;#39;s &lt;code&gt;master&lt;/code&gt; branch is&lt;/p&gt; &lt;p&gt;&lt;code&gt; Cloning into &amp;#39;langchain&amp;#39;... remote: Enumerating objects: 137116, done. remote: Counting objects: 100% (5275/5275), done. remote: Compressing objects: 100% (481/481), done. remote: Total 137116 (delta 5003), reused 4829 (delta 4794), pack-reused 131841 Receiving objects: 100% (137116/137116), 224.32 MiB | 4.70 MiB/s, done. Resolving deltas: 100% (101282/101282), done. Updating files: 100% (7595/7595), done. &lt;/code&gt;&lt;/p&gt; &lt;p&gt;and LangGraph repository&amp;#39;s &lt;code&gt;main&lt;/code&gt; branch is&lt;/p&gt; &lt;p&gt;&lt;code&gt; Cloning into &amp;#39;langgraph&amp;#39;... remote: Enumerating objects: 10436, done. remote: Counting objects: 100% (1815/1815), done. remote: Compressing objects: 100% (1015/1015), done. remote: Total 10436 (delta 1090), reused 1371 (delta 774), pack-reused 8621 Receiving objects: 100% (10436/10436), 327.76 MiB | 3.13 MiB/s, done. Resolving deltas: 100% (6828/6828), done. &lt;/code&gt;&lt;/p&gt; &lt;p&gt;For comparision, this is React&amp;#39;s &lt;code&gt;main&lt;/code&gt; brach is&lt;/p&gt; &lt;p&gt;&lt;code&gt; Cloning into &amp;#39;react&amp;#39;... remote: Enumerating objects: 326918, done. remote: Counting objects: 100% (813/813), done. remote: Compressing objects: 100% (324/324), done. remote: Total 326918 (delta 470), reused 718 (delta 422), pack-reused 326105 Receiving objects: 100% (326918/326918), 532.16 MiB | 5.97 MiB/s, done. Resolving deltas: 100% (232896/232896), done. &lt;/code&gt; and it doesn&amp;#39;t even have rich text files like .ipynb.&lt;/p&gt; &lt;p&gt;There are couple of observations. 1. Maintaining an open-source repository with Jupyter Notebooks is not for easy, I think. Any updates to libraries used need notebooks to rerun and reflect latest outputs. Even if there is no change in output, the git diff changes drastically. I have heard about nbdime but have no idea about it. 2. LangGraph repo is bigger in size than LangChain after decompressing. ``` du -sh langgraph 475M langgraph&lt;/p&gt; &lt;p&gt;du -sh langchain 459M langchain``` This size by du depends on multiple factors, block size being on of them.&lt;/p&gt; &lt;p&gt;What did you find interesting? Do share more insights and fun facts about the projects!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/MagentaSpark&quot;&gt; /u/MagentaSpark &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1eb19ri/langchain_vs_langgraph_git/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1eb19ri/langchain_vs_langgraph_git/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1eb19ri</id><link href="https://www.reddit.com/r/LangChain/comments/1eb19ri/langchain_vs_langgraph_git/" /><updated>2024-07-24T13:18:20+00:00</updated><published>2024-07-24T13:18:20+00:00</published><title>LangChain VS LangGraph: Git</title></entry><entry><author><name>/u/Senior_Union_393</name><uri>https://www.reddit.com/user/Senior_Union_393</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I am trying to make a planning agent using langgraph in which we can revert back to the planner node using conditions from other agent nodes . I am stuck on the reverting back function.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Senior_Union_393&quot;&gt; /u/Senior_Union_393 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1eazoaz/reverting_back_to_planning_node_based_on_a/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1eazoaz/reverting_back_to_planning_node_based_on_a/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1eazoaz</id><link href="https://www.reddit.com/r/LangChain/comments/1eazoaz/reverting_back_to_planning_node_based_on_a/" /><updated>2024-07-24T12:00:29+00:00</updated><published>2024-07-24T12:00:29+00:00</published><title>Reverting back to planning node based on a condition</title></entry><entry><author><name>/u/gwen_from_nile</name><uri>https://www.reddit.com/user/gwen_from_nile</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I am rather new to the AI space (my background is data infrastructure), so I am documenting my journey as I&amp;#39;m learning. This time I built an AI Code Assistant that uses RAG to answer questions about different repositories.&lt;/p&gt; &lt;p&gt;I blogged everything I learned while building this - Schema design, use of LangChain (tbh, not sure it was a good choice...), choice of models, streaming chat UX...&lt;/p&gt; &lt;p&gt;You can see the app here: &lt;a href=&quot;https://code-assist-nile.vercel.app&quot;&gt;https://code-assist-nile.vercel.app&lt;/a&gt;&lt;br/&gt; And the blog: &lt;a href=&quot;https://www.thenile.dev/blog/building_code_assistant&quot;&gt;https://www.thenile.dev/blog/building_code_assistant&lt;/a&gt;&lt;br/&gt; The code is here: &lt;a href=&quot;https://github.com/niledatabase/niledatabase/tree/main/examples/ai/code_assist/&quot;&gt;https://github.com/niledatabase/niledatabase/tree/main/examples/ai/code_assist/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/gwen_from_nile&quot;&gt; /u/gwen_from_nile &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1eadv0e/i_build_a_ragbased_multitenant_ai_code_assistant/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1eadv0e/i_build_a_ragbased_multitenant_ai_code_assistant/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1eadv0e</id><link href="https://www.reddit.com/r/LangChain/comments/1eadv0e/i_build_a_ragbased_multitenant_ai_code_assistant/" /><updated>2024-07-23T17:35:24+00:00</updated><published>2024-07-23T17:35:24+00:00</published><title>I build a RAG-based multi-tenant AI Code Assistant with OpenAI, LangChain, Postgres and PG Vector</title></entry><entry><author><name>/u/Acanthocephala_Salt</name><uri>https://www.reddit.com/user/Acanthocephala_Salt</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1eageaq/exciting_news_from_meta_llama_31_is_here/&quot;&gt; &lt;img src=&quot;https://a.thumbs.redditmedia.com/4gnJFgu6Xo73DTSmQcJ5IIl2IzPp6vj2y5cONfUo6q4.jpg&quot; alt=&quot;Exciting News from Meta [Llama 3.1 is Here]&quot; title=&quot;Exciting News from Meta [Llama 3.1 is Here]&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Meta has just released its latest LLM model, Llama 3.1, marking a significant step in accessible artificial intelligence. Here are the key points from the announcement:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;405B version.&lt;/strong&gt; There is a new Llama 3.1 405B version. That’s right &lt;em&gt;405 Billion parameters.&lt;/em&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Expanded context length&lt;/strong&gt;: Now all llama 3.1 models offer a context length of &lt;strong&gt;128K tokens&lt;/strong&gt;, 16 times its previous 8K context length from Llama 3. This allows for more advanced use cases, such as long-form text summarization, multilingual conversational agents, and coding assistants&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Model evaluations&lt;/strong&gt;: The model evaluations released by Meta are as follows:&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/zjcxaf93jbed1.png?width=3201&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=31191792e788799899102d882d3170acc34ea19b&quot;&gt;Llama 405B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/h1x4jcy6jbed1.png?width=3201&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4fb34e2d110345a34e1715d16be8951d0edc637b&quot;&gt;Llama 8B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;4. API Coming Soon:&lt;/strong&gt; Users will be able to access and utilize Llama 3.1 models through &lt;a href=&quot;http://awanllm.com/&quot;&gt;awanllm.com&lt;/a&gt; soon. Stay tuned for updates in this post!&lt;/p&gt; &lt;p&gt;Source: &lt;a href=&quot;https://ai.meta.com/blog/meta-llama-3-1/&quot;&gt;https://ai.meta.com/blog/meta-llama-3-1/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Acanthocephala_Salt&quot;&gt; /u/Acanthocephala_Salt &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1eageaq/exciting_news_from_meta_llama_31_is_here/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1eageaq/exciting_news_from_meta_llama_31_is_here/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1eageaq</id><media:thumbnail url="https://a.thumbs.redditmedia.com/4gnJFgu6Xo73DTSmQcJ5IIl2IzPp6vj2y5cONfUo6q4.jpg" /><link href="https://www.reddit.com/r/LangChain/comments/1eageaq/exciting_news_from_meta_llama_31_is_here/" /><updated>2024-07-23T19:17:49+00:00</updated><published>2024-07-23T19:17:49+00:00</published><title>Exciting News from Meta [Llama 3.1 is Here]</title></entry><entry><author><name>/u/NasserAAA</name><uri>https://www.reddit.com/user/NasserAAA</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;What are your thoughts on using MongoDB as vectorstore for your apps.&lt;/p&gt; &lt;p&gt;I was working on prototype locally for the most of its time but right now we are moving to hosting on streamlit, what are your recommendations for vectorstores.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/NasserAAA&quot;&gt; /u/NasserAAA &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1eaffzv/mongodb_as_vectorstore/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1eaffzv/mongodb_as_vectorstore/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1eaffzv</id><link href="https://www.reddit.com/r/LangChain/comments/1eaffzv/mongodb_as_vectorstore/" /><updated>2024-07-23T18:39:44+00:00</updated><published>2024-07-23T18:39:44+00:00</published><title>MongoDB as vectorstore</title></entry><entry><author><name>/u/DifficultArugula8304</name><uri>https://www.reddit.com/user/DifficultArugula8304</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I want to give my client the option to construct new agents and create flows for input and output like vectorizing input and parsing output and storing it in a database. Is there any opensource tool with a UI that can do this? The language it&amp;#39;s written in doesn&amp;#39;t really matter, all options are welcome.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/DifficultArugula8304&quot;&gt; /u/DifficultArugula8304 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1eaedlh/looking_for_an_opensource_framework_to_manage/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1eaedlh/looking_for_an_opensource_framework_to_manage/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1eaedlh</id><link href="https://www.reddit.com/r/LangChain/comments/1eaedlh/looking_for_an_opensource_framework_to_manage/" /><updated>2024-07-23T17:56:20+00:00</updated><published>2024-07-23T17:56:20+00:00</published><title>Looking for an opensource framework to manage agents</title></entry><entry><author><name>/u/srvking</name><uri>https://www.reddit.com/user/srvking</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Guys,&lt;/p&gt; &lt;p&gt;Has anybody used Qdrant in the cloud, especially Azure and has gone live and in production? We are trying to insert 884 points with a production grade cluster in azure eastus and it takes about 6-8 seconds and that too with gRPC. Http takes even longer.&lt;/p&gt; &lt;p&gt;We are absolutely sure that this is the time taken by Qdrant Remote Client provided by their official package because we have enabled all the logging and can pin-point which operation takes time.&lt;/p&gt; &lt;p&gt;We created a support ticket with the Qdrant team as well, but have been ghosted by them. &lt;/p&gt; &lt;p&gt;Wondering if Qdrant is right choice and if it is, how do people insert points faster? We do have metadata and chunk text in the point. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/srvking&quot;&gt; /u/srvking &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1eaabwv/is_qdrant_cloud_production_ready/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1eaabwv/is_qdrant_cloud_production_ready/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1eaabwv</id><link href="https://www.reddit.com/r/LangChain/comments/1eaabwv/is_qdrant_cloud_production_ready/" /><updated>2024-07-23T15:14:06+00:00</updated><published>2024-07-23T15:14:06+00:00</published><title>Is Qdrant cloud Production Ready?</title></entry><entry><author><name>/u/Expensive-Rub3117</name><uri>https://www.reddit.com/user/Expensive-Rub3117</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ea7g23/multiagentdataanalysis_aidriven_data_analysis/&quot;&gt; &lt;img src=&quot;https://external-preview.redd.it/1JX3PzIaTRXFic9OMARqVvTPnyE1x5FcNLG2jrAgYEU.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=eca0d0f2c1e87731dab2321618072174b07b7430&quot; alt=&quot;Multi-agent-DataAnalysis AI-Driven Data Analysis System&quot; title=&quot;Multi-agent-DataAnalysis AI-Driven Data Analysis System&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;h1&gt;# Advanced AI-Driven Data Analysis System: A LangGraph Implementation&lt;/h1&gt; &lt;h2&gt;Project Overview&lt;/h2&gt; &lt;p&gt;I&amp;#39;ve developed a sophisticated data analysis system that leverages the power of LangGraph, showcasing its capabilities in integrating various AI architectures and methodologies. This system is designed to serve as a comprehensive example of how LangGraph can be used to streamline complex data analysis tasks by orchestrating multiple AI agents and architectural patterns.&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/lntq41fap9ed1.jpg?width=610&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=7b2f31c450c495ebe2eb3d5a1e75522223ae1267&quot;&gt;https://preview.redd.it/lntq41fap9ed1.jpg?width=610&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=7b2f31c450c495ebe2eb3d5a1e75522223ae1267&lt;/a&gt;&lt;/p&gt; &lt;h2&gt;Key Features&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;**LangGraph-Powered Architecture**: The system demonstrates LangGraph&amp;#39;s flexibility by incorporating:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Supervisor agents for overseeing the analysis process&lt;/li&gt; &lt;li&gt;Chain-of-thought reasoning for complex problem-solving&lt;/li&gt; &lt;li&gt;Critic agents for quality assurance and error checking&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;**Innovative Note Taker Agent**: A standout feature that highlights LangGraph&amp;#39;s extensibility:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Continuously records the current state of the project&lt;/li&gt; &lt;li&gt;Provides a more efficient alternative to transmitting complete historical information&lt;/li&gt; &lt;li&gt;Enhances the system&amp;#39;s ability to maintain context and continuity across different analysis stages&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;**Adaptive Workflow**: Showcases LangGraph&amp;#39;s dynamic routing capabilities, adjusting the analysis approach based on the data and task at hand.&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;Why It&amp;#39;s a Valuable LangGraph Example&lt;/h2&gt; &lt;p&gt;This implementation serves as an excellent case study for LangGraph users by demonstrating:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Integration of diverse AI agent types within a unified framework&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Efficient state management using the innovative Note Taker agent&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Real-world application of LangGraph in complex data analysis scenarios&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;h2&gt;Contribution to LangGraph&lt;/h2&gt; &lt;p&gt;I am eager to contribute this project as an example in the official LangGraph repository. My goals are to:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;Provide a comprehensive, real-world example of LangGraph&amp;#39;s capabilities&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Help other developers understand advanced LangGraph implementations&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Contribute to the growth and adoption of LangGraph in the AI community&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;Project Repository&lt;/h2&gt; &lt;p&gt;For a deeper dive into the codebase, architecture, and implementation details, please visit the project&amp;#39;s GitHub repository:&lt;/p&gt; &lt;p&gt;[AI-Driven Data Analysis System on GitHub](&lt;a href=&quot;https://github.com/starpig1129/Multi-agent-DataAnalysis&quot;&gt;https://github.com/starpig1129/Multi-agent-DataAnalysis&lt;/a&gt;)&lt;/p&gt; &lt;p&gt;I welcome feedback and collaboration to refine this example for potential inclusion in the LangGraph documentation or example collection.&lt;/p&gt; &lt;h2&gt;Next Steps&lt;/h2&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;I am open to adapting the project to better align with LangGraph&amp;#39;s documentation standards.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;I would appreciate guidance on the best way to submit this as a potential official example.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;I&amp;#39;m eager to collaborate with the LangGraph community to enhance this example and make it as valuable as possible for other users.&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Feel free to explore the repository, and I look forward to any feedback or suggestions for improving this as a LangGraph example!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Expensive-Rub3117&quot;&gt; /u/Expensive-Rub3117 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ea7g23/multiagentdataanalysis_aidriven_data_analysis/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ea7g23/multiagentdataanalysis_aidriven_data_analysis/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1ea7g23</id><media:thumbnail url="https://external-preview.redd.it/1JX3PzIaTRXFic9OMARqVvTPnyE1x5FcNLG2jrAgYEU.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=eca0d0f2c1e87731dab2321618072174b07b7430" /><link href="https://www.reddit.com/r/LangChain/comments/1ea7g23/multiagentdataanalysis_aidriven_data_analysis/" /><updated>2024-07-23T13:09:57+00:00</updated><published>2024-07-23T13:09:57+00:00</published><title>Multi-agent-DataAnalysis AI-Driven Data Analysis System</title></entry><entry><author><name>/u/mehul_gupta1997</name><uri>https://www.reddit.com/user/mehul_gupta1997</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/mehul_gupta1997&quot;&gt; /u/mehul_gupta1997 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;/r/ArtificialInteligence/comments/1ead2of/how_to_use_llama_31_in_local_explained/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ead6zf/how_to_use_llama_31_codes_explained/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1ead6zf</id><link href="https://www.reddit.com/r/LangChain/comments/1ead6zf/how_to_use_llama_31_codes_explained/" /><updated>2024-07-23T17:08:15+00:00</updated><published>2024-07-23T17:08:15+00:00</published><title>How to use Llama 3.1? Codes explained</title></entry><entry><author><name>/u/jscraft</name><uri>https://www.reddit.com/user/jscraft</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Made this 2 part tutorial about Tool Calling in LangChain.js&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Part 1️: &lt;a href=&quot;https://www.js-craft.io/blog/tool-calling-langchain-js/&quot;&gt;https://www.js-craft.io/blog/tool-calling-langchain-js/&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Part 2: &lt;a href=&quot;https://www.js-craft.io/blog/tool-calling-langchain-js-toolmessage-schemas/&quot;&gt;https://www.js-craft.io/blog/tool-calling-langchain-js-toolmessage-schemas/&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Hope you will find it useful and any feedback is welcomed!&lt;/p&gt; &lt;p&gt;PS: I think it was one of the most time-consuming tutorials to make, as things here are not quite intuitive. At least for me :) &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/jscraft&quot;&gt; /u/jscraft &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ea79dp/tool_calling_tutorial_for_langchainjs/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ea79dp/tool_calling_tutorial_for_langchainjs/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1ea79dp</id><link href="https://www.reddit.com/r/LangChain/comments/1ea79dp/tool_calling_tutorial_for_langchainjs/" /><updated>2024-07-23T13:01:21+00:00</updated><published>2024-07-23T13:01:21+00:00</published><title>Tool Calling tutorial for LangChain.js</title></entry></feed>