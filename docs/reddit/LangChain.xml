<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/"><category term="LangChain" label="r/LangChain"/><updated>2024-08-06T16:55:03+00:00</updated><icon>https://www.redditstatic.com/icon.png/</icon><id>/r/LangChain.rss</id><link rel="self" href="https://www.reddit.com/r/LangChain.rss" type="application/atom+xml" /><link rel="alternate" href="https://www.reddit.com/r/LangChain" type="text/html" /><subtitle>LangChain is an open-source framework and developer toolkit that helps developers get LLM applications from prototype to production. It is available for Python and Javascript at https://www.langchain.com/.</subtitle><title>LangChain</title><entry><author><name>/u/zchaarm</name><uri>https://www.reddit.com/user/zchaarm</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;A place for members of &lt;a href=&quot;/r/LangChain&quot;&gt;r/LangChain&lt;/a&gt; to chat with each other&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/zchaarm&quot;&gt; /u/zchaarm &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_10ljho9</id><link href="https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/" /><updated>2023-01-26T04:33:05+00:00</updated><published>2023-01-26T04:33:05+00:00</published><title>r/LangChain Lounge</title></entry><entry><author><name>/u/moonbunR</name><uri>https://www.reddit.com/user/moonbunR</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1elcgt3/customer_review_analysis_ai_agent/&quot;&gt; &lt;img src=&quot;https://a.thumbs.redditmedia.com/pZgg5gsUcAK_aEdnnGnjE5DIsQs1HHdDTNiAo1mfyf8.jpg&quot; alt=&quot;Customer review analysis Ai Agent&quot; title=&quot;Customer review analysis Ai Agent&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;&lt;a href=&quot;https://preview.redd.it/bb6kdv6oh0hd1.jpg?width=974&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=078ba4f7babb1c827c9cb413fa236fc9d1cd5c2f&quot;&gt;https://preview.redd.it/bb6kdv6oh0hd1.jpg?width=974&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=078ba4f7babb1c827c9cb413fa236fc9d1cd5c2f&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This &lt;a href=&quot;https://app.smythos.com/builder?templateId=customer-review-analysis-lxxqii7231.smyth&quot;&gt;~agent~&lt;/a&gt; uses sentiment analysis and an LLM to automatically review and give responses to customer reviews. When the agent receives the customer review, it is first taken through sentiment analysis to understand the sentiment of the feedback. Together with the sentiment for context, the review is sent to the LLM which crafts an appropriate response for the review.&lt;/p&gt; &lt;p&gt;Personally, I think the secret lies in the prompting of the LLM, in my case I set up the LLM to do 3 things mainly, &lt;/p&gt; &lt;ol&gt; &lt;li&gt;Empathize with the customer&lt;/li&gt; &lt;li&gt;Offer a solution&lt;/li&gt; &lt;li&gt;Provide additional information&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;These three gave me some pretty decent results, you can try your own or just use these on the template, but I still think that with more robust prompting, the agent can produce more natural and efficient responses that can help out your brand in a huge way. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/moonbunR&quot;&gt; /u/moonbunR &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1elcgt3/customer_review_analysis_ai_agent/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1elcgt3/customer_review_analysis_ai_agent/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1elcgt3</id><media:thumbnail url="https://a.thumbs.redditmedia.com/pZgg5gsUcAK_aEdnnGnjE5DIsQs1HHdDTNiAo1mfyf8.jpg" /><link href="https://www.reddit.com/r/LangChain/comments/1elcgt3/customer_review_analysis_ai_agent/" /><updated>2024-08-06T08:28:24+00:00</updated><published>2024-08-06T08:28:24+00:00</published><title>Customer review analysis Ai Agent</title></entry><entry><author><name>/u/GPT-Claude-Gemini</name><uri>https://www.reddit.com/user/GPT-Claude-Gemini</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey everyone I want to share a Langchain-based project that I have been working on for the last few months — &lt;a href=&quot;https://www.jenova.ai/&quot;&gt;JENOVA&lt;/a&gt;, an AI (similar to ChatGPT) that integrates the best foundation models and tools into one seamless experience.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;AI is advancing too fast for most people to follow.&lt;/strong&gt; New state-of-the-art models emerge constantly, each with unique strengths and specialties. Currently:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Claude 3.5 Sonnet is the best at reasoning, math, and coding.&lt;/li&gt; &lt;li&gt;Gemini 1.5 Pro excels in business/financial analysis and language translations.&lt;/li&gt; &lt;li&gt;Llama 3.1 405B is most performative in roleplaying and creativity.&lt;/li&gt; &lt;li&gt;GPT-4o is most knowledgeable in areas such as art, entertainment, and travel.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This rapidly changing and fragmenting AI landscape is leading to the following problems for consumers:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Awareness Gap:&lt;/strong&gt; Most people are unaware of the latest models and their specific strengths, and are often paying for AI (e.g. ChatGPT) that is suboptimal for their tasks.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Constant Switching:&lt;/strong&gt; Due to constant changes in SOTA models, consumers have to frequently switch their preferred AI and subscription.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;User Friction:&lt;/strong&gt; Switching AI results in significant user experience disruptions, such as losing chat histories or critical features such as web browsing.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;JENOVA is built to solve this.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;When you ask JENOVA a question, it automatically routes your query to the model that can provide the optimal answer (built on top of Langchain).&lt;/strong&gt; For example, if your first question is about coding, then Claude 3.5 Sonnet will respond. If your second question is about tourist spots in Tokyo, then GPT-4o will respond. All this happens seamlessly in the background.&lt;/p&gt; &lt;p&gt;JENOVA&amp;#39;s model ranking is continuously updated to incorporate the latest AI models and performance benchmarks, ensuring you are always using the best models for your specific needs.&lt;/p&gt; &lt;p&gt;In addition to the best AI models, JENOVA also provides you with an expanding suite of the most useful tools, starting with:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Web browsing&lt;/strong&gt; for real-time information (performs surprisingly well, nearly on par with Perplexity)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Multi-format document analysis&lt;/strong&gt; including PDF, Word, Excel, PowerPoint, and more&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Image interpretation&lt;/strong&gt; for visual tasks&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Your privacy is very important to us. Your conversations and data are never used for training, either by us or by third-party AI providers.&lt;/p&gt; &lt;p&gt;Try it out at &lt;a href=&quot;https://www.jenova.ai/&quot;&gt;&lt;strong&gt;www.jenova.ai&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/GPT-Claude-Gemini&quot;&gt; /u/GPT-Claude-Gemini &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ellpk9/sharing_my_project_that_was_built_on_langchain_an/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ellpk9/sharing_my_project_that_was_built_on_langchain_an/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1ellpk9</id><link href="https://www.reddit.com/r/LangChain/comments/1ellpk9/sharing_my_project_that_was_built_on_langchain_an/" /><updated>2024-08-06T16:07:14+00:00</updated><published>2024-08-06T16:07:14+00:00</published><title>Sharing my project that was built on Langchain: An all-in-one AI that integrates the best foundation models (GPT, Claude, Gemini, Llama) and tools into one seamless experience.</title></entry><entry><author><name>/u/BigYesterday2785</name><uri>https://www.reddit.com/user/BigYesterday2785</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I wanted to ask, if I want to run local LLMs only on CPU.&lt;/p&gt; &lt;p&gt;I do not have access to GPUs and wanted to ask how much slower CPU would be, compared to GPU.&lt;/p&gt; &lt;p&gt;I would love to run a small Open Source LLM only on CPUs to read 500 pages PDFs and be able to ask it questions.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/BigYesterday2785&quot;&gt; /u/BigYesterday2785 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1eli67w/run_local_llm_on_cpu_how_bad_would_would_it_be/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1eli67w/run_local_llm_on_cpu_how_bad_would_would_it_be/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1eli67w</id><link href="https://www.reddit.com/r/LangChain/comments/1eli67w/run_local_llm_on_cpu_how_bad_would_would_it_be/" /><updated>2024-08-06T13:44:24+00:00</updated><published>2024-08-06T13:44:24+00:00</published><title>Run local LLM on CPU. how Bad would would it be compared to GPUs</title></entry><entry><author><name>/u/mehul_gupta1997</name><uri>https://www.reddit.com/user/mehul_gupta1997</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/mehul_gupta1997&quot;&gt; /u/mehul_gupta1997 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;/r/ArtificialInteligence/comments/1eli3xb/ragflow_ui_for_rag_framework/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1eli4qr/ragflow_ui_for_rag_framework/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1eli4qr</id><link href="https://www.reddit.com/r/LangChain/comments/1eli4qr/ragflow_ui_for_rag_framework/" /><updated>2024-08-06T13:42:38+00:00</updated><published>2024-08-06T13:42:38+00:00</published><title>RAGflow : UI for RAG framework</title></entry><entry><author><name>/u/TimeTravellingCat</name><uri>https://www.reddit.com/user/TimeTravellingCat</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1elkjcz/building_multiagent_workflows_with_open_and/&quot;&gt; &lt;img src=&quot;https://external-preview.redd.it/emRxMDJpOTA1MmhkMQOeLRWbXZLLqGF7C1pE6u8v4bV4Eov2GX0h5P2dipJi.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=efb7632c3d308cdeef92241156ced31db3fcbf57&quot; alt=&quot;Building multi-agent workflows with open and closed models using an open-source low-code platform&quot; title=&quot;Building multi-agent workflows with open and closed models using an open-source low-code platform&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/TimeTravellingCat&quot;&gt; /u/TimeTravellingCat &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://v.redd.it/nzs7bi9052hd1&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1elkjcz/building_multiagent_workflows_with_open_and/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1elkjcz</id><media:thumbnail url="https://external-preview.redd.it/emRxMDJpOTA1MmhkMQOeLRWbXZLLqGF7C1pE6u8v4bV4Eov2GX0h5P2dipJi.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=efb7632c3d308cdeef92241156ced31db3fcbf57" /><link href="https://www.reddit.com/r/LangChain/comments/1elkjcz/building_multiagent_workflows_with_open_and/" /><updated>2024-08-06T15:20:54+00:00</updated><published>2024-08-06T15:20:54+00:00</published><title>Building multi-agent workflows with open and closed models using an open-source low-code platform</title></entry><entry><author><name>/u/mehul_gupta1997</name><uri>https://www.reddit.com/user/mehul_gupta1997</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1el7jc9/langchain_in_your_pocket_completes_6_months/&quot;&gt; &lt;img src=&quot;https://b.thumbs.redditmedia.com/aEiKKct7MORRNB8Sihet1ABmGQk9Ug9_q_P-0jOItBQ.jpg&quot; alt=&quot;LangChain in your Pocket completes 6 months !!&quot; title=&quot;LangChain in your Pocket completes 6 months !!&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;m glad to share that my debut book, &lt;strong&gt;&amp;quot;LangChain in your Pocket: Beginner&amp;#39;s Guide to Building Generative AI Applications using LLMs&lt;/strong&gt;&amp;quot; completed 6 months last week and what a dream run it has been.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;The book has been &lt;strong&gt;republished by Packt.&lt;/strong&gt; And is now available with all major publishers including O&amp;#39;Reilly.&lt;/li&gt; &lt;li&gt;So far, the book has sold over &lt;strong&gt;500 copies&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;It is the &lt;strong&gt;highest-rated book on LangChain&lt;/strong&gt; on Amazon (Amazon.in: 4.7; Amazon.com: 4.3 ).&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;The best part is that the book hasn&amp;#39;t received a bad review regarding the content from anyone, making this even more special for me&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;&lt;em&gt;A big thanks to the community for all the support.&lt;/em&gt;&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/7wtmrl2nnygd1.png?width=901&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=da3d9b2ea43d771ee738bcbb611c6331a36ef580&quot;&gt;https://preview.redd.it/7wtmrl2nnygd1.png?width=901&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=da3d9b2ea43d771ee738bcbb611c6331a36ef580&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/mehul_gupta1997&quot;&gt; /u/mehul_gupta1997 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1el7jc9/langchain_in_your_pocket_completes_6_months/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1el7jc9/langchain_in_your_pocket_completes_6_months/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1el7jc9</id><media:thumbnail url="https://b.thumbs.redditmedia.com/aEiKKct7MORRNB8Sihet1ABmGQk9Ug9_q_P-0jOItBQ.jpg" /><link href="https://www.reddit.com/r/LangChain/comments/1el7jc9/langchain_in_your_pocket_completes_6_months/" /><updated>2024-08-06T03:24:06+00:00</updated><published>2024-08-06T03:24:06+00:00</published><title>LangChain in your Pocket completes 6 months !!</title></entry><entry><author><name>/u/franckeinstein24</name><uri>https://www.reddit.com/user/franckeinstein24</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1elmowa/large_language_models_productivity_and_profits/&quot;&gt; &lt;img src=&quot;https://external-preview.redd.it/0Gpr0l9wOWJpMWNIC8nus2jHb5fhiP1sqUtyjaWdipU.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f97adc2c91917c889a6db9abbf2946942cc53166&quot; alt=&quot;Large Language Models, Productivity, and Profits&quot; title=&quot;Large Language Models, Productivity, and Profits&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/franckeinstein24&quot;&gt; /u/franckeinstein24 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.lycee.ai/blog/large-language-models-productivity-and-profits&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1elmowa/large_language_models_productivity_and_profits/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1elmowa</id><media:thumbnail url="https://external-preview.redd.it/0Gpr0l9wOWJpMWNIC8nus2jHb5fhiP1sqUtyjaWdipU.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=f97adc2c91917c889a6db9abbf2946942cc53166" /><link href="https://www.reddit.com/r/LangChain/comments/1elmowa/large_language_models_productivity_and_profits/" /><updated>2024-08-06T16:46:01+00:00</updated><published>2024-08-06T16:46:01+00:00</published><title>Large Language Models, Productivity, and Profits</title></entry><entry><author><name>/u/skipvdm</name><uri>https://www.reddit.com/user/skipvdm</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi Guys,&lt;/p&gt; &lt;p&gt;I am working on an issue and can&amp;#39;t seem to figure it out. I want to retrieve all the embeddings that are stored in a Azure AI Search instance.&lt;/p&gt; &lt;p&gt;At this moment i&amp;#39;m trying it like this:&lt;/p&gt; &lt;p&gt;&lt;code&gt;# Create a SearchClient instance&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;endpoint = SEARCH_ENDPOINT&lt;/code&gt;&lt;br/&gt; &lt;code&gt;search_client = SearchClient(endpoint=endpoint, index_name=index_name, credential=AzureKeyCredential(api_key))&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;# Formulate a search request&lt;/code&gt;&lt;br/&gt; &lt;code&gt;search_text = &amp;quot;*&amp;quot;&lt;/code&gt;&lt;br/&gt; &lt;code&gt;select_fields = &amp;quot;*&amp;quot;&lt;/code&gt; &lt;/p&gt; &lt;p&gt;&lt;code&gt;# Execute the search&lt;/code&gt;&lt;br/&gt; &lt;code&gt;results = search_client.search(search_text, select=select_fields)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;print(results)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;However, this return me the following:&lt;/p&gt; &lt;p&gt;{&amp;#39;content&amp;#39;: &amp;#39;RANDOM CONTENT&amp;#39;,&lt;br/&gt; &amp;#39;metadata&amp;#39;:&lt;br/&gt; &amp;#39;{&amp;quot;source&amp;quot;: &amp;quot;DOC SOURCE,&lt;br/&gt; &amp;quot;page&amp;quot;: 0,&lt;br/&gt; &amp;quot;start_index&amp;quot;: 4}&amp;#39;,&lt;br/&gt; &amp;#39;id&amp;#39;: &amp;#39;OTQ4MTUwOWYtNTFiNC00NTViLWE0MzQtNTJlYjQxZDJiMmI0&amp;#39;,&lt;br/&gt; &amp;#39;@search.score&amp;#39;: 1.0, &amp;#39;@search.reranker_score&amp;#39;: None,&lt;br/&gt; &amp;#39;@search.highlights&amp;#39;: None,&lt;br/&gt; &amp;#39;@search.captions&amp;#39;: None}&lt;/p&gt; &lt;p&gt;However not the embeddings. Can anybody help me?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/skipvdm&quot;&gt; /u/skipvdm &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1elllb7/retrieving_all_embeddings_from_an_azure_ai_search/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1elllb7/retrieving_all_embeddings_from_an_azure_ai_search/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1elllb7</id><link href="https://www.reddit.com/r/LangChain/comments/1elllb7/retrieving_all_embeddings_from_an_azure_ai_search/" /><updated>2024-08-06T16:02:37+00:00</updated><published>2024-08-06T16:02:37+00:00</published><title>Retrieving all embeddings from an Azure Ai Search instance</title></entry><entry><author><name>/u/Mental-Ad-7783</name><uri>https://www.reddit.com/user/Mental-Ad-7783</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I am trying to build an application that analyses all my transcripts and answer user question based on the context by performing similarity search. I am loading all my transcripts into vector database(in my case milvus) and storing them as embeddings, along with the metadata of the transcripts. I am using rag retrieval process to get the answer, but I didn&amp;#39;t get the expected output on most cases. Any better way of doing this, any suggestions on choosing the appropriate embeddings and dimensions.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Mental-Ad-7783&quot;&gt; /u/Mental-Ad-7783 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1elldx1/extracting_insights_from_conversational/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1elldx1/extracting_insights_from_conversational/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1elldx1</id><link href="https://www.reddit.com/r/LangChain/comments/1elldx1/extracting_insights_from_conversational/" /><updated>2024-08-06T15:54:39+00:00</updated><published>2024-08-06T15:54:39+00:00</published><title>Extracting insights from conversational transcripts</title></entry><entry><author><name>/u/Relevant_Ebb_3633</name><uri>https://www.reddit.com/user/Relevant_Ebb_3633</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hello everyone, I&amp;#39;m looking for a low-code platform to implement RAG in business processes. I&amp;#39;ve tested tools like Dify, RAGflow, Flowise, and langflow, but none of them seem to be well-optimized for RAG. Does anyone know of any low-code platforms that offer better RAG parameter optimization? Thanks in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Relevant_Ebb_3633&quot;&gt; /u/Relevant_Ebb_3633 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1elarmg/looking_for_lowcode_tools_for_building_and/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1elarmg/looking_for_lowcode_tools_for_building_and/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1elarmg</id><link href="https://www.reddit.com/r/LangChain/comments/1elarmg/looking_for_lowcode_tools_for_building_and/" /><updated>2024-08-06T06:34:58+00:00</updated><published>2024-08-06T06:34:58+00:00</published><title>Looking for low-code tools for building and optimizing RAG</title></entry><entry><author><name>/u/Just_Guide7361</name><uri>https://www.reddit.com/user/Just_Guide7361</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I am currently building a Q&amp;amp;A interface with Streamlit and Langchain. Our initial vector database was in Pinecone. We have documents about the same topic, but different industries. Pure embedding search is not optimal, as it will match the same concepts across industries. So, we build a simple selector option where users pick their industry, and then ask the question. In pinecone each industry had their own namespace, we then simply filter on this:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;vectorstore = PineconeVectorStore(index_name=index_name, embedding=embeddings, namespace=namespace) retriever = vectorstore.as_retriever(search_type=&amp;quot;similarity&amp;quot;, search_kwargs={&amp;quot;k&amp;quot;: 3}) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Hybrid search with pinecone is not as convenient as with Weaviate, and since we noticed beter performance with hybrid search we are switching to Weaviate. The downside is that filters are not so clear for the Weaviate retriever. &lt;/p&gt; &lt;pre&gt;&lt;code&gt;retriever = WeaviateHybridSearchRetriever( client=client, index_name=WEAVIATE_INDEX_NAME, text_key=&amp;quot;page_content&amp;quot;, k=5, alpha=0.75, attributes=[&amp;quot;file_name&amp;quot;, &amp;quot;industry], create_schema_if_missing=False, ) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Our Langchain Chain looks similar to this ( &lt;a href=&quot;https://github.com/langchain-ai/langchain/blob/master/templates/hybrid-search-weaviate/hybrid_search_weaviate/chain.py&quot;&gt;https://github.com/langchain-ai/langchain/blob/master/templates/hybrid-search-weaviate/hybrid_search_weaviate/chain.py&lt;/a&gt; ):&lt;/p&gt; &lt;pre&gt;&lt;code&gt;# RAG prompt template = &amp;quot;&amp;quot;&amp;quot;Answer the question based only on the following context: {context} Question: {question} &amp;quot;&amp;quot;&amp;quot; prompt = ChatPromptTemplate.from_template(template) # RAG model = ChatOpenAI() chain = ( RunnableParallel({&amp;quot;context&amp;quot;: retriever, &amp;quot;question&amp;quot;: RunnablePassthrough()}) | prompt | model | StrOutputParser() ) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The docs do show this:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;retriever.invoke( &amp;quot;AI integration in society&amp;quot;, where_filter={ &amp;quot;path&amp;quot;: [&amp;quot;author&amp;quot;], &amp;quot;operator&amp;quot;: &amp;quot;Equal&amp;quot;, &amp;quot;valueString&amp;quot;: &amp;quot;Prof. Jonathan K. Sterling&amp;quot;, }, ) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;a href=&quot;https://python.langchain.com/v0.2/docs/integrations/retrievers/weaviate-hybrid/&quot;&gt;https://python.langchain.com/v0.2/docs/integrations/retrievers/weaviate-hybrid/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Does anyone know how/where to add the &lt;code&gt;where_filter&lt;/code&gt; parameter for Weaviate hybrid search in the Chain?&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/langchain-ai/langchain/blob/master/templates/hybrid-search-weaviate/hybrid_search_weaviate/chain.py&quot;&gt;&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Just_Guide7361&quot;&gt; /u/Just_Guide7361 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1elk3g2/weaviatehybridsearchretriever_with_filters/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1elk3g2/weaviatehybridsearchretriever_with_filters/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1elk3g2</id><link href="https://www.reddit.com/r/LangChain/comments/1elk3g2/weaviatehybridsearchretriever_with_filters/" /><updated>2024-08-06T15:03:27+00:00</updated><published>2024-08-06T15:03:27+00:00</published><title>WeaviateHybridSearchRetriever with filters?</title></entry><entry><author><name>/u/Danidre</name><uri>https://www.reddit.com/user/Danidre</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;ve been keeping my eye on the LangGraph documentation, there was one titled &amp;quot;Help Desk RAG&amp;quot; or &amp;quot;Make a Help Desk Agent with RAG&amp;quot; or something. This was back when the documentation also showed a &amp;quot;Design Patterns&amp;quot; category for the graphs.&lt;/p&gt; &lt;p&gt;I&amp;#39;m finally ready to start the tutorial but it has all but vanished. I only see about 6 other RAG based links. All of which are long, and with names that don&amp;#39;t really indicate the differences. (Corrective RAG, Self-RAG, Agentic RAG...)&lt;/p&gt; &lt;p&gt;I may end up spending the time diving into them all eventually, but would anyone know which one specifically used to be the &amp;quot;Help Desk RAG&amp;quot;?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Danidre&quot;&gt; /u/Danidre &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1elilnx/help_desk_rag_documentation_new_link_langgraph/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1elilnx/help_desk_rag_documentation_new_link_langgraph/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1elilnx</id><link href="https://www.reddit.com/r/LangChain/comments/1elilnx/help_desk_rag_documentation_new_link_langgraph/" /><updated>2024-08-06T14:02:40+00:00</updated><published>2024-08-06T14:02:40+00:00</published><title>Help Desk RAG Documentation New Link? [LangGraph]</title></entry><entry><author><name>/u/Jen1888Mik</name><uri>https://www.reddit.com/user/Jen1888Mik</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;My application logic is - i give some text from database, and using this context generated answer.But if i ask something but its dont exist in context - LLm still answer me. Why?.Here my code &lt;/p&gt; &lt;pre&gt;&lt;code&gt;const prompt = await pull&amp;lt;ChatPromptTemplate&amp;gt;(&amp;quot;rlm/rag-prompt&amp;quot;); const ragChain = await createStuffDocumentsChain({ llm, prompt:prompt, outputParser: parcer, }); const stream = await ragChain.stream({ question:question, context: [ new Document({ pageContent: text as string, }), ], }); &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Jen1888Mik&quot;&gt; /u/Jen1888Mik &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1eli48o/llm_answer_to_the_question_context_for_answer_did/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1eli48o/llm_answer_to_the_question_context_for_answer_did/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1eli48o</id><link href="https://www.reddit.com/r/LangChain/comments/1eli48o/llm_answer_to_the_question_context_for_answer_did/" /><updated>2024-08-06T13:42:00+00:00</updated><published>2024-08-06T13:42:00+00:00</published><title>LLM answer to the question - context for answer did not even provide</title></entry><entry><author><name>/u/Sorre33</name><uri>https://www.reddit.com/user/Sorre33</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;ve been building an SQLagent-based chatbot, testing and prototyping with langserve. I&amp;#39;ve obtained pretty good results and the interaction with the database and query production tasks work surprisingly well. Now, I&amp;#39;m about to upgrade to multi-agent to include some RAG functionalities and being able to produce more comprehensive answers and to help users integrate information from docs etc. For that I will probably using langgraph.&lt;/p&gt; &lt;p&gt;Up to this point, I&amp;#39;ve been using the langchain SQLAgent, which works pretty well considering that it handles all in one the sql dialect, the prompt construction and the tools.&lt;/p&gt; &lt;p&gt;Yesterday though, starting to design the langgraph upgrade, I found out that the new langchain docs now suggest to build SQLagents from scratch using langgraph, through &amp;quot;create_react_agent&amp;quot; and including the sql toolkit, which to me looks like a less intuitive way compared to SQLAgent, adding extra steps to the process and messing up the old prompt building approach (the react_agent takes in input only llm, tools and messages_modifier, which is a simple SystemMessage, and the db is passed to the SQLDatabaseToolkit. To me this is way less intuitive than just having an sql_agent that takes llm, db and an actual full prompt template).&lt;/p&gt; &lt;p&gt;Is there a reason for this? Why did they change the documentations and tutorials from create_sql_agent to this new approach? Is it worth it to make the change if I&amp;#39;m going to overall switch to a langgraph-based design or will I be ok integrating the &amp;quot;old&amp;quot; SQLAgent?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Sorre33&quot;&gt; /u/Sorre33 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1eldqp7/create_sql_agent_vs_create_react_agent/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1eldqp7/create_sql_agent_vs_create_react_agent/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1eldqp7</id><link href="https://www.reddit.com/r/LangChain/comments/1eldqp7/create_sql_agent_vs_create_react_agent/" /><updated>2024-08-06T09:55:22+00:00</updated><published>2024-08-06T09:55:22+00:00</published><title>create_sql_agent VS create_react_agent</title></entry><entry><author><name>/u/Common-Comedian7128</name><uri>https://www.reddit.com/user/Common-Comedian7128</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;&lt;strong&gt;Context:&lt;/strong&gt; I have an ambitious idea about using agents for long-horizon text-only storytelling but have limited technical knowledge. I hope more experienced individuals can offer input on feasibility and potential challenges.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Project Overview:&lt;/strong&gt; My goal is to create very detailed, long form AI roleplaying software where users aren&amp;#39;t just reading a novel, they are the protagonist. In my experience, AI can produce passable prose (Claude 3.5), but it struggles with story progression and planning, often resulting in circular narratives. To address this, I plan to use agents to create a detailed plot outline based on a user-supplied premise. The AI will narrate, and the user will act as the protagonist. Every time the protagonist acts, the AI will follow a series of escalating questions to assess and rewrite the outline as needed to keep the story cohesive:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Does this fit with the next planned beat? If yes, the writer AI continues; if not,&lt;/li&gt; &lt;li&gt;Does this fit with the scene? If yes, rewrite the beat; if not,&lt;/li&gt; &lt;li&gt;Does this fit with the chapter outline? If yes, rewrite the scene and beat; if not,&lt;/li&gt; &lt;li&gt;Does this fit with the arc?&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;This way, the story always has an outline to stay on track but can pivot as needed based on the protagonist&amp;#39;s actions.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Assumptions:&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Using a series of repeating agents, AI can generate and refine variations of a user-supplied premise into hyper-specific instructions. AI-written content often lacks specificity, which I plan to address through careful prompting and QA agents.&lt;/li&gt; &lt;li&gt;I can break up the long horizon task of impromptu novel writing (AI roleplaying) into discrete chunks I can hand off to specific agents that will only see their little section of the walled garden. This feels risky because different sections might need to know about each other to be cohesive and it could be hard to predict what that shared knowledge pool might need to be without making it so large the cost and wait time balloon beyond toleration.&lt;/li&gt; &lt;li&gt;That this won’t run afoul of the bitter lesson - where I have to get so granular I stray into deterministic logic which is inherently limiting and better solved with waiting for GPT-4.5.&lt;/li&gt; &lt;li&gt;I can get the response time from user input to the next beat written to a tolerable experience. I plan to make what I can run concurrently and use a bag of tricks to lessen the perceived waiting time.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;The plot outline to keep things on track is table stakes for me. My real goal is far more ambitious (and far-fetched) about making the characters come alive:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Give all / many of the characters encountered their own AI which fully fleshes out their backstory, goals, personality, outlook, relationships with other AI characters, and gives them agency to pursue those minor/major goals as the book unfolds. The events other characters set in motion will be experienced by the protagonist whether it’s breaking and entering or being crabby in a conversation because they get hangry and it’s lunch time.&lt;/li&gt; &lt;li&gt;Create true repercussions for actions the protagonist or other characters take. If the protagonist as a student is rude, the AI controlling the teacher will assign detention, the unpleasantness determined by the AI of the teacher administering the detention &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I know a tiny amount of code (enough to hard code Towers of Hanoi in terminal) and with AI’s help, have Open Router hooked up to a local python program in VS Code where it writes to a local document. I plan to create tons of little documents the agents will write/read to. In my day job I work at a startup as a UX designer, have lots of senior engineering FAANG friends and have played extensively with AI, particularly with storytelling including programs like Sudowrite and Novelcrafter. &lt;/p&gt; &lt;p&gt;I haven’t yet messed with agents. What do you predict will be the hardest parts of this project and what are the odds of success? I suspect the programming will be simple, the prompting will be finicky, the lag challenging and the cost fairly high even with GPT-4o mini. Even if I succeed, the output will probably be average until GPT-4.5 comes along.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Common-Comedian7128&quot;&gt; /u/Common-Comedian7128 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1eli34m/beginner_advice_on_extremely_ambitious_agentbased/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1eli34m/beginner_advice_on_extremely_ambitious_agentbased/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1eli34m</id><link href="https://www.reddit.com/r/LangChain/comments/1eli34m/beginner_advice_on_extremely_ambitious_agentbased/" /><updated>2024-08-06T13:40:43+00:00</updated><published>2024-08-06T13:40:43+00:00</published><title>Beginner: Advice on Extremely Ambitious Agent-Based Long-Horizon Storytelling</title></entry><entry><author><name>/u/Plane_Past129</name><uri>https://www.reddit.com/user/Plane_Past129</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I know it&amp;#39;s not right place to ask, but I need some suggestions from people who have worked with automation thing. My manager assigned me a task to fetch all the data from a website. I know to write a simple web scraper. But, first we have to log into their website. So, they suggested me to do automating website. I tried to write selenium in python. But, this has to be hosted on server and the data from the website should be accessed via an API call. Every time I run selenium code, It opens an instance of browser and performing automation. Is there any other way to handle this without opening a browser? I think this should be hosted on EC2 instance?? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Plane_Past129&quot;&gt; /u/Plane_Past129 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1eldor2/automation/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1eldor2/automation/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1eldor2</id><link href="https://www.reddit.com/r/LangChain/comments/1eldor2/automation/" /><updated>2024-08-06T09:51:56+00:00</updated><published>2024-08-06T09:51:56+00:00</published><title>Automation??</title></entry><entry><author><name>/u/phicreative1997</name><uri>https://www.reddit.com/user/phicreative1997</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1elgkz4/adding_memory_agent_interaction_into_the/&quot;&gt; &lt;img src=&quot;https://external-preview.redd.it/8KOjNt_QWxAuMrglcZz6T7GEcc4UE4-ife3v361FEU8.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=16a76ae363963d0e6b15bf595f936a6ebf7fbf63&quot; alt=&quot;Adding Memory &amp;amp; Agent interaction into the “Auto-Analyst”&quot; title=&quot;Adding Memory &amp;amp; Agent interaction into the “Auto-Analyst”&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/phicreative1997&quot;&gt; /u/phicreative1997 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://medium.com/firebird-technologies/adding-memory-agent-interaction-into-the-auto-analyst-01aa7a2d3614&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1elgkz4/adding_memory_agent_interaction_into_the/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1elgkz4</id><media:thumbnail url="https://external-preview.redd.it/8KOjNt_QWxAuMrglcZz6T7GEcc4UE4-ife3v361FEU8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=16a76ae363963d0e6b15bf595f936a6ebf7fbf63" /><link href="https://www.reddit.com/r/LangChain/comments/1elgkz4/adding_memory_agent_interaction_into_the/" /><updated>2024-08-06T12:34:07+00:00</updated><published>2024-08-06T12:34:07+00:00</published><title>Adding Memory &amp; Agent interaction into the “Auto-Analyst”</title></entry><entry><author><name>/u/Anmsacx</name><uri>https://www.reddit.com/user/Anmsacx</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1elbugn/deprecated_pydantic_library_is_not_working_for/&quot;&gt; &lt;img src=&quot;https://b.thumbs.redditmedia.com/kMbJRyrHEP98eWpF7LHLPGKz9G9rRkwt3r7gJy4W-0A.jpg&quot; alt=&quot;Deprecated pydantic library is not working for RAG pipeline development&quot; title=&quot;Deprecated pydantic library is not working for RAG pipeline development&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I am new to langflow and langchain. Running the new ollama- llama3.1 locally on my machine. The error is in the main.py file of pydantic and states that the existing json, fields and other such are deprecated. As a result, i am unable to run the model for my task. Please help😿&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Anmsacx&quot;&gt; /u/Anmsacx &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/gallery/1elbugn&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1elbugn/deprecated_pydantic_library_is_not_working_for/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1elbugn</id><media:thumbnail url="https://b.thumbs.redditmedia.com/kMbJRyrHEP98eWpF7LHLPGKz9G9rRkwt3r7gJy4W-0A.jpg" /><link href="https://www.reddit.com/r/LangChain/comments/1elbugn/deprecated_pydantic_library_is_not_working_for/" /><updated>2024-08-06T07:45:48+00:00</updated><published>2024-08-06T07:45:48+00:00</published><title>Deprecated pydantic library is not working for RAG pipeline development</title></entry><entry><author><name>/u/Exotic_Show3271</name><uri>https://www.reddit.com/user/Exotic_Show3271</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;hi guys, I want to do the retrieval part in my rag without a limit on the K. so it can answer such questions, how many times X appears for example. what is the best approach? I tried increasing the k to the total number of documents but it gave me a lot of irrelevant documents. and the vectorstore.as_retriver() always return 4 docs only&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Exotic_Show3271&quot;&gt; /u/Exotic_Show3271 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1eld8ep/increase_number_of_retrieved_docs/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1eld8ep/increase_number_of_retrieved_docs/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1eld8ep</id><link href="https://www.reddit.com/r/LangChain/comments/1eld8ep/increase_number_of_retrieved_docs/" /><updated>2024-08-06T09:21:10+00:00</updated><published>2024-08-06T09:21:10+00:00</published><title>increase number of retrieved docs</title></entry><entry><author><name>/u/CharmingViolinist962</name><uri>https://www.reddit.com/user/CharmingViolinist962</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;index_name = f&amp;quot;vs_spn_chatrules&amp;quot;&lt;/p&gt; &lt;p&gt;search_client = SearchClient(&lt;br/&gt; endpoint=service_endpoint,&lt;br/&gt; index_name=index_name,&lt;br/&gt; credential=cognitive_search_credential,&lt;br/&gt; )&lt;/p&gt; &lt;h1&gt;Creating an Azure AI Search Vector Store&lt;/h1&gt; &lt;p&gt;vector_store = AzureAISearchVectorStore(&lt;br/&gt; search_or_index_client=index_client,&lt;br/&gt; filterable_metadata_field_keys=metadata_fields,&lt;br/&gt; index_name=index_name,&lt;br/&gt; index_management=IndexManagement.CREATE_IF_NOT_EXISTS,&lt;br/&gt; id_field_key=&amp;quot;id&amp;quot;,&lt;br/&gt; chunk_field_key=&amp;quot;chunk&amp;quot;, #content&lt;br/&gt; embedding_field_key=&amp;quot;embedding&amp;quot;, #content_vector&lt;br/&gt; embedding_dimensionality=1536,&lt;br/&gt; metadata_string_field_key=&amp;quot;metadata&amp;quot;,&lt;br/&gt; doc_id_field_key=&amp;quot;doc_id&amp;quot;,&lt;br/&gt; language_analyzer=&amp;quot;en.lucene&amp;quot;,&lt;br/&gt; vector_algorithm_type=&amp;quot;exhaustiveKnn&amp;quot;, #HNSW focuses on approximate methods for efficiency, KNN ensures exactness through exhaustive searches.&lt;br/&gt; )&lt;/p&gt; &lt;p&gt;Settings.llm = llm&lt;br/&gt; Settings.embed_model = embed_model&lt;/p&gt; &lt;p&gt;storage_context = StorageContext.from_defaults(vector_store=vector_store)&lt;/p&gt; &lt;h1&gt;VectorStoreIndex.from_documents&lt;/h1&gt; &lt;p&gt;index = VectorStoreIndex.from_documents(&lt;br/&gt; all_docs, storage_context=storage_context&lt;br/&gt; )&lt;/p&gt; &lt;p&gt;all_docs is a list of documents with document metadata and documents&lt;/p&gt; &lt;p&gt;while im trying to create index its gives error&lt;/p&gt; &lt;p&gt;HttpResponseError: () The request is invalid. Details: An unexpected &amp;#39;StartArray&amp;#39; node was found when reading from the JSON reader. A &amp;#39;PrimitiveValue&amp;#39; node was expected.&lt;br/&gt; Code:&lt;br/&gt; Message: The request is invalid. Details: An unexpected &amp;#39;StartArray&amp;#39; node was found when reading from the JSON reader. A &amp;#39;PrimitiveValue&amp;#39; node was expected.&lt;/p&gt; &lt;p&gt;anyone faced this&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/CharmingViolinist962&quot;&gt; /u/CharmingViolinist962 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1elbo4f/issue_while_trying_to_create_index_with_error_the/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1elbo4f/issue_while_trying_to_create_index_with_error_the/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1elbo4f</id><link href="https://www.reddit.com/r/LangChain/comments/1elbo4f/issue_while_trying_to_create_index_with_error_the/" /><updated>2024-08-06T07:34:15+00:00</updated><published>2024-08-06T07:34:15+00:00</published><title>Issue while trying to create index with error The request is invalid. Details: An unexpected 'StartArray' node was found when reading from the JSON reader. A 'PrimitiveValue' node was expected.</title></entry><entry><author><name>/u/anujtomar_17</name><uri>https://www.reddit.com/user/anujtomar_17</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1el9edu/javascript_revolution_nodejs_in_backend/&quot;&gt; &lt;img src=&quot;https://external-preview.redd.it/4VCwe6CcHO5MeC--bZyzxCK5GSWjgRY8cnCRF7SgID8.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=804d791203a8e670b391f9a21caab8e2e265d88e&quot; alt=&quot;JavaScript Revolution: Node.js in Back-End Development&quot; title=&quot;JavaScript Revolution: Node.js in Back-End Development&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/anujtomar_17&quot;&gt; /u/anujtomar_17 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.quickwayinfosystems.com/blog/javascript-revolution-nodejs-backend-development/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1el9edu/javascript_revolution_nodejs_in_backend/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1el9edu</id><media:thumbnail url="https://external-preview.redd.it/4VCwe6CcHO5MeC--bZyzxCK5GSWjgRY8cnCRF7SgID8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=804d791203a8e670b391f9a21caab8e2e265d88e" /><link href="https://www.reddit.com/r/LangChain/comments/1el9edu/javascript_revolution_nodejs_in_backend/" /><updated>2024-08-06T05:08:28+00:00</updated><published>2024-08-06T05:08:28+00:00</published><title>JavaScript Revolution: Node.js in Back-End Development</title></entry><entry><author><name>/u/MeltingHippos</name><uri>https://www.reddit.com/user/MeltingHippos</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Post by an AI researcher describing how their team made a modification to OpenAI’s Whisper model architecture that results in a 1.5x increase in speed with comparable accuracy. The improvement is achieved using a multi-head attention mechanism (hence Medusa). The post gives an overview of Whisper&amp;#39;s architecture and a detailed explanation of the method used to achieve the increase in speed:&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://medium.com/@sgl.yael/whisper-medusa-using-multiple-decoding-heads-to-achieve-1-5x-speedup-7344348ef89b&quot;&gt;https://medium.com/@sgl.yael/whisper-medusa-using-multiple-decoding-heads-to-achieve-1-5x-speedup-7344348ef89b&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/MeltingHippos&quot;&gt; /u/MeltingHippos &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1eks2cm/whispermedusa_uses_multiple_decoding_heads_for/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1eks2cm/whispermedusa_uses_multiple_decoding_heads_for/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1eks2cm</id><link href="https://www.reddit.com/r/LangChain/comments/1eks2cm/whispermedusa_uses_multiple_decoding_heads_for/" /><updated>2024-08-05T16:22:51+00:00</updated><published>2024-08-05T16:22:51+00:00</published><title>Whisper-Medusa: uses multiple decoding heads for 1.5X speedup</title></entry><entry><author><name>/u/mehul_gupta1997</name><uri>https://www.reddit.com/user/mehul_gupta1997</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;LangFlow is an extension of LangChain which provides GUI options to build Generative AI applications using LLMs with drag and drop options. Checkout how to install and use it in this tutorial : &lt;a href=&quot;https://youtu.be/LpxeE_eTGOU&quot;&gt;https://youtu.be/LpxeE_eTGOU&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/mehul_gupta1997&quot;&gt; /u/mehul_gupta1997 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1eknnho/langflow_ui_for_langchain/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1eknnho/langflow_ui_for_langchain/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1eknnho</id><link href="https://www.reddit.com/r/LangChain/comments/1eknnho/langflow_ui_for_langchain/" /><updated>2024-08-05T13:22:29+00:00</updated><published>2024-08-05T13:22:29+00:00</published><title>LangFlow : UI for LangChain</title></entry><entry><author><name>/u/tisi3000</name><uri>https://www.reddit.com/user/tisi3000</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ekr1de/langgraph_fanout_ui/&quot;&gt; &lt;img src=&quot;https://external-preview.redd.it/mgH19MDRIzUoIy6LkDG_eHazz8u3kwXBx1KVuKNm3uc.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=92049c27bd138737e4722f1c39707b435d1ed0d5&quot; alt=&quot;LangGraph fan-out UI&quot; title=&quot;LangGraph fan-out UI&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;So LangGraph can execute nodes in parallel via fan-out. For example: Select topics -&amp;gt; research each -&amp;gt; ...&lt;br/&gt; What&amp;#39;s a good way to visualize this in a UI? Can be tricky in a 1-dimensional, linear chat interface, but also generally if there are multiple steps running in parallel.&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://reddit.com/link/1ekr1de/video/3z2zywmt7vgd1/player&quot;&gt;fan-out in gotoHuman&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We are using these tabs for now, but maybe people have come up with smarter solutions already?!&lt;br/&gt; (Code is &lt;a href=&quot;https://github.com/gotohuman/gth-demo-fanout-content-creator&quot;&gt;here&lt;/a&gt;) &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/tisi3000&quot;&gt; /u/tisi3000 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ekr1de/langgraph_fanout_ui/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ekr1de/langgraph_fanout_ui/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1ekr1de</id><media:thumbnail url="https://external-preview.redd.it/mgH19MDRIzUoIy6LkDG_eHazz8u3kwXBx1KVuKNm3uc.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=92049c27bd138737e4722f1c39707b435d1ed0d5" /><link href="https://www.reddit.com/r/LangChain/comments/1ekr1de/langgraph_fanout_ui/" /><updated>2024-08-05T15:41:44+00:00</updated><published>2024-08-05T15:41:44+00:00</published><title>LangGraph fan-out UI</title></entry><entry><author><name>/u/gabbom_XCII</name><uri>https://www.reddit.com/user/gabbom_XCII</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I’m building an application made of 3 agents:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Router Agent;&lt;/li&gt; &lt;li&gt;Agent A&lt;/li&gt; &lt;li&gt;Agent B&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The router agent receives the initial user prompt and based on context it routes to one of the Agents.&lt;/p&gt; &lt;p&gt;When defining my LangGraph workflow I’m using a in-memory SqliteSaver. And also setting the thread_id when running my workflow.&lt;/p&gt; &lt;p&gt;But I’m not sure how LangGraph uses this chat history. Does it sends all agents history to all agent calls or it just sends the agent’s specific history to each agent? I’m experiencing some kind of “amnesia” in between interactions.&lt;/p&gt; &lt;p&gt;Am I missing something in my prompt building? Like including a {history} along with my {input} and {context} ? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/gabbom_XCII&quot;&gt; /u/gabbom_XCII &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ekn1rt/langgraph_orchestrating_langchain_agents_with/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ekn1rt/langgraph_orchestrating_langchain_agents_with/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1ekn1rt</id><link href="https://www.reddit.com/r/LangChain/comments/1ekn1rt/langgraph_orchestrating_langchain_agents_with/" /><updated>2024-08-05T12:54:58+00:00</updated><published>2024-08-05T12:54:58+00:00</published><title>LangGraph orchestrating LangChain agents with chat history.</title></entry></feed>