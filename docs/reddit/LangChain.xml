<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/"><category term="LangChain" label="r/LangChain"/><updated>2024-01-09T03:19:31+00:00</updated><icon>https://www.redditstatic.com/icon.png/</icon><id>/r/LangChain.rss</id><link rel="self" href="https://www.reddit.com/r/LangChain.rss" type="application/atom+xml" /><link rel="alternate" href="https://www.reddit.com/r/LangChain" type="text/html" /><subtitle>LangChain is an open-source framework and developer toolkit that helps developers get LLM applications from prototype to production. It is available for Python and Javascript at https://www.langchain.com/.</subtitle><title>LangChain</title><entry><author><name>/u/zchaarm</name><uri>https://www.reddit.com/user/zchaarm</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;A place for members of &lt;a href=&quot;/r/LangChain&quot;&gt;r/LangChain&lt;/a&gt; to chat with each other&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/zchaarm&quot;&gt; /u/zchaarm &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_10ljho9</id><link href="https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/" /><updated>2023-01-26T04:33:05+00:00</updated><published>2023-01-26T04:33:05+00:00</published><title>r/LangChain Lounge</title></entry><entry><author><name>/u/function-devs</name><uri>https://www.reddit.com/user/function-devs</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey everyone!&lt;/p&gt; &lt;p&gt;Excited to share this with you.&lt;/p&gt; &lt;p&gt;![image](&lt;a href=&quot;https://github.com/thestriver/ai-for-javascript-course/assets/16709708/95237a88-63e2-48b6-a2c6-fc45ff49fe7b&quot;&gt;https://github.com/thestriver/ai-for-javascript-course/assets/16709708/95237a88-63e2-48b6-a2c6-fc45ff49fe7b&lt;/a&gt;)&lt;/p&gt; &lt;p&gt;I just released an open-source course for Javascript developers who want to build AI applications on GitHub. All 60 pages of them (if you want the PDF format of the primer). (The markdown file is at over 1600 lines right now and growing.) üôÇ&lt;/p&gt; &lt;p&gt;Structured to take Javascript developers from 0-1, I put in everything I know from building AI-powered apps over the past year, and I hope you find it useful too.&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/thestriver/ai-for-javascript-course&quot;&gt;Github Link&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Here are some of the topics touched on in the modules:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Introduction to LLMs üß©&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Advanced Prompt Engineering and Optimization ‚úèÔ∏è&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Integrating&lt;/strong&gt; &lt;strong&gt;OpenAI GPT 3.5 and Mistral 7B Instruct v0.2 into JS apps&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Retrieval Augmented Generation üí¨&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Using Vercel AI SDK, Pinecone, and Langchain to build a Research Assistant Tool&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Function Calling&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Building&lt;/strong&gt; 3 *&lt;strong&gt;&lt;em&gt;AI Agents with different levels of complexity ü§ñ&lt;/em&gt;&lt;/strong&gt;*&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Security, Ethics, and Performance in AI Development&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;A relevant project accompanies each course.&lt;/p&gt; &lt;p&gt;I created this course hoping it would be an excellent guide for aspiring AI developers and a valuable resource for the wider JavaScript developer community.&lt;/p&gt; &lt;p&gt;I would love to get your feedback and, of course, would appreciate it if you shared any bugs or mistakes you discover or questions with me.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/function-devs&quot;&gt; /u/function-devs &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/191r21r/i_released_a_new_opensource_practical_ai/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/191r21r/i_released_a_new_opensource_practical_ai/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_191r21r</id><link href="https://www.reddit.com/r/LangChain/comments/191r21r/i_released_a_new_opensource_practical_ai/" /><updated>2024-01-08T17:47:53+00:00</updated><published>2024-01-08T17:47:53+00:00</published><title>I released a new opensource Practical AI Development for Javascript Developers course! (Heavily uses langchain)</title></entry><entry><author><name>/u/gswithai</name><uri>https://www.reddit.com/user/gswithai</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi üëã &lt;/p&gt; &lt;p&gt;I wrote this &lt;a href=&quot;https://www.gettingstarted.ai/everything-you-need-to-know-when-getting-started-with-langchain/&quot;&gt;introductory post&lt;/a&gt; for anyone just getting started with LangChain.&lt;/p&gt; &lt;p&gt;I try to keep it simple while going over important points so you can get started in no time.&lt;/p&gt; &lt;p&gt;If you‚Äôre new to LangChain, you‚Äôll want to &lt;a href=&quot;https://www.gettingstarted.ai/everything-you-need-to-know-when-getting-started-with-langchain/&quot;&gt;read this introductory post&lt;/a&gt; and then dive deeper into more advanced topics as you progress.&lt;/p&gt; &lt;p&gt;Check it out and let me know if you have any questions or feedback.&lt;/p&gt; &lt;p&gt;Cheers!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/gswithai&quot;&gt; /u/gswithai &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/191q3qk/new_to_langchain_start_here/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/191q3qk/new_to_langchain_start_here/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_191q3qk</id><link href="https://www.reddit.com/r/LangChain/comments/191q3qk/new_to_langchain_start_here/" /><updated>2024-01-08T17:10:01+00:00</updated><published>2024-01-08T17:10:01+00:00</published><title>New to LangChain? Start here!</title></entry><entry><author><name>/u/30299578815310</name><uri>https://www.reddit.com/user/30299578815310</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I like the tools but I&amp;#39;m not a huge fan of the default agent prompts. Just wondering if anybody has done this? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/30299578815310&quot;&gt; /u/30299578815310 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/191sm46/has_anybody_used_langchain_tools_but_totally_used/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/191sm46/has_anybody_used_langchain_tools_but_totally_used/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_191sm46</id><link href="https://www.reddit.com/r/LangChain/comments/191sm46/has_anybody_used_langchain_tools_but_totally_used/" /><updated>2024-01-08T18:50:31+00:00</updated><published>2024-01-08T18:50:31+00:00</published><title>has anybody used langchain tools but totally used custom logic for parsing LLM responses and invoking them?</title></entry><entry><author><name>/u/areebmianoor</name><uri>https://www.reddit.com/user/areebmianoor</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi &lt;/p&gt; &lt;p&gt;I&amp;#39;m fairly new to this so pardon the basic question, I poked around this community but couldn&amp;#39;t find the response so some help would be kindly appreciated. Here&amp;#39;s the situation:&lt;/p&gt; &lt;p&gt;I&amp;#39;m using RAG to build an assistant for my employer. Currently I&amp;#39;ve set up a chatbot that uses Langchain, OpenAI embedding, Deeplake as a vector database. And my current setup works well from a demo perspective where I can show the chatbot responding over some code files and giving some value. However, when I combine other data types into the database (after chunking and embedding) such as my companies Confluence documentation, it doesn&amp;#39;t seem to mesh well with the codebase data. I believe i should be doinf this in a way where the vector database has files stored in such a way that makes it clear for the retriever what part of the vector database came from a codebase and what part came from documentation (confluence) and they need to be referenced together but stored seperately to work effienciently. &lt;/p&gt; &lt;p&gt;Moreover i need some kind of an agent setup which can identify whether to respond with context from the codebase&amp;#39;s vector files or from the confluence documentations vector files or an appropriate combination of both (that would be ideal). . &lt;/p&gt; &lt;p&gt;It would also be extremely powerful if I could also somehow add on the companies PDFs, word files, PowerPoint, excel files etc. Into the mix and have it all be part of the same RAG flow. &lt;/p&gt; &lt;p&gt;I would appreciate some guidance from people who have done similar or the knowhow to solve this.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/areebmianoor&quot;&gt; /u/areebmianoor &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/191d8td/rag_responding_with_multiple_data_types/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/191d8td/rag_responding_with_multiple_data_types/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_191d8td</id><link href="https://www.reddit.com/r/LangChain/comments/191d8td/rag_responding_with_multiple_data_types/" /><updated>2024-01-08T05:09:58+00:00</updated><published>2024-01-08T05:09:58+00:00</published><title>RAG - Responding with multiple data types</title></entry><entry><author><name>/u/InternationalMail954</name><uri>https://www.reddit.com/user/InternationalMail954</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi everyone I&amp;#39;m trying to create a document retrieval app with memory. Currently it will only answer document questions and will not answer successive questions.&lt;br/&gt; For example if I ask for 3*3 it returns correctly but if i ask it to multiply the previous result by 3 it will tell me the document doesn&amp;#39;t have a number it knows to multiply by 3. Any help would be greatly appreciated.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;import streamlit as st from langchain.memory.chat_message_histories import StreamlitChatMessageHistory from io import StringIO import pinecone pinecone.init(api_key=&amp;quot;&amp;quot;, environment=&amp;quot;gcp-starter&amp;quot;) from langchain import PromptTemplate from langchain.chat_models import ChatOpenAI from langchain.chains import LLMChain from langchain.chains import ConversationalRetrievalChain from langchain.embeddings import OpenAIEmbeddings from langchain.vectorstores import Pinecone from langchain.text_splitter import RecursiveCharacterTextSplitter from langchain.memory import ConversationBufferMemory from langchain.schema import SystemMessage OPENAI_API_KEY = &amp;quot;&amp;quot; OPENAI_DIMENSION = 1536 embedding = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY) st.reupload_file = False vector_store = Pinecone.from_existing_index(&amp;quot;legal-cases&amp;quot;, embedding) index = pinecone.Index(&amp;quot;legal-cases&amp;quot;) def upload_new_file_to_pinecone(text): embedding = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY) text_splitter = RecursiveCharacterTextSplitter( chunk_size=100, chunk_overlap=0, length_function=len ) chunks = text_splitter.create_documents([text]) result = Pinecone.from_documents(chunks, embedding, index_name=&amp;quot;legal-cases&amp;quot;) st.title(&amp;quot;üìù File Q&amp;amp;A&amp;quot;) uploaded_file = st.file_uploader(&amp;quot;Upload an article&amp;quot;, type=(&amp;quot;txt&amp;quot;, &amp;quot;md&amp;quot;)) if st.reupload_file: print(&amp;quot;updated file&amp;quot;) # To read file as bytes: bytes_data = uploaded_file.getvalue() # To convert to a string based IO: stringio = StringIO(uploaded_file.getvalue().decode(&amp;quot;utf-8&amp;quot;)) # To read file as string: string_data = stringio.read() index.delete(delete_all=True) upload_new_file_to_pinecone(string_data) from langchain.chains import RetrievalQA from langchain.chat_models import ChatOpenAI k = 3 # user&amp;#39;s question text input widget q = st.text_input(&amp;quot;Ask a question about the content of your file:&amp;quot;) if q: # if the user entered a question and hit enter standard_answer = &amp;quot;&amp;quot; q = f&amp;quot;{q} {standard_answer}&amp;quot; system_message = f&amp;quot;&amp;quot;&amp;quot; You are an assistant which helps to user find answers to his question with internal company data. This data will be provided by a vector db as context. You also help with normal stuff like answering questions or generating text by ignoring this system message. When asked to summarize a specific page only summarize pages which match the page id within the url if appliable. &amp;quot;&amp;quot;&amp;quot; system_message = SystemMessage(content=system_message) memory = ConversationBufferMemory( memory_key=&amp;quot;chat_history&amp;quot;, input_key=&amp;quot;question&amp;quot;, output_key=&amp;quot;answer&amp;quot;, return_messages=True, ) memory.chat_memory.add_message(system_message) llm = ChatOpenAI(model=&amp;quot;gpt-3.5-turbo&amp;quot;, temperature=1, api_key=OPENAI_API_KEY) retriever = vector_store.as_retriever( search_type=&amp;quot;similarity&amp;quot;, search_kwargs={&amp;quot;k&amp;quot;: k} ) chain = ConversationalRetrievalChain.from_llm( llm, retriever=retriever, memory=memory, return_source_documents=True, ) answer = chain({&amp;quot;question&amp;quot; : q})[&amp;quot;chat_history&amp;quot;][-1].content # text area widget for the LLM answer st.text_area(&amp;quot;LLM Answer: &amp;quot;, value=answer) st.divider() # if there&amp;#39;s no chat history in the session state, create it if &amp;quot;history&amp;quot; not in st.session_state: st.session_state.history = &amp;quot;&amp;quot; # the current question and answer value = f&amp;quot;Q: {q} \nA: {answer}&amp;quot; st.session_state.history = ( f&amp;#39;{value} \n {&amp;quot;-&amp;quot; * 100} \n {st.session_state.history}&amp;#39; ) h = st.session_state.history # text area widget for the chat history st.text_area(label=&amp;quot;Chat History&amp;quot;, value=h, key=&amp;quot;history&amp;quot;, height=400) &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/InternationalMail954&quot;&gt; /u/InternationalMail954 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/191bsyn/can_someone_please_help_me_understand_why_this/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/191bsyn/can_someone_please_help_me_understand_why_this/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_191bsyn</id><link href="https://www.reddit.com/r/LangChain/comments/191bsyn/can_someone_please_help_me_understand_why_this/" /><updated>2024-01-08T03:53:36+00:00</updated><published>2024-01-08T03:53:36+00:00</published><title>Can someone please help me understand why this document lookup app can't remember previous questions</title></entry><entry><author><name>/u/SignificantSuit5561</name><uri>https://www.reddit.com/user/SignificantSuit5561</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi, I&amp;#39;m doing an experiment with langchain and chatgpt to see if I can get chatgpt to make business recommendations based on keywords provided in the prompt. I am using the following code:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;loader = TextLoader(&amp;#39;/tmp/training.txt&amp;#39;) index = VectorstoreIndexCreator().from_loaders([loader]) chain = ConversationalRetrievalChain.from_llm( llm=ChatOpenAI(model=&amp;quot;gpt-4&amp;quot;), retriever=index.vectorstore.as_retriever(search_kwargs={&amp;quot;k&amp;quot;: 10}), ) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;training.txt is just a text file with a JSON array e.g.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;[ { &amp;quot;name&amp;quot;: &amp;quot;Some Software Shop&amp;quot;, &amp;quot;keywords&amp;quot;: [ &amp;quot;SaaS&amp;quot;, &amp;quot;sales&amp;quot; ], &amp;quot;description&amp;quot;: &amp;quot;...&amp;quot;, } ] &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;When I prompt with something like &amp;quot;What businesses deal with SaaS?&amp;quot;, it never seems to return the relevant results i.e. those with &amp;quot;SaaS&amp;quot; in the keywords. Moreover, it seems that I always get the same handful of results out of hundreds in the file.&lt;/p&gt; &lt;p&gt;Any thoughts on where I&amp;#39;m going wrong? Thank you.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/SignificantSuit5561&quot;&gt; /u/SignificantSuit5561 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/191b6is/please_teach_me_how_to_get_relevant_vector_store/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/191b6is/please_teach_me_how_to_get_relevant_vector_store/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_191b6is</id><link href="https://www.reddit.com/r/LangChain/comments/191b6is/please_teach_me_how_to_get_relevant_vector_store/" /><updated>2024-01-08T03:21:51+00:00</updated><published>2024-01-08T03:21:51+00:00</published><title>Please teach me how to get relevant vector store results</title></entry><entry><author><name>/u/Appropriate_Egg6118</name><uri>https://www.reddit.com/user/Appropriate_Egg6118</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;How to add ouput parser to ConversationalRetrievalQAChain or ConversationChain?&lt;/p&gt; &lt;p&gt;I can able to add outputparser to LLMChain but it&amp;#39;s not working for above conversation chains. &lt;/p&gt; &lt;pre&gt;&lt;code&gt;Code: from langchain.output_parsers import ResponseSchema, StructuredOutputParser from langchain.memory import ConversationBufferMemory from langchain.prompts import PromptTemplate, ChatPromptTemplate, HumanMessagePromptTemplate from langchain.chat_models import ChatOpenAI from langchain.chains import LLMChain, ConversationalRetrievalChain checker_tmpl = &amp;quot;&amp;quot;&amp;quot; # Task Description: As a customer support representative, your role is to provide accurate and helpful responses to customer inquiries. Use the provided context to understand the customer&amp;#39;s issue and answer their questions directly. Avoid any extraneous information or explanations that are not directly relevant to the customer&amp;#39;s query. {format_instructions} Chat History:\n\n{history} \n\n # Given Context: {context} # Customer&amp;#39;s Question: {question} {human_input} # Your Response: [Please type your answer here, ensuring it is concise, relevant, and directly addresses the customer&amp;#39;s question based on the given context.] &amp;quot;&amp;quot;&amp;quot; checker_response_schemas = [ ResponseSchema( name=&amp;quot;requires_customer_support_contact&amp;quot;, description=&amp;quot;Indicates whether the user needs to contact customer support. Set to True if the context does not sufficiently address the user&amp;#39;s issue and further assistance is needed. Set to False if the provided context is adequate to answer the user&amp;#39;s query.&amp;quot;, type=&amp;quot;boolean&amp;quot; ), ResponseSchema( name=&amp;quot;contextual_answer&amp;quot;, description=&amp;quot;Provides a direct answer to the user&amp;#39;s question, utilizing the given context. The response should be concise, accurate, and specifically tailored to address the query based on the context provided.&amp;quot;, ), ] check_output_parser = StructuredOutputParser.from_response_schemas(checker_response_schemas) resume_checker_prompt = ChatPromptTemplate( messages=[ HumanMessagePromptTemplate.from_template(checker_tmpl) ], input_variables=[&amp;quot;history&amp;quot;,&amp;quot;question&amp;quot;,&amp;quot;context&amp;quot;], partial_variables={&amp;quot;format_instructions&amp;quot;: check_output_parser.get_format_instructions()} ) memory = ConversationBufferMemory( memory_key=&amp;quot;history&amp;quot;, input_key=&amp;quot;human_input&amp;quot; ) llm = ChatOpenAI(model_name=&amp;quot;gpt-3.5-turbo-1106&amp;quot;) chat_chain = LLMChain( llm=llm, prompt=resume_checker_prompt, memory = memory ) # I wanna use below chain with output parser. chain = ConversationalRetrievalChain.from_llm( llm=llm, memory=memory, chain_type=&amp;quot;stuff&amp;quot;, retriever=retriever, return_source_documents=True, get_chat_history=lambda h : h, verbose=False , ) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Appropriate_Egg6118&quot;&gt; /u/Appropriate_Egg6118 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/191b28p/how_to_use_output_parser_with/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/191b28p/how_to_use_output_parser_with/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_191b28p</id><link href="https://www.reddit.com/r/LangChain/comments/191b28p/how_to_use_output_parser_with/" /><updated>2024-01-08T03:16:02+00:00</updated><published>2024-01-08T03:16:02+00:00</published><title>How to use Output parser with ConversationalRetrievalQAChain?</title></entry><entry><author><name>/u/Flaky_Shame6323</name><uri>https://www.reddit.com/user/Flaky_Shame6323</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/19129gt/structured_data_extraction_from_text_using/&quot;&gt; &lt;img src=&quot;https://b.thumbs.redditmedia.com/TwUjNMB3aDPRMc1vY0XM5iTPJm50TzNIOA2SYDQ_7PA.jpg&quot; alt=&quot;Structured data extraction from text using Langchain?&quot; title=&quot;Structured data extraction from text using Langchain?&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I usually work on knowledge graph extraction from text and I&amp;#39;ve been looking at LLMs for quite some time now. I tried using Langchain for the past few days. However, it seems to be very pleonastic compared to doing the same things myself, except for the JsonOutputParser part which I&amp;#39;ve found useful.&lt;br/&gt; I had however a strange behaviour which probably could be answered by someone here: I used Pydantic do describe the schema to be extracted. I need the model to choose one word from the list as a vocabulary given the text it is provided, but it always returns the whole list again. I had different mistakes when I just sent a single string, but this seems rather odd. Any clue in what might be going on? &lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/in1z5l1sz2bc1.png?width=1496&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=080d65fa037264f6dbb9173eca4fd9d8578cd42c&quot;&gt;https://preview.redd.it/in1z5l1sz2bc1.png?width=1496&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=080d65fa037264f6dbb9173eca4fd9d8578cd42c&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Flaky_Shame6323&quot;&gt; /u/Flaky_Shame6323 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/19129gt/structured_data_extraction_from_text_using/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/19129gt/structured_data_extraction_from_text_using/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_19129gt</id><media:thumbnail url="https://b.thumbs.redditmedia.com/TwUjNMB3aDPRMc1vY0XM5iTPJm50TzNIOA2SYDQ_7PA.jpg" /><link href="https://www.reddit.com/r/LangChain/comments/19129gt/structured_data_extraction_from_text_using/" /><updated>2024-01-07T20:55:18+00:00</updated><published>2024-01-07T20:55:18+00:00</published><title>Structured data extraction from text using Langchain?</title></entry><entry><author><name>/u/Due_Leader2644</name><uri>https://www.reddit.com/user/Due_Leader2644</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I have a large csv file of about 50mb with a few columns. I would like to summarize the information that I have in this file using an LLM. My major concern would be dealing with the context length. Can anyone provide me with any existing implementations of this concept or guide me with links which can help me? Thank you.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Due_Leader2644&quot;&gt; /u/Due_Leader2644 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/19137og/summary_generation_on_large_csv_files/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/19137og/summary_generation_on_large_csv_files/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_19137og</id><link href="https://www.reddit.com/r/LangChain/comments/19137og/summary_generation_on_large_csv_files/" /><updated>2024-01-07T21:32:30+00:00</updated><published>2024-01-07T21:32:30+00:00</published><title>Summary Generation on large CSV files</title></entry><entry><author><name>/u/PookieCooch</name><uri>https://www.reddit.com/user/PookieCooch</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi Everyone,&lt;/p&gt; &lt;p&gt;I am new to Langchain and been using it to query SQL databases. I want to query a database where the output I get is in tabular form like how would you query in a mysql workbench. The output should have only the relevant columns and data attributes. Then I have a next requirement to output in a graph form for a graph database.&lt;/p&gt; &lt;p&gt;Can anyone guide me or share examples on how to do that?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/PookieCooch&quot;&gt; /u/PookieCooch &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/190sud6/tabular_and_graph_output/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/190sud6/tabular_and_graph_output/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_190sud6</id><link href="https://www.reddit.com/r/LangChain/comments/190sud6/tabular_and_graph_output/" /><updated>2024-01-07T14:05:13+00:00</updated><published>2024-01-07T14:05:13+00:00</published><title>Tabular and Graph Output</title></entry><entry><author><name>/u/silent-spiral</name><uri>https://www.reddit.com/user/silent-spiral</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I have some chains setup, but how do I set it up so, if the json fails to parse, you re-ask the chatbot to correct the errors? and if it does parse, it just goes forward. &lt;/p&gt; &lt;p&gt;Iv&amp;#39;e read through the docs but found this very difficult. I&amp;quot;m not using agents btw, just regular chains. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/silent-spiral&quot;&gt; /u/silent-spiral &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/190k71t/best_way_to_do_error_handling_with_langchain/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/190k71t/best_way_to_do_error_handling_with_langchain/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_190k71t</id><link href="https://www.reddit.com/r/LangChain/comments/190k71t/best_way_to_do_error_handling_with_langchain/" /><updated>2024-01-07T04:59:56+00:00</updated><published>2024-01-07T04:59:56+00:00</published><title>Best way to do error handling with langchain?</title></entry><entry><author><name>/u/GW_MC</name><uri>https://www.reddit.com/user/GW_MC</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;It is my first-time use LangChain and I have no idea how to apply the chat / instruct template of a model in the prompt using Llama-cpp.&lt;/p&gt; &lt;p&gt;For example, tiny-llama uses the following prompt template:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;&amp;lt;|system|&amp;gt; {system_message}&amp;lt;/s&amp;gt; &amp;lt;|user|&amp;gt; {prompt}&amp;lt;/s&amp;gt; &amp;lt;|assistant|&amp;gt; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;And I &amp;quot;apply&amp;quot; the template using the following runnable chain following LangChain doc:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;[ promptTemplate, model.bind({ stop: [&amp;quot;&amp;lt;|system|&amp;gt;&amp;quot;, &amp;quot;&amp;lt;|user|&amp;gt;&amp;quot;, &amp;quot;&amp;lt;|assistant|&amp;gt;&amp;quot;, &amp;quot;&amp;lt;/s&amp;gt;&amp;quot;], }), outputParser ] &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;When I apply invoke the chain, I got the following output:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;[llm/start] [1:llm:LlamaCpp] Entering LLM run with input: { &amp;quot;prompts&amp;quot;: [ &amp;quot;&amp;lt;|system|&amp;gt;\nYou are a helpful AI assistant that provides information based on your knowledge without any creation of unknown information.\nIf you do not know the answer to a question, you must say \&amp;quot;I don&amp;#39;t know\&amp;quot;.&amp;lt;/s&amp;gt;\n&amp;lt;|user|&amp;gt;\nSystem Context:\n\nyour name is John\n\nAre you Amy?&amp;lt;/s&amp;gt;\n&amp;lt;|assistant|&amp;gt;Response:&amp;quot; ] } &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;I have no idea if I have applied the template correctly, because of the &amp;quot;prompts&amp;quot; key feels like there is another place for providing the system info.&lt;/p&gt; &lt;p&gt;Are there any missing steps I have to perform before passing it into the invoke function? Do I mix up the meaning of &amp;quot;prompt template&amp;quot;? Any help in any languages is highly appreciated.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/GW_MC&quot;&gt; /u/GW_MC &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/190i8ne/apply_model_chat_instruct_template/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/190i8ne/apply_model_chat_instruct_template/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_190i8ne</id><link href="https://www.reddit.com/r/LangChain/comments/190i8ne/apply_model_chat_instruct_template/" /><updated>2024-01-07T03:17:05+00:00</updated><published>2024-01-07T03:17:05+00:00</published><title>Apply Model Chat / Instruct Template</title></entry><entry><author><name>/u/Sacred_5oul</name><uri>https://www.reddit.com/user/Sacred_5oul</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/190r1uz/error429insufficient_quota_for_new_openai_account/&quot;&gt; &lt;img src=&quot;https://b.thumbs.redditmedia.com/dN1DU3g3vMw-YG0AVkWnn0E0UWV4kJNXXqGUKuKQHDc.jpg&quot; alt=&quot;Error429(insufficient quota) for new openai account&quot; title=&quot;Error429(insufficient quota) for new openai account&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I built a basic LLM. Code &amp;amp; error are in the attached pics. My OpenAI account is new and I used 0 credits until now. But it&amp;#39;s giving me error 429. There isn&amp;#39;t any loop to send multiple requests so I don&amp;#39;t get why it&amp;#39;s throwing that error&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Sacred_5oul&quot;&gt; /u/Sacred_5oul &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/gallery/190r1uz&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/190r1uz/error429insufficient_quota_for_new_openai_account/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_190r1uz</id><media:thumbnail url="https://b.thumbs.redditmedia.com/dN1DU3g3vMw-YG0AVkWnn0E0UWV4kJNXXqGUKuKQHDc.jpg" /><link href="https://www.reddit.com/r/LangChain/comments/190r1uz/error429insufficient_quota_for_new_openai_account/" /><updated>2024-01-07T12:25:17+00:00</updated><published>2024-01-07T12:25:17+00:00</published><title>Error429(insufficient quota) for new openai account</title></entry><entry><author><name>/u/modularmindapp</name><uri>https://www.reddit.com/user/modularmindapp</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/190q1vz/in_under_60_seconds_learn_how_to_automate_news/&quot;&gt; &lt;img src=&quot;https://external-preview.redd.it/NXNsMnVybzI1MGJjMeAnmOtl8lo8ROnIz5nV_e7iMlPBFQfUw6LG5ByW5DPX.png?width=140&amp;amp;height=140&amp;amp;crop=140:140,smart&amp;amp;format=jpg&amp;amp;v=enabled&amp;amp;lthumb=true&amp;amp;s=b48338f975ca137ecc9783db39e283fe358f7e0b&quot; alt=&quot;In under 60 seconds, learn how to automate news report generation on AI topics using ModularMind&quot; title=&quot;In under 60 seconds, learn how to automate news report generation on AI topics using ModularMind&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/modularmindapp&quot;&gt; /u/modularmindapp &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://v.redd.it/cp3swqm150bc1&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/190q1vz/in_under_60_seconds_learn_how_to_automate_news/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_190q1vz</id><media:thumbnail url="https://external-preview.redd.it/NXNsMnVybzI1MGJjMeAnmOtl8lo8ROnIz5nV_e7iMlPBFQfUw6LG5ByW5DPX.png?width=140&amp;height=140&amp;crop=140:140,smart&amp;format=jpg&amp;v=enabled&amp;lthumb=true&amp;s=b48338f975ca137ecc9783db39e283fe358f7e0b" /><link href="https://www.reddit.com/r/LangChain/comments/190q1vz/in_under_60_seconds_learn_how_to_automate_news/" /><updated>2024-01-07T11:19:10+00:00</updated><published>2024-01-07T11:19:10+00:00</published><title>In under 60 seconds, learn how to automate news report generation on AI topics using ModularMind</title></entry><entry><author><name>/u/Glass_Journalist6022</name><uri>https://www.reddit.com/user/Glass_Journalist6022</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey Reddit community! üöÄ&lt;/p&gt; &lt;p&gt;Whilst building various things with LLMs I‚Äôve found one of the most time-consuming, annoying things to be writing &amp;amp; optimising prompts, then testing them across different models. That&amp;#39;s why I want to share something I&amp;#39;ve been working on: &lt;a href=&quot;http://composo.ai/&quot;&gt;Composo.ai&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Automated Prompt Writing&lt;/strong&gt;: Wish GPT4 would just write its prompts for itself? Now you can üôÇ. Great for those moments when you&amp;#39;re stuck, just starting out or are trying to do the final bit of refining to a well-tuned prompt.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Compare Gemini vs GPT4 &amp;amp; more&lt;/strong&gt;: Want to test how Gemini stacks up against Claude or GPT-4 for a certain prompt? Our platform lets you compare them side by side.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;No sign-up to try it out&lt;/strong&gt;: Jump straight into action at &lt;a href=&quot;http://www.composo.ai/&quot;&gt;www.composo.ai&lt;/a&gt; - no sign-up, no barriers.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Robust testing &amp;amp; evaluation platform too:&lt;/strong&gt; We‚Äôre also building a robust testing &amp;amp; evaluation platform for production applications, so you can automate all that subjective ‚Äòtesting by vibes‚Äô of your application outputs.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The automated prompt writing is the easiest to get started with, and you can also test those prompts across models without having to sign up too.&lt;/p&gt; &lt;p&gt;Super keen to hear what you think!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Glass_Journalist6022&quot;&gt; /u/Glass_Journalist6022 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/19051ef/automating_prompt_writing_llm_evaluation/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/19051ef/automating_prompt_writing_llm_evaluation/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_19051ef</id><link href="https://www.reddit.com/r/LangChain/comments/19051ef/automating_prompt_writing_llm_evaluation/" /><updated>2024-01-06T17:28:26+00:00</updated><published>2024-01-06T17:28:26+00:00</published><title>Automating prompt writing &amp; LLM evaluation</title></entry><entry><author><name>/u/leandrosq</name><uri>https://www.reddit.com/user/leandrosq</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey folks,&lt;/p&gt; &lt;p&gt;So I am new to LLMs but as a learning I want to rewrite the ending of Game of Thrones using a LLM.&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;So far here&amp;#39;s what I got:&lt;/p&gt; &lt;p&gt;- Made my own dataset with the scripts for each episode on all seasons. (I was going to use the books... but they are just too much text for the LLM)&lt;/p&gt; &lt;p&gt;- Simple inference using LLamaCpp, Mistral and Langchain&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;But I can&amp;#39;t feed all the episodes into one prompt, as it exceeds the token window size. I played with document splitting and Chroma as a Vector database, but it does not solve the problem.&lt;/p&gt; &lt;p&gt;So how can I proceed to rewrite it?&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;My code so far:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;# %% !pip install langchain langchain-community chromadb # %% !CMAKE_ARGS=&amp;quot;-DLLAMA_METAL=on&amp;quot; FORCE_CMAKE=1 pip install --upgrade --force-reinstall llama-cpp-python # %% from langchain.callbacks.manager import CallbackManager from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler from langchain.chains import LLMChain from langchain.prompts import PromptTemplate from langchain.prompts.few_shot import FewShotPromptTemplate from langchain_community.llms import LlamaCpp # %% import os examples = [] # Read .txt files under &amp;quot;./dataset/&amp;quot; DATASET_PATH = &amp;quot;./scripts/&amp;quot; files = os.listdir(DATASET_PATH) for file in files: if file.endswith(&amp;quot;.txt&amp;quot;): with open(os.path.join(DATASET_PATH, file), &amp;quot;r&amp;quot;) as f: content = f.read() title = content[:content.find(&amp;quot;\n&amp;quot;)] # Start content on the third line content = content[content.find(&amp;quot;\n&amp;quot;, content.find(&amp;quot;\n&amp;quot;) + 1) + 1 :] season = file[: file.find(&amp;quot;x&amp;quot;)] episode = file[file.find(&amp;quot;x&amp;quot;) + 1 : file.find(&amp;quot;.&amp;quot;)] examples.append({ &amp;quot;title&amp;quot;: title, &amp;quot;content&amp;quot;: content, &amp;quot;season&amp;quot;: season, &amp;quot;episode&amp;quot;: episode }) # examples.append([&amp;quot;title: &amp;quot; + title, &amp;quot;book content: &amp;quot; + content, &amp;quot;season: &amp;quot; + season, &amp;quot;episode: &amp;quot; + episode]) print(examples[0]) # %% from langchain.schema import Document # prompt = &amp;quot;Given the following scripts, rewrite the final two seasons so that the plots are more consistent with the seasons before.&amp;quot; # source = &amp;quot;&amp;quot; # currentSeason = 0 # for example in examples: # if example[&amp;quot;season&amp;quot;] != currentSeason: # if currentSeason != 0: # source += &amp;quot;&amp;lt;END OF SEASON&amp;gt;\n&amp;quot; # source += &amp;quot;\nSEASON &amp;quot; + str(example[&amp;quot;season&amp;quot;]) + &amp;quot;\n&amp;quot; # currentSeason = example[&amp;quot;season&amp;quot;] # source += example[&amp;quot;content&amp;quot;] + &amp;quot;&amp;lt;END OF EPISODE&amp;gt;\n&amp;quot; documents = [] for example in examples: documents.append( Document( page_content=example[&amp;quot;content&amp;quot;], metadata={ &amp;quot;title&amp;quot;: example[&amp;quot;title&amp;quot;], &amp;quot;season&amp;quot;: example[&amp;quot;season&amp;quot;], &amp;quot;episode&amp;quot;: example[&amp;quot;episode&amp;quot;] } ) ) # %% # from llama_cpp import Llama # model = &amp;quot;./mistral-7b-v0.1.Q6_K.gguf&amp;quot; # llm = Llama( # model_path=model, # n_ctx=8196, # n_batch=512, # n_threads=7, # n_gpu_layers=2, # verbose=True, # seed=42 # ) # message = f&amp;quot;&amp;lt;s&amp;gt;[INST] {prompt} [/INST]&amp;lt;/s&amp;gt;{source}&amp;quot; # # output = llm(message, echo=True, stream=True, max_tokens=4096) # stream = llm.create_completion( # message, # stream=True, # echo=True, # repeat_penalty=1.1, # max_tokens=4096, # stop=[&amp;quot;&amp;lt;END OF SEASON&amp;gt;&amp;quot;], #) # for output in stream: # print(output[&amp;quot;choices&amp;quot;][0][&amp;quot;text&amp;quot;].replace(message, &amp;quot;&amp;quot;)) # print(output[&amp;quot;usage&amp;quot;]) # output = output[&amp;quot;choices&amp;quot;][0][&amp;quot;text&amp;quot;].replace(message, &amp;quot;&amp;quot;) # print(output) # %% callbackManager = CallbackManager([StreamingStdOutCallbackHandler()]) llm = LlamaCpp( model_path=&amp;quot;./mistral-7b-v0.1.Q6_K.gguf&amp;quot;, n_ctx=8196, n_batch=512, n_threads=7, n_gpu_layers=1, f16_kv=True, verbose=False, seed=42, callback_manager=callbackManager ) pmt = PromptTemplate( template=&amp;quot;Question: {input}\nAnswer: Hello!&amp;lt;&amp;quot;, input_variables=[&amp;quot;input&amp;quot;] ) llm_chain = LLMChain(llm=llm, prompt=pmt) # %% from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter chunkSize = 4096 chunkOverlap = 4 splitter = RecursiveCharacterTextSplitter( chunk_size=chunkSize, chunk_overlap=chunkOverlap ) docs = splitter.split_documents(documents) print(f&amp;quot;from {len(documents)} to {len(docs)}&amp;quot;) # %% from langchain.vectorstores import Chroma from langchain_community.embeddings import LlamaCppEmbeddings storagePath = &amp;quot;./storage/chroma&amp;quot; if not os.path.exists(storagePath): os.makedirs(storagePath) embeddings = LlamaCppEmbeddings( model_path=&amp;quot;./mistral-7b-v0.1.Q6_K.gguf&amp;quot;, n_ctx=8196, n_batch=512, n_threads=7, n_gpu_layers=1, f16_kv=True, verbose=False, seed=42 ) # %% db = Chroma.from_documents( documents=docs, embedding=embeddings, persist_directory=storagePath, ) # %% %%capture captured --no-stdout llm_chain.run(&amp;quot;Hey, what&amp;#39;s up?&amp;quot;) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/leandrosq&quot;&gt; /u/leandrosq &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18zpl3w/book_rewrite_with_large_context_window/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18zpl3w/book_rewrite_with_large_context_window/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18zpl3w</id><link href="https://www.reddit.com/r/LangChain/comments/18zpl3w/book_rewrite_with_large_context_window/" /><updated>2024-01-06T02:55:24+00:00</updated><published>2024-01-06T02:55:24+00:00</published><title>Book rewrite with large context window</title></entry><entry><author><name>/u/Adventurous_Key_5341</name><uri>https://www.reddit.com/user/Adventurous_Key_5341</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Let&amp;#39;s say I have a CSV file with 100,000 comments related to a product. I would like to summarize all the comments and provide some recommendations on how to improve the product. &lt;/p&gt; &lt;p&gt;What is the best way to summarize this? Using RAG or chunking and feeding the entire list of comments to the LLM to process?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Adventurous_Key_5341&quot;&gt; /u/Adventurous_Key_5341 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18z51mu/rag_vs_batch_processing_in_large_document/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18z51mu/rag_vs_batch_processing_in_large_document/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18z51mu</id><link href="https://www.reddit.com/r/LangChain/comments/18z51mu/rag_vs_batch_processing_in_large_document/" /><updated>2024-01-05T11:46:34+00:00</updated><published>2024-01-05T11:46:34+00:00</published><title>RAG Vs. Batch Processing in Large Document Summarization</title></entry><entry><author><name>/u/tatyanaaaaaa</name><uri>https://www.reddit.com/user/tatyanaaaaaa</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18z5pch/endtoend_observability_for_langchain_script/&quot;&gt; &lt;img src=&quot;https://external-preview.redd.it/mO85YzUfer9KVyiJT_eCFGAbSyXAkrbUsykXu0xdDqk.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bced1e137c9eeaf99b33937b2bda15adac7124f4&quot; alt=&quot;End-to-end observability for LangChain script&quot; title=&quot;End-to-end observability for LangChain script&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Prompt: &amp;quot;What are the number of parameters in GPT5 and GPT4? What is the logarithm (base e) of the difference between the number of parameters?&amp;quot;&lt;/p&gt; &lt;p&gt;AimOS providing essential information about the trace:&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/u4walgoh5mac1.jpg?width=2880&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=ecdef81eb609552d333134a962397b4b483761dd&quot;&gt;AimOS&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Exploring the process: Steps tab provides a detailed walkthrough of the sequence of actions undertaken throughout the pipeline to achieve the specified goal. &lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/ju843lmi4mac1.jpg?width=2880&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=6c7c44b724f584d1f70bdf0027a2a17cbbdc9418&quot;&gt;AimOS&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The final answer to our question: The logarithm (base e) of the difference between the number of parameters in GPT5 and GPT4 is 4.605170185988092.&lt;/p&gt; &lt;p&gt;Cost tab including three graphs showing token-usage, token-usage-input, and token-usage-output, providing a detailed breakdown of the computational costs associated LangChain activities.&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/w3xgkjcv5mac1.jpg?width=2880&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=7e9460d625fd1a2c30291ba599ccd0ed2b33d1ad&quot;&gt;AimOS&lt;/a&gt;&lt;/p&gt; &lt;p&gt;AimOS has a Debugger for LangChain that logs LLMs prompts and generations, tools inputs/outputs, and chains metadata. &lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/aimhubio/aimos&quot;&gt;https://github.com/aimhubio/aimos&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/tatyanaaaaaa&quot;&gt; /u/tatyanaaaaaa &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18z5pch/endtoend_observability_for_langchain_script/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18z5pch/endtoend_observability_for_langchain_script/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_18z5pch</id><media:thumbnail url="https://external-preview.redd.it/mO85YzUfer9KVyiJT_eCFGAbSyXAkrbUsykXu0xdDqk.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=bced1e137c9eeaf99b33937b2bda15adac7124f4" /><link href="https://www.reddit.com/r/LangChain/comments/18z5pch/endtoend_observability_for_langchain_script/" /><updated>2024-01-05T12:24:47+00:00</updated><published>2024-01-05T12:24:47+00:00</published><title>End-to-end observability for LangChain script</title></entry><entry><author><name>/u/sarthak_uchiha</name><uri>https://www.reddit.com/user/sarthak_uchiha</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Is there any library or any way which helps in extracting pdf containing complex tables data and store , and how can we chunk that pdf data such that table data preserves in vector db ? Assuming each pdf contains around 5-10 pages&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/sarthak_uchiha&quot;&gt; /u/sarthak_uchiha &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18yxacm/extracting_data_from_pdf_containing_complex_tables/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18yxacm/extracting_data_from_pdf_containing_complex_tables/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18yxacm</id><link href="https://www.reddit.com/r/LangChain/comments/18yxacm/extracting_data_from_pdf_containing_complex_tables/" /><updated>2024-01-05T03:54:45+00:00</updated><published>2024-01-05T03:54:45+00:00</published><title>Extracting data from pdf containing complex tables</title></entry><entry><author><name>/u/Useful_Ad_7882</name><uri>https://www.reddit.com/user/Useful_Ad_7882</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;While it is easy to create streamlit/hosted apps using vector databases; i am looking to create a solution which ensures that user data [including vector database information] never leaves user device, leading to utmost privacy [unless search results for a RAG solution are sent to an LLM]&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;Anyone has had luck running chromaDB on mobile ? or any other vector databases that would work accordingly ? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Useful_Ad_7882&quot;&gt; /u/Useful_Ad_7882 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18yxpdg/chromadb_or_any_vector_database_for_mobile_devices/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18yxpdg/chromadb_or_any_vector_database_for_mobile_devices/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18yxpdg</id><link href="https://www.reddit.com/r/LangChain/comments/18yxpdg/chromadb_or_any_vector_database_for_mobile_devices/" /><updated>2024-01-05T04:15:53+00:00</updated><published>2024-01-05T04:15:53+00:00</published><title>ChromaDB or any vector database for mobile devices</title></entry><entry><author><name>/u/SustainedSuspense</name><uri>https://www.reddit.com/user/SustainedSuspense</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I want an LLM like GPT to be the brain of my automated workflow. I would define an SOP (standard operating procedure) for how to handle a transaction from start to finish. The transaction will be added to a Postgres DB and trigger an event via Amazon SNS, at which point id like my LangChain app to start processing the transaction (which has a JSON structure). There will be points in the processing of this transaction where API requests will need to be made based on certain conditions in the data. Ill need a way to store the current state of the transaction to know what should happen next (or possibly have the transaction in the db be the source of truth). There will be a step where ill need to send reminder emails to users once a day until they complete an action. &lt;/p&gt; &lt;p&gt;Can LangChain handle most of the thinking in my automated workflow? Eg ‚Äúbased on the current state of the transaction you should probably do X next.‚Äù&lt;/p&gt; &lt;p&gt;Any tutorials or reading you could recommend would be helpful. Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/SustainedSuspense&quot;&gt; /u/SustainedSuspense &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18ypaes/would_i_use_langchain_for_this/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18ypaes/would_i_use_langchain_for_this/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18ypaes</id><link href="https://www.reddit.com/r/LangChain/comments/18ypaes/would_i_use_langchain_for_this/" /><updated>2024-01-04T22:01:45+00:00</updated><published>2024-01-04T22:01:45+00:00</published><title>Would i use LangChain for this?</title></entry><entry><author><name>/u/modularmindapp</name><uri>https://www.reddit.com/user/modularmindapp</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18z5q0l/automate_marketing_content_generation_effortlessly/&quot;&gt; &lt;img src=&quot;https://external-preview.redd.it/MDllbzZ0bHY2bWFjMRVzvsqQW8F5a8H9mQhX4DL55qVzfrzQ7DPsaV4qySwB.png?width=140&amp;amp;height=140&amp;amp;crop=140:140,smart&amp;amp;format=jpg&amp;amp;v=enabled&amp;amp;lthumb=true&amp;amp;s=048153d7b3cff3ebe75b083a63527fd3b2acc727&quot; alt=&quot;Automate marketing content generation effortlessly&quot; title=&quot;Automate marketing content generation effortlessly&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/modularmindapp&quot;&gt; /u/modularmindapp &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://v.redd.it/5h7apd4t6mac1&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18z5q0l/automate_marketing_content_generation_effortlessly/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_18z5q0l</id><media:thumbnail url="https://external-preview.redd.it/MDllbzZ0bHY2bWFjMRVzvsqQW8F5a8H9mQhX4DL55qVzfrzQ7DPsaV4qySwB.png?width=140&amp;height=140&amp;crop=140:140,smart&amp;format=jpg&amp;v=enabled&amp;lthumb=true&amp;s=048153d7b3cff3ebe75b083a63527fd3b2acc727" /><link href="https://www.reddit.com/r/LangChain/comments/18z5q0l/automate_marketing_content_generation_effortlessly/" /><updated>2024-01-05T12:25:47+00:00</updated><published>2024-01-05T12:25:47+00:00</published><title>Automate marketing content generation effortlessly</title></entry><entry><author><name>/u/NickWang_</name><uri>https://www.reddit.com/user/NickWang_</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I have been developing based on langchain for a period of time recently, and later I came into contact with the service langsmith provided by it. The overall feeling is that the trace capability is quite powerful, but the test set management is not convenient to use. What did you use during the testing process? Test methods or dataset management tools.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/NickWang_&quot;&gt; /u/NickWang_ &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18yws8j/how_do_you_test_and_manage_use_cases_based_on_the/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18yws8j/how_do_you_test_and_manage_use_cases_based_on_the/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18yws8j</id><link href="https://www.reddit.com/r/LangChain/comments/18yws8j/how_do_you_test_and_manage_use_cases_based_on_the/" /><updated>2024-01-05T03:29:00+00:00</updated><published>2024-01-05T03:29:00+00:00</published><title>How do you test and manage use cases based on the langchain framework?</title></entry><entry><author><name>/u/InternationalMail954</name><uri>https://www.reddit.com/user/InternationalMail954</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi everyone I am trying to use langchain to reference and answer questions about documents and keep memory during the conversation about my document questions. I keep running into issues where it only references the documents in my Pinecone data base but has no conversation memory. Any help would be greatly appreciated. Code below:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;import streamlit as st from io import StringIO import pinecone pinecone.init(api_key=&amp;quot;&amp;quot;, environment=&amp;quot;gcp-starter&amp;quot;) from langchain import PromptTemplate from langchain.chat_models import ChatOpenAI from langchain.chains import LLMChain from langchain.chains import RetrievalQA from langchain.embeddings import OpenAIEmbeddings from langchain.vectorstores import Pinecone from langchain.text_splitter import RecursiveCharacterTextSplitter from langchain.memory import ConversationBufferMemory OPENAI_API_KEY = &amp;quot;&amp;quot; OPENAI_DIMENSION = 1536 embedding = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY) st.reupload_file = False vector_store = Pinecone.from_existing_index(&amp;quot;legal-cases&amp;quot;, embedding) index = pinecone.Index(&amp;#39;legal-cases&amp;#39;) if not hasattr(st.session_state, &amp;quot;convo_memory&amp;quot;): st.session_state.convo_memory = ConversationBufferMemory(return_messages=True) def upload_new_file_to_pinecone(text): embedding = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY) text_splitter = RecursiveCharacterTextSplitter( chunk_size=100, chunk_overlap=0, length_function=len ) chunks = text_splitter.create_documents([text]) result = Pinecone.from_documents(chunks, embedding, index_name=&amp;quot;legal-cases&amp;quot;) st.title(&amp;quot;üìù File Q&amp;amp;A&amp;quot;) uploaded_file = st.file_uploader(&amp;quot;Upload an article&amp;quot;, type=(&amp;quot;txt&amp;quot;, &amp;quot;md&amp;quot;)) if st.reupload_file: print(&amp;quot;updated file&amp;quot;) # To read file as bytes: bytes_data = uploaded_file.getvalue() # To convert to a string based IO: stringio = StringIO(uploaded_file.getvalue().decode(&amp;quot;utf-8&amp;quot;)) # To read file as string: string_data = stringio.read() index.delete(delete_all=True) upload_new_file_to_pinecone(string_data) def ask_and_get_answer(vector_store, q, k=3): from langchain.chains import RetrievalQA from langchain.chat_models import ChatOpenAI llm = ChatOpenAI(model=&amp;#39;gpt-3.5-turbo&amp;#39;, temperature=1, api_key=OPENAI_API_KEY) retriever = vector_store.as_retriever(search_type=&amp;#39;similarity&amp;#39;, search_kwargs={&amp;#39;k&amp;#39;: k}) chain = RetrievalQA.from_chain_type(llm=llm, chain_type=&amp;quot;stuff&amp;quot;, retriever=retriever, memory=st.session_state.convo_memory, ) answer = chain.run(q) return answer st.session_state.vs = vector_store k = 3 # user&amp;#39;s question text input widget q = st.text_input(&amp;#39;Ask a question about the content of your file:&amp;#39;) if q: # if the user entered a question and hit enter standard_answer = &amp;quot;&amp;quot; q = f&amp;quot;{q} {standard_answer}&amp;quot; if &amp;#39;vs&amp;#39; in st.session_state: # if there&amp;#39;s the vector store (user uploaded, split and embedded a file) vector_store = st.session_state.vs st.write(f&amp;#39;k: {k}&amp;#39;) answer = ask_and_get_answer(vector_store, q, k) # text area widget for the LLM answer st.text_area(&amp;#39;LLM Answer: &amp;#39;, value=answer) st.divider() # if there&amp;#39;s no chat history in the session state, create it if &amp;#39;history&amp;#39; not in st.session_state: st.session_state.history = &amp;#39;&amp;#39; # the current question and answer value = f&amp;#39;Q: {q} \nA: {answer}&amp;#39; st.session_state.history = f&amp;#39;{value} \n {&amp;quot;-&amp;quot; * 100} \n {st.session_state.history}&amp;#39; h = st.session_state.history # text area widget for the chat history st.text_area(label=&amp;#39;Chat History&amp;#39;, value=h, key=&amp;#39;history&amp;#39;, height=400) print(st.session_state.convo_memory) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/InternationalMail954&quot;&gt; /u/InternationalMail954 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18yovcm/please_help_with_langchain_want_both_document/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18yovcm/please_help_with_langchain_want_both_document/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18yovcm</id><link href="https://www.reddit.com/r/LangChain/comments/18yovcm/please_help_with_langchain_want_both_document/" /><updated>2024-01-04T21:44:41+00:00</updated><published>2024-01-04T21:44:41+00:00</published><title>Please help with langchain, want both document retrieval and conversation memory</title></entry><entry><author><name>/u/Ill_Bodybuilder3499</name><uri>https://www.reddit.com/user/Ill_Bodybuilder3499</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;I have built a RAG App with Langchain and used the &lt;a href=&quot;https://huggingface.co/intfloat/multilingual-e5-large&quot;&gt;intfloat/multilingual-e5-large&lt;/a&gt; embeddings so far. At the moment I tried oout different chunk sizes (100-2000) and I am wondering if the embedding model is relevant for the chunk size? I was wondering because I saw that the embedding size is 1024.&lt;/p&gt; &lt;p&gt;Thanks for suggestions!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Ill_Bodybuilder3499&quot;&gt; /u/Ill_Bodybuilder3499 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18yd77i/rag_embedding_model_relevant_to_chunk_size/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18yd77i/rag_embedding_model_relevant_to_chunk_size/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18yd77i</id><link href="https://www.reddit.com/r/LangChain/comments/18yd77i/rag_embedding_model_relevant_to_chunk_size/" /><updated>2024-01-04T13:29:22+00:00</updated><published>2024-01-04T13:29:22+00:00</published><title>RAG Embedding Model relevant to Chunk Size?</title></entry></feed>