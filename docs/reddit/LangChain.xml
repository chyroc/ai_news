<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/"><category term="LangChain" label="r/LangChain"/><updated>2024-08-04T02:44:45+00:00</updated><icon>https://www.redditstatic.com/icon.png/</icon><id>/r/LangChain.rss</id><link rel="self" href="https://www.reddit.com/r/LangChain.rss" type="application/atom+xml" /><link rel="alternate" href="https://www.reddit.com/r/LangChain" type="text/html" /><subtitle>LangChain is an open-source framework and developer toolkit that helps developers get LLM applications from prototype to production. It is available for Python and Javascript at https://www.langchain.com/.</subtitle><title>LangChain</title><entry><author><name>/u/zchaarm</name><uri>https://www.reddit.com/user/zchaarm</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;A place for members of &lt;a href=&quot;/r/LangChain&quot;&gt;r/LangChain&lt;/a&gt; to chat with each other&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/zchaarm&quot;&gt; /u/zchaarm &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_10ljho9</id><link href="https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/" /><updated>2023-01-26T04:33:05+00:00</updated><published>2023-01-26T04:33:05+00:00</published><title>r/LangChain Lounge</title></entry><entry><author><name>/u/AccordingPipe7759</name><uri>https://www.reddit.com/user/AccordingPipe7759</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I found this to be a really good use case for LLM agents but struggling to figure out the architecture or approach they took. Would anyone be able to decipher the high level architecture? &lt;a href=&quot;https://x.com/tryramp/status/1792659194996281478/mediaviewer&quot;&gt;https://x.com/tryramp/status/1792659194996281478/mediaviewer&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/AccordingPipe7759&quot;&gt; /u/AccordingPipe7759 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ejhc5n/how_did_ramp_build_llm_based_product_tours/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ejhc5n/how_did_ramp_build_llm_based_product_tours/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1ejhc5n</id><link href="https://www.reddit.com/r/LangChain/comments/1ejhc5n/how_did_ramp_build_llm_based_product_tours/" /><updated>2024-08-03T23:41:09+00:00</updated><published>2024-08-03T23:41:09+00:00</published><title>How did Ramp build LLM based product tours</title></entry><entry><author><name>/u/abhinavkimothi</name><uri>https://www.reddit.com/user/abhinavkimothi</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ej8nta/generating_contextual_llm_responses/&quot;&gt; &lt;img src=&quot;https://b.thumbs.redditmedia.com/ljLMRjQw2bCrR6swmG_8mUyksQcDdrWErXF037NS3Gk.jpg&quot; alt=&quot;Generating Contextual LLM Responses&quot; title=&quot;Generating Contextual LLM Responses&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/abhinavkimothi&quot;&gt; /u/abhinavkimothi &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/gallery/1ej8nta&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ej8nta/generating_contextual_llm_responses/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1ej8nta</id><media:thumbnail url="https://b.thumbs.redditmedia.com/ljLMRjQw2bCrR6swmG_8mUyksQcDdrWErXF037NS3Gk.jpg" /><link href="https://www.reddit.com/r/LangChain/comments/1ej8nta/generating_contextual_llm_responses/" /><updated>2024-08-03T17:17:02+00:00</updated><published>2024-08-03T17:17:02+00:00</published><title>Generating Contextual LLM Responses</title></entry><entry><author><name>/u/Repulsive-Bedroom883</name><uri>https://www.reddit.com/user/Repulsive-Bedroom883</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ejkbip/created_an_emotional_companion_you_can_call/&quot;&gt; &lt;img src=&quot;https://preview.redd.it/1uqwnhe53kgd1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=81fd30b247b058716e0c10a0c02724f047adbfe4&quot; alt=&quot;Created an Emotional Companion You Can Call Anytime, Anywhere&quot; title=&quot;Created an Emotional Companion You Can Call Anytime, Anywhere&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey Reddit community!&lt;/p&gt; &lt;p&gt;I’m excited to introduce a project I’ve been working on: an AI Companion that offers friendly support and companionship whenever you need it, accessible through a simple call.&lt;/p&gt; &lt;p&gt;This AI Companion is more than just a chatbot—it connects with you on a deeper level, recognizing your emotions through your voice and engaging in meaningful, personalized interactions. Whether you’re feeling alone, stressed, or just in need of a friendly chat, it’s here to provide empathy and understanding.&lt;/p&gt; &lt;p&gt;Why is this important? Finding genuine companionship and emotional support can be challenging, and this AI Companion aims to fill that gap, offering a comforting presence that’s always available.&lt;/p&gt; &lt;p&gt;Your privacy is a top priority. We ensure that no personal data or voice recordings are stored, so you can feel confident in your confidentiality and emotional safety.&lt;/p&gt; &lt;p&gt;In addition to conversational support, the AI Companion includes features like guided sleep sessions, meditation exercises, mindfulness practices, and mental health tools. These options allow for tailored guidance to support your well-being.&lt;/p&gt; &lt;p&gt;I’m keen to hear your feedback on what features you find most valuable and how this AI Companion can better assist you. If you’re interested in trying it out or have any questions, don’t hesitate to reach out!&lt;/p&gt; &lt;p&gt;Experience compassionate support, anytime, anywhere.&lt;/p&gt; &lt;p&gt;I look forward to hearing from you!&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://sakooon.vercel.app&quot;&gt;https://sakooon.vercel.app&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Repulsive-Bedroom883&quot;&gt; /u/Repulsive-Bedroom883 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://i.redd.it/1uqwnhe53kgd1.jpeg&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ejkbip/created_an_emotional_companion_you_can_call/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1ejkbip</id><media:thumbnail url="https://preview.redd.it/1uqwnhe53kgd1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=81fd30b247b058716e0c10a0c02724f047adbfe4" /><link href="https://www.reddit.com/r/LangChain/comments/1ejkbip/created_an_emotional_companion_you_can_call/" /><updated>2024-08-04T02:12:08+00:00</updated><published>2024-08-04T02:12:08+00:00</published><title>Created an Emotional Companion You Can Call Anytime, Anywhere</title></entry><entry><author><name>/u/DifficultNerve6992</name><uri>https://www.reddit.com/user/DifficultNerve6992</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ejh5zq/top_5_platforms_for_building_ai_agents/&quot;&gt; &lt;img src=&quot;https://external-preview.redd.it/95dqlyWw1xusGsLCWYRT9EW7QmnzJkG2BpWGFNyEJRQ.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=913dd133e4f30a84c8dacb7875d27e0c6f2b8f59&quot; alt=&quot;Top 5 Platforms for Building AI Agents&quot; title=&quot;Top 5 Platforms for Building AI Agents&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/DifficultNerve6992&quot;&gt; /u/DifficultNerve6992 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;/r/AIAgentsDirectory/comments/1ejggwi/top_5_platforms_for_building_ai_agents/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ejh5zq/top_5_platforms_for_building_ai_agents/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1ejh5zq</id><media:thumbnail url="https://external-preview.redd.it/95dqlyWw1xusGsLCWYRT9EW7QmnzJkG2BpWGFNyEJRQ.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=913dd133e4f30a84c8dacb7875d27e0c6f2b8f59" /><link href="https://www.reddit.com/r/LangChain/comments/1ejh5zq/top_5_platforms_for_building_ai_agents/" /><updated>2024-08-03T23:32:56+00:00</updated><published>2024-08-03T23:32:56+00:00</published><title>Top 5 Platforms for Building AI Agents</title></entry><entry><author><name>/u/Present_Owl742</name><uri>https://www.reddit.com/user/Present_Owl742</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;How do you actually get practical skills/exposure to build llm based apps? I&amp;#39;m quite new in the app development space amd have been playing with building a RAG based application for documentation. All well and good there...my question is now: what next? What are other exercises to upskill and gain more exposure? Are there hackathons or small assignments? What are you working on as a side project?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Present_Owl742&quot;&gt; /u/Present_Owl742 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ej2l1w/how_do_you_upskill_with_practical_assignments/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ej2l1w/how_do_you_upskill_with_practical_assignments/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1ej2l1w</id><link href="https://www.reddit.com/r/LangChain/comments/1ej2l1w/how_do_you_upskill_with_practical_assignments/" /><updated>2024-08-03T12:49:31+00:00</updated><published>2024-08-03T12:49:31+00:00</published><title>How do you upskill with practical assignments?</title></entry><entry><author><name>/u/ramo109</name><uri>https://www.reddit.com/user/ramo109</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;My challenge is to match an incoming query containing a description of a mechanical part and match it to a list of parts. The part list contains 30K parts with pretty short descriptions. Keyword search kind of works but would prefer for semantic understanding of the description of the part. &lt;/p&gt; &lt;p&gt;Currently, I’ve created embedding for every part but similarity search doesn’t always return good results. I’ve also tried feeding in the parts list chunk-wise in a chain to see if groups match up and then break that group down further. &lt;/p&gt; &lt;p&gt;Any other ideas that I might be missing?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/ramo109&quot;&gt; /u/ramo109 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ejbjhf/matching_query_to_category_list/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ejbjhf/matching_query_to_category_list/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1ejbjhf</id><link href="https://www.reddit.com/r/LangChain/comments/1ejbjhf/matching_query_to_category_list/" /><updated>2024-08-03T19:18:50+00:00</updated><published>2024-08-03T19:18:50+00:00</published><title>Matching query to category list</title></entry><entry><author><name>/u/litchiTheGreat</name><uri>https://www.reddit.com/user/litchiTheGreat</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;My company started using langchain and yaml templates to describe the response format to the prompts, issue is, the response sometime returns as unparsable yaml format. And we have no good way of validating it Aside from sending it again with the error hoping it will fix it. How do you go about handling the responses? I heard json responses are really popular option but it&amp;#39;s a bit restricting as you do not have comments in it so you can&amp;#39;t really specify things like which fields are optional or not and what is the desired return type for each field&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/litchiTheGreat&quot;&gt; /u/litchiTheGreat &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ej1ef4/how_are_u_all_validating_responses_in_different/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ej1ef4/how_are_u_all_validating_responses_in_different/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1ej1ef4</id><link href="https://www.reddit.com/r/LangChain/comments/1ej1ef4/how_are_u_all_validating_responses_in_different/" /><updated>2024-08-03T11:45:28+00:00</updated><published>2024-08-03T11:45:28+00:00</published><title>How are u all validating responses in different formats from gpt?</title></entry><entry><author><name>/u/Jazzlike_Tooth929</name><uri>https://www.reddit.com/user/Jazzlike_Tooth929</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;m thinking about founding a marketplace of AI agents for developers.&lt;/p&gt; &lt;p&gt;As far as I know, there is currently no platform for creating and sharing agents: if I build an agent for,say, financial analysis of a fortune 500 company, the only way to share it would be to share the source code. Monetizing it would be extremely hard. On the other hand, if I want to use (multi)-agents to solve a particular problem, I need to create and maintain the code for all the agents, and I&amp;#39;ll prbably be reinventing the wheel, as some of the agents would have been created by someone else before.&lt;/p&gt; &lt;p&gt;The idea is to create a platform where: &lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Devs who create agents could turn them into APIs and easily monetize&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Devs who want to use (multi)-agents to automate complex worflows could pick the best agents for certain common tasks from the platform by simply calling the API, instead of having to maintain the code and infra to run them.&lt;/li&gt; &lt;/ol&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Kinda like GPT store but from developers to developers. Wdyt? Would you use this?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Jazzlike_Tooth929&quot;&gt; /u/Jazzlike_Tooth929 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1eiqa8c/ai_agent_marketplace_validaterefute_this_idea/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1eiqa8c/ai_agent_marketplace_validaterefute_this_idea/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1eiqa8c</id><link href="https://www.reddit.com/r/LangChain/comments/1eiqa8c/ai_agent_marketplace_validaterefute_this_idea/" /><updated>2024-08-03T00:50:56+00:00</updated><published>2024-08-03T00:50:56+00:00</published><title>AI agent marketplace – validate/refute this idea</title></entry><entry><author><name>/u/NoInfluence5257</name><uri>https://www.reddit.com/user/NoInfluence5257</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;h1&gt;&lt;a href=&quot;https://stackoverflow.com/questions/78828381/getting-valueerror-missing-some-input-keys-n-id-in-langchain&quot;&gt;Getting ValueError: Missing some input keys: {&amp;#39;\n &amp;quot;_id&amp;quot;&amp;#39;} in Langchain&lt;/a&gt;&lt;/h1&gt; &lt;p&gt;I am trying to generate MongoDB aggregation queries using LangChain with OpenAI&amp;#39;s GPT-3.5. However, I am encountering a &lt;code&gt;ValueError&lt;/code&gt; stating that some input keys are missing.&lt;br/&gt; &lt;a href=&quot;https://stackoverflow.com/questions/78828381/getting-valueerror-missing-some-input-keys-n-id-in-langchain&quot;&gt;https://stackoverflow.com/questions/78828381/getting-valueerror-missing-some-input-keys-n-id-in-langchain&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/NoInfluence5257&quot;&gt; /u/NoInfluence5257 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1eizloe/can_anyone_solve_this_error/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1eizloe/can_anyone_solve_this_error/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1eizloe</id><link href="https://www.reddit.com/r/LangChain/comments/1eizloe/can_anyone_solve_this_error/" /><updated>2024-08-03T09:54:34+00:00</updated><published>2024-08-03T09:54:34+00:00</published><title>Can Anyone solve this error</title></entry><entry><author><name>/u/stolendog-1</name><uri>https://www.reddit.com/user/stolendog-1</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hello.&lt;br/&gt; I&amp;#39;m working on implementing a RAG solution on AWS and I&amp;#39;m curious about best practices for document storage (chunks). I&amp;#39;d love to hear about your experiences and preferences.&lt;/p&gt; &lt;p&gt;Specifically, I&amp;#39;m wondering:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Do you keep your document chunks in the same database as your vectors, or do you use separate storage? like S3 object storage? in S3 case you need download chunks each time?&lt;/li&gt; &lt;li&gt;If you use separate storage, what solution do you prefer? (e.g., S3 buckets, document databases, etc.)&lt;/li&gt; &lt;li&gt;For those using combined storage, what vector databases are you using that handle this well? I&amp;#39;m planning to use pg_vector on PostgreSQL&lt;/li&gt; &lt;li&gt;How do you handle metadata and linking between vectors and original documents in your setup?&lt;/li&gt; &lt;li&gt;Any pitfalls or lessons learned you&amp;#39;d be willing to share?&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;I&amp;#39;m particularly interested in solutions that scale well and remain cost-effective then document collection grows.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/stolendog-1&quot;&gt; /u/stolendog-1 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1eibcqw/document_storage_in_rag_solutions_separate_or/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1eibcqw/document_storage_in_rag_solutions_separate_or/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1eibcqw</id><link href="https://www.reddit.com/r/LangChain/comments/1eibcqw/document_storage_in_rag_solutions_separate_or/" /><updated>2024-08-02T14:23:59+00:00</updated><published>2024-08-02T14:23:59+00:00</published><title>Document Storage in RAG solutions: separate or combined with Vector DB?</title></entry><entry><author><name>/u/NTXL</name><uri>https://www.reddit.com/user/NTXL</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;so I&amp;#39;m working on a chatbot and I&amp;#39;m trying to implement a feature similar to the memory system used in ChatGPT. &lt;/p&gt; &lt;p&gt;Here&amp;#39;s my current idea:&lt;/p&gt; &lt;p&gt;Whenever the chatbot needs to store new information, it will first pass this data through a function that assesses its similarity to existing memories stored in a vector database. This function will identify the most similar match. then, both the new information and the closest match will be fed into a secondary model. that generates a structured object in my case a true or false to determine whether the new information is truly unique or just a variant of what’s already stored. &lt;/p&gt; &lt;p&gt;The outcome from this model will then dictate whether the i update a memory or insert a new one. i tried just checking the cosine similarity between the embeddings but it was very inconsistent. i am also not knowledgeable in NLP so this was all i could think of. i&amp;#39;d like to know if this approach is overkill and if their is an easier way?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/NTXL&quot;&gt; /u/NTXL &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1eigf6i/need_help_designing_a_persistent_memory_feature/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1eigf6i/need_help_designing_a_persistent_memory_feature/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1eigf6i</id><link href="https://www.reddit.com/r/LangChain/comments/1eigf6i/need_help_designing_a_persistent_memory_feature/" /><updated>2024-08-02T17:47:04+00:00</updated><published>2024-08-02T17:47:04+00:00</published><title>need help Designing a Persistent Memory Feature for a Chatbot like the chatgpt memory feature</title></entry><entry><author><name>/u/abcdedcbaa</name><uri>https://www.reddit.com/user/abcdedcbaa</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi. I&amp;#39;m new to langchain, literally was only able to learn how to create an llm through bedrock. I&amp;#39;m getting a feeling that it would help me with the chatbot simulating a person we&amp;#39;re developing rn but somehow I&amp;#39;m still not sold yet. Right now backend is running on Python (aws is accessed via boto3) including ways to fill out the variables in a template prompt. About four of the variables are supposed to have two to three sub events with equal chances being instructed to the LLM (e.g. equal chance to say 1. he found his wallet he lost yesterday and 2. he cannot find it anymore and he&amp;#39;s letting go of it). I can code this with Python but I can imagine it would be hard to scale up if say there are already ten scenarios of five variables. Is there any functionality of langchain framework that can help me with this?&lt;/p&gt; &lt;p&gt;Also, another question which is probably related or not to langchain as a solution. We are planning to append a whole ass transcript or two of what we expect the chat should go. Right now we have two whole transcripts on the main prompt. It seems like it&amp;#39;s not giving us any issue right now but leads are saying two transcripts aren&amp;#39;t enough and we should add more, about three or four for each of the eight intents. Has anyone already tried appending transcripts to their main prompt for chatbot and had issues with hallucination? Is there any langchain stuff that could help us with this?&lt;/p&gt; &lt;p&gt;We are using Claude 3.5 btw via bedrock.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/abcdedcbaa&quot;&gt; /u/abcdedcbaa &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1eilbh3/chatbot_dev_pure_python_with_langchain/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1eilbh3/chatbot_dev_pure_python_with_langchain/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1eilbh3</id><link href="https://www.reddit.com/r/LangChain/comments/1eilbh3/chatbot_dev_pure_python_with_langchain/" /><updated>2024-08-02T21:08:14+00:00</updated><published>2024-08-02T21:08:14+00:00</published><title>Chatbot dev pure python with langchain</title></entry><entry><author><name>/u/GPT-Claude-Gemini</name><uri>https://www.reddit.com/user/GPT-Claude-Gemini</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Not sure if I am the only one that notice this, but the performance of Gemini on Langchain has been highly unreliable, a few examples:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Gemini&amp;#39;s stream would often just stop midway without ever being completed (making Gemini mostly unuseable)&lt;/li&gt; &lt;li&gt;Can&amp;#39;t get the input/output token count after each Gemini API request&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Is this a problem on Gemini&amp;#39;s side or with the Langchain abstraction? Is there an estimated timeline that these issues can be solved?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/GPT-Claude-Gemini&quot;&gt; /u/GPT-Claude-Gemini &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ei45mq/is_the_poor_performance_of_gemini_on_langchain/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ei45mq/is_the_poor_performance_of_gemini_on_langchain/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1ei45mq</id><link href="https://www.reddit.com/r/LangChain/comments/1ei45mq/is_the_poor_performance_of_gemini_on_langchain/" /><updated>2024-08-02T07:40:37+00:00</updated><published>2024-08-02T07:40:37+00:00</published><title>Is the poor performance of Gemini on Langchain caused by Langchain or Google?</title></entry><entry><author><name>/u/MagentaSpark</name><uri>https://www.reddit.com/user/MagentaSpark</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;h1&gt;TLDR&lt;/h1&gt; &lt;p&gt;What do you use?&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Enums as tool parameter arguments&lt;/li&gt; &lt;li&gt;Arguments transposed as parameters with their value set to Boolean&lt;/li&gt; &lt;/ol&gt; &lt;h1&gt;How to define a tool/function for LLM&lt;/h1&gt; &lt;p&gt;While it is convenient to use the actual function/tool in your code to be sent as tool schema to the LLM and receive arguments, it might not be always be ideal. Here are two common examples:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Tasks like conditional routing where reasoning is done by an LLM is more dependent on tool definition and less code logic.&lt;/li&gt; &lt;li&gt;Database Query filtering: The unreliable (multiple points of failure) Text-to-SQL/DSL workload of databases with low complexity and expectations can be abstracted away with premade query with filter parameters of which, arguments will be computed by LLM.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;How do you define such tools? I see two approaches.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Functions with parameter type as Enum, which confines the number of possible arguments for that parameter&lt;/li&gt; &lt;li&gt;Functions with possible arguments itself as parameters, and the actual arguments as boolean value.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Which approach as worked for you and your LLM/system? Are there any more things you would like to add?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/MagentaSpark&quot;&gt; /u/MagentaSpark &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ei9rbi/tool_calling_patterns_langgraph/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ei9rbi/tool_calling_patterns_langgraph/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1ei9rbi</id><link href="https://www.reddit.com/r/LangChain/comments/1ei9rbi/tool_calling_patterns_langgraph/" /><updated>2024-08-02T13:14:04+00:00</updated><published>2024-08-02T13:14:04+00:00</published><title>Tool calling patterns: LangGraph</title></entry><entry><author><name>/u/Spare-Badger2244</name><uri>https://www.reddit.com/user/Spare-Badger2244</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Is there a way to use langchain and Ollama with with an api key. Similar to how OpenAI works ?&lt;/p&gt; &lt;p&gt;Currently I have Ollama running and am able to set up a retrieval chain and query the llm but is there a way to query Ollama with langchain using an api key ? Cause currently any one on the network can just start using that resource. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Spare-Badger2244&quot;&gt; /u/Spare-Badger2244 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1eihjbb/can_i_use_ollama_and_langchain_with_api_key/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1eihjbb/can_i_use_ollama_and_langchain_with_api_key/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1eihjbb</id><link href="https://www.reddit.com/r/LangChain/comments/1eihjbb/can_i_use_ollama_and_langchain_with_api_key/" /><updated>2024-08-02T18:32:57+00:00</updated><published>2024-08-02T18:32:57+00:00</published><title>Can I use Ollama and langchain with api key ?</title></entry><entry><author><name>/u/lzyTitan412</name><uri>https://www.reddit.com/user/lzyTitan412</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=pLPJoFvq4_M&quot;&gt;LangGraph Studio: The first agent IDE (youtube.com)&lt;/a&gt; -- check this out.&lt;/p&gt; &lt;p&gt;Just a week back, I was thinking of developing a web app kind of interface for langgraph, and they just launched it. Now, what if there were a drag-and-drop-like application for creating a complex graph chain?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/lzyTitan412&quot;&gt; /u/lzyTitan412 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ehp7h5/langgraph_studio_is_amazing/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ehp7h5/langgraph_studio_is_amazing/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1ehp7h5</id><link href="https://www.reddit.com/r/LangChain/comments/1ehp7h5/langgraph_studio_is_amazing/" /><updated>2024-08-01T19:18:21+00:00</updated><published>2024-08-01T19:18:21+00:00</published><title>LangGraph Studio is amazing</title></entry><entry><author><name>/u/Jen1888Mik</name><uri>https://www.reddit.com/user/Jen1888Mik</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I use this code for write embeddings in my database:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;const res = await ConvexVectorStore.fromDocuments(splitDocs, embeddings, { ctx }); &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Logic my app is - get id embedding from db and pass it in Langchain context . But i need get id my embeddings from db - variable res don`t give me this info. How to do this?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Jen1888Mik&quot;&gt; /u/Jen1888Mik &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1eib3t3/how_to_return_id_from_db/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1eib3t3/how_to_return_id_from_db/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1eib3t3</id><link href="https://www.reddit.com/r/LangChain/comments/1eib3t3/how_to_return_id_from_db/" /><updated>2024-08-02T14:13:37+00:00</updated><published>2024-08-02T14:13:37+00:00</published><title>How to return id from db ConvexVectoreStore.fromDocuments</title></entry><entry><author><name>/u/RiverOtterBae</name><uri>https://www.reddit.com/user/RiverOtterBae</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I have my existent backend set up as a bunch of serverless functions at the moment (cloudflare workers). I wanted to set up a new `/chat` endpoint as just another serverless function which uses langchain on the server. But as I get deep into the code I&amp;#39;m not sure if it makes sense to do it this way...&lt;/p&gt; &lt;p&gt;Basically if I have Langchain running on this endpoint, since servelerless functions are stateless, that means each time the user sends a new message I need to fetch the chat history from the database, load it into context, process the request (generate the next response) and then tear it all down only to have to build it all up again with the next request. Since there is also no persistent connection.&lt;/p&gt; &lt;p&gt;This all seems a bit wasteful in my opinion. If I host langchain on the client I&amp;#39;m thinking I can avoid all this extra work since the langchain &amp;quot;instance&amp;quot; will stay put for the duration of the chat session. Once the long context is loaded in memory I only need to add new messages to it vs redoing the whole thing which can get very taxing for loooong conversations.&lt;/p&gt; &lt;p&gt;But I would prefer to handle it on the server side to hide the prompt magic &amp;quot;special sauce&amp;quot; if possible... &lt;/p&gt; &lt;p&gt;How are ya&amp;#39;ll serving your langchain apps in production?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/RiverOtterBae&quot;&gt; /u/RiverOtterBae &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ehwzcr/where_are_you_running_langchain_in_your/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ehwzcr/where_are_you_running_langchain_in_your/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1ehwzcr</id><link href="https://www.reddit.com/r/LangChain/comments/1ehwzcr/where_are_you_running_langchain_in_your/" /><updated>2024-08-02T00:54:32+00:00</updated><published>2024-08-02T00:54:32+00:00</published><title>Where are you running Langchain in your production apps? (serverless / on the client / somewhere else)???</title></entry><entry><author><name>/u/Top_Raccoon_1493</name><uri>https://www.reddit.com/user/Top_Raccoon_1493</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I am implementing Weaviate from yesterday with docker on langchain, its tough implementing&lt;br/&gt; Can anyone share some tutorials or is it better as compared to Qdrant&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Top_Raccoon_1493&quot;&gt; /u/Top_Raccoon_1493 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1eia9w0/how_to_implement_weaviate_with_docker/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1eia9w0/how_to_implement_weaviate_with_docker/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1eia9w0</id><link href="https://www.reddit.com/r/LangChain/comments/1eia9w0/how_to_implement_weaviate_with_docker/" /><updated>2024-08-02T13:37:49+00:00</updated><published>2024-08-02T13:37:49+00:00</published><title>How to implement Weaviate with docker?</title></entry><entry><author><name>/u/tys203831</name><uri>https://www.reddit.com/user/tys203831</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Let&amp;#39;s say I have 50 website API endpoints to get the data, and I am going to wrap them inside 50 agents... I&amp;#39;m not sure if this is the right way to do, as I&amp;#39;m afraid the agent will possibly messed up when the API endpoints keep growing...&lt;/p&gt; &lt;p&gt;Instead, I am thinking if I could RAG on tools to be used based on the user query, and then trigger those function tool based the output returned by RAG... Is this something feasible and perhaps more scalable than agents?&lt;/p&gt; &lt;p&gt;I&amp;#39;m not that sure the scalability of agents when there is a lot of data endpoints to access with. Hope for help, thanks.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/tys203831&quot;&gt; /u/tys203831 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ehyf74/using_rag_to_choose_tools_vs_agents_which_is/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ehyf74/using_rag_to_choose_tools_vs_agents_which_is/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1ehyf74</id><link href="https://www.reddit.com/r/LangChain/comments/1ehyf74/using_rag_to_choose_tools_vs_agents_which_is/" /><updated>2024-08-02T02:05:20+00:00</updated><published>2024-08-02T02:05:20+00:00</published><title>Using RAG to choose tools vs agents, which is better choices? If accuracy matters</title></entry><entry><author><name>/u/The_Wolfiee</name><uri>https://www.reddit.com/user/The_Wolfiee</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I am using an LLM that is a fine tuned version of Llama 3 on a cybersecurity dataset that recognises vulnerable code blocks and suggests steps to remediates the vulnerablities with fixed code. &lt;/p&gt; &lt;p&gt;I tried the same LLM and prompt with LangChain and Llama CPP but I get different results from each of them. &lt;/p&gt; &lt;p&gt;In Llama CPP, I get the suggested steps and fixed code block but with LangChain (using the Llama CPP abstraction), I get only the steps. &lt;/p&gt; &lt;p&gt;The prompt format is Llama-Chat 2 and the prompt specifically says &amp;quot;provide a code block that fixes the vulnerability&amp;quot;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/The_Wolfiee&quot;&gt; /u/The_Wolfiee &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ei1nol/different_results_with_same_prompt_and_llm_but/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ei1nol/different_results_with_same_prompt_and_llm_but/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1ei1nol</id><link href="https://www.reddit.com/r/LangChain/comments/1ei1nol/different_results_with_same_prompt_and_llm_but/" /><updated>2024-08-02T04:57:50+00:00</updated><published>2024-08-02T04:57:50+00:00</published><title>Different results with same prompt and LLM but different framework?</title></entry><entry><author><name>/u/Grand_Worker_7417</name><uri>https://www.reddit.com/user/Grand_Worker_7417</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hello, I am building an AI assistant in langgraph and while one of the agents works very fast, the other has issues because of 2 reasons: first one can&amp;#39;t be solved as we do need gpt4 for it, while the previously mentioned agent runs on gpt3.5 turbo, but the other reason is like to tackle is that for this agent, state messages build up very fast very long because of the amount of tools being used. Is there a way to compress the state or reduce state size to send less tokens to the model? Anyone knows?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Grand_Worker_7417&quot;&gt; /u/Grand_Worker_7417 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ei7fvd/reducing_length_of_state_in_langgraph/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ei7fvd/reducing_length_of_state_in_langgraph/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1ei7fvd</id><link href="https://www.reddit.com/r/LangChain/comments/1ei7fvd/reducing_length_of_state_in_langgraph/" /><updated>2024-08-02T11:16:59+00:00</updated><published>2024-08-02T11:16:59+00:00</published><title>Reducing length of State in LangGraph</title></entry><entry><author><name>/u/Shiro_94</name><uri>https://www.reddit.com/user/Shiro_94</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hello everyone,&lt;/p&gt; &lt;p&gt;I am currently looking at some opensource RAG such as Langchain and Llama index. Quite like Llama index but it does not seem suitable for production. I did not find the capability of doing batch inference especially for retrieving the closest chunks for a batch of query. (so lack of scalability here)&lt;br/&gt; Langchain seems to have this feature (correct me if I am wrong but they are extracting the embeddings of queries by batch and not using multiple workers =&amp;gt; one embedding model call instead of N call if we have N queries)&lt;/p&gt; &lt;p&gt;I was wondering if there are others open source RAG worth for production other than langchain allowing:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Vector Store&lt;/li&gt; &lt;li&gt;Chunking &amp;amp; Document upload of different type (pdf, docx, raw text etc)&lt;/li&gt; &lt;li&gt;scalability (such as batch for queries =&amp;gt; embedding model call made by batch)&lt;/li&gt; &lt;li&gt;flexible about choosing the embedding model (HF, OpenAI etc)&lt;/li&gt; &lt;li&gt;good feature about the retriever such as filtering from metadata&lt;/li&gt; &lt;li&gt;good postprocess function (or possibility to add custom function) such as chunk merging etc&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Thanks for the help!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Shiro_94&quot;&gt; /u/Shiro_94 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ehtdhs/open_source_rag_best_for_production/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ehtdhs/open_source_rag_best_for_production/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1ehtdhs</id><link href="https://www.reddit.com/r/LangChain/comments/1ehtdhs/open_source_rag_best_for_production/" /><updated>2024-08-01T22:10:49+00:00</updated><published>2024-08-01T22:10:49+00:00</published><title>Open Source RAG - best for production?</title></entry><entry><author><name>/u/Select-Coconut-1161</name><uri>https://www.reddit.com/user/Select-Coconut-1161</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey everyone. I am trying to filter my documents by metadata. I was using ChromaDB and the code below was working just fine.&lt;/p&gt; &lt;p&gt;For basic_retriever:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;basic_retriever = _vector_store.as_retriever( search_kwargs={ &amp;quot;k&amp;quot;: 20, &amp;quot;filter&amp;quot;: { &amp;quot;$and&amp;quot;: [ {&amp;quot;speaker&amp;quot;: {&amp;quot;$eq&amp;quot;: &amp;quot;Participant&amp;quot;}}, {&amp;quot;participant&amp;quot;: {&amp;quot;$eq&amp;quot;: participant_id}}, {&amp;quot;part&amp;quot;: {&amp;quot;$in&amp;quot;: parts}} ] } } ) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;For self_query_retriever:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;self_query_retriever = SelfQueryRetriever.from_llm( llm, _vector_store, metadata_field_info=metadata_field_info, document_contents=&amp;quot;text&amp;quot;, verbose=True, enable_limit=False, search_kwargs={ &amp;quot;filter&amp;quot;: { &amp;#39;$and&amp;#39;: [ {&amp;quot;speaker&amp;quot;: {&amp;quot;$eq&amp;quot;: &amp;quot;Participant&amp;quot;}}, {&amp;quot;participant&amp;quot;: {&amp;quot;$eq&amp;quot;: participant_id}}, {&amp;quot;part&amp;quot;: {&amp;quot;$in&amp;quot;: parts}} ] } }, context_similarity=&amp;quot;context&amp;quot;, ) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;However. I had to migrate to MongoDB and the structure above does not work. I tried to follow documentation and did these:&lt;/p&gt; &lt;p&gt;For basic retriever:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;basic_retriever = _vector_store.as_retriever( search_kwargs={ &amp;quot;k&amp;quot;: 20, &amp;quot;pre_filter&amp;quot;: { &amp;quot;$and&amp;quot;: [ { &amp;quot;text&amp;quot;: { &amp;quot;path&amp;quot;: &amp;quot;speaker&amp;quot;, &amp;quot;query&amp;quot;: &amp;quot;Participant&amp;quot; } }, { &amp;quot;text&amp;quot;: { &amp;quot;path&amp;quot;: &amp;quot;participant&amp;quot;, &amp;quot;query&amp;quot;: participant_id } }, { &amp;quot;text&amp;quot;: { &amp;quot;path&amp;quot;: &amp;quot;part&amp;quot;, &amp;quot;query&amp;quot;: { &amp;quot;$in&amp;quot;: parts } } } ] } } ) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;For self_query_retriever:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;self_query_retriever = SelfQueryRetriever.from_llm( llm, _vector_store, metadata_field_info=metadata_field_info, document_contents=&amp;quot;text&amp;quot;, verbose=True, enable_limit=False, search_kwargs={ &amp;quot;pre_filter&amp;quot;: { &amp;quot;$and&amp;quot;: [ { &amp;quot;text&amp;quot;: { &amp;quot;path&amp;quot;: &amp;quot;speaker&amp;quot;, &amp;quot;query&amp;quot;: &amp;quot;Participant&amp;quot; } }, { &amp;quot;text&amp;quot;: { &amp;quot;path&amp;quot;: &amp;quot;participant&amp;quot;, &amp;quot;query&amp;quot;: participant_id } }, { &amp;quot;text&amp;quot;: { &amp;quot;path&amp;quot;: &amp;quot;part&amp;quot;, &amp;quot;query&amp;quot;: { &amp;quot;$in&amp;quot;: parts } } } ] } }, context_similarity=&amp;quot;context&amp;quot;, ) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;But it does not work. I get the following error:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;PlanExecutor error during aggregation :: caused by :: &amp;quot;filter[0]&amp;quot; must be a boolean, objectId, number, string, date, uuid, or null, full error: {&amp;#39;ok&amp;#39;: 0.0, &amp;#39;errmsg&amp;#39;: &amp;#39;PlanExecutor error during aggregation :: caused by :: &amp;quot;filter[0]&amp;quot; must be a boolean, objectId, number, string, date, uuid, or null&amp;#39;, &amp;#39;code&amp;#39;: 8, &amp;#39;codeName&amp;#39;: &amp;#39;UnknownError&amp;#39;, &amp;#39;$clusterTime&amp;#39;: {&amp;#39;clusterTime&amp;#39;: Timestamp(1722587696, 1), &amp;#39;signature&amp;#39;: {&amp;#39;hash&amp;#39;: b&amp;#39;&amp;quot;\xb7\x80\xdbA\xc1o\xe0?\x91\xce\x9b\x7f\xbd\xe2l\xe6\xfc&amp;#39;, &amp;#39;keyId&amp;#39;: 7335879767352147970}}, &amp;#39;operationTime&amp;#39;: Timestamp(1722587696, 1)} &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Also, I set my vector_search_index like that:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;{ &amp;quot;fields&amp;quot;: [ { &amp;quot;numDimensions&amp;quot;: 1536, &amp;quot;path&amp;quot;: &amp;quot;embedding&amp;quot;, &amp;quot;similarity&amp;quot;: &amp;quot;cosine&amp;quot;, &amp;quot;type&amp;quot;: &amp;quot;vector&amp;quot; }, { &amp;quot;path&amp;quot;: &amp;quot;timestamp&amp;quot;, &amp;quot;type&amp;quot;: &amp;quot;filter&amp;quot; }, { &amp;quot;path&amp;quot;: &amp;quot;speaker&amp;quot;, &amp;quot;type&amp;quot;: &amp;quot;filter&amp;quot; }, { &amp;quot;path&amp;quot;: &amp;quot;context&amp;quot;, &amp;quot;type&amp;quot;: &amp;quot;filter&amp;quot; }, { &amp;quot;path&amp;quot;: &amp;quot;participant&amp;quot;, &amp;quot;type&amp;quot;: &amp;quot;filter&amp;quot; }, { &amp;quot;path&amp;quot;: &amp;quot;metadata.part&amp;quot;, &amp;quot;type&amp;quot;: &amp;quot;filter&amp;quot; } ] } &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;How do I properly filter my metadata?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Select-Coconut-1161&quot;&gt; /u/Select-Coconut-1161 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ei4yts/having_problems_with_filtering_by_metadata_when/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ei4yts/having_problems_with_filtering_by_metadata_when/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1ei4yts</id><link href="https://www.reddit.com/r/LangChain/comments/1ei4yts/having_problems_with_filtering_by_metadata_when/" /><updated>2024-08-02T08:36:36+00:00</updated><published>2024-08-02T08:36:36+00:00</published><title>Having problems with filtering by metadata when using MongoDB</title></entry><entry><author><name>/u/moonbunR</name><uri>https://www.reddit.com/user/moonbunR</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/moonbunR&quot;&gt; /u/moonbunR &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;/r/SmythOS/comments/1efnjke/how_does_an_llm_orchestrator_decide_which_agent/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ehjtby/how_does_an_llm_orchestrator_decide_which_agent/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1ehjtby</id><link href="https://www.reddit.com/r/LangChain/comments/1ehjtby/how_does_an_llm_orchestrator_decide_which_agent/" /><updated>2024-08-01T15:40:52+00:00</updated><published>2024-08-01T15:40:52+00:00</published><title>How does an LLM orchestrator decide which agent to use in a multi-agent system?</title></entry></feed>