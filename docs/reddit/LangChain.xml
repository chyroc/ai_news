<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/"><category term="LangChain" label="r/LangChain"/><updated>2024-06-28T05:14:52+00:00</updated><icon>https://www.redditstatic.com/icon.png/</icon><id>/r/LangChain.rss</id><link rel="self" href="https://www.reddit.com/r/LangChain.rss" type="application/atom+xml" /><link rel="alternate" href="https://www.reddit.com/r/LangChain" type="text/html" /><subtitle>LangChain is an open-source framework and developer toolkit that helps developers get LLM applications from prototype to production. It is available for Python and Javascript at https://www.langchain.com/.</subtitle><title>LangChain</title><entry><author><name>/u/zchaarm</name><uri>https://www.reddit.com/user/zchaarm</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;A place for members of &lt;a href=&quot;/r/LangChain&quot;&gt;r/LangChain&lt;/a&gt; to chat with each other&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/zchaarm&quot;&gt; /u/zchaarm &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_10ljho9</id><link href="https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/" /><updated>2023-01-26T04:33:05+00:00</updated><published>2023-01-26T04:33:05+00:00</published><title>r/LangChain Lounge</title></entry><entry><author><name>/u/trance_dude19</name><uri>https://www.reddit.com/user/trance_dude19</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hello, sorry for posting something technical here but I can&amp;#39;t find a better forum. I am using LangSmith to track LangChain runs per this:&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://docs.smith.langchain.com/old/tracing/integrations/python&quot;&gt;https://docs.smith.langchain.com/old/tracing/integrations/python&lt;/a&gt;&lt;/p&gt; &lt;p&gt;which only requires two lines of config code and not the repeated use of the &lt;strong&gt;traceable&lt;/strong&gt; decorator. I now wish to add metadata to all traces. But the only way I can find in the docs to do that is to use traceable(metadata). Is there a way to add metadata to all runs without the use of traceable? thx&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/trance_dude19&quot;&gt; /u/trance_dude19 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dq00g3/add_metadata_to_langsmith_traces/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dq00g3/add_metadata_to_langsmith_traces/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dq00g3</id><link href="https://www.reddit.com/r/LangChain/comments/1dq00g3/add_metadata_to_langsmith_traces/" /><updated>2024-06-27T19:36:17+00:00</updated><published>2024-06-27T19:36:17+00:00</published><title>add metadata to langsmith traces</title></entry><entry><author><name>/u/Txflip</name><uri>https://www.reddit.com/user/Txflip</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;m trying to set up a &lt;code&gt;PydanticOutPutParser&lt;/code&gt; instance at the end of a RAG LCEL chain, but am receiving the error&lt;/p&gt; &lt;p&gt;&lt;code&gt;TypeError: argument &amp;#39;text&amp;#39;: &amp;#39;dict&amp;#39; object cannot be converted to &amp;#39;PyString&amp;#39;&lt;/code&gt;&lt;/p&gt; &lt;p&gt;This is my associated code&lt;/p&gt; &lt;pre&gt;&lt;code&gt;from langchain_core.runnables import ( RunnableParallel, RunnablePassthrough ) from langchain_core.output_parsers import PydanticOutputParser from langchain_core.pydantic_v1 import ( BaseModel, Field ) from langchain_core.prompts import PromptTemplate from langchain.schema.output_parser import StrOutputParser class Fee(BaseModel): fee_subject: str = Field(description=&amp;quot;The subject in which the fee relates to.&amp;quot;) fee_amount: float = Field(description=&amp;quot;The dollar cost of the fee.&amp;quot;) class Fees(BaseModel): fees: List[Fee] = Field(description=&amp;quot;List of fees.&amp;quot;) vectorstore = Milvus.from_texts( texts=all_texts, embedding=OpenAIEmbeddings(), connection_args={&amp;quot;uri&amp;quot;: URI}, drop_old=True ) retriever = vectorstore.as_retriever() pydantic_output_parser = PydanticOutputParser(pydantic_object=Fees) test_prompt = &amp;quot;&amp;quot;&amp;quot; You are a fee-finding support assistant. Your job is to find any applicable fees relating to a person&amp;#39;s query. Return the fee and fee amount related to each part of a person&amp;#39;s query. If you don&amp;#39;t find anything, then return $0. Do not make up fees. You are given supporting context to pull information from along with the original question. \n{format_instructions}\n Question: {question} Context: {context} Answer: &amp;quot;&amp;quot;&amp;quot; test_prompt_template = PromptTemplate( template=test_prompt, input_variables=[&amp;#39;question&amp;#39;, &amp;#39;context&amp;#39;], partial_variables={&amp;quot;format_instructions&amp;quot;: pydantic_output_parser.get_format_instructions()}) retrieval = RunnableParallel( {&amp;#39;context&amp;#39;: retriever, &amp;#39;question&amp;#39;: RunnablePassthrough()} ) model = Ollama( model=&amp;quot;llama3&amp;quot;, temperature=0 ) str_output_parser = StrOutputParser() chain = retrieval | test_prompt_template | model | pydantic_output_parser question = &amp;quot;I have a shipment being delivered to an airport. What amount in fees can I expect from shipping with XPO?&amp;quot; output = chain.invoke({&amp;quot;question&amp;quot;: question}) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The error is happening when I invoke the chain. What am I missing here?&lt;/p&gt; &lt;p&gt;When I then change the &lt;code&gt;output = chain.invoke({&amp;quot;question&amp;quot;: question})&lt;/code&gt; to &lt;code&gt;output = chain.invoke(question)&lt;/code&gt;, I get a new error&lt;/p&gt; &lt;pre&gt;&lt;code&gt;OutputParserException: Invalid json output: A treasure trove of fees! &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The &amp;quot;treasure trove...&amp;quot; part is output from the model. It is not following the Pydantic output format. What is happening here, and why couldn&amp;#39;t I use the dictionary format for &lt;code&gt;invoke()&lt;/code&gt;?&lt;/p&gt; &lt;p&gt;FYI, I have the &lt;code&gt;{format_instructions}&lt;/code&gt; in the prompt because that is what I did in a previous piece of code, but not sure if that is correct in this context.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Txflip&quot;&gt; /u/Txflip &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dq8yob/trouble_setting_up_pydanticoutputparser_with_lcel/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dq8yob/trouble_setting_up_pydanticoutputparser_with_lcel/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dq8yob</id><link href="https://www.reddit.com/r/LangChain/comments/1dq8yob/trouble_setting_up_pydanticoutputparser_with_lcel/" /><updated>2024-06-28T02:32:45+00:00</updated><published>2024-06-28T02:32:45+00:00</published><title>Trouble setting up PydanticOutputParser with LCEL RAG</title></entry><entry><author><name>/u/dccpt</name><uri>https://www.reddit.com/user/dccpt</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dpt9iz/extract_data_from_chat_history_quickly_and/&quot;&gt; &lt;img src=&quot;https://external-preview.redd.it/9f3qHXFEDc5moxlRaP4wYclBrxl1FfQFS0lbxr1ol8s.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7cc629947259c03fb2bbebe47efc15b73e5319d5&quot; alt=&quot;Extract Data From Chat History: Quickly and Accurately&quot; title=&quot;Extract Data From Chat History: Quickly and Accurately&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi all - several recent posts here have discussed the challenges of extracting structured data from chat histories. This is a common challenge: fulfilling sales orders, collecting support info, booking meetings/appointments, and more.&lt;/p&gt; &lt;p&gt;Zep’s new &lt;a href=&quot;https://blog.getzep.com/structured-data-extraction/&quot;&gt;Structured Data Extraction&lt;/a&gt; is a high-accuracy tool for extracting data from chat histories. It&amp;#39;s also 10x faster than gpt-4o.&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://i.redd.it/nwrcdkgwo49d1.gif&quot;&gt;https://i.redd.it/nwrcdkgwo49d1.gif&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Versus OpenAI JSON Mode&lt;/h1&gt; &lt;p&gt;OpenAI (or other LLM provider) JSON Mode (with something like a LangChain&amp;#39;s &lt;code&gt;with_structured_output&lt;/code&gt;), only guarantees that the result will be well-formed JSON, but the LLM may still return hallucinated values, incorrectly structured fields (think a phone number or date in an incorrect format), or even fields that don&amp;#39;t exist in your &lt;code&gt;pydantic&lt;/code&gt; model!&lt;/p&gt; &lt;p&gt;It can also be super slow, and the more fields you add to your &lt;code&gt;pydantic&lt;/code&gt; model, the longer it takes.&lt;/p&gt; &lt;p&gt;To ensure fast, accurate results, Zep uses a combination of:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;dialog preprocessing, which, amongst other things, improves accuracy for machine-transcribed dialogs and allows partial dates to be extracted;&lt;/li&gt; &lt;li&gt;guided output inference techniques on fine-tuned LLMs running on our own infrastructure;&lt;/li&gt; &lt;li&gt;and post-inference validation.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Using Zep with LangChain&lt;/h1&gt; &lt;p&gt;It&amp;#39;s simple to &lt;a href=&quot;https://help.getzep.com/langchain/overview&quot;&gt;drop Zep into a LangChain application&lt;/a&gt;. Once you&amp;#39;re persisting memory to Zep, you can extract data from this dialogue.&lt;/p&gt; &lt;h1&gt;Low or zero marginal latency cost to adding additional fields&lt;/h1&gt; &lt;p&gt;Zep&amp;#39;s extraction latency scales sub-linearly with the number of fields in your model. That is, you may add additional fields with a low or no marginal increase in latency.&lt;/p&gt; &lt;h1&gt;Support for Partial and Relative Dates&lt;/h1&gt; &lt;p&gt;Zep understands various date and time formats, including relative times such as “yesterday” or “last week.” It can also parse partial dates and times, such as “at 3pm” or “on the 15th.”&lt;/p&gt; &lt;h1&gt;Extracting from Speech Transcripts&lt;/h1&gt; &lt;p&gt;Zep can understand and extract data from machine-transcribed transcripts. Spelled out numbers and dates will be parsed as if written language. Utterances such as “uh” or “um” are ignored.&lt;/p&gt; &lt;p&gt;You can read more &lt;a href=&quot;https://blog.getzep.com/structured-data-extraction/&quot;&gt;in our announcement&lt;/a&gt; and the &lt;a href=&quot;https://help.getzep.com/langchain/overview&quot;&gt;Structured Data Extraction guide&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;This was a ton of work to build and lots of fun. Would love your feedback if you give it a spin!&lt;/p&gt; &lt;p&gt;-Daniel&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/dccpt&quot;&gt; /u/dccpt &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dpt9iz/extract_data_from_chat_history_quickly_and/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dpt9iz/extract_data_from_chat_history_quickly_and/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1dpt9iz</id><media:thumbnail url="https://external-preview.redd.it/9f3qHXFEDc5moxlRaP4wYclBrxl1FfQFS0lbxr1ol8s.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=7cc629947259c03fb2bbebe47efc15b73e5319d5" /><link href="https://www.reddit.com/r/LangChain/comments/1dpt9iz/extract_data_from_chat_history_quickly_and/" /><updated>2024-06-27T14:55:57+00:00</updated><published>2024-06-27T14:55:57+00:00</published><title>Extract Data From Chat History: Quickly and Accurately</title></entry><entry><author><name>/u/Money_Cabinet_3404</name><uri>https://www.reddit.com/user/Money_Cabinet_3404</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Today, we are excited to announce the latest integration of &lt;a href=&quot;https://zenguard.ai&quot;&gt;ZenGuard AI&lt;/a&gt; with LangChain - &lt;a href=&quot;https://python.langchain.com/v0.2/docs/integrations/tools/zenguard&quot;&gt;https://python.langchain.com/v0.2/docs/integrations/tools/zenguard&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Highlights of this integration:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Prompt Injection Protection: Automatically guards against malicious prompt injections.&lt;/li&gt; &lt;li&gt;Jailbreak Prevention: Keeps your applications safe from unauthorized access.&lt;/li&gt; &lt;li&gt;Data Leak Prevention: Protects sensitive PII/IP, secrets, and keywords from exposure.&lt;/li&gt; &lt;li&gt;Topicality Restrictions: Ensures content remains relevant and appropriate.&lt;/li&gt; &lt;li&gt;Toxicity Protection: Filters out harmful or offensive language.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;At ZenGuard AI, we are dedicated to fortifying your data security. We welcome your feedback and questions to help us serve you better. PS: If you would like to leave feedback, please file a request on &lt;a href=&quot;https://github.com/langchain-ai/langchain/issues/new?assignees=&amp;amp;labels=03+-+Documentation&amp;amp;projects=&amp;amp;template=documentation.yml&amp;amp;title=DOC%3A+&quot;&gt;GitHub&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Stay safe and secure,&lt;br/&gt; The ZenGuard AI Team&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Money_Cabinet_3404&quot;&gt; /u/Money_Cabinet_3404 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dpyk87/secure_your_langchain_applications_with_zenguard/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dpyk87/secure_your_langchain_applications_with_zenguard/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dpyk87</id><link href="https://www.reddit.com/r/LangChain/comments/1dpyk87/secure_your_langchain_applications_with_zenguard/" /><updated>2024-06-27T18:35:25+00:00</updated><published>2024-06-27T18:35:25+00:00</published><title>Secure Your LangChain applications with ZenGuard AI Integration</title></entry><entry><author><name>/u/Sevyten</name><uri>https://www.reddit.com/user/Sevyten</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey guys!&lt;/p&gt; &lt;p&gt;Just wanted to give you all a heads up about a live workshop we&amp;#39;re hosting tonight. We&amp;#39;ll be showing how to build an AI-powered tool similar to GitHub Copilot using &lt;a href=&quot;http://superduperdb.com&quot;&gt;SuperDuperDB&amp;#39;s&lt;/a&gt; latest release (v0.2). 🚀&lt;/p&gt; &lt;p&gt;🎥 Today (27/06/2024) at 9 PM CET&lt;br/&gt; 🔗 &lt;a href=&quot;https://www.youtube.com/watch?v=JgavM6QDmxQ&quot;&gt;https://www.youtube.com/watch?v=JgavM6QDmxQ&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;What to Expect:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;AI and Databases:&lt;/strong&gt; How to integrate AI models directly with your database.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Vector Search &amp;amp; Model Chaining:&lt;/strong&gt; Learn about vector search and setting up workflows by chaining models and APIs.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Real-time AI Outputs:&lt;/strong&gt; Implementing real-time AI outputs as new data arrives.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;If you&amp;#39;re into AI, databases, or just curious about how it all works, this session is for you. &lt;/p&gt; &lt;p&gt;Feel free to drop any questions or comments below. Excited to see what you all think!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Sevyten&quot;&gt; /u/Sevyten &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dpnjdx/build_your_own_github_copilot_with_superduperdb/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dpnjdx/build_your_own_github_copilot_with_superduperdb/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dpnjdx</id><link href="https://www.reddit.com/r/LangChain/comments/1dpnjdx/build_your_own_github_copilot_with_superduperdb/" /><updated>2024-06-27T09:57:36+00:00</updated><published>2024-06-27T09:57:36+00:00</published><title>Build Your Own GitHub Copilot with SuperDuperDB: Live Workshop</title></entry><entry><author><name>/u/Ashamed-Amphibian-71</name><uri>https://www.reddit.com/user/Ashamed-Amphibian-71</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;hello devs, my first post here. need some urgent help!&lt;/p&gt; &lt;p&gt;I&amp;#39;ve a dataset with 1000+ datapoints, having a column &amp;#39;CONTENT&amp;#39;, some rows contain customer feedback, some have dialogues between customer and agent, some are one-liner reviews and so on. &lt;/p&gt; &lt;p&gt;I want to extract the &amp;#39;key information&amp;#39; (what it basically conveys) from these data points using an LLM. what is the best way to go about it folks? &lt;/p&gt; &lt;p&gt;any help is highly appreciated :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Ashamed-Amphibian-71&quot;&gt; /u/Ashamed-Amphibian-71 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dq5aim/information_extraction_from_a_complex_dataset/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dq5aim/information_extraction_from_a_complex_dataset/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dq5aim</id><link href="https://www.reddit.com/r/LangChain/comments/1dq5aim/information_extraction_from_a_complex_dataset/" /><updated>2024-06-27T23:26:22+00:00</updated><published>2024-06-27T23:26:22+00:00</published><title>information extraction from a complex dataset.</title></entry><entry><author><name>/u/coolcloud</name><uri>https://www.reddit.com/user/coolcloud</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey all,&lt;/p&gt; &lt;p&gt;We&amp;#39;ve spent a lot of time building new techniques for parsing and searching PDFs. They&amp;#39;ve lead to a significant improvement in our RAG search and I wanted to share what we&amp;#39;ve learned.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Some examples:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Table - SEC Docs are notoriously hard for PDF -&amp;gt; tables. We tried the top results on google &amp;amp; some opensource thins not a single one succeeded on this table. &lt;/p&gt; &lt;p&gt;Couple examples of who we looked at:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;ilovepdf&lt;/li&gt; &lt;li&gt;Adobe&lt;/li&gt; &lt;li&gt;Gonitro&lt;/li&gt; &lt;li&gt;PDFtables&lt;/li&gt; &lt;li&gt;OCR 2 Edit&lt;/li&gt; &lt;li&gt;microsoft/table-transformer-structure-recognition&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Results - our result (can be accurately converted into CSV,MD,JSON)&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/5wju5gedmy8d1.png?width=1035&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2a336bd0e1af14760fbb5ca4291284c99edaa27e&quot;&gt;https://preview.redd.it/5wju5gedmy8d1.png?width=1035&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2a336bd0e1af14760fbb5ca4291284c99edaa27e&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Example: identifying headers, paragraphs, lists/list items (purple), and ignoring the &amp;quot;junk&amp;quot; at the top aka the table of contents in the header.&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/ix7747bjmy8d1.png?width=1018&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ea0b65ae6a35581d955da282353ff63509602a38&quot;&gt;https://preview.redd.it/ix7747bjmy8d1.png?width=1018&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ea0b65ae6a35581d955da282353ff63509602a38&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why did we do this?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;W ran into a bunch of issues with existing approaches that boils down to one thing: hallucinations often happen because the chunk doesn&amp;#39;t provide enough information.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;chunking by word count doesn&amp;#39;t work. It often chunks mid-paragraph or sentence.&lt;/li&gt; &lt;li&gt;Chunking by sentence or paragraph doesn&amp;#39;t work. If the answer spans 2-3 paragraphs, you still are SOL.&lt;/li&gt; &lt;li&gt;Semantic chunking is better but still fail quite often on lists or &amp;quot;somewhat&amp;quot; different pieces of info.&lt;/li&gt; &lt;li&gt;LLM&amp;#39;s deal better with structured/semi-structured data, i.e. knowing what you&amp;#39;re sending it is a header, paragraph list etc., makes the model perform better.&lt;/li&gt; &lt;li&gt;Headers often aren&amp;#39;t included because they&amp;#39;re too far away from the relevant vector, although often times headers contain important information.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;What are we doing different?&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;We are dynamically generating chunks when a search happens, sending headers &amp;amp; sub-headers to the LLM along with the chunk/chunks that were relevant to the search.&lt;/p&gt; &lt;p&gt;Example of how this is helpful: you have 7 documents that talk about how to reset a device, and the header says the device name, but it isn&amp;#39;t talked about the paragraphs. The 7 chunks that talked about how to reset a device would come back, but the LLM wouldn&amp;#39;t know which one was relevant to which product. That is, unless the chunk happened to include both the paragraphs and the headers, which often times in our experience, it doesn&amp;#39;t.&lt;/p&gt; &lt;p&gt;This is a simplified version of what our structure looks like:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;{ &amp;quot;type&amp;quot;: &amp;quot;Root&amp;quot;, &amp;quot;children&amp;quot;: [ { &amp;quot;type&amp;quot;: &amp;quot;Header&amp;quot;, &amp;quot;text&amp;quot;: &amp;quot;How to reset an iphone&amp;quot;, &amp;quot;children&amp;quot;: [ { &amp;quot;type&amp;quot;: &amp;quot;Header&amp;quot;, &amp;quot;text&amp;quot;: &amp;quot;iphone 10 reset&amp;quot;, &amp;quot;children&amp;quot;: [ { &amp;quot;type&amp;quot;: &amp;quot;Paragraph&amp;quot;, &amp;quot;text&amp;quot;: &amp;quot;Example Paragraph.&amp;quot; }, { &amp;quot;type&amp;quot;: &amp;quot;List&amp;quot;, &amp;quot;children&amp;quot;: [ &amp;quot;Item 1&amp;quot;, &amp;quot;Item 2&amp;quot;, &amp;quot;Item 3&amp;quot; ] } ] }, { &amp;quot;type&amp;quot;: &amp;quot;Header&amp;quot;, &amp;quot;text&amp;quot;: &amp;quot;iphone 11 reset&amp;quot;, &amp;quot;children&amp;quot;: [ { &amp;quot;type&amp;quot;: &amp;quot;Paragraph&amp;quot;, &amp;quot;text&amp;quot;: &amp;quot;Example Paragraph 2&amp;quot; }, { &amp;quot;type&amp;quot;: &amp;quot;Table&amp;quot;, &amp;quot;children&amp;quot;: [ { &amp;quot;type&amp;quot;: &amp;quot;TableCell&amp;quot;, &amp;quot;row&amp;quot;: 0, &amp;quot;col&amp;quot;: 0, &amp;quot;text&amp;quot;: &amp;quot;Column 1&amp;quot;}, { &amp;quot;type&amp;quot;: &amp;quot;TableCell&amp;quot;, &amp;quot;row&amp;quot;: 0, &amp;quot;col&amp;quot;: 1, &amp;quot;text&amp;quot;: &amp;quot;Column 2&amp;quot;}, { &amp;quot;type&amp;quot;: &amp;quot;TableCell&amp;quot;, &amp;quot;row&amp;quot;: 0, &amp;quot;col&amp;quot;: 2, &amp;quot;text&amp;quot;: &amp;quot;Column 3&amp;quot;}, { &amp;quot;type&amp;quot;: &amp;quot;TableCell&amp;quot;, &amp;quot;row&amp;quot;: 1, &amp;quot;col&amp;quot;: 0, &amp;quot;text&amp;quot;: &amp;quot;Row 1, Cell 1&amp;quot;}, { &amp;quot;type&amp;quot;: &amp;quot;TableCell&amp;quot;, &amp;quot;row&amp;quot;: 1, &amp;quot;col&amp;quot;: 1, &amp;quot;text&amp;quot;: &amp;quot;Row 1, Cell 2&amp;quot;}, { &amp;quot;type&amp;quot;: &amp;quot;TableCell&amp;quot;, &amp;quot;row&amp;quot;: 1, &amp;quot;col&amp;quot;: 2, &amp;quot;text&amp;quot;: &amp;quot;Row 1, Cell 3&amp;quot;} ] } ] } ] } ] } &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;How do we get PDF&amp;#39;s into this format?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;At a high level, we are identifying different portions of PDF&amp;#39;s based on PDF metadata and heuristics. This helps solve three problems:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;OCR can often mis-identify letters/numbers, or entirely crop out words. &lt;/li&gt; &lt;li&gt;Most other companies are trying to use OCR/ML models to identify layout elements, which seems to work decent on data it&amp;#39;s seen before but fails pretty hard unexpectedly. When it fails, it&amp;#39;s a black box. For example, Microsoft released a paper a few days ago saying they trained a model on over 500M documents and still fails on a bunch of use cases that we have working&lt;/li&gt; &lt;li&gt;We can look at layout, font analysis etc. throughout the entire doc allowing us to understand the &amp;quot;structure&amp;quot; of the document more. We&amp;#39;ll talk about this more when looking at font classes&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;How?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;First, we extract tables. We use a small OCR model to identify bounding boxes, then we do use white space analysis to find cells. This is the only portion of OCR we use (we&amp;#39;re looking at doing line analysis but have punted on that thus far.) We have found OCR to poorly identify cells on more complex tables, and often turn a 4 into a 5 or a 8 into a 2 etc.&lt;/p&gt; &lt;p&gt;When we find a table, we find characters that we believe to be a cell based on distance between each other, trying to read the table as a human would. An example would be 1345 would be a &amp;quot;cell&amp;quot; or text block, where 1 345 would be two text blocks due to the distance between them. A re-occurring theme is white space can get you pretty far.&lt;/p&gt; &lt;p&gt;Second, we extract character data from the PDF:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Fonts&lt;/strong&gt;: Information about the fonts used in the document, including the font name, type (e.g., TrueType, Type 1), and embedded font files.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Character Positions:&lt;/strong&gt; The exact bounding box of each character on the page.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Character Color:&lt;/strong&gt; PDFs usually give this correctly, and when it&amp;#39;s wrong it&amp;#39;s still good enough&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;PDFs provide a other metadata, but we found them to either be inaccurate or not necessary:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Content Streams:&lt;/strong&gt; Sequences of instructions that describe the content of the page, including text, images, and vector graphics. We found these to be surprisingly inaccurate. Newline characters inserted in the middle of words, characters and words placed out of order, and whitespace is handled really inconsistently (more below)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Annotations:&lt;/strong&gt; Information about interactive elements such as links, form fields, and comments. There are useful details here that we may use in the future, but, again, a lot of PDF tools generate these incorrectly.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Third, we strip out all space, newline, and other invisible characters. We do whitespace analysis to build words from individual characters. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;After extracting PDF metadata:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;We extract out character locations, font sizes, and fonts. We then do multiple passes of whitespace analysis and clustering algorithms to find groups, then try to identify what category they fall into based on heuristics. We used to rely more heavily on clustering (DBScan specifically), but found that simpler whitespace analysis often outperformed it. &lt;/p&gt; &lt;ul&gt; &lt;li&gt;If you look at a PDF and see only a handful of characters, let&amp;#39;s say 1% that are font 32, color blue, and each time they&amp;#39;re identified together it&amp;#39;s only 2-3 words it&amp;#39;s likely a header. &lt;/li&gt; &lt;li&gt;Now you see 2% are font 28, red, it&amp;#39;s probably a sub-header. (That is if the font spans multiple pages.) If it instead is only in a single location, it&amp;#39;s most likely something important in the text that the author wants us to &amp;#39;flag&amp;#39;. &lt;/li&gt; &lt;li&gt;This makes font analysis across the document important, and another reason we stay away from OCR&lt;/li&gt; &lt;li&gt;If, the document is 80% font 12, black. It&amp;#39;s probably &amp;#39;normal text.&amp;#39; Normal text needs to be categorized into two different formats, one is paragraphs, the other is bullet points/lists. &lt;/li&gt; &lt;li&gt;For bullet points we look primarily at the white space, identifying that there&amp;#39;s a significant amount of white space, often follow by a bullet point, number, or dash. &lt;/li&gt; &lt;li&gt;For paragraphs, we text together in a &amp;#39;normal&amp;#39; format without bullet points, traditionally spanning a majority of the document.&lt;/li&gt; &lt;li&gt;Junk detection. A lot of PDF&amp;#39;s have junk in them. An example would be a header that&amp;#39;s at the top of every single document, or a footer on every document saying who wrote it, the page number etc. This junk otherwise is sent to the chunking algorithm meaning you can often have random information mid-paragraph. We generate character ngram vectors and cluster then based on L1 distance (rather than cosine). That lets us find variations like &amp;quot;Page 1&amp;quot;, &amp;quot;Page 2&amp;quot;, etc. If those appear in roughly the same location on more than 20-35% of pages, it&amp;#39;s likely just repeat junk.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The product is still in beta so if you&amp;#39;re actively trying to solve this, or a similar problem, we&amp;#39;re letting people use it for free, in exchange for feedback.&lt;/p&gt; &lt;p&gt;Have additional questions? Shoot!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/coolcloud&quot;&gt; /u/coolcloud &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dpbc4g/how_we_chunk_turning_pdfs_into_hierarchical/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dpbc4g/how_we_chunk_turning_pdfs_into_hierarchical/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dpbc4g</id><link href="https://www.reddit.com/r/LangChain/comments/1dpbc4g/how_we_chunk_turning_pdfs_into_hierarchical/" /><updated>2024-06-26T22:21:08+00:00</updated><published>2024-06-26T22:21:08+00:00</published><title>How we Chunk - turning PDF's into hierarchical structure for RAG</title></entry><entry><author><name>/u/fakestudy69</name><uri>https://www.reddit.com/user/fakestudy69</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hello everyone,&lt;/p&gt; &lt;p&gt;I&amp;#39;m currently working on a project to develop a customer support live agent that not only assists in resolving issues but also understands the tone of the conversation and guides the agent to ensure successful call closures. I&amp;#39;m seeking advice and suggestions from those who have experience or expertise in this area.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Project Overview:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The goal is to create a live support agent that can:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Understand the Tone of the Conversation:&lt;/strong&gt; Analyze the emotional tone (e.g., frustration, satisfaction, confusion) of customer interactions in real-time.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Guide Agent Responses:&lt;/strong&gt; Provide suggestions to human agents on how to respond effectively based on the detected tone and context.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Ensure Successful Call Closures:&lt;/strong&gt; Help agents navigate conversations towards a satisfactory resolution for the customer.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;Key Features I&amp;#39;m Aiming For:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Tone Detection:&lt;/strong&gt; Implement natural language processing (NLP) techniques to analyze and understand the customer&amp;#39;s emotional state.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Response Recommendations:&lt;/strong&gt; Develop an AI-driven system that offers response suggestions tailored to the detected tone and context of the conversation.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Real-Time Feedback:&lt;/strong&gt; Provide live feedback to agents during the call to adjust their approach if necessary.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Learning and Improvement:&lt;/strong&gt; Incorporate machine learning to continuously improve the accuracy of tone detection and response suggestions based on historical data.&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/fakestudy69&quot;&gt; /u/fakestudy69 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dq2dwy/seeking_guidance_on_building_a_customer_support/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dq2dwy/seeking_guidance_on_building_a_customer_support/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dq2dwy</id><link href="https://www.reddit.com/r/LangChain/comments/1dq2dwy/seeking_guidance_on_building_a_customer_support/" /><updated>2024-06-27T21:15:49+00:00</updated><published>2024-06-27T21:15:49+00:00</published><title>Seeking Guidance on Building a Customer Support Live Agent with Tone Analysis Capabilities</title></entry><entry><author><name>/u/northwolf56</name><uri>https://www.reddit.com/user/northwolf56</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;&lt;a href=&quot;https://youtu.be/-OKC7CY2bbQ&quot;&gt;https://youtu.be/-OKC7CY2bbQ&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Enjoy! Coming soon &lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://visualagents.ai&quot;&gt;https://visualagents.ai&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/northwolf56&quot;&gt; /u/northwolf56 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dq1w28/no_code_chrome_extension_chat_bot_using_visual/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dq1w28/no_code_chrome_extension_chat_bot_using_visual/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dq1w28</id><link href="https://www.reddit.com/r/LangChain/comments/1dq1w28/no_code_chrome_extension_chat_bot_using_visual/" /><updated>2024-06-27T20:54:31+00:00</updated><published>2024-06-27T20:54:31+00:00</published><title>No Code Chrome Extension Chat Bot Using Visual LangChain</title></entry><entry><author><name>/u/MagentaSpark</name><uri>https://www.reddit.com/user/MagentaSpark</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;There are 2 ways of doing same things now. Chains and Graphs. They both offer almost identical control in most of the small workflows. Advantages, disadvantages and use cases for chains as nodes vs compiled graphs as nodes.&lt;/p&gt; &lt;p&gt;I do realise that both are inherit from runnable primitive, but application wise, practically, there are 2 distinct way of doing thing, right?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/MagentaSpark&quot;&gt; /u/MagentaSpark &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dpqltj/any_experiences_with_graph_within_a_graph_in/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dpqltj/any_experiences_with_graph_within_a_graph_in/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dpqltj</id><link href="https://www.reddit.com/r/LangChain/comments/1dpqltj/any_experiences_with_graph_within_a_graph_in/" /><updated>2024-06-27T12:55:21+00:00</updated><published>2024-06-27T12:55:21+00:00</published><title>Any experiences with Graph within a Graph in LangGraph?</title></entry><entry><author><name>/u/upandfastLFGG</name><uri>https://www.reddit.com/user/upandfastLFGG</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dpuny1/my_agent_will_sometimes_treat_the_on_tool_end/&quot;&gt; &lt;img src=&quot;https://b.thumbs.redditmedia.com/74Jui9TaQ3EhIRjJdrxvaeeec3uHP47O96xrpgsl6Zo.jpg&quot; alt=&quot;My agent will sometimes treat the on_tool_end event as a string. Anyone know why?&quot; title=&quot;My agent will sometimes treat the on_tool_end event as a string. Anyone know why?&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;My agent works 80-85% of the time. For some reason, there&amp;#39;ll be random moments when it doesn&amp;#39;t work as intended because a certain agent astream event doesn&amp;#39;t seem to get processed correctly. &lt;/p&gt; &lt;p&gt;Anyone know the reason behind this kind of interaction?&lt;/p&gt; &lt;p&gt;The first image will show the langsmith trace of an agent that works as intended. The second image will show the langsmith trace of an agent that doesn&amp;#39;t work as intended.&lt;/p&gt; &lt;p&gt;If you look at the second image, it seems like the tool call is being treated as a string and gets added to the AI Message. No idea what causes this or why this happens&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/xaozguj1z49d1.jpg?width=2013&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=b89af5f20ad43e5dd39136bdaff62e0aec0ee4e3&quot;&gt;Langsmith trace for agent that behaves as expected&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/2wwefdn4z49d1.jpg?width=2007&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=1a5ce10261ddb357ea02b26d7fabbd96870a90ec&quot;&gt;Langsmith trace for an agent that doesn&amp;#39;t work&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/upandfastLFGG&quot;&gt; /u/upandfastLFGG &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dpuny1/my_agent_will_sometimes_treat_the_on_tool_end/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dpuny1/my_agent_will_sometimes_treat_the_on_tool_end/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1dpuny1</id><media:thumbnail url="https://b.thumbs.redditmedia.com/74Jui9TaQ3EhIRjJdrxvaeeec3uHP47O96xrpgsl6Zo.jpg" /><link href="https://www.reddit.com/r/LangChain/comments/1dpuny1/my_agent_will_sometimes_treat_the_on_tool_end/" /><updated>2024-06-27T15:54:38+00:00</updated><published>2024-06-27T15:54:38+00:00</published><title>My agent will sometimes treat the on_tool_end event as a string. Anyone know why?</title></entry><entry><author><name>/u/Fresh_Skin130</name><uri>https://www.reddit.com/user/Fresh_Skin130</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I have built a first proof of concept of agents that generate a flow chart given some text as input. The flowchart is generated in graphML format and is compatible with yED chart editor (free).&lt;/p&gt; &lt;p&gt;The project is available here: &lt;a href=&quot;https://github.com/marco-marchesi/FlowChartGenerator&quot;&gt;https://github.com/marco-marchesi/FlowChartGenerator&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Note: it&amp;#39;s my first github project, any suggestion and contribution are very welcome.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Fresh_Skin130&quot;&gt; /u/Fresh_Skin130 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dpr1yz/text_2_flowchart_agent/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dpr1yz/text_2_flowchart_agent/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dpr1yz</id><link href="https://www.reddit.com/r/LangChain/comments/1dpr1yz/text_2_flowchart_agent/" /><updated>2024-06-27T13:17:05+00:00</updated><published>2024-06-27T13:17:05+00:00</published><title>Text 2 FlowChart agent</title></entry><entry><author><name>/u/HotRepresentative325</name><uri>https://www.reddit.com/user/HotRepresentative325</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;How do I start with langchain, am I even using the right tool?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/HotRepresentative325&quot;&gt; /u/HotRepresentative325 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dpymhw/i_want_to_create_a_vector_database_input_pdfs_and/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dpymhw/i_want_to_create_a_vector_database_input_pdfs_and/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dpymhw</id><link href="https://www.reddit.com/r/LangChain/comments/1dpymhw/i_want_to_create_a_vector_database_input_pdfs_and/" /><updated>2024-06-27T18:38:02+00:00</updated><published>2024-06-27T18:38:02+00:00</published><title>I want to create a vector database input pdfs and website chunks and do searches</title></entry><entry><author><name>/u/bubble_h13</name><uri>https://www.reddit.com/user/bubble_h13</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dpx7af/how_do_agent_select_tool_properly/&quot;&gt; &lt;img src=&quot;https://external-preview.redd.it/UallS6eTqMQ6Q_VovzOpEJke6ZgDb6GzqRJtFsGug9s.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=82f0fba791dd0f75899e3dc5480f915fe8c0cafd&quot; alt=&quot;How do agent select tool properly&quot; title=&quot;How do agent select tool properly&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;In my program, I use react agent, but the agent usually says, &amp;quot;xxx is not a valid tool&amp;quot;.&lt;br/&gt; for example, I have a tool named &lt;code&gt;regonition_image_click&lt;/code&gt; when I got correct, like&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/kiifjjo6c59d1.png?width=1004&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bbf6587ac9217104bee34d49205399f877569333&quot;&gt;https://preview.redd.it/kiifjjo6c59d1.png?width=1004&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bbf6587ac9217104bee34d49205399f877569333&lt;/a&gt;&lt;/p&gt; &lt;p&gt;but mostly the Action will get redundant or Chinese ( I think it will directly be the tool name), then it will get error&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/j6ol1887d59d1.png?width=1576&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d6a1da911d287722a8739beac62594ca3c806c9b&quot;&gt;https://preview.redd.it/j6ol1887d59d1.png?width=1576&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d6a1da911d287722a8739beac62594ca3c806c9b&lt;/a&gt;&lt;/p&gt; &lt;p&gt;so now I try 2 method&lt;br/&gt; First, using call tools &lt;a href=&quot;https://python.langchain.com/v0.1/docs/use_cases/tool_use/multiple_tools/&quot;&gt;https://python.langchain.com/v0.1/docs/use_cases/tool_use/multiple_tools/&lt;/a&gt;&lt;br/&gt; I still try to understand how it work&lt;/p&gt; &lt;pre&gt;&lt;code&gt;AgentExecutor(agent_executor_kwargs={&amp;quot;call_tools&amp;quot;: call_tools}) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Second, using plan in agent executor: &lt;a href=&quot;https://github.com/langchain-ai/langchain/discussions/18698&quot;&gt;https://github.com/langchain-ai/langchain/discussions/18698&lt;/a&gt;&lt;br/&gt; But I&amp;#39;m not sure where to place the plan function to override the original (which comes from&lt;code&gt;RunnableSequence&lt;/code&gt;?).&lt;/p&gt; &lt;pre&gt;&lt;code&gt;from langchain.chains.base import Chain from typing import Any, List, Tuple, Union from langchain_core.agents import AgentAction, AgentFinish from langchain_core.callbacks import Callbacks class FastAgent(Chain): def plan( self, intermediate_steps: List[Tuple[AgentAction, str]], callbacks: Callbacks = None, **kwargs: Any, ) -&amp;gt; Union[AgentAction, AgentFinish]: &amp;quot;&amp;quot;&amp;quot;Given input, decided what to do. Args: intermediate_steps: Steps the LLM has taken to date, along with observations callbacks: Callbacks to run. **kwargs: User inputs. Returns: Action specifying what tool to use. &amp;quot;&amp;quot;&amp;quot; inputs = {**kwargs, **{&amp;quot;intermediate_steps&amp;quot;: intermediate_steps}} action_input = {&amp;quot;para1&amp;quot;: &amp;quot;val1&amp;quot;, &amp;quot;para2&amp;quot;: &amp;quot;val2&amp;quot;} inputs[&amp;quot;action_input&amp;quot;] = action_input final_output: Any = None for chunk in self.runnable.stream(inputs, config={&amp;quot;callbacks&amp;quot;: callbacks}): if final_output is None: final_output = chunk else: final_output += chunk return final_output from model_setting import get_llm from langchain import hub from langchain.chains import LLMChain from agent_tool.fast_tool import type_text_tool, reg_image_click from langchain.agents import AgentExecutor, create_react_agent prompt = hub.pull(&amp;quot;hwchase17/react&amp;quot;) tools = [type_text_tool, reg_image_click] llm = get_llm() LLMChain(llm=llm, prompt=prompt) agent = create_react_agent(llm, tools, prompt) question = &amp;quot;我想要點擊申請人旁邊的按鈕&amp;quot; agent.invoke({&amp;quot;input&amp;quot;: question}) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Could someone please provide some advice on which method is better and how to do it?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/bubble_h13&quot;&gt; /u/bubble_h13 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dpx7af/how_do_agent_select_tool_properly/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dpx7af/how_do_agent_select_tool_properly/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1dpx7af</id><media:thumbnail url="https://external-preview.redd.it/UallS6eTqMQ6Q_VovzOpEJke6ZgDb6GzqRJtFsGug9s.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=82f0fba791dd0f75899e3dc5480f915fe8c0cafd" /><link href="https://www.reddit.com/r/LangChain/comments/1dpx7af/how_do_agent_select_tool_properly/" /><updated>2024-06-27T17:39:04+00:00</updated><published>2024-06-27T17:39:04+00:00</published><title>How do agent select tool properly</title></entry><entry><author><name>/u/ms-atomicbomb</name><uri>https://www.reddit.com/user/ms-atomicbomb</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;m hiring a fully-remote Agentic Software Developers to build, test and refine our agents, as well as the infrastructure around them. We&amp;#39;re a stealth-mode start up backed top VCs. Please feel free to reach out to me here or via Discord (@thebirthdaygirl) and I&amp;#39;d love to chat! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/ms-atomicbomb&quot;&gt; /u/ms-atomicbomb &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dpl6ks/hiring_fullyremote_agentic_software_developers/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dpl6ks/hiring_fullyremote_agentic_software_developers/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dpl6ks</id><link href="https://www.reddit.com/r/LangChain/comments/1dpl6ks/hiring_fullyremote_agentic_software_developers/" /><updated>2024-06-27T07:08:04+00:00</updated><published>2024-06-27T07:08:04+00:00</published><title>Hiring fully-remote Agentic Software Developers!</title></entry><entry><author><name>/u/d2clon</name><uri>https://www.reddit.com/user/d2clon</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hello, people. I am a veteran programmer who is new to AI and its business use cases.&lt;/p&gt; &lt;p&gt;I am fascinated by it, and I am now working on a small prototype for a client. It is an out-of-the-book RAG case:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;~1.5K 1-page PDFs with product specs.&lt;/li&gt; &lt;li&gt;Build a chatbot to ask questions about the products.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;In our team, we are making great progress in the basic setup. The PDFs are indexed in a VectorDB and we are able to use GPT4 to interact with the VectorDB data and generate human friendly answers.&lt;/p&gt; &lt;p&gt;But there is a lot to improve about the generated recomendations, conclusions, filtering, best results, ...&lt;/p&gt; &lt;p&gt;All the tutorials and documentation we are seeing end up here, in the basic setup. And don&amp;#39;t go further in the details and improvements needed to go to &amp;quot;production&amp;quot; level. Further more, I have seen that many people on this community and others are mentioning their dissapointment with the actual state of the technology and their abandom of building a RAG architecture.&lt;/p&gt; &lt;p&gt;I just want a confirmation that it is possible. That some of you have managed to build a RAG architecture that is used satisfactorily in production. Is this the case? :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/d2clon&quot;&gt; /u/d2clon &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dp7p9j/are_there_any_rag_successful_real_production_use/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dp7p9j/are_there_any_rag_successful_real_production_use/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dp7p9j</id><link href="https://www.reddit.com/r/LangChain/comments/1dp7p9j/are_there_any_rag_successful_real_production_use/" /><updated>2024-06-26T19:47:52+00:00</updated><published>2024-06-26T19:47:52+00:00</published><title>Are there any RAG successful real production use cases out there?</title></entry><entry><author><name>/u/harshit_nariya</name><uri>https://www.reddit.com/user/harshit_nariya</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/harshit_nariya&quot;&gt; /u/harshit_nariya &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;/r/AnyBodyCanAI/comments/1dpkjvq/we_built_an_opensource_lowcode_multiagent/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dpo9li/we_built_an_opensource_lowcode_multiagent/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dpo9li</id><link href="https://www.reddit.com/r/LangChain/comments/1dpo9li/we_built_an_opensource_lowcode_multiagent/" /><updated>2024-06-27T10:45:04+00:00</updated><published>2024-06-27T10:45:04+00:00</published><title>We built an open-source low-code multi-agent automation framework</title></entry><entry><author><name>/u/simaodiego</name><uri>https://www.reddit.com/user/simaodiego</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;pre&gt;&lt;code&gt;Is it possible to create a model using RAG with specific content and if my model has no response it can access ChatGPT for example? Just when RAG is not enough, how to find and develop this type of solution? &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/simaodiego&quot;&gt; /u/simaodiego &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dprerw/mix_of_rag_and_public_apis/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dprerw/mix_of_rag_and_public_apis/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dprerw</id><link href="https://www.reddit.com/r/LangChain/comments/1dprerw/mix_of_rag_and_public_apis/" /><updated>2024-06-27T13:33:47+00:00</updated><published>2024-06-27T13:33:47+00:00</published><title>Mix of RAG and public APIs</title></entry><entry><author><name>/u/inez-dolly</name><uri>https://www.reddit.com/user/inez-dolly</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hello. I&amp;#39;m new to LangChain and I&amp;#39;ve been wondering how to achieve shared memory/session between independent agents, without using a graph with a supervisor. I have an agent which is responsible for breaking down complex question to steps that can be executed by other agents. It is aware other agents exist.&lt;/p&gt; &lt;p&gt;For example:&lt;/p&gt; &lt;p&gt;Main question: What will be the weather tomorrow in Oslo? Will it be warmer than in Bergen?&lt;/p&gt; &lt;p&gt;Which is broken down to steps by an agent:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;{&amp;quot;agent&amp;quot;: &amp;quot;DateTimeAgent, &amp;quot;task&amp;quot;: &amp;quot;Get tomorrow&amp;#39;s date&amp;quot;}&lt;/li&gt; &lt;li&gt;{&amp;quot;agent&amp;quot;: &amp;quot;ForecastAgent, &amp;quot;task&amp;quot;: &amp;quot;Get the weather in Oslo, Norway for 2024/06/28&amp;quot;}&lt;/li&gt; &lt;li&gt;{&amp;quot;agent&amp;quot;: &amp;quot;ForecastAgent, &amp;quot;task&amp;quot;: &amp;quot;Get the weather in Bergen, Norway for 2024/06/28&amp;quot;}&lt;/li&gt; &lt;li&gt;{&amp;quot;agent&amp;quot;: &amp;quot;CalcAgent, &amp;quot;task&amp;quot;: &amp;quot;Calculate the difference between the temperatures&amp;quot;}&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;As you can see, step 4 is related to the outcome of the previous ones. What&amp;#39;s the best way to make the agents aware of the results of their peers? Is the only way to use langgraph? It seems a bit inefficient to me to have a wrapper agent using the RunnableWithMessageHistory class and have an LLM manage the routing and conversation.&lt;/p&gt; &lt;p&gt;Thank you for your assistance beforehand!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/inez-dolly&quot;&gt; /u/inez-dolly &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dpqtfw/sharing_history_between_independent_agents/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dpqtfw/sharing_history_between_independent_agents/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dpqtfw</id><link href="https://www.reddit.com/r/LangChain/comments/1dpqtfw/sharing_history_between_independent_agents/" /><updated>2024-06-27T13:05:38+00:00</updated><published>2024-06-27T13:05:38+00:00</published><title>Sharing history between independent agents</title></entry><entry><author><name>/u/MajorTuttle</name><uri>https://www.reddit.com/user/MajorTuttle</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Caveat: I am very new to using langchain.&lt;/p&gt; &lt;p&gt;Langsmith seems like an excellent product for enterprise and large-scale production, but beyond my needs and pricing, and seems not an easy way to export data.&lt;/p&gt; &lt;p&gt;I just want to be able to capture what is going on under the hood -- including all LLM API call inputs/outputs -- mostly to better understand how the implemented patterns (structured outputs, tool calling, etc) are done in practice. Ideally either as a data-structure for me to persist or directly to a file (JSON or JSONL or similar I can peruse and process).&lt;/p&gt; &lt;p&gt;Is there an easy way to do this?&lt;/p&gt; &lt;p&gt;Its not clear to me if the chain design just makes it challenging to implement observability (and why langsmith is needed), or if its somewhat intentionally not made clear or well-documented as langsmith and langserve are the profit centers.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/MajorTuttle&quot;&gt; /u/MajorTuttle &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dpjf0d/trace_to_datastructure_or_file_instead_of/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dpjf0d/trace_to_datastructure_or_file_instead_of/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dpjf0d</id><link href="https://www.reddit.com/r/LangChain/comments/1dpjf0d/trace_to_datastructure_or_file_instead_of/" /><updated>2024-06-27T05:12:12+00:00</updated><published>2024-06-27T05:12:12+00:00</published><title>Trace to data-structure or file instead of langsmith?</title></entry><entry><author><name>/u/link2ani</name><uri>https://www.reddit.com/user/link2ani</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/link2ani&quot;&gt; /u/link2ani &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dpprkp/anyone_building_rag_app_in_javascript_what_stack/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dpprkp/anyone_building_rag_app_in_javascript_what_stack/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dpprkp</id><link href="https://www.reddit.com/r/LangChain/comments/1dpprkp/anyone_building_rag_app_in_javascript_what_stack/" /><updated>2024-06-27T12:10:55+00:00</updated><published>2024-06-27T12:10:55+00:00</published><title>anyone building RAG app in javascript? what stack are you using?</title></entry><entry><author><name>/u/Specialist-Cloud-448</name><uri>https://www.reddit.com/user/Specialist-Cloud-448</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I have data stored in my &lt;strong&gt;DynamoDB&lt;/strong&gt; which is frequently updated through back-end services. Now I want to create a PG vector based &lt;strong&gt;AuroraDB vector database&lt;/strong&gt; for storing &lt;strong&gt;embeddings&lt;/strong&gt;, which I want to be automatically updated whenever there is the change in the DynamoDB.&lt;/p&gt; &lt;p&gt;I thought about using the &lt;strong&gt;EventBridge&lt;/strong&gt; but need more suggestion on that.&lt;/p&gt; &lt;p&gt;My aim is to create the new embeddings everytime there is the change (Upsert) in the DynamoDB and store them in the PG Vector Database. So that I can perform the RAG on &lt;strong&gt;latest embeddings&lt;/strong&gt; to so the answer from LLM must be context aware.&lt;/p&gt; &lt;p&gt;In the phase of architectural designing and ideation of this feature.&lt;/p&gt; &lt;p&gt;Any suggestions are welcomed .&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Specialist-Cloud-448&quot;&gt; /u/Specialist-Cloud-448 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dpphgh/data_ingestion_for_the_rag_from_dynamo_db_to/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dpphgh/data_ingestion_for_the_rag_from_dynamo_db_to/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dpphgh</id><link href="https://www.reddit.com/r/LangChain/comments/1dpphgh/data_ingestion_for_the_rag_from_dynamo_db_to/" /><updated>2024-06-27T11:56:30+00:00</updated><published>2024-06-27T11:56:30+00:00</published><title>Data Ingestion for the RAG from Dynamo DB to AuroraDB with pgVector to store embeddings</title></entry><entry><author><name>/u/byrocuy</name><uri>https://www.reddit.com/user/byrocuy</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hello, I am currently developing a chatbot using LangGraph, and I&amp;#39;m facing some challenges with managing state for multiple users. Specifically, I&amp;#39;m dealing with the following constraints and setup:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The state is limited only to last 10-15 messages due to the structure of API I am interacting with.&lt;/li&gt; &lt;li&gt;All the chat history will be stored in a MySQL database. I do it by storing each input and response manually to the db, as the checkpointer implementation in MySQL is not supported yet.&lt;/li&gt; &lt;li&gt;The messages will be stored in the database with the corresponding user ID. For now the chat history in the database has no function in the chatbot flow. It only serve for the frontend to load previous interaction when user open the chatbot. &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;If I understand correctly, the state is stored in the runtime and shared across multiple users, right? I think this might lead to a memory problem if I don&amp;#39;t implement some way of handler or even if I limit the previous messages for each user it will lead to a problem.&lt;/p&gt; &lt;p&gt;My idea to handle this is as follows: - Store the chat history (user ID and message) in the database. - When a new query comes in, load the last 10 last messages from the database for the appropriate user ID. - Append this history with the new query and pass it to the chatbot.&lt;/p&gt; &lt;p&gt;How does my idea sound? Are there any potential pitfalls or improvements you would suggest? I&amp;#39;m open to any suggestions and feedback.&lt;/p&gt; &lt;p&gt;Thanks in advance for your help!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/byrocuy&quot;&gt; /u/byrocuy &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dpgr6p/how_to_manage_state_in_langgraph_for_multiple/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dpgr6p/how_to_manage_state_in_langgraph_for_multiple/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dpgr6p</id><link href="https://www.reddit.com/r/LangChain/comments/1dpgr6p/how_to_manage_state_in_langgraph_for_multiple/" /><updated>2024-06-27T02:42:53+00:00</updated><published>2024-06-27T02:42:53+00:00</published><title>How to Manage State in LangGraph for Multiple Users?</title></entry><entry><author><name>/u/sujihai</name><uri>https://www.reddit.com/user/sujihai</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I am currently working on integrating several components into a comprehensive chat application using LangServe and LangChain. Below, I detail the components involved and the specific issues I&amp;#39;m encountering. Any guidance or suggestions would be greatly appreciated.&lt;/p&gt; &lt;h1&gt;Components and Setup:&lt;/h1&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;History Aware Retriever and Question Answer Chain&lt;/strong&gt;: &lt;ul&gt; &lt;li&gt;I&amp;#39;ve created a chain that consists of a history-aware retriever and a question-answer chain.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&amp;#8203;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;contextualize_q_system_prompt = &amp;quot;&amp;quot;&amp;quot;Given a chat history and the latest user question \ which might reference context in the chat history, formulate a standalone question \ which can be understood without the chat history. Do NOT answer the question, \ just reformulate it if needed and otherwise return it as is.&amp;quot;&amp;quot;&amp;quot; contextualize_q_prompt = ChatPromptTemplate.from_messages( [ (&amp;quot;system&amp;quot;, contextualize_q_system_prompt), MessagesPlaceholder(&amp;quot;chat_history&amp;quot;), (&amp;quot;human&amp;quot;, &amp;quot;{input}&amp;quot;), ] ) history_aware_retriever = create_history_aware_retriever( llm, retriever, contextualize_q_prompt ) ### Answer question ### qa_system_prompt = &amp;quot;&amp;quot;&amp;quot;You are an assistant for question-answering tasks. \ Use the following pieces of retrieved context to answer the question. \ If you don&amp;#39;t know the answer, just say that you don&amp;#39;t know. \ Use three sentences maximum and keep the answer concise.\ {context}&amp;quot;&amp;quot;&amp;quot; qa_prompt = ChatPromptTemplate.from_messages( [ (&amp;quot;system&amp;quot;, qa_system_prompt), MessagesPlaceholder(&amp;quot;chat_history&amp;quot;), (&amp;quot;human&amp;quot;, &amp;quot;{input}&amp;quot;), ] ) question_answer_chain = create_stuff_documents_chain(llm, qa_prompt) rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain) &lt;/code&gt;&lt;/pre&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Message History Implementation&lt;/strong&gt;: &lt;ul&gt; &lt;li&gt;The application incorporates &lt;code&gt;RedisChatMessageHistory&lt;/code&gt; along with &lt;code&gt;RunnableWithMessageHistory&lt;/code&gt;. The intention is to leverage Redis for managing chat message history, tracking conversations by User ID and Conversation ID.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;LangGraph Integration&lt;/strong&gt;: &lt;ul&gt; &lt;li&gt;I&amp;#39;m attempting to integrate this setup into LangGraph. However, I&amp;#39;m facing challenges because LangGraph documentation suggests using Checkpoints with SQLite, and it&amp;#39;s unclear how to integrate RedisChatMessageHistory which is essential for my application.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ol&gt; &lt;h1&gt;Issues:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Integration with LangGraph&lt;/strong&gt;: How can I integrate &lt;code&gt;RedisChatMessageHistory&lt;/code&gt; within LangGraph, given that LangGraph primarily supports SQLite for Checkpoints?&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Consistent Message History&lt;/strong&gt;: I need to ensure that message history capabilities are maintained across the entire application, allowing tracking of conversations by User ID and Conversation ID.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Resources:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;For the retrieval chain setup, please refer to the LangChain documentation on question answering with chat history: &lt;a href=&quot;https://python.langchain.com/v0.1/docs/use_cases/question_answering/chat_history/#tying-it-together&quot;&gt;LangChain QA with Chat History&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;For details on managing persistent chat with user IDs and conversation IDs, see this example from LangServe: &lt;a href=&quot;https://github.com/langchain-ai/langserve/blob/main/examples/chat_with_persistence_and_user/server.py&quot;&gt;LangServe Chat with Persistence&lt;/a&gt;.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Request:&lt;/h1&gt; &lt;p&gt;I am seeking advice or examples on how to properly integrate RedisChatMessageHistory with LangGraph in a manner that maintains full functionality of the message history features. Any insights or pointers towards documentation or similar implementations would be incredibly helpful.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/sujihai&quot;&gt; /u/sujihai &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dposfd/integration_issues_with_langgraph/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dposfd/integration_issues_with_langgraph/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dposfd</id><link href="https://www.reddit.com/r/LangChain/comments/1dposfd/integration_issues_with_langgraph/" /><updated>2024-06-27T11:16:17+00:00</updated><published>2024-06-27T11:16:17+00:00</published><title>Integration Issues with LangGraph, RedisChatMessageHistory, and RunnableWithMessageHistory</title></entry><entry><author><name>/u/Early_Low8914</name><uri>https://www.reddit.com/user/Early_Low8914</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;When calling the retriever directly, I get a response which includes the content + metadata.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;retriever = documents.as_retriever(search_kwargs={&amp;quot;k&amp;quot;: 1}) retriever.get_relevant_documents(&amp;quot;foo&amp;quot;) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The response:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;[Document(page_content=&amp;#39;foo&amp;#39;, metadata={&amp;#39;tenant_id&amp;#39;: &amp;#39;0d122190-b761-43f7-9ea3-f1842bbe1c4d&amp;#39;, &amp;#39;page&amp;#39;: 7})] &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;When i wrap the retriever with the utiliy function provided by langchain: &amp;quot;create_retriever_tool&amp;quot;:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;tool: Tool = create_retriever_tool(documents.as_retriever(search_kwargs={ &amp;quot;k&amp;quot;: 6}), name=&amp;quot;search_documents&amp;quot;, description=&amp;quot;Search documents&amp;quot;) tool.invoke(&amp;quot;foo&amp;quot;) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The response:&lt;/p&gt; &lt;p&gt;&amp;#39;foo&amp;#39;&lt;/p&gt; &lt;p&gt;So in this case, the metadata part is completely missing. I understand that I could use a prompt_template which includes the metadata:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;document_prompt = PromptTemplate.from_template( &amp;quot;Document chunk metadata: tenant_id: {tenant_id}...\n&amp;quot; &amp;quot;Document chunk content: {page_content}. &amp;quot; ) tool: Tool = create_retriever_tool(documents.as_retriever(search_kwargs={ &amp;quot;k&amp;quot;: 6}), name=&amp;quot;search_documents&amp;quot;, description=&amp;quot;Search documents&amp;quot;, document_prompt=document_prompt) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;and this works but the output from directly calling retriever.get_relevant_documents(&amp;quot;foo&amp;quot;) is a document array which makes it easier to work with.&lt;/p&gt; &lt;p&gt;I would like to have the exact same output from the response of calling the tool. How can this be achieved? Is the only solution to create a custom tool function instead of using the utility function?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Early_Low8914&quot;&gt; /u/Early_Low8914 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dpntgo/how_to_get_a_structured_response_from_create/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dpntgo/how_to_get_a_structured_response_from_create/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dpntgo</id><link href="https://www.reddit.com/r/LangChain/comments/1dpntgo/how_to_get_a_structured_response_from_create/" /><updated>2024-06-27T10:16:00+00:00</updated><published>2024-06-27T10:16:00+00:00</published><title>How to get a structured response from &quot;create_retriever_tool&quot;?</title></entry></feed>