<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/"><category term="LangChain" label="r/LangChain"/><updated>2024-03-23T14:24:07+00:00</updated><icon>https://www.redditstatic.com/icon.png/</icon><id>/r/LangChain.rss</id><link rel="self" href="https://www.reddit.com/r/LangChain.rss" type="application/atom+xml" /><link rel="alternate" href="https://www.reddit.com/r/LangChain" type="text/html" /><subtitle>LangChain is an open-source framework and developer toolkit that helps developers get LLM applications from prototype to production. It is available for Python and Javascript at https://www.langchain.com/.</subtitle><title>LangChain</title><entry><author><name>/u/zchaarm</name><uri>https://www.reddit.com/user/zchaarm</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;A place for members of &lt;a href=&quot;/r/LangChain&quot;&gt;r/LangChain&lt;/a&gt; to chat with each other&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/zchaarm&quot;&gt; /u/zchaarm &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_10ljho9</id><link href="https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/" /><updated>2023-01-26T04:33:05+00:00</updated><published>2023-01-26T04:33:05+00:00</published><title>r/LangChain Lounge</title></entry><entry><author><name>/u/danipudani</name><uri>https://www.reddit.com/user/danipudani</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1blr3w4/large_language_models_and_bert_chris_manning/&quot;&gt; &lt;img src=&quot;https://external-preview.redd.it/W--MEh6AUpBEnN6r2bPTw9FOTA3jL1LGUDn8_deGKVU.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=edc8b0aef1b55dfced7712f6af44fceb3431ebfc&quot; alt=&quot;Large Language Models and BERT - Chris Manning Stanford CoreNLP&quot; title=&quot;Large Language Models and BERT - Chris Manning Stanford CoreNLP&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/danipudani&quot;&gt; /u/danipudani &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://youtu.be/SA90w6vYPlo?si=EX0SbzvTbmyVCptM&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1blr3w4/large_language_models_and_bert_chris_manning/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1blr3w4</id><media:thumbnail url="https://external-preview.redd.it/W--MEh6AUpBEnN6r2bPTw9FOTA3jL1LGUDn8_deGKVU.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=edc8b0aef1b55dfced7712f6af44fceb3431ebfc" /><link href="https://www.reddit.com/r/LangChain/comments/1blr3w4/large_language_models_and_bert_chris_manning/" /><updated>2024-03-23T12:34:38+00:00</updated><published>2024-03-23T12:34:38+00:00</published><title>Large Language Models and BERT - Chris Manning Stanford CoreNLP</title></entry><entry><author><name>/u/Budget-Juggernaut-68</name><uri>https://www.reddit.com/user/Budget-Juggernaut-68</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I am trying to make use of langgraph, but I&amp;#39;m getting stuck at using langchain with my own &amp;quot;customllm&amp;quot; &lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://python.langchain.com/docs/modules/model_io/llms/custom_llm&quot;&gt;https://python.langchain.com/docs/modules/model_io/llms/custom_llm&lt;/a&gt; &lt;/p&gt; &lt;p&gt;It doesn&amp;#39;t say any where how I am suppose to link all that with my API, any help will be appreciated.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Budget-Juggernaut-68&quot;&gt; /u/Budget-Juggernaut-68 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1blrpbz/langchain_with_custom_local_llm_api/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1blrpbz/langchain_with_custom_local_llm_api/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1blrpbz</id><link href="https://www.reddit.com/r/LangChain/comments/1blrpbz/langchain_with_custom_local_llm_api/" /><updated>2024-03-23T13:05:34+00:00</updated><published>2024-03-23T13:05:34+00:00</published><title>Langchain with custom local LLM api</title></entry><entry><author><name>/u/FillOk5686</name><uri>https://www.reddit.com/user/FillOk5686</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;ve recently been exploring the development of projects using large language models (LLMs) like the GPT series and encountered a question: Why should we consider using tools like LlamaIndex or LangChain as intermediaries for communication with LLMs, rather than directly interacting with LLMs through the API in the repository layer?&lt;/p&gt; &lt;p&gt;From my understanding, directly using the API seems to offer a more simplified and direct control path, potentially avoiding the introduction of additional complexity and potential performance bottlenecks. However, I&amp;#39;ve also heard that these tools (LlamaIndex and LangChain) can enhance the efficiency and effectiveness of communication with LLMs.&lt;/p&gt; &lt;p&gt;Specifically, I have the following questions:&lt;/p&gt; &lt;p&gt;How do LlamaIndex and LangChain optimize the efficiency of communication with LLMs? What specific performance optimizations and functional enhancements do they provide that make interactions with LLMs faster and more effective?&lt;/p&gt; &lt;p&gt;In what situations might direct communication with LLMs via the API not be the best choice? Are these tools primarily addressing certain specific technical challenges or needs?&lt;/p&gt; &lt;p&gt;What are the potential advantages and disadvantages of using LlamaIndex or LangChain compared to direct communication with LLMs? Especially considering the impact on architectural complexity, development and maintenance costs, as well as scalability and flexibility.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/FillOk5686&quot;&gt; /u/FillOk5686 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bliwzp/why_do_we_need_llamaindex_or_langchain_for/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bliwzp/why_do_we_need_llamaindex_or_langchain_for/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bliwzp</id><link href="https://www.reddit.com/r/LangChain/comments/1bliwzp/why_do_we_need_llamaindex_or_langchain_for/" /><updated>2024-03-23T03:48:03+00:00</updated><published>2024-03-23T03:48:03+00:00</published><title>Why do we need LlamaIndex or LangChain for communicating with Large Language Models (LLM)?</title></entry><entry><author><name>/u/gamalibr</name><uri>https://www.reddit.com/user/gamalibr</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey folks!&lt;/p&gt; &lt;p&gt;I&amp;#39;m building a function calling prompt, but the results are terrible.&lt;/p&gt; &lt;h1&gt;Prompt&lt;/h1&gt; &lt;blockquote&gt; &lt;p&gt;Please select the best function to answer user questions and context. Follow this instructions:&lt;/p&gt; &lt;/blockquote&gt; &lt;h1&gt;Functions structure&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;GetDeliveryStatusForWorkItems&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Get the delivery status (whether late or on track) for work items. Response structure: {{&amp;quot;workItemsWithDeliveryStatus&amp;quot;: [{&amp;quot;id&amp;quot;: &amp;quot;string&amp;quot;, &amp;quot;key&amp;quot;: &amp;quot;string&amp;quot;, &amp;quot;title&amp;quot;: &amp;quot;string&amp;quot;, &amp;quot;actualStatus&amp;quot;: &amp;quot;string&amp;quot;, &amp;quot;assignedTo&amp;quot;: &amp;quot;string&amp;quot;, &amp;quot;leadTimeToEnd&amp;quot;: &amp;quot;number&amp;quot;, &amp;quot;leadTimeUsed&amp;quot;: &amp;quot;number&amp;quot;, &amp;quot;percentageLeadTimeAlreadyUsed&amp;quot;: &amp;quot;number&amp;quot;, &amp;quot;leadTimeToEndWithLeadTimeAlreadyUsed&amp;quot;: &amp;quot;number&amp;quot;, &amp;quot;percentageLeadTimeExceeded&amp;quot;: &amp;quot;number&amp;quot;, &amp;quot;isLate&amp;quot;: &amp;quot;boolean&amp;quot;, &amp;quot;onTrackFlag&amp;quot;: &amp;quot;string&amp;quot;}]}}&lt;/p&gt; &lt;p&gt;&lt;strong&gt;GetWorkItensTool&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Get work itens by keys or ids passed as parameters. Response structure: { &amp;quot;WorkItems&amp;quot;: [{&amp;quot;key&amp;quot;:&amp;quot;string&amp;quot;,&amp;quot;id&amp;quot;:&amp;quot;string&amp;quot;,&amp;quot;name&amp;quot;:&amp;quot;string&amp;quot;,&amp;quot;description&amp;quot;:&amp;quot;string&amp;quot;,&amp;quot;created&amp;quot;:&amp;quot;string&amp;quot;,&amp;quot;updated&amp;quot;:&amp;quot;string&amp;quot;,&amp;quot;changelog&amp;quot;:[{&amp;quot;id&amp;quot;:&amp;quot;string&amp;quot;,&amp;quot;createdAt&amp;quot;:&amp;quot;string&amp;quot;,&amp;quot;movements&amp;quot;:[{&amp;quot;field&amp;quot;:&amp;quot;string&amp;quot;,&amp;quot;fromColumnId&amp;quot;:&amp;quot;string&amp;quot;,&amp;quot;fromColumnName&amp;quot;:&amp;quot;string&amp;quot;,&amp;quot;toColumnId&amp;quot;:&amp;quot;string&amp;quot;,&amp;quot;toColumnName&amp;quot;:&amp;quot;string&amp;quot;}]}],&amp;quot;workItemCreatedAt&amp;quot;:&amp;quot;string&amp;quot;,&amp;quot;columnName&amp;quot;:&amp;quot;string&amp;quot;,&amp;quot;priority&amp;quot;:&amp;quot;string&amp;quot;,&amp;quot;flagged&amp;quot;:&amp;quot;boolean&amp;quot;,&amp;quot;assignee&amp;quot;:{&amp;quot;accountId&amp;quot;:&amp;quot;string&amp;quot;,&amp;quot;userName&amp;quot;:&amp;quot;string&amp;quot;},&amp;quot;workItemType&amp;quot;:{&amp;quot;name&amp;quot;:&amp;quot;string&amp;quot;,&amp;quot;id&amp;quot;:&amp;quot;string&amp;quot;,&amp;quot;description&amp;quot;:&amp;quot;string&amp;quot;,&amp;quot;subtask&amp;quot;:&amp;quot;boolean&amp;quot;},&amp;quot;status&amp;quot;:{&amp;quot;name&amp;quot;:&amp;quot;string&amp;quot;,&amp;quot;id&amp;quot;:&amp;quot;string&amp;quot;,&amp;quot;statusCategory&amp;quot;:{&amp;quot;name&amp;quot;:&amp;quot;string&amp;quot;,&amp;quot;id&amp;quot;:&amp;quot;number&amp;quot;}}}]}&lt;/p&gt; &lt;h1&gt;Functions JSON Schema&lt;/h1&gt; &lt;p&gt;```TOOLS&lt;/p&gt; &lt;ul&gt; &lt;li&gt;0: { &amp;quot;type&amp;quot;: &amp;quot;function&amp;quot;, &amp;quot;function&amp;quot;: { &amp;quot;name&amp;quot;: &amp;quot;GetColumnsConfigTool&amp;quot;, &amp;quot;description&amp;quot;: &amp;quot;Get the Id and Name for columns(To Do, WIP, and Done) from a board along with their respective order. Response Structure: \&amp;quot;{\&amp;quot;wipColumnsAndDoneColumns\&amp;quot;:[\&amp;quot;string\&amp;quot;],\&amp;quot;columnsConfig\&amp;quot;:{\&amp;quot;allColumns\&amp;quot;:[{\&amp;quot;id\&amp;quot;:\&amp;quot;string\&amp;quot;,\&amp;quot;name\&amp;quot;:\&amp;quot;string\&amp;quot;,\&amp;quot;order\&amp;quot;:\&amp;quot;number|null\&amp;quot;,\&amp;quot;column\&amp;quot;:\&amp;quot;string\&amp;quot;}],\&amp;quot;wipColumns\&amp;quot;:[\&amp;quot;string\&amp;quot;],\&amp;quot;doneColumns\&amp;quot;:[\&amp;quot;string\&amp;quot;],\&amp;quot;todoColumns\&amp;quot;:[\&amp;quot;string\&amp;quot;]}}\&amp;quot;&amp;quot;, &amp;quot;parameters&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;object&amp;quot;, &amp;quot;properties&amp;quot;: {}, &amp;quot;additionalProperties&amp;quot;: false, &amp;quot;$schema&amp;quot;: &amp;quot;&lt;a href=&quot;http://json-schema.org/draft-07/schema#&quot;&gt;http://json-schema.org/draft-07/schema#&lt;/a&gt;&amp;quot; } } }&lt;/li&gt; &lt;li&gt;1: { &amp;quot;type&amp;quot;: &amp;quot;function&amp;quot;, &amp;quot;function&amp;quot;: { &amp;quot;name&amp;quot;: &amp;quot;GetWorkItemsDeliveryStatusTool&amp;quot;, &amp;quot;description&amp;quot;: &amp;quot;Get the delivery status (whether late or on track) for work items. Response structure: {{\&amp;quot;workItemsWithDeliveryStatus\&amp;quot;: [{\&amp;quot;id\&amp;quot;: \&amp;quot;string\&amp;quot;, \&amp;quot;key\&amp;quot;: \&amp;quot;string\&amp;quot;, \&amp;quot;title\&amp;quot;: \&amp;quot;string\&amp;quot;, \&amp;quot;actualStatus\&amp;quot;: \&amp;quot;string\&amp;quot;, \&amp;quot;assignedTo\&amp;quot;: \&amp;quot;string\&amp;quot;, \&amp;quot;leadTimeToEnd\&amp;quot;: \&amp;quot;number\&amp;quot;, \&amp;quot;leadTimeUsed\&amp;quot;: \&amp;quot;number\&amp;quot;, \&amp;quot;percentageLeadTimeAlreadyUsed\&amp;quot;: \&amp;quot;number\&amp;quot;, \&amp;quot;leadTimeToEndWithLeadTimeAlreadyUsed\&amp;quot;: \&amp;quot;number\&amp;quot;, \&amp;quot;percentageLeadTimeExceeded\&amp;quot;: \&amp;quot;number\&amp;quot;, \&amp;quot;isLate\&amp;quot;: \&amp;quot;boolean\&amp;quot;, \&amp;quot;onTrackFlag\&amp;quot;: \&amp;quot;string\&amp;quot;}]}}\n This function need a get workItem function before.&amp;quot;, &amp;quot;parameters&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;object&amp;quot;, &amp;quot;properties&amp;quot;: { &amp;quot;parameters&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;array&amp;quot;, &amp;quot;items&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;object&amp;quot;, &amp;quot;properties&amp;quot;: { &amp;quot;workItems&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;array&amp;quot;, &amp;quot;items&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;string&amp;quot;, &amp;quot;description&amp;quot;: &amp;quot;Work Item Id&amp;quot; }, &amp;quot;description&amp;quot;: &amp;quot;value extracted from a text and returned as a parameter, examples of what the value looks like (GE-18, KDZ-20, NT-10, HYPER-44, APP-538)&amp;quot; } }, &amp;quot;additionalProperties&amp;quot;: false } } }, &amp;quot;required&amp;quot;: [ &amp;quot;parameters&amp;quot; ], &amp;quot;additionalProperties&amp;quot;: false, &amp;quot;$schema&amp;quot;: &amp;quot;&lt;a href=&quot;http://json-schema.org/draft-07/schema#&quot;&gt;http://json-schema.org/draft-07/schema#&lt;/a&gt;&amp;quot; } } }&lt;/li&gt; &lt;li&gt;2: { &amp;quot;type&amp;quot;: &amp;quot;function&amp;quot;, &amp;quot;function&amp;quot;: { &amp;quot;name&amp;quot;: &amp;quot;GetWorkItensTool&amp;quot;, &amp;quot;description&amp;quot;: &amp;quot;Get work itens by keys or ids passed as parameters. Response structure: { \&amp;quot;WorkItems\&amp;quot;: [{\&amp;quot;key\&amp;quot;:\&amp;quot;string\&amp;quot;,\&amp;quot;id\&amp;quot;:\&amp;quot;string\&amp;quot;,\&amp;quot;name\&amp;quot;:\&amp;quot;string\&amp;quot;,\&amp;quot;description\&amp;quot;:\&amp;quot;string\&amp;quot;,\&amp;quot;created\&amp;quot;:\&amp;quot;string\&amp;quot;,\&amp;quot;updated\&amp;quot;:\&amp;quot;string\&amp;quot;,\&amp;quot;changelog\&amp;quot;:[{\&amp;quot;id\&amp;quot;:\&amp;quot;string\&amp;quot;,\&amp;quot;createdAt\&amp;quot;:\&amp;quot;string\&amp;quot;,\&amp;quot;movements\&amp;quot;:[{\&amp;quot;field\&amp;quot;:\&amp;quot;string\&amp;quot;,\&amp;quot;fromColumnId\&amp;quot;:\&amp;quot;string\&amp;quot;,\&amp;quot;fromColumnName\&amp;quot;:\&amp;quot;string\&amp;quot;,\&amp;quot;toColumnId\&amp;quot;:\&amp;quot;string\&amp;quot;,\&amp;quot;toColumnName\&amp;quot;:\&amp;quot;string\&amp;quot;}]}],\&amp;quot;workItemCreatedAt\&amp;quot;:\&amp;quot;string\&amp;quot;,\&amp;quot;columnName\&amp;quot;:\&amp;quot;string\&amp;quot;,\&amp;quot;priority\&amp;quot;:\&amp;quot;string\&amp;quot;,\&amp;quot;flagged\&amp;quot;:\&amp;quot;boolean\&amp;quot;,\&amp;quot;assignee\&amp;quot;:{\&amp;quot;accountId\&amp;quot;:\&amp;quot;string\&amp;quot;,\&amp;quot;userName\&amp;quot;:\&amp;quot;string\&amp;quot;},\&amp;quot;workItemType\&amp;quot;:{\&amp;quot;name\&amp;quot;:\&amp;quot;string\&amp;quot;,\&amp;quot;id\&amp;quot;:\&amp;quot;string\&amp;quot;,\&amp;quot;description\&amp;quot;:\&amp;quot;string\&amp;quot;,\&amp;quot;subtask\&amp;quot;:\&amp;quot;boolean\&amp;quot;},\&amp;quot;status\&amp;quot;:{\&amp;quot;name\&amp;quot;:\&amp;quot;string\&amp;quot;,\&amp;quot;id\&amp;quot;:\&amp;quot;string\&amp;quot;,\&amp;quot;statusCategory\&amp;quot;:{\&amp;quot;name\&amp;quot;:\&amp;quot;string\&amp;quot;,\&amp;quot;id\&amp;quot;:\&amp;quot;number\&amp;quot;}}}]}&amp;quot;, &amp;quot;parameters&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;object&amp;quot;, &amp;quot;properties&amp;quot;: { &amp;quot;parameters&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;object&amp;quot;, &amp;quot;properties&amp;quot;: { &amp;quot;workItemsIds&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;array&amp;quot;, &amp;quot;items&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;string&amp;quot;, &amp;quot;description&amp;quot;: &amp;quot;WorkItemId. identifier for a work item. Examples: GE-18, KDZ-20, NT-10, HYPER-44, APP-538. Each ID represents a specific work item in a project or system.&amp;quot; } } }, &amp;quot;additionalProperties&amp;quot;: false, &amp;quot;description&amp;quot;: &amp;quot;Array of paramaters mentioned in text. You can use chat history to provide more context and get parameters&amp;quot; }, &amp;quot;operation&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;array&amp;quot;, &amp;quot;items&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;string&amp;quot;, &amp;quot;enum&amp;quot;: [ &amp;quot;ByWeek&amp;quot;, &amp;quot;inWIP&amp;quot;, &amp;quot;ByIds&amp;quot;, &amp;quot;Last24hours&amp;quot;, &amp;quot;ByTypes&amp;quot; ], &amp;quot;description&amp;quot;: &amp;quot;The inner filter for Work Items.&amp;quot; } } }, &amp;quot;required&amp;quot;: [ &amp;quot;parameters&amp;quot;, &amp;quot;operation&amp;quot; ], &amp;quot;additionalProperties&amp;quot;: false, &amp;quot;$schema&amp;quot;: &amp;quot;&lt;a href=&quot;http://json-schema.org/draft-07/schema#&quot;&gt;http://json-schema.org/draft-07/schema#&lt;/a&gt;&amp;quot; } } }```&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;User question&lt;/h1&gt; &lt;p&gt;What are the delivery dates for tasks that are in progress?&lt;/p&gt; &lt;h1&gt;Expected result&lt;/h1&gt; &lt;p&gt;functions GetWorkItensTool and GetDeliveryStatusForWorkItems.&lt;/p&gt; &lt;h1&gt;Actual result&lt;/h1&gt; &lt;p&gt;function GetWorkItensTool&lt;/p&gt; &lt;hr/&gt; &lt;p&gt;Would you happen to have any tips about how to improve it? Is there any better model than openAI GPT-4-turbo for it?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/gamalibr&quot;&gt; /u/gamalibr &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bley1d/improve_function_calling/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bley1d/improve_function_calling/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bley1d</id><link href="https://www.reddit.com/r/LangChain/comments/1bley1d/improve_function_calling/" /><updated>2024-03-23T00:31:07+00:00</updated><published>2024-03-23T00:31:07+00:00</published><title>Improve function calling</title></entry><entry><author><name>/u/Dear_Insect_5295</name><uri>https://www.reddit.com/user/Dear_Insect_5295</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I am working on a rag application where I need to rerank the docs, I tried langchain integrations like colbert, flashrank but I wanted to implement other rankers now I have a list of relevant docs after the user query, any suggestions on how can I send this as parameter to llm chain.&lt;br/&gt; Below is sample code from langchain cookbook. &lt;/p&gt; &lt;p&gt;from langchain_core.runnables import RunnableParallel&lt;br/&gt; rag_chain_from_docs = (&lt;br/&gt; RunnablePassthrough.assign(context=(lambda x: format_docs(x[&amp;quot;context&amp;quot;])))&lt;br/&gt; | prompt&lt;br/&gt; | llm&lt;br/&gt; | StrOutputParser()&lt;br/&gt; )&lt;br/&gt; rag_chain_with_source = RunnableParallel(&lt;br/&gt; {&amp;quot;context&amp;quot;: retriever, &amp;quot;question&amp;quot;: RunnablePassthrough()}&lt;br/&gt; ).assign(answer=rag_chain_from_docs)&lt;br/&gt; rag_chain_with_source.invoke(&amp;quot;What is Task Decomposition&amp;quot;) &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Dear_Insect_5295&quot;&gt; /u/Dear_Insect_5295 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bllgb8/how_can_we_send_reranked_documents_to_any_llm/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bllgb8/how_can_we_send_reranked_documents_to_any_llm/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bllgb8</id><link href="https://www.reddit.com/r/LangChain/comments/1bllgb8/how_can_we_send_reranked_documents_to_any_llm/" /><updated>2024-03-23T06:19:10+00:00</updated><published>2024-03-23T06:19:10+00:00</published><title>How can we send reranked documents to any llm chain instead of retriever?</title></entry><entry><author><name>/u/ridiculoys</name><uri>https://www.reddit.com/user/ridiculoys</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi! I&amp;#39;m new to Langchain and tinkering with LLMs in general, I&amp;#39;m just doing a small project on Langchain&amp;#39;s capabilities on document loading, chunking, and of course using a similarity search on a vectorstore and then using the information I retrieve in a chain to get an answer.&lt;/p&gt; &lt;p&gt;I&amp;#39;m only testing on a small dataset, so it&amp;#39;s easy for me to see the specific files and pages to cross check whether it is the best result among the different files. But it got me thinking: if I try to work with a larger dataset, how exactly do I verify if the answer is the best result in the ranking and if it is indeed correct?&lt;/p&gt; &lt;p&gt;Is it possible to get datasets where it contains a PDF, some test input prompts, and an expected certain correct output? This way, I would be able to use my project to ingest that data and see if I get similar results? Or is this too good to be true?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/ridiculoys&quot;&gt; /u/ridiculoys &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bl1p6d/how_do_you_verify_aside_from_manually_checking/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bl1p6d/how_do_you_verify_aside_from_manually_checking/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bl1p6d</id><link href="https://www.reddit.com/r/LangChain/comments/1bl1p6d/how_do_you_verify_aside_from_manually_checking/" /><updated>2024-03-22T15:16:39+00:00</updated><published>2024-03-22T15:16:39+00:00</published><title>How do you verify, aside from manually checking the PDFs, that your answers are correct from a simple RAG implementation using Langchain?</title></entry><entry><author><name>/u/marclelamy</name><uri>https://www.reddit.com/user/marclelamy</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;m trying to implement RAG with semantic search and have been using OpenAI text embedding 3 small, but the results aren&amp;#39;t particularly good. Do you know any that is at least better than this one on which I can experiment on? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/marclelamy&quot;&gt; /u/marclelamy &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1blfg7i/what_is_the_current_best_embedding_model_for/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1blfg7i/what_is_the_current_best_embedding_model_for/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1blfg7i</id><link href="https://www.reddit.com/r/LangChain/comments/1blfg7i/what_is_the_current_best_embedding_model_for/" /><updated>2024-03-23T00:54:52+00:00</updated><published>2024-03-23T00:54:52+00:00</published><title>What is the current best embedding model for semantic search?</title></entry><entry><author><name>/u/SustainedSuspense</name><uri>https://www.reddit.com/user/SustainedSuspense</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Im passing in my Graphql documentation as html and splitting it by section tag. I want to search that data and return the relevant information for that query or mutation. However, both &lt;code&gt;retriever.get_relevant_documents&lt;/code&gt; and &lt;code&gt;vector.similarity_search&lt;/code&gt; returns all the same data that &lt;code&gt;print(documents)&lt;/code&gt; prints out (the entire document&amp;#39;s text). Any help would be appreciated. Thank you.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;from langchain_openai import OpenAIEmbeddings from langchain_community.vectorstores import FAISS from langchain_text_splitters import HTMLHeaderTextSplitter embeddings = OpenAIEmbeddings() html = open(&amp;quot;src/bff/index.html&amp;quot;, &amp;#39;r&amp;#39;).read() headers_to_split_on = [ (&amp;quot;section &amp;gt; h2&amp;quot;, &amp;quot;GraphQL query, mutation or type definition&amp;quot;), ] html_splitter = HTMLHeaderTextSplitter(headers_to_split_on=headers_to_split_on) documents = html_splitter.split_text(html) print(documents) vector = FAISS.from_documents(documents, embeddings) retriever = vector.as_retriever() response1 = retriever.get_relevant_documents(&amp;quot;WorkflowDto definition&amp;quot;) print(response1) response2 = vector.similarity_search(&amp;quot;WorkflowDto definition&amp;quot;) print(response2) &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/SustainedSuspense&quot;&gt; /u/SustainedSuspense &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bljqco/beginner_trying_make_a_simple_rag_with_graphql/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bljqco/beginner_trying_make_a_simple_rag_with_graphql/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bljqco</id><link href="https://www.reddit.com/r/LangChain/comments/1bljqco/beginner_trying_make_a_simple_rag_with_graphql/" /><updated>2024-03-23T04:32:52+00:00</updated><published>2024-03-23T04:32:52+00:00</published><title>Beginner trying make a simple RAG with Graphql documentation. Not getting anywhere.</title></entry><entry><author><name>/u/Ambitious_Ordinary50</name><uri>https://www.reddit.com/user/Ambitious_Ordinary50</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;anyone have done a project with azure open ai and different azure services like postgress for storing the embedding and azure blob for storing the files &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Ambitious_Ordinary50&quot;&gt; /u/Ambitious_Ordinary50 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1blhhxq/langchain_with_azure_openai/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1blhhxq/langchain_with_azure_openai/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1blhhxq</id><link href="https://www.reddit.com/r/LangChain/comments/1blhhxq/langchain_with_azure_openai/" /><updated>2024-03-23T02:34:53+00:00</updated><published>2024-03-23T02:34:53+00:00</published><title>langchain with azure openai</title></entry><entry><author><name>/u/okahuAI</name><uri>https://www.reddit.com/user/okahuAI</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/okahuAI&quot;&gt; /u/okahuAI &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;/r/AIObservability/comments/1bkknxl/okahu_ai_observability_is_now_in_preview/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1blh2xh/okahu_ai_observability_is_now_in_preview/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1blh2xh</id><link href="https://www.reddit.com/r/LangChain/comments/1blh2xh/okahu_ai_observability_is_now_in_preview/" /><updated>2024-03-23T02:14:23+00:00</updated><published>2024-03-23T02:14:23+00:00</published><title>Okahu AI Observability is now in preview!</title></entry><entry><author><name>/u/drunkmute</name><uri>https://www.reddit.com/user/drunkmute</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi, currently I am working on an RAG tool for my company. I have to load multiple PDFs and Powerpoints, for which I use the UnstructuredPDFLoader and UnstructuredPowerPointLoader, because a lot of these documents contain images with text on them and these loaders allow you to extract said text through OCR. However, when I run this and the program goes through the retrieval steps, I get an error coming from a deprecated function, along with the output of each document split that will be used to answer my query. The answers to my questions are not of a high quality, and I believe that it may be attributed to something being wrong with my loaders because the output I am seeing for some document splits is &amp;quot;NO_OUTPUT&amp;quot; or &amp;quot;NO_OUTPUT_OUTPUT&amp;quot;.&lt;/p&gt; &lt;p&gt;I am wondering if any of you have run into this problem. In addition, as a bonus question, how do you all maintain metadata like source information in your document splits? I always lose mine.&lt;/p&gt; &lt;p&gt;Below is the retrieval code:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;# Vector Database u/st.cache_resource def vector_db_init(_folder_id, _model): &amp;quot;&amp;quot;&amp;quot;Vector db initializer, plus contextual compression addition&amp;quot;&amp;quot;&amp;quot; persist_directory = &amp;quot;./db/&amp;quot; # Persist directory path # Embeddings to be applied embeddings = VertexAIEmbeddings( model_name=&amp;quot;textembedding-gecko-multilingual&amp;quot;, credentials=CREDENTIALS, project_id=PROJECT_ID, ) # Document splitting, embedding and vector database loading # DOES NOT have to be done in every run, just once and after you can simply refer to the db if not os.path.exists(persist_directory): # Data Pre-processing # TODO- loader that can correct typos and what-not pdf_loader = DirectoryLoader(&amp;quot;/Users/marconardoneguerra/Desktop/e3_Consulting/Other/AI/Proposal RAG/docs&amp;quot;, glob=&amp;quot;**/*.pdf&amp;quot;, recursive=True, show_progress=True, loader_cls=UnstructuredPDFLoader, loader_kwargs={ #&amp;quot;extract_images&amp;quot;:True, &amp;quot;post_processors&amp;quot;:[clean_extra_whitespace, clean_non_ascii_chars, clean], # data cleaning &amp;quot;mode&amp;quot;:&amp;quot;single&amp;quot;, &amp;quot;strategy&amp;quot;:&amp;quot;hi_res&amp;quot;, &amp;quot;high_res_model_name&amp;quot;:&amp;quot;detectron2_onnx&amp;quot;, #&amp;quot;encoding&amp;quot;:&amp;quot;unicode&amp;quot; }) ppt_loader = DirectoryLoader(&amp;quot;/Users/marconardoneguerra/Desktop/e3_Consulting/Other/AI/Proposal RAG/docs&amp;quot;, glob=&amp;quot;**/*.pptx&amp;quot;, recursive=True, show_progress=True, loader_cls=UnstructuredPowerPointLoader, loader_kwargs={ #&amp;quot;extract_images&amp;quot;:True, &amp;quot;post_processors&amp;quot;:[clean_extra_whitespace, clean_non_ascii_chars, clean], # data cleaning &amp;quot;mode&amp;quot;:&amp;quot;single&amp;quot;, &amp;quot;strategy&amp;quot;:&amp;quot;hi_res&amp;quot;, &amp;quot;high_res_model_name&amp;quot;:&amp;quot;detectron2_onnx&amp;quot;, #&amp;quot;encoding&amp;quot;:&amp;quot;unicode&amp;quot; }) loaded_pdfs = pdf_loader.load() loaded_ppts = ppt_loader.load() print(&amp;quot;# of PDFs:&amp;quot; + str(len(loaded_pdfs))) print(&amp;quot;# of PPTs:&amp;quot; + str(len(loaded_ppts))) loaded_docs = loaded_pdfs + loaded_ppts # gdrive_documents = doc_processor(gdrive_documents, image_captioning) context = &amp;quot;\n\n&amp;quot;.join(str(p.page_content) for p in loaded_docs) # had to remove below because this splitter is new and does not have a max token size, hence has given chunks too large to handle # splitter = SemanticChunker(embeddings) splitter = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=25) data = splitter.split_text(context) print(&amp;quot;Data Processing Complete&amp;quot;) vectordb = Chroma.from_texts( data, embeddings, persist_directory=persist_directory ) vectordb.persist() print(&amp;quot;Vector DB Creating Complete\n&amp;quot;) elif os.path.exists(persist_directory): vectordb = Chroma( persist_directory=persist_directory, embedding_function=embeddings ) print(&amp;quot;Vector DB Loaded\n&amp;quot;) # Compresses what is contextually needed for query answer (?) compressor = LLMChainExtractor.from_llm(_model, ) compression_retriever = ContextualCompressionRetriever(base_compressor=compressor, base_retriever=vectordb.as_retriever(search_type=&amp;quot;similarity&amp;quot;, search_kwargs={&amp;quot;k&amp;quot;: 24})) return compression_retriever @st.cache_resource def chain_init(_model, _retriever): &amp;quot;&amp;quot;&amp;quot;Initializes chain for retrieval&amp;quot;&amp;quot;&amp;quot; # DONE- conversational template template = &amp;quot;&amp;quot;&amp;quot; Who you are: You are an expert on everything about e3 Consulting, AKA e3, a consulting firm based in San Juan, Puerto Rico. Your firm specializes in IT consulting. \ ------ Instructions: You will be receiving questions about e3 and their previous work. \ You will gather knowledge to deliver a good response to the user (separated with &amp;lt;ctx&amp;gt;&amp;lt;/ctx&amp;gt;). \ If you don&amp;#39;t know the answer, answer with &amp;quot;Unfortunately, I don&amp;#39;t have the information.&amp;quot; \ If you don&amp;#39;t find enough information below, also answer with &amp;quot;Unfortunately, I don&amp;#39;t have the information.&amp;quot; \ The context will most likely have typos in it, please correct them when you formulate your answer. \ ------ &amp;lt;ctx&amp;gt; {context} &amp;lt;/ctx&amp;gt; ------ {question} Answer: &amp;quot;&amp;quot;&amp;quot; # template above question_prompt_template = PromptTemplate(template=template, input_variables=[&amp;quot;question&amp;quot;, &amp;quot;context&amp;quot;]) # We create a qa chain with our llm, retriever, and memory # Use chain_type refine so we cna build off of different information, # in addition to being wary of our context window # TODO make this conversational (could be complex) qa_chain = RetrievalQA.from_chain_type( llm=_model, chain_type=&amp;quot;stuff&amp;quot;, return_source_documents=True, retriever=_retriever, verbose=True, ) # qa_chain = RetrievalQA | StrOutputParser() return qa_chain &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Error examples:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;NO_OUTPUT_OUTPUT/Users/marconardoneguerra/anaconda3/envs/proposal-rag/lib/python3.12/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain. warnings.warn( NO_OUTPUT_OUTPUT/Users/marconardoneguerra/anaconda3/envs/proposal-rag/lib/python3.12/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain. warnings.warn( NO_OUTPUT_OUTPUT/Users/marconardoneguerra/anaconda3/envs/proposal-rag/lib/python3.12/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain. warnings.warn( NO_OUTPUT_OUTPUT/Users/marconardoneguerra/anaconda3/envs/proposal-rag/lib/python3.12/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain. &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/drunkmute&quot;&gt; /u/drunkmute &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1blc6q2/document_loaders_outputting_no_output/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1blc6q2/document_loaders_outputting_no_output/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1blc6q2</id><link href="https://www.reddit.com/r/LangChain/comments/1blc6q2/document_loaders_outputting_no_output/" /><updated>2024-03-22T22:31:29+00:00</updated><published>2024-03-22T22:31:29+00:00</published><title>Document Loaders Outputting &quot;NO_OUTPUT&quot;</title></entry><entry><author><name>/u/e3e6</name><uri>https://www.reddit.com/user/e3e6</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi community, looking for some guidance to explain how this is called when I specify the Tool&amp;#39;s parameters as a description rather than Fields. &lt;/p&gt; &lt;p&gt;For some reason Fields does not work in that project, giving me exception like&lt;/p&gt; &lt;pre&gt;&lt;code&gt;ERROR:root:An error occurred ZeroShotAgent does not support multi-input tool &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;Here is some code&lt;/p&gt; &lt;pre&gt;&lt;code&gt;class SimpleInputs(BaseModel): input: str class GetEvents(BaseTool): name = &amp;quot;get_calendar_event&amp;quot; description = &amp;#39;Use this tool with arguments like &amp;quot;{{&amp;quot;start_date&amp;quot;: &amp;quot;yyyy-mm-dd&amp;quot;, &amp;quot;max_results&amp;quot;: int}}&amp;quot; when you need to retrieve events from Calendar.&amp;#39; args_schema: Type[BaseModel] = SimpleInputs return_direct: bool = False --- agent = initialize_agent( agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, tools=tools, llm=self.llm, verbose=True, &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/e3e6&quot;&gt; /u/e3e6 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1blaz9d/langchain_single_input_agent_params/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1blaz9d/langchain_single_input_agent_params/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1blaz9d</id><link href="https://www.reddit.com/r/LangChain/comments/1blaz9d/langchain_single_input_agent_params/" /><updated>2024-03-22T21:41:07+00:00</updated><published>2024-03-22T21:41:07+00:00</published><title>LangChain single input Agent params</title></entry><entry><author><name>/u/digital-bolkonsky</name><uri>https://www.reddit.com/user/digital-bolkonsky</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Does anyone have a codebase or tutorial for using LLMs with LangChain to summarize each row in a database and generate output for each? I have a database that is updated weekly, which you can think of as a record of transactions. I&amp;#39;m looking for a way to read each row of this database weekly, summarize the contents of those records, and have it tweeted out. I&amp;#39;m curious if there&amp;#39;s a tutorial or codebase somewhere that does this. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/digital-bolkonsky&quot;&gt; /u/digital-bolkonsky &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bl7k4g/does_anyone_have_a_codebase_or_tutorial_for_using/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bl7k4g/does_anyone_have_a_codebase_or_tutorial_for_using/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bl7k4g</id><link href="https://www.reddit.com/r/LangChain/comments/1bl7k4g/does_anyone_have_a_codebase_or_tutorial_for_using/" /><updated>2024-03-22T19:18:59+00:00</updated><published>2024-03-22T19:18:59+00:00</published><title>Does anyone have a codebase or tutorial for using LLMs with LangChain to summarize each row in a database and generate output for each?</title></entry><entry><author><name>/u/Fit-Set6851</name><uri>https://www.reddit.com/user/Fit-Set6851</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;If I have 100 documents in my vector db. In the metadata t are total of 5 sources and each source have 20 documents in the vector db. &lt;/p&gt; &lt;p&gt;So now as query is given by the user I want to process relevant documents of each source separately and then combine the answers. &lt;/p&gt; &lt;p&gt;Can somebody help me on how to do this in an optimized way? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Fit-Set6851&quot;&gt; /u/Fit-Set6851 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bkysyf/how_to_process_each_source_in_vector_db/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bkysyf/how_to_process_each_source_in_vector_db/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bkysyf</id><link href="https://www.reddit.com/r/LangChain/comments/1bkysyf/how_to_process_each_source_in_vector_db/" /><updated>2024-03-22T13:07:15+00:00</updated><published>2024-03-22T13:07:15+00:00</published><title>How to process each source in Vector db individually ?</title></entry><entry><author><name>/u/smtabatabaie</name><uri>https://www.reddit.com/user/smtabatabaie</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey everyone, just wanted to ask if you know any Langchain GPTs that actually works and is updated with the latest Langchain documents?&lt;br/&gt; Thanks&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/smtabatabaie&quot;&gt; /u/smtabatabaie &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bl3lpg/langchain_gpt/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bl3lpg/langchain_gpt/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bl3lpg</id><link href="https://www.reddit.com/r/LangChain/comments/1bl3lpg/langchain_gpt/" /><updated>2024-03-22T16:36:17+00:00</updated><published>2024-03-22T16:36:17+00:00</published><title>Langchain GPT</title></entry><entry><author><name>/u/cryptokaykay</name><uri>https://www.reddit.com/user/cryptokaykay</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;What are some challenges you face after deploying your LLM based application in production? &lt;/p&gt; &lt;p&gt;My only goal is to improve the accuracy of my chatbot. It seems like everything boils down to this unless there are any other special usecases you are using the LLMs for. Basically. I try to monitor for all the responses of my chatbot and measure them objectively so I can tweak and improve the accuracy. This seems pretty basic. But, what are some of the other levers that I can pull to improve the accuracy of my RAG based chat application?&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;I am also building a tooling for tracing and monitoring the responses with higher cardinality compared to the ones that are in the market. Plan to open source it pretty soon.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/cryptokaykay&quot;&gt; /u/cryptokaykay &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bklgf7/daily_struggles_with_my_llm_based_chatbot_in/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bklgf7/daily_struggles_with_my_llm_based_chatbot_in/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bklgf7</id><link href="https://www.reddit.com/r/LangChain/comments/1bklgf7/daily_struggles_with_my_llm_based_chatbot_in/" /><updated>2024-03-21T23:53:39+00:00</updated><published>2024-03-21T23:53:39+00:00</published><title>Daily struggles with my LLM based chatbot in production</title></entry><entry><author><name>/u/heisenberg-principle</name><uri>https://www.reddit.com/user/heisenberg-principle</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Couldn&amp;#39;t find any other post on this topic but I&amp;#39;m having an issue with langchain_mistralai library. We&amp;#39;re using weaviate_client 4.5.4 which requires httpx version 0.27.0. However langchain_mistralai is not compatible with httpx versions &amp;gt; 0.26.0. Will this be fixed at some point or should I give up and find a workaround? (downgrading weaviate_client is not an option, since it has needed functionalities which can&amp;#39;t be sacrificed XD)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/heisenberg-principle&quot;&gt; /u/heisenberg-principle &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bkxdyu/dependency_issues_when_adding_langchain_mistralai/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bkxdyu/dependency_issues_when_adding_langchain_mistralai/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bkxdyu</id><link href="https://www.reddit.com/r/LangChain/comments/1bkxdyu/dependency_issues_when_adding_langchain_mistralai/" /><updated>2024-03-22T11:51:14+00:00</updated><published>2024-03-22T11:51:14+00:00</published><title>Dependency issues when adding langchain_mistralai to the project dependencies</title></entry><entry><author><name>/u/Putrid_Spinach3961</name><uri>https://www.reddit.com/user/Putrid_Spinach3961</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey i am getting this error:&lt;/p&gt; &lt;p&gt;openai.NotFoundError: Error code: 404 - {&amp;#39;error&amp;#39;:{&amp;#39;code&amp;#39;:&amp;#39;404&amp;#39;, &amp;#39;message&amp;#39;: &amp;#39;Resource not found&amp;#39;}}&lt;/p&gt; &lt;p&gt;I used: From langchain_community.llms import OpenAI&lt;/p&gt; &lt;p&gt;From langchain.chains import LLMChain&lt;/p&gt; &lt;p&gt;Code: llm= OpenAI(model_name=&amp;quot;modelname&amp;quot;)&lt;/p&gt; &lt;p&gt;Output=LLMChain(prompt=prompt, llm=llm).run(&amp;#39;query&amp;#39;)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Putrid_Spinach3961&quot;&gt; /u/Putrid_Spinach3961 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bkxck3/why_this_error/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bkxck3/why_this_error/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bkxck3</id><link href="https://www.reddit.com/r/LangChain/comments/1bkxck3/why_this_error/" /><updated>2024-03-22T11:49:06+00:00</updated><published>2024-03-22T11:49:06+00:00</published><title>Why this error?</title></entry><entry><author><name>/u/s8ntinel69</name><uri>https://www.reddit.com/user/s8ntinel69</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bkwdjn/why_is_my_chaininvoke_command_giving_the_full/&quot;&gt; &lt;img src=&quot;https://b.thumbs.redditmedia.com/tkmq7LaPtW_OrBODlSdRs4PFlx5mNx5A-oAsIBwbBhQ.jpg&quot; alt=&quot;Why is my chain.invoke({}) command giving the full model response instead of just AIMessage(content=' ')&quot; title=&quot;Why is my chain.invoke({}) command giving the full model response instead of just AIMessage(content=' ')&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I am using ChatVertexAI and the ChatPromptTemplate to provide the model with a system message, and a user message, both of which are stored in separate variables which return a string. &lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/6raj796a7vpc1.png?width=831&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=61b9af92cc549d82da840310606772c3d3d0c51e&quot;&gt;Prompt template&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The chain uses LCEL to define the chain for the invoke command &lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/1ud0215i7vpc1.png?width=847&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=604c30b0fdf719ff723c86abda5757ccd1146613&quot;&gt;The chain and invoke command&lt;/a&gt;&lt;/p&gt; &lt;p&gt;However, the output that I get includes details that I should get if the command was chain.generate() and not chain.invoke(). It should not include all of the response metadata that is being printed here. &lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/vzocmpoy7vpc1.png?width=1103&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3890c438dfef3a70cb5801c820380236a7a62435&quot;&gt;https://preview.redd.it/vzocmpoy7vpc1.png?width=1103&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3890c438dfef3a70cb5801c820380236a7a62435&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Should&amp;#39;nt the output contain only AIMessage(content=&amp;#39;.........&amp;#39;) and not anything else? I Know I can use definition.content in this case, but in reality I cannot use that as this output is going to be used by langgraph for creating a reflection agent, in which I need to use the output like it is. &lt;/p&gt; &lt;p&gt;I checked all documentation and my prompt template as well the call to the LLM is exactly as it should be, but in the examples they show, the chain.invoke command should not print response metadata.&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/s8ntinel69&quot;&gt; /u/s8ntinel69 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bkwdjn/why_is_my_chaininvoke_command_giving_the_full/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bkwdjn/why_is_my_chaininvoke_command_giving_the_full/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1bkwdjn</id><media:thumbnail url="https://b.thumbs.redditmedia.com/tkmq7LaPtW_OrBODlSdRs4PFlx5mNx5A-oAsIBwbBhQ.jpg" /><link href="https://www.reddit.com/r/LangChain/comments/1bkwdjn/why_is_my_chaininvoke_command_giving_the_full/" /><updated>2024-03-22T10:50:34+00:00</updated><published>2024-03-22T10:50:34+00:00</published><title>Why is my chain.invoke({}) command giving the full model response instead of just AIMessage(content=' ')</title></entry><entry><author><name>/u/BichonFrise_</name><uri>https://www.reddit.com/user/BichonFrise_</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;[New in my AI agent journey]&lt;/p&gt; &lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;As I mentionned in the title of this post, I&amp;#39;m wondering if Langchain is the best framework to build AI agents that are able to retrieve information online. &lt;/p&gt; &lt;p&gt;I&amp;#39;ll give you one example : &lt;/p&gt; &lt;ul&gt; &lt;li&gt;Step 1 :I would like to give my agent a list of website and ask them if this company is a B2C company or a B2B company.&lt;/li&gt; &lt;li&gt;Step 2 : Chain this agent to another : if it&amp;#39;s a B2B company find the pricing&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Is is possible to do so with Langchain ? &lt;/p&gt; &lt;p&gt;If yes, do you know where I could find a tutorial to get me started ?&lt;/p&gt; &lt;p&gt;If not what is the best framework out there ? I saw &lt;a href=&quot;https://github.com/joaomdmoura/crewai/&quot;&gt;https://github.com/joaomdmoura/crewai/&lt;/a&gt; &amp;amp; &lt;a href=&quot;https://superagent.sh&quot;&gt;superagent.sh&lt;/a&gt; but I&amp;#39;m not sure if these are exactly what I&amp;#39;m looking for. &lt;/p&gt; &lt;p&gt;Thanks for your help ! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/BichonFrise_&quot;&gt; /u/BichonFrise_ &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bktz0j/how_to_build_ai_agents_for_information_retrieval/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bktz0j/how_to_build_ai_agents_for_information_retrieval/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bktz0j</id><link href="https://www.reddit.com/r/LangChain/comments/1bktz0j/how_to_build_ai_agents_for_information_retrieval/" /><updated>2024-03-22T08:00:15+00:00</updated><published>2024-03-22T08:00:15+00:00</published><title>How to build AI agents for information retrieval online</title></entry><entry><author><name>/u/o3omoomin</name><uri>https://www.reddit.com/user/o3omoomin</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I wrote this after looking at the Ensemble Retriever docs.&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://docs.llamaindex.ai/en/stable/examples/retrievers/ensemble_retrieval/&quot;&gt;https://docs.llamaindex.ai/en/stable/examples/retrievers/ensemble_retrieval/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;And can you explain the code for the evaluation part in detail with comments? A post is happening, but I don&amp;#39;t know what it is. Below is the evaluation code&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;from llama_index.core.evaluation import ( CorrectnessEvaluator, SemanticSimilarityEvaluator, RelevancyEvaluator, FaithfulnessEvaluator, PairwiseComparisonEvaluator, ) from llama_index.core.evaluation.eval_utils import ( get_responses; get_results_df; ) from llama_index.core.evaluation import BatchEvalRunner import numpy as np evaluator_c = CorrectnessEvaluator(llm=eval_llm) evaluator_s = SemanticSimilarityEvaluator() evaluator_r = RelevancyEvaluator(llm=eval_llm) evaluator_f = FaithfulnessEvaluator(llm=eval_llm) pairwise_evaluator = PairwiseComparisonEvaluator(llm=eval_llm) max_samples = 5 eval_qs = eval_dataset.questions qr_pairs = eval_dataset.qr_pairs ref_response_strs = [r for (_, r) in qr_pairs] base_query_engine = vector_indices[-1].as_query_engine(similarity_top_k=2) query_engine = RetrieverQueryEngine(retriever, node_postprocessors=[reranker]) base_pred_responses = get_responses( eval_qs[:max_samples], base_query_engine, show_progress=True ) pred_responses = get_responses( eval_qs[:max_samples], query_engine, show_progress=True ) sponse_strs = [str(p) for p in pred_responses] base_pred_response_strs = [str(p) for p in base_pred_responses] evaluator_dict = { &amp;quot;correctness&amp;quot;: evaluator_c, &amp;quot;faithfulness&amp;quot;: evaluator_f, &amp;quot;semantic_similarity&amp;quot;: evaluator_s, } batch_runner = BatchEvalRunner(evaluator_dict, workers=1, show_progress=True) eval_results = await batch_runner. evaluate_responses( queries=eval_qs[:max_samples], responses=pred_responses[:max_samples], reference=ref_response_strs[:max_samples], ) base_eval_results = await batch_runner.aevaluate_responses( queries=eval_qs[:max_samples], responses=base_pred_responses[:max_samples], reference=ref_response_strs[:max_samples], ) results_df = get_results_df( [eval_results, base_eval_results], [&amp;quot;Ensemble Retriever&amp;quot;, &amp;quot;Base Retriever&amp;quot;], [&amp;quot;correctness&amp;quot;, &amp;quot;faithfulness&amp;quot;, &amp;quot;semantic_similarity&amp;quot;], ) display(results_df) &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/o3omoomin&quot;&gt; /u/o3omoomin &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bktatx/please_explain_the_logic_behind_the_evaluation_of/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bktatx/please_explain_the_logic_behind_the_evaluation_of/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bktatx</id><link href="https://www.reddit.com/r/LangChain/comments/1bktatx/please_explain_the_logic_behind_the_evaluation_of/" /><updated>2024-03-22T07:09:51+00:00</updated><published>2024-03-22T07:09:51+00:00</published><title>Please explain the logic behind the evaluation of the llama index.</title></entry><entry><author><name>/u/The-Tank-849</name><uri>https://www.reddit.com/user/The-Tank-849</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Any of you are happy and have almost perfect result either their LLM chatbots with business data? Happy to discuss&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/The-Tank-849&quot;&gt; /u/The-Tank-849 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bkmo3b/chatbot_in_production/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bkmo3b/chatbot_in_production/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bkmo3b</id><link href="https://www.reddit.com/r/LangChain/comments/1bkmo3b/chatbot_in_production/" /><updated>2024-03-22T00:49:45+00:00</updated><published>2024-03-22T00:49:45+00:00</published><title>Chatbot in production</title></entry><entry><author><name>/u/rpatel09</name><uri>https://www.reddit.com/user/rpatel09</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;m looking to build a RAG app on our github repositories but wanted to ask if anyone has done something like this and what chunking strategies worked and didn&amp;#39;t work. I&amp;#39;ve been looking at semantic chunking but unsure how this would work with code? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/rpatel09&quot;&gt; /u/rpatel09 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bkkqel/chunking_strategies_for_code/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bkkqel/chunking_strategies_for_code/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bkkqel</id><link href="https://www.reddit.com/r/LangChain/comments/1bkkqel/chunking_strategies_for_code/" /><updated>2024-03-21T23:21:06+00:00</updated><published>2024-03-21T23:21:06+00:00</published><title>chunking strategies for code?</title></entry><entry><author><name>/u/internetcookiez</name><uri>https://www.reddit.com/user/internetcookiez</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I have a ChromaDB database which I can query information about a specific data, however, this data also has numerical data that I would like to transform into a SQL database, in .db form.&lt;/p&gt; &lt;p&gt;However, I want to be able to infer whether the LLM should call the vector db, and go through a ChromaDB chain for the answer, or go through an SQL chain. &lt;/p&gt; &lt;p&gt;How can I make this happen, automatically as the user asks questions? Basically, how can I create an agent that can determine which one to run?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/internetcookiez&quot;&gt; /u/internetcookiez &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bkqww1/how_can_i_combine_chromadb_and_sql_in_langchain/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bkqww1/how_can_i_combine_chromadb_and_sql_in_langchain/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bkqww1</id><link href="https://www.reddit.com/r/LangChain/comments/1bkqww1/how_can_i_combine_chromadb_and_sql_in_langchain/" /><updated>2024-03-22T04:29:18+00:00</updated><published>2024-03-22T04:29:18+00:00</published><title>How can I combine ChromaDB and SQL in langchain?</title></entry><entry><author><name>/u/FigureClassic6675</name><uri>https://www.reddit.com/user/FigureClassic6675</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/FigureClassic6675&quot;&gt; /u/FigureClassic6675 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bkp8fv/how_to_create_lead_capture_chatbot_with_function/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bkp8fv/how_to_create_lead_capture_chatbot_with_function/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bkp8fv</id><link href="https://www.reddit.com/r/LangChain/comments/1bkp8fv/how_to_create_lead_capture_chatbot_with_function/" /><updated>2024-03-22T02:57:18+00:00</updated><published>2024-03-22T02:57:18+00:00</published><title>How to create lead capture chatbot with function calling feature</title></entry></feed>