<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/"><category term="LangChain" label="r/LangChain"/><updated>2024-08-02T06:37:02+00:00</updated><icon>https://www.redditstatic.com/icon.png/</icon><id>/r/LangChain.rss</id><link rel="self" href="https://www.reddit.com/r/LangChain.rss" type="application/atom+xml" /><link rel="alternate" href="https://www.reddit.com/r/LangChain" type="text/html" /><subtitle>LangChain is an open-source framework and developer toolkit that helps developers get LLM applications from prototype to production. It is available for Python and Javascript at https://www.langchain.com/.</subtitle><title>LangChain</title><entry><author><name>/u/zchaarm</name><uri>https://www.reddit.com/user/zchaarm</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;A place for members of &lt;a href=&quot;/r/LangChain&quot;&gt;r/LangChain&lt;/a&gt; to chat with each other&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/zchaarm&quot;&gt; /u/zchaarm &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_10ljho9</id><link href="https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/" /><updated>2023-01-26T04:33:05+00:00</updated><published>2023-01-26T04:33:05+00:00</published><title>r/LangChain Lounge</title></entry><entry><author><name>/u/lzyTitan412</name><uri>https://www.reddit.com/user/lzyTitan412</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=pLPJoFvq4_M&quot;&gt;LangGraph Studio: The first agent IDE (youtube.com)&lt;/a&gt; -- check this out.&lt;/p&gt; &lt;p&gt;Just a week back, I was thinking of developing a web app kind of interface for langgraph, and they just launched it. Now, what if there were a drag-and-drop-like application for creating a complex graph chain?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/lzyTitan412&quot;&gt; /u/lzyTitan412 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ehp7h5/langgraph_studio_is_amazing/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ehp7h5/langgraph_studio_is_amazing/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1ehp7h5</id><link href="https://www.reddit.com/r/LangChain/comments/1ehp7h5/langgraph_studio_is_amazing/" /><updated>2024-08-01T19:18:21+00:00</updated><published>2024-08-01T19:18:21+00:00</published><title>LangGraph Studio is amazing</title></entry><entry><author><name>/u/The_Wolfiee</name><uri>https://www.reddit.com/user/The_Wolfiee</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I am using an LLM that is a fine tuned version of Llama 3 on a cybersecurity dataset that recognises vulnerable code blocks and suggests steps to remediates the vulnerablities with fixed code. &lt;/p&gt; &lt;p&gt;I tried the same LLM and prompt with LangChain and Llama CPP but I get different results from each of them. &lt;/p&gt; &lt;p&gt;In Llama CPP, I get the suggested steps and fixed code block but with LangChain (using the Llama CPP abstraction), I get only the steps. &lt;/p&gt; &lt;p&gt;The prompt format is Llama-Chat 2 and the prompt specifically says &amp;quot;provide a code block that fixes the vulnerability&amp;quot;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/The_Wolfiee&quot;&gt; /u/The_Wolfiee &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ei1nol/different_results_with_same_prompt_and_llm_but/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ei1nol/different_results_with_same_prompt_and_llm_but/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1ei1nol</id><link href="https://www.reddit.com/r/LangChain/comments/1ei1nol/different_results_with_same_prompt_and_llm_but/" /><updated>2024-08-02T04:57:50+00:00</updated><published>2024-08-02T04:57:50+00:00</published><title>Different results with same prompt and LLM but different framework?</title></entry><entry><author><name>/u/RiverOtterBae</name><uri>https://www.reddit.com/user/RiverOtterBae</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I have my existent backend set up as a bunch of serverless functions at the moment (cloudflare workers). I wanted to set up a new `/chat` endpoint as just another serverless function which uses langchain on the server. But as I get deep into the code I&amp;#39;m not sure if it makes sense to do it this way...&lt;/p&gt; &lt;p&gt;Basically if I have Langchain running on this endpoint, since servelerless functions are stateless, that means each time the user sends a new message I need to fetch the chat history from the database, load it into context, process the request (generate the next response) and then tear it all down only to have to build it all up again with the next request. Since there is also no persistent connection.&lt;/p&gt; &lt;p&gt;This all seems a bit wasteful in my opinion. If I host langchain on the client I&amp;#39;m thinking I can avoid all this extra work since the langchain &amp;quot;instance&amp;quot; will stay put for the duration of the chat session. Once the long context is loaded in memory I only need to add new messages to it vs redoing the whole thing which can get very taxing for loooong conversations.&lt;/p&gt; &lt;p&gt;But I would prefer to handle it on the server side to hide the prompt magic &amp;quot;special sauce&amp;quot; if possible... &lt;/p&gt; &lt;p&gt;How are ya&amp;#39;ll serving your langchain apps in production?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/RiverOtterBae&quot;&gt; /u/RiverOtterBae &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ehwzcr/where_are_you_running_langchain_in_your/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ehwzcr/where_are_you_running_langchain_in_your/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1ehwzcr</id><link href="https://www.reddit.com/r/LangChain/comments/1ehwzcr/where_are_you_running_langchain_in_your/" /><updated>2024-08-02T00:54:32+00:00</updated><published>2024-08-02T00:54:32+00:00</published><title>Where are you running Langchain in your production apps? (serverless / on the client / somewhere else)???</title></entry><entry><author><name>/u/tys203831</name><uri>https://www.reddit.com/user/tys203831</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Let&amp;#39;s say I have 50 website API endpoints to get the data, and I am going to wrap them inside 50 agents... I&amp;#39;m not sure if this is the right way to do, as I&amp;#39;m afraid the agent will possibly messed up when the API endpoints keep growing...&lt;/p&gt; &lt;p&gt;Instead, I am thinking if I could RAG on tools to be used based on the user query, and then trigger those function tool based the output returned by RAG... Is this something feasible and perhaps more scalable than agents?&lt;/p&gt; &lt;p&gt;I&amp;#39;m not that sure the scalability of agents when there is a lot of data endpoints to access with. Hope for help, thanks.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/tys203831&quot;&gt; /u/tys203831 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ehyf74/using_rag_to_choose_tools_vs_agents_which_is/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ehyf74/using_rag_to_choose_tools_vs_agents_which_is/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1ehyf74</id><link href="https://www.reddit.com/r/LangChain/comments/1ehyf74/using_rag_to_choose_tools_vs_agents_which_is/" /><updated>2024-08-02T02:05:20+00:00</updated><published>2024-08-02T02:05:20+00:00</published><title>Using RAG to choose tools vs agents, which is better choices? If accuracy matters</title></entry><entry><author><name>/u/moonbunR</name><uri>https://www.reddit.com/user/moonbunR</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/moonbunR&quot;&gt; /u/moonbunR &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;/r/SmythOS/comments/1efnjke/how_does_an_llm_orchestrator_decide_which_agent/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ehjtby/how_does_an_llm_orchestrator_decide_which_agent/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1ehjtby</id><link href="https://www.reddit.com/r/LangChain/comments/1ehjtby/how_does_an_llm_orchestrator_decide_which_agent/" /><updated>2024-08-01T15:40:52+00:00</updated><published>2024-08-01T15:40:52+00:00</published><title>How does an LLM orchestrator decide which agent to use in a multi-agent system?</title></entry><entry><author><name>/u/banenvy</name><uri>https://www.reddit.com/user/banenvy</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I think the resources I am coming across are outdated as it says bind_tools not found.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/banenvy&quot;&gt; /u/banenvy &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ei08g7/does_anyone_have_resources_to_build_a_google/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ei08g7/does_anyone_have_resources_to_build_a_google/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1ei08g7</id><link href="https://www.reddit.com/r/LangChain/comments/1ei08g7/does_anyone_have_resources_to_build_a_google/" /><updated>2024-08-02T03:39:50+00:00</updated><published>2024-08-02T03:39:50+00:00</published><title>Does anyone have resources to build a Google Gemini agent that can use tools?</title></entry><entry><author><name>/u/Shiro_94</name><uri>https://www.reddit.com/user/Shiro_94</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hello everyone,&lt;/p&gt; &lt;p&gt;I am currently looking at some opensource RAG such as Langchain and Llama index. Quite like Llama index but it does not seem suitable for production. I did not find the capability of doing batch inference especially for retrieving the closest chunks for a batch of query. (so lack of scalability here)&lt;br/&gt; Langchain seems to have this feature (correct me if I am wrong but they are extracting the embeddings of queries by batch and not using multiple workers =&amp;gt; one embedding model call instead of N call if we have N queries)&lt;/p&gt; &lt;p&gt;I was wondering if there are others open source RAG worth for production other than langchain allowing:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Vector Store&lt;/li&gt; &lt;li&gt;Chunking &amp;amp; Document upload of different type (pdf, docx, raw text etc)&lt;/li&gt; &lt;li&gt;scalability (such as batch for queries =&amp;gt; embedding model call made by batch)&lt;/li&gt; &lt;li&gt;flexible about choosing the embedding model (HF, OpenAI etc)&lt;/li&gt; &lt;li&gt;good feature about the retriever such as filtering from metadata&lt;/li&gt; &lt;li&gt;good postprocess function (or possibility to add custom function) such as chunk merging etc&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Thanks for the help!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Shiro_94&quot;&gt; /u/Shiro_94 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ehtdhs/open_source_rag_best_for_production/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ehtdhs/open_source_rag_best_for_production/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1ehtdhs</id><link href="https://www.reddit.com/r/LangChain/comments/1ehtdhs/open_source_rag_best_for_production/" /><updated>2024-08-01T22:10:49+00:00</updated><published>2024-08-01T22:10:49+00:00</published><title>Open Source RAG - best for production?</title></entry><entry><author><name>/u/Complete-Pie5760</name><uri>https://www.reddit.com/user/Complete-Pie5760</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;ReAct agents are powerful, but I&amp;#39;ve found a way to make them even better for real-world use. Here&amp;#39;s how:&lt;/p&gt; &lt;p&gt;Multi-LLM Integration: I&amp;#39;ve set up my ReAct agents to work with over 200 different LLMs. This flexibility lets you choose the best model for each task. &lt;/p&gt; &lt;p&gt;Performance Tracking: By monitoring costs, token usage, and latency, I&amp;#39;ve optimized my agents&amp;#39; efficiency. This is crucial for large-scale applications.&lt;/p&gt; &lt;p&gt;Improved Reliability: I&amp;#39;ve implemented fallbacks between LLMs, load-balancing, and automatic retries. This makes the agents much more stable in production environments.&lt;/p&gt; &lt;p&gt;Smart Caching: By storing frequently accessed data, I&amp;#39;ve significantly reduced API calls, making the agents faster and more cost-effective.&lt;/p&gt; &lt;p&gt;Detailed Logging: Comprehensive action tracking has been a game-changer for debugging complex ReAct runs.&lt;/p&gt; &lt;p&gt;Easy Prompt Management- I can now update prompts without touching the code, which speeds up experimentation and optimization.&lt;/p&gt; &lt;p&gt;I&amp;#39;ve based my implementation on Simon Willison&amp;#39;s work. You can find the starting point here: &lt;a href=&quot;https://git.new/ReAct-framework&quot;&gt;https://git.new/ReAct-framework&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Has anyone else been working on improving ReAct agents? What challenges have you faced in real-world applications?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Complete-Pie5760&quot;&gt; /u/Complete-Pie5760 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ehcpct/how_to_build_production_grade_langchain_agents/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ehcpct/how_to_build_production_grade_langchain_agents/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1ehcpct</id><link href="https://www.reddit.com/r/LangChain/comments/1ehcpct/how_to_build_production_grade_langchain_agents/" /><updated>2024-08-01T09:50:53+00:00</updated><published>2024-08-01T09:50:53+00:00</published><title>How to build Production grade Langchain Agents using ReAct</title></entry><entry><author><name>/u/Apart-Damage143</name><uri>https://www.reddit.com/user/Apart-Damage143</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Apart-Damage143&quot;&gt; /u/Apart-Damage143 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ehslq6/does_anyone_know_any_tools_or_libraries_that_can/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ehslq6/does_anyone_know_any_tools_or_libraries_that_can/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1ehslq6</id><link href="https://www.reddit.com/r/LangChain/comments/1ehslq6/does_anyone_know_any_tools_or_libraries_that_can/" /><updated>2024-08-01T21:37:49+00:00</updated><published>2024-08-01T21:37:49+00:00</published><title>Does anyone know any tools or libraries that can generate diagrams based on input?</title></entry><entry><author><name>/u/BenkattoRamunan</name><uri>https://www.reddit.com/user/BenkattoRamunan</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;So the main idea is that given logs I need RAG to get answers to analyse them. The LLM model works well here upto a level of base knowledge it has been pretrained on. Now I want the answers to be more accurate. So I have a couple of template info and domain knowledge like: &lt;/p&gt; &lt;pre&gt;&lt;code&gt;Brief scenarios for Registration Cases 1. - Registration always begins with REGISTER msg. - REGISTER message is one of the most important message in protocol and it contains a lot of important information in it. Unterstanding the meaning of each parameters in registration would help you greatly with various troubleshooting situation. Followings are some of the examples of REGISTER message you may see in the field. Keep reading this page as often as possible until you become very familiar with all the details of the contents. Blah Blah .... &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Now I can give parts of these in the prompts for RAG. (Note: here the vector store contains embeddings of my logs). Now the reference gets larger and larger and more sophisticated. I am looking for alternate ways to make sure RAG references this domain info before it answers questions on the log vector store. So I can keep expanding this document of domain knowledge to refer to, and RAG analyses logs based on this domain knowledge. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/BenkattoRamunan&quot;&gt; /u/BenkattoRamunan &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ehwxbh/rag_with_prior_knowledge_or_reference_to_follow/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ehwxbh/rag_with_prior_knowledge_or_reference_to_follow/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1ehwxbh</id><link href="https://www.reddit.com/r/LangChain/comments/1ehwxbh/rag_with_prior_knowledge_or_reference_to_follow/" /><updated>2024-08-02T00:51:46+00:00</updated><published>2024-08-02T00:51:46+00:00</published><title>RAG with prior knowledge or reference to follow</title></entry><entry><author><name>/u/g_pal</name><uri>https://www.reddit.com/user/g_pal</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I recently had our AI interviewer speak with 22 developers who are building with LangGraph. The interviews covered various topics, including how they&amp;#39;re using LangGraph, what they like about it, and areas for improvement. I wanted to share the key findings because I thought you might find it interesting.&lt;/p&gt; &lt;h1&gt;Use Cases and Attractions&lt;/h1&gt; &lt;p&gt;LangGraph is attracting developers from a wide range of industries due to its versatility in managing complex AI workflows. Here are some interesting use cases:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Content Generation:&lt;/strong&gt; Teams are using LangGraph to create systems where multiple AI agents collaborate to draft, fact-check, and refine research papers in real-time.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Customer Service:&lt;/strong&gt; Developers are building dynamic response systems that analyze sentiment, retrieve relevant information, and generate personalized replies with built-in clarification mechanisms.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Financial Modeling:&lt;/strong&gt; Some are building valuation models in real estate that adapt in real-time based on market fluctuations and simulated scenarios.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Academic Research&lt;/strong&gt;: Institutions are developing adaptive research assistants capable of gathering data, synthesizing insights, and proposing new hypotheses within a single integrated system.&lt;/li&gt; &lt;/ol&gt; &lt;h1&gt;What Attracts Developers to LangGraph?&lt;/h1&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Multi-Agent System Orchestration&lt;/strong&gt;: LangGraph excels at managing multiple AI agents, allowing for a divide-and-conquer approach to complex problems.&amp;quot;We are working on a project that requires multiple AI agents to communicate and talk to one another. LangGraph helps with thinking through the problem using a divide-and-conquer approach with graphs, nodes, and edges.&amp;quot; - Founder, Property Technology Startup&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Workflow Visualization and Debugging&lt;/strong&gt;: The platform&amp;#39;s visualization capabilities are highly valued for development and debugging.&amp;quot;LangGraph can visualize all the requests and all the payloads instantly, and I can debug by taking LangGraph. It&amp;#39;s very convenient for the development experience.&amp;quot; - Cloud Solutions Architect, Microsoft&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Complex Problem-Solving&lt;/strong&gt;: Developers appreciate LangGraph&amp;#39;s ability to tackle intricate challenges that traditional programming struggles with.&amp;quot;Solving complex problems that are not, um, possible with traditional programming.&amp;quot; - AI Researcher, Nokia&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Abstraction of Flow Logic&lt;/strong&gt;: LangGraph simplifies the implementation of complex workflows by abstracting flow logic.&amp;quot;[LangGraph helped] abstract the flow logic and avoid having to write all of the boilerplate code to get started with the project.&amp;quot; - AI Researcher, Nokia&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Flexible Agentic Workflows&lt;/strong&gt;: The tool&amp;#39;s adaptability for various AI agent scenarios is a key attraction.&amp;quot;Being able to create an agentic workflow that is easy to visualize abstractly with graphs, nodes, and edges.&amp;quot; - Founder, Property Technology Startup&lt;/li&gt; &lt;/ol&gt; &lt;h1&gt;LangGraph vs Alternatives&lt;/h1&gt; &lt;p&gt;The most commonly considered alternatives were CrewAI and Microsoft&amp;#39;s Autogen. However, developers noted several areas where LangGraph stands out:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Handling Complex Workflows:&lt;/strong&gt; Unlike some competitors limited to simple, linear processes, LangGraph can handle complex graph flows, including cycles.&amp;quot;CrewAI can only handle DAGs and cannot handle cycles, whereas LangGraph can handle complex graph flows, including cycles.&amp;quot; - Developer&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Developer Control:&lt;/strong&gt; LangGraph offers a level of control that many find unmatched, especially for custom use cases.&amp;quot;We did tinker a bit with CrewAI and Meta GPT. But those could not come even near as powerful as LangGraph. And we did combine with LangChain because we have very custom use cases, and we need to have a lot of control. And the competitor frameworks just don&amp;#39;t offer that amount of, control over the code.&amp;quot; - Founder, GenAI Startup&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Mature Ecosystem:&lt;/strong&gt; LangGraph&amp;#39;s longer market presence has resulted in more resources, tools, and infrastructure.&amp;quot;LangGraph has the advantage of being in the market longer, offering more resources, tools, and infrastructure. The ability to use LangSmith in conjunction with LangGraph for debugging and performance analysis is a significant differentiator.&amp;quot; - Developer&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Market Leadership:&lt;/strong&gt; Despite a volatile market, LangGraph is currently seen as a leader in functionality and tooling for developing workflows.&amp;quot;Currently, LangGraph is one of the leaders in terms of functionality and tooling for developing workflows. The market is volatile, and I hope LangGraph continues to innovate and create more tools to facilitate developers&amp;#39; work.&amp;quot; - Developer&lt;/li&gt; &lt;/ol&gt; &lt;h1&gt;Areas for Improvement&lt;/h1&gt; &lt;p&gt;While LangGraph has garnered praise, developers also identified several areas for improvement:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Simplify Syntax and Reduce Complexity:&lt;/strong&gt; Some developers noted that the graph-based approach, while powerful, can be complex to maintain.&amp;quot;Some syntax can be made a lot simpler.&amp;quot; - Senior Engineering Director, BlackRock&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Enhance Documentation and Community Resources:&lt;/strong&gt; There&amp;#39;s a need for more in-depth, complex examples and community-driven documentation.&amp;quot;The lack of how-to articles and community-driven documentation... There&amp;#39;s a lot of entry-level stuff, but nothing really in-depth or complex.&amp;quot; - Research Assistant, BYU&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Improve Debugging Capabilities:&lt;/strong&gt; Developers expressed a need for more detailed debugging information, especially for tracking state within the graph.&amp;quot;There is a need for more debugging information. Sometimes, the bug information starts from the instantiation of the workflow, and it&amp;#39;s hard to track the state within the graph.&amp;quot; - Senior Software Engineer, Canadian Government Agency&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Better Human-in-the-Loop Integration:&lt;/strong&gt; Some users aren&amp;#39;t satisfied with the current implementation of human-in-the-loop concepts.&amp;quot;More options around the human-in-the-loop concept. I&amp;#39;m not a very big fan of their current implementation of that.&amp;quot; - AI Researcher, Nokia&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Enhanced Subgraph Integration:&lt;/strong&gt; Multiple developers mentioned issues with integrating and combining subgraphs.&amp;quot;The possibility to integrate subgraphs isn&amp;#39;t compatible with [graph drawing].&amp;quot; - Engineer, IT Consulting Company &amp;quot;I wish you could combine smaller graphs into bigger graphs more easily.&amp;quot; - Research Assistant, BYU&lt;/li&gt; &lt;li&gt;&lt;strong&gt;More Complex Examples:&lt;/strong&gt; There&amp;#39;s a desire for more complex examples that developers can use as starting points.&amp;quot;Creating more examples online that people can use as inspiration would be fantastic.&amp;quot; - Senior Engineering Director, BlackRock&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;____&lt;br/&gt; You can check out the interview transcripts here: &lt;a href=&quot;http://kgrid.ai/company/langgraph&quot;&gt;kgrid.ai/company/langgraph&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Curious to know whether this aligns with your experience? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/g_pal&quot;&gt; /u/g_pal &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1eh0ly3/spoke_to_22_langgraph_devs_and_heres_what_we_found/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1eh0ly3/spoke_to_22_langgraph_devs_and_heres_what_we_found/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1eh0ly3</id><link href="https://www.reddit.com/r/LangChain/comments/1eh0ly3/spoke_to_22_langgraph_devs_and_heres_what_we_found/" /><updated>2024-07-31T22:36:11+00:00</updated><published>2024-07-31T22:36:11+00:00</published><title>Spoke to 22 LangGraph devs and here's what we found</title></entry><entry><author><name>/u/sharrajesh</name><uri>https://www.reddit.com/user/sharrajesh</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;m working on a production FastAPI application that uses LangChain with a cascade of tools for various AI tasks. I&amp;#39;m looking to add asynchronous streaming support to my API and would appreciate feedback on my proposed design:&lt;/p&gt; &lt;h2&gt;Current Setup:&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;FastAPI endpoints that use LangChain agents with multiple tools&lt;/li&gt; &lt;li&gt;Synchronous API calls that return complete responses, including main content and metadata (e.g., sources used)&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;Proposed Design:&lt;/h2&gt; &lt;ol&gt; &lt;li&gt;Keep existing synchronous API endpoints as-is for backward compatibility&lt;/li&gt; &lt;li&gt;Add new streaming endpoints for real-time token generation of the main response body&lt;/li&gt; &lt;li&gt;Use Redis as a message broker to collect and stream responses&lt;/li&gt; &lt;li&gt;Synchronous API continues to return full response with all fields (main content, sources, etc.)&lt;/li&gt; &lt;/ol&gt; &lt;h2&gt;Implementation Idea:&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;Modify existing endpoints to publish responses to Redis&lt;/li&gt; &lt;li&gt;Create new streaming endpoints that subscribe to Redis channels&lt;/li&gt; &lt;li&gt;Update LangChain agents to publish chunks and full responses to Redis&lt;/li&gt; &lt;li&gt;Client can use either sync API for full response or streaming API for real-time updates&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;Questions:&lt;/h2&gt; &lt;ol&gt; &lt;li&gt;Is this a sensible approach for adding streaming to an existing production API?&lt;/li&gt; &lt;li&gt;Are there better alternatives to using Redis for this purpose?&lt;/li&gt; &lt;li&gt;How can I ensure efficient resource usage and low latency with this design?&lt;/li&gt; &lt;li&gt;Any potential pitfalls or considerations I should be aware of?&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;I&amp;#39;d greatly appreciate any insights, alternative approaches, or best practices for implementing streaming in a FastAPI LangChain application. Thanks in advance for your help!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/sharrajesh&quot;&gt; /u/sharrajesh &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ehggqs/adding_streaming_support_to_fastapi_langchain/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ehggqs/adding_streaming_support_to_fastapi_langchain/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1ehggqs</id><link href="https://www.reddit.com/r/LangChain/comments/1ehggqs/adding_streaming_support_to_fastapi_langchain/" /><updated>2024-08-01T13:18:27+00:00</updated><published>2024-08-01T13:18:27+00:00</published><title>Adding Streaming Support to FastAPI LangChain Application with Agents</title></entry><entry><author><name>/u/philwinder</name><uri>https://www.reddit.com/user/philwinder</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ehkocs/a_comparison_of_open_source_llm_frameworks_for/&quot;&gt; &lt;img src=&quot;https://external-preview.redd.it/zJvoLGvOZMDIVae9TJd22PSmRb2UpCVRfc5JMBOgqaM.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d4f941cd37872f3dd0d705cf4caeb73d0cbc347a&quot; alt=&quot;A Comparison of Open Source LLM Frameworks for Pipelining&quot; title=&quot;A Comparison of Open Source LLM Frameworks for Pipelining&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/philwinder&quot;&gt; /u/philwinder &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://winder.ai/comparison-open-source-llm-frameworks-pipelining/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ehkocs/a_comparison_of_open_source_llm_frameworks_for/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1ehkocs</id><media:thumbnail url="https://external-preview.redd.it/zJvoLGvOZMDIVae9TJd22PSmRb2UpCVRfc5JMBOgqaM.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=d4f941cd37872f3dd0d705cf4caeb73d0cbc347a" /><link href="https://www.reddit.com/r/LangChain/comments/1ehkocs/a_comparison_of_open_source_llm_frameworks_for/" /><updated>2024-08-01T16:15:12+00:00</updated><published>2024-08-01T16:15:12+00:00</published><title>A Comparison of Open Source LLM Frameworks for Pipelining</title></entry><entry><author><name>/u/Longjumping-Try1191</name><uri>https://www.reddit.com/user/Longjumping-Try1191</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Could someone show me how I can use &lt;a href=&quot;https://huggingface.co/DAMO-NLP-SG/Video-LLaMA-2-7B-Finetuned&quot;&gt;https://huggingface.co/DAMO-NLP-SG/Video-LLaMA-2-7B-Finetuned&lt;/a&gt; with Python, please? I am still a beginner. Thank you to those who take the time to answer my question.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Longjumping-Try1191&quot;&gt; /u/Longjumping-Try1191 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ehsukn/how_can_i_run_videollama/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ehsukn/how_can_i_run_videollama/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1ehsukn</id><link href="https://www.reddit.com/r/LangChain/comments/1ehsukn/how_can_i_run_videollama/" /><updated>2024-08-01T21:48:14+00:00</updated><published>2024-08-01T21:48:14+00:00</published><title>How can I run Video-LLaMa ?</title></entry><entry><author><name>/u/man_rech</name><uri>https://www.reddit.com/user/man_rech</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;I’m developing a natural language to SQL to natural language agent using LangGraph. While my agent can generate good responses with nice visualizations for some questions, it’s often inconsistent. I get different answers for the same question even though I have top_p set to 1 and temperature set to 0.&lt;/p&gt; &lt;p&gt;Here are some details:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The process involves several steps before creating the query and composing the response like watching the db schema, executing tools and reflecting on the temporary response. &lt;/li&gt; &lt;li&gt;The median token utilization per run is around 40k tokens, except for very basic questions. Is this context too large?&lt;/li&gt; &lt;li&gt;Occasionally, the agent doesn’t follow my instructions. Sometimes changing the wording slightly improves performance, but I have to do so many attempts that I get crazy. Is it normal?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The database structure:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;2 tables with around 300 ish columns each&lt;/li&gt; &lt;li&gt;3 smaller tables with around 10 ish columns each&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Am I on the right track, or is there a fundamental issue, possibly related to the large context size? Any insights or suggestions would be greatly appreciated!&lt;/p&gt; &lt;p&gt;Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/man_rech&quot;&gt; /u/man_rech &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ehhzzq/need_help_with_nl_to_sql_agent_inconsistent/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ehhzzq/need_help_with_nl_to_sql_agent_inconsistent/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1ehhzzq</id><link href="https://www.reddit.com/r/LangChain/comments/1ehhzzq/need_help_with_nl_to_sql_agent_inconsistent/" /><updated>2024-08-01T14:25:12+00:00</updated><published>2024-08-01T14:25:12+00:00</published><title>Need Help with NL to SQL Agent: Inconsistent Responses and Large Context Issues</title></entry><entry><author><name>/u/Adventurous_Joke3397</name><uri>https://www.reddit.com/user/Adventurous_Joke3397</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;When you build RAG, where do you store all the vectors? &lt;/p&gt; &lt;p&gt;I am using Postgres + pg_vector, and just storing the vectors in the same DB as the rest of my application data. It is convenient and works well with my toolchain.&lt;/p&gt; &lt;p&gt;But I also heard (without explanation) that it is better to use a separate database for vectors. &lt;/p&gt; &lt;p&gt;Is this true? Any thoughts on why? Does another Postgres database on the same instance “count”? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Adventurous_Joke3397&quot;&gt; /u/Adventurous_Joke3397 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ehpb1d/where_to_store_vectors/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ehpb1d/where_to_store_vectors/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1ehpb1d</id><link href="https://www.reddit.com/r/LangChain/comments/1ehpb1d/where_to_store_vectors/" /><updated>2024-08-01T19:22:19+00:00</updated><published>2024-08-01T19:22:19+00:00</published><title>Where to store vectors?</title></entry><entry><author><name>/u/Upstairs-Belt8255</name><uri>https://www.reddit.com/user/Upstairs-Belt8255</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Is anyone working on Conversational technology that allows a third person to &amp;quot;take over&amp;quot; the conversation, given the answer relevancy confidence is low, with a RAG pipeline?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Upstairs-Belt8255&quot;&gt; /u/Upstairs-Belt8255 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ehmxa9/allowing_a_real_person_to_take_over_the/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ehmxa9/allowing_a_real_person_to_take_over_the/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1ehmxa9</id><link href="https://www.reddit.com/r/LangChain/comments/1ehmxa9/allowing_a_real_person_to_take_over_the/" /><updated>2024-08-01T17:46:05+00:00</updated><published>2024-08-01T17:46:05+00:00</published><title>Allowing a real person to 'take over' the conversation?</title></entry><entry><author><name>/u/Best_Sail5</name><uri>https://www.reddit.com/user/Best_Sail5</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hello guys,&lt;/p&gt; &lt;p&gt;I have a question concerning how to prompt a model.&lt;br/&gt; I&amp;#39;m currently using LLaMa 3.1 to interact with a tool. The model is given an objective and generate multiple rounds of tool input to achieve it.&lt;br/&gt; Currently i&amp;#39;m simply using the following format:&lt;/p&gt; &lt;p&gt;ChatPromptTemplate([(&amp;#39;system&amp;#39;,system_prompt),(&amp;#39;user&amp;#39;,user_prompt)])&lt;br/&gt; where user_prompt contains the previous rounds of him generating commands and tool output like this:&lt;br/&gt; user_prompt=&amp;quot;&amp;quot;&amp;quot;&lt;br/&gt; {prompt}&lt;br/&gt; previous commands executed:{previous_rounds_of_tool_call}&lt;br/&gt; &amp;quot;&amp;quot;&amp;quot;&lt;br/&gt; This is inspired of ReAct prompt formatting.&lt;/p&gt; &lt;p&gt;But I&amp;#39;m thinking about changing that to prompt him in the following format:&lt;br/&gt; ChatPromptTemplate([(&amp;#39;system&amp;#39;,system_prompt),(&amp;#39;user&amp;#39;,user_prompt)&lt;br/&gt; ,(&amp;#39;tool&amp;#39;,tool_message),(&amp;#39;user&amp;#39;,user_prompt).....])&lt;/p&gt; &lt;p&gt;adding each turns as separate message.&lt;br/&gt; I would like to know if someone already used that? Does it change something ? How to do the training with multi steps setup like that?simply train each step separately?&lt;/p&gt; &lt;p&gt;Thanks for your advices!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Best_Sail5&quot;&gt; /u/Best_Sail5 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1eheoc1/multiple_turns_prompting_strategy/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1eheoc1/multiple_turns_prompting_strategy/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1eheoc1</id><link href="https://www.reddit.com/r/LangChain/comments/1eheoc1/multiple_turns_prompting_strategy/" /><updated>2024-08-01T11:49:44+00:00</updated><published>2024-08-01T11:49:44+00:00</published><title>Multiple turns prompting strategy</title></entry><entry><author><name>/u/maniac_runner</name><uri>https://www.reddit.com/user/maniac_runner</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1eh73u7/github_pytorchtorchchat_run_pytorch_llms_locally/&quot;&gt; &lt;img src=&quot;https://external-preview.redd.it/KnyLvUQLSlhqwDrJ5al_7_sHY4CasKEA7RvCEIrbcO0.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=70859b752fa3814100646807363b0b2e1ac8d99c&quot; alt=&quot;GitHub - pytorch/torchchat: Run PyTorch LLMs locally on servers, desktop and mobile&quot; title=&quot;GitHub - pytorch/torchchat: Run PyTorch LLMs locally on servers, desktop and mobile&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/maniac_runner&quot;&gt; /u/maniac_runner &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://github.com/pytorch/torchchat&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1eh73u7/github_pytorchtorchchat_run_pytorch_llms_locally/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1eh73u7</id><media:thumbnail url="https://external-preview.redd.it/KnyLvUQLSlhqwDrJ5al_7_sHY4CasKEA7RvCEIrbcO0.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=70859b752fa3814100646807363b0b2e1ac8d99c" /><link href="https://www.reddit.com/r/LangChain/comments/1eh73u7/github_pytorchtorchchat_run_pytorch_llms_locally/" /><updated>2024-08-01T03:48:53+00:00</updated><published>2024-08-01T03:48:53+00:00</published><title>GitHub - pytorch/torchchat: Run PyTorch LLMs locally on servers, desktop and mobile</title></entry><entry><author><name>/u/mehul_gupta1997</name><uri>https://www.reddit.com/user/mehul_gupta1997</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/mehul_gupta1997&quot;&gt; /u/mehul_gupta1997 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;/r/ArtificialInteligence/comments/1ehgr4f/graphrag_vs_rag_which_one_is_better/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ehgtik/graphrag_vs_rag_which_one_is_better/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1ehgtik</id><link href="https://www.reddit.com/r/LangChain/comments/1ehgtik/graphrag_vs_rag_which_one_is_better/" /><updated>2024-08-01T13:34:48+00:00</updated><published>2024-08-01T13:34:48+00:00</published><title>GraphRAG vs RAG: Which one is better?</title></entry><entry><author><name>/u/Longjumping-Try1191</name><uri>https://www.reddit.com/user/Longjumping-Try1191</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hello, does anyone know of a model capable of understanding videos and answering my questions via an LLM? Something like Video-LLaMA but that allows commercial use? Thank you very much to those who take the time to respond.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Longjumping-Try1191&quot;&gt; /u/Longjumping-Try1191 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ehgnig/looking_for_a_video_understanding_model_with/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ehgnig/looking_for_a_video_understanding_model_with/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1ehgnig</id><link href="https://www.reddit.com/r/LangChain/comments/1ehgnig/looking_for_a_video_understanding_model_with/" /><updated>2024-08-01T13:27:03+00:00</updated><published>2024-08-01T13:27:03+00:00</published><title>Looking for a Video Understanding Model with Commercial Use License</title></entry><entry><author><name>/u/Jen1888Mik</name><uri>https://www.reddit.com/user/Jen1888Mik</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hello. I use convex and langchain to create my rag application. In my case i need add to context specific data from my db - logic my application is serching some embeddings by id from my vectoredb then i use its embeddings for generate answer - but if i just add to context from RunnableSequence my embeddings i need use some format or embeddings are enough for this? I dont find this example in langchain documentation&lt;/p&gt; &lt;p&gt;&lt;code&gt;const vectorStore = new ConvexVectorStore(new OpenAIEmbeddings(), { ctx });&lt;/code&gt;&lt;br/&gt; &lt;code&gt;const llm = new ChatOpenAI({&lt;/code&gt;&lt;br/&gt; &lt;code&gt;apiKey: process.env.OPENAI_API_KEY,&lt;/code&gt;&lt;br/&gt; &lt;code&gt;model: &amp;quot;gpt-3.5-turbo&amp;quot;,&lt;/code&gt;&lt;br/&gt; &lt;code&gt;temperature: 0,&lt;/code&gt;&lt;br/&gt; &lt;code&gt;});&lt;/code&gt;&lt;br/&gt; &lt;code&gt;const parcer = new StringOutputParser();&lt;/code&gt;&lt;br/&gt; &lt;code&gt;const retriever = vectorStore.asRetriever()&lt;/code&gt;&lt;br/&gt; &lt;code&gt;const prompt = await pull&amp;lt;ChatPromptTemplate&amp;gt;(&amp;quot;rlm/rag-prompt&amp;quot;);&lt;/code&gt;&lt;br/&gt; &lt;code&gt;const ragChain = RunnableSequence.from([&lt;/code&gt;&lt;br/&gt; &lt;code&gt;{&lt;/code&gt;&lt;br/&gt; &lt;code&gt;context: retriever.pipe(formatDocumentsAsString) ,&lt;/code&gt;&lt;br/&gt; &lt;code&gt;question: new RunnablePassthrough(),&lt;/code&gt;&lt;br/&gt; &lt;code&gt;},&lt;/code&gt;&lt;br/&gt; &lt;code&gt;prompt,&lt;/code&gt;&lt;br/&gt; &lt;code&gt;llm,&lt;/code&gt;&lt;br/&gt; &lt;code&gt;parcer,&lt;/code&gt;&lt;br/&gt; &lt;code&gt;]);&lt;/code&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Jen1888Mik&quot;&gt; /u/Jen1888Mik &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ehggmb/how_to_add_specific_source_from_db_to/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ehggmb/how_to_add_specific_source_from_db_to/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1ehggmb</id><link href="https://www.reddit.com/r/LangChain/comments/1ehggmb/how_to_add_specific_source_from_db_to/" /><updated>2024-08-01T13:18:17+00:00</updated><published>2024-08-01T13:18:17+00:00</published><title>How to add specific source from db to RunnableSequence using Langchain</title></entry><entry><author><name>/u/Traditional_Art_6943</name><uri>https://www.reddit.com/user/Traditional_Art_6943</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi guys I have created a PDF Chat/ Web Search RAG application deployed on Hugging Face Spaces &lt;a href=&quot;https://shreyas094-searchgpt.hf.space&quot;&gt;https://shreyas094-searchgpt.hf.space&lt;/a&gt;. Providing the model documentation below please feel free to contribute.&lt;/p&gt; &lt;h1&gt;AI-powered Web Search and PDF Chat Assistant&lt;/h1&gt; &lt;p&gt;This project combines the power of large language models with web search capabilities and PDF document analysis to create a versatile chat assistant. Users can interact with their uploaded PDF documents or leverage web search to get informative responses to their queries.&lt;/p&gt; &lt;h2&gt;Features&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;PDF Document Chat&lt;/strong&gt;: Upload and interact with multiple PDF documents.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Web Search Integration&lt;/strong&gt;: Option to use web search for answering queries.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Multiple AI Models&lt;/strong&gt;: Choose from a selection of powerful language models.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Customizable Responses&lt;/strong&gt;: Adjust temperature and API call settings for fine-tuned outputs.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;User-friendly Interface&lt;/strong&gt;: Built with Gradio for an intuitive chat experience.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Document Selection&lt;/strong&gt;: Choose which uploaded documents to include in your queries.&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;How It Works&lt;/h2&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;Document Processing&lt;/strong&gt;: &lt;/p&gt; &lt;ul&gt; &lt;li&gt;Upload PDF documents using either PyPDF or LlamaParse.&lt;/li&gt; &lt;li&gt;Documents are processed and stored in a FAISS vector database for efficient retrieval.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;Embedding&lt;/strong&gt;: &lt;/p&gt; &lt;ul&gt; &lt;li&gt;Utilizes HuggingFace embeddings (default: &amp;#39;sentence-transformers/all-mpnet-base-v2&amp;#39;) for document indexing and query matching.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;Query Processing&lt;/strong&gt;:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;For PDF queries, relevant document sections are retrieved from the FAISS database.&lt;/li&gt; &lt;li&gt;For web searches, results are fetched using the DuckDuckGo search API.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;Response Generation&lt;/strong&gt;:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Queries are processed using the selected AI model (options include Mistral, Mixtral, and others).&lt;/li&gt; &lt;li&gt;Responses are generated based on the retrieved context (from PDFs or web search).&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;User Interaction&lt;/strong&gt;:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Users can chat with the AI, asking questions about uploaded documents or general queries.&lt;/li&gt; &lt;li&gt;The interface allows for adjusting model parameters and switching between PDF and web search modes.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ol&gt; &lt;h2&gt;Setup and Usage&lt;/h2&gt; &lt;ol&gt; &lt;li&gt;Install the required dependencies (list of dependencies to be added).&lt;/li&gt; &lt;li&gt;Set up the necessary API keys and tokens in your environment variables.&lt;/li&gt; &lt;li&gt;Run the main script to launch the Gradio interface.&lt;/li&gt; &lt;li&gt;Upload PDF documents using the file input at the top of the interface.&lt;/li&gt; &lt;li&gt;Select documents to query using the checkboxes.&lt;/li&gt; &lt;li&gt;Toggle between PDF chat and web search modes as needed.&lt;/li&gt; &lt;li&gt;Adjust temperature and number of API calls to fine-tune responses.&lt;/li&gt; &lt;li&gt;Start chatting and asking questions!&lt;/li&gt; &lt;/ol&gt; &lt;h2&gt;Models&lt;/h2&gt; &lt;p&gt;The project supports multiple AI models, including: - mistralai/Mistral-7B-Instruct-v0.3 - mistralai/Mixtral-8x7B-Instruct-v0.1 - meta/llama-3.1-8b-instruct - mistralai/Mistral-Nemo-Instruct-2407&lt;/p&gt; &lt;h2&gt;Future Improvements&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;Integration of more embedding models for improved performance.&lt;/li&gt; &lt;li&gt;Enhanced PDF parsing capabilities.&lt;/li&gt; &lt;li&gt;Support for additional file formats beyond PDF.&lt;/li&gt; &lt;li&gt;Improved caching for faster response times.&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;Contribution&lt;/h2&gt; &lt;p&gt;Contributions to this project are welcome!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Traditional_Art_6943&quot;&gt; /u/Traditional_Art_6943 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1egyn3g/rag_pdf_chat_web_search/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1egyn3g/rag_pdf_chat_web_search/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1egyn3g</id><link href="https://www.reddit.com/r/LangChain/comments/1egyn3g/rag_pdf_chat_web_search/" /><updated>2024-07-31T21:14:26+00:00</updated><published>2024-07-31T21:14:26+00:00</published><title>RAG PDF Chat + Web Search</title></entry><entry><author><name>/u/o3omoomin</name><uri>https://www.reddit.com/user/o3omoomin</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I participated in the nvidia gen ai contest on June 18th in Korea time, or maybe June 17th in Pacific time.&lt;/p&gt; &lt;p&gt;And I&amp;#39;ve been waiting for today. But the result email and certificate haven&amp;#39;t arrived to me. What&amp;#39;s the reason?&lt;/p&gt; &lt;p&gt;Or is there anyone who is experiencing the same situation as me?&lt;/p&gt; &lt;p&gt;I also talked about this issue in the nvidia developer discord channel, but the people involved don&amp;#39;t input any chat.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/o3omoomin&quot;&gt; /u/o3omoomin &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ehajmd/i_did_not_receive_the_results_email_from_nvidia/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ehajmd/i_did_not_receive_the_results_email_from_nvidia/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1ehajmd</id><link href="https://www.reddit.com/r/LangChain/comments/1ehajmd/i_did_not_receive_the_results_email_from_nvidia/" /><updated>2024-08-01T07:21:44+00:00</updated><published>2024-08-01T07:21:44+00:00</published><title>I did not receive the results email from Nvidia generative AI contset. Also the certificate.</title></entry><entry><author><name>/u/No_Storm5504</name><uri>https://www.reddit.com/user/No_Storm5504</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I have been deloyed an langgraph app which running at LangServe. One question is that I have a feature which required human-in-the-loop that&amp;#39;s send a None Type to the langgraph, that&amp;#39;s the way to continue langgraph execution. I know how to do it in Python SDK but still no clues in LangServe client way，which I mimic request with Python &lt;code&gt;requests&lt;/code&gt; through payloads. &lt;/p&gt; &lt;p&gt;Any good ideas?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/No_Storm5504&quot;&gt; /u/No_Storm5504 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1eh9wwv/how_to_pass_none_type_as_an_input_to_langgraph/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1eh9wwv/how_to_pass_none_type_as_an_input_to_langgraph/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1eh9wwv</id><link href="https://www.reddit.com/r/LangChain/comments/1eh9wwv/how_to_pass_none_type_as_an_input_to_langgraph/" /><updated>2024-08-01T06:39:08+00:00</updated><published>2024-08-01T06:39:08+00:00</published><title>How to pass None Type as an input to LangGraph which deployed in LangServe</title></entry></feed>