<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/"><category term="LangChain" label="r/LangChain"/><updated>2024-03-21T21:42:42+00:00</updated><icon>https://www.redditstatic.com/icon.png/</icon><id>/r/LangChain.rss</id><link rel="self" href="https://www.reddit.com/r/LangChain.rss" type="application/atom+xml" /><link rel="alternate" href="https://www.reddit.com/r/LangChain" type="text/html" /><subtitle>LangChain is an open-source framework and developer toolkit that helps developers get LLM applications from prototype to production. It is available for Python and Javascript at https://www.langchain.com/.</subtitle><title>LangChain</title><entry><author><name>/u/zchaarm</name><uri>https://www.reddit.com/user/zchaarm</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;A place for members of &lt;a href=&quot;/r/LangChain&quot;&gt;r/LangChain&lt;/a&gt; to chat with each other&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/zchaarm&quot;&gt; /u/zchaarm &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_10ljho9</id><link href="https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/" /><updated>2023-01-26T04:33:05+00:00</updated><published>2023-01-26T04:33:05+00:00</published><title>r/LangChain Lounge</title></entry><entry><author><name>/u/zkid18</name><uri>https://www.reddit.com/user/zkid18</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey folks,&lt;br/&gt; I am in the process of building my first custom GPT and have some questions regarding how to work properly with multimodal data. Let me explain what I am trying to achieve.&lt;br/&gt; I am creating a helper tool that will assist me in analyzing various pricing strategies of different SaaS tools. I have a dataset of 100k SaaS companies that have been labeled in some way, so I can cluster them based on their industry, category, etc.&lt;/p&gt; &lt;p&gt;Here is what I have as an input for my GPT so far:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;I have collected screenshots of their pricing pages, which are stored in S3.&lt;/li&gt; &lt;li&gt;I have collected the HTML for the pricing pages, which is stored in MongoDB.&lt;/li&gt; &lt;li&gt;I have a table of the companies with enriched data.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;I would like to build RAG on top of these documents, but I am a bit concerned about the next steps. My plan is to start with a simple one and use LlamaIndex. Here are the steps I have in mind:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Connect the data to the LlamaHub and pick the proper database. I want to keep the connection between the three mediums. and thus, I am not sure which database is best for my case. Should I use a vector database, graph database, or key-value database here?&lt;/li&gt; &lt;li&gt;Query the data and come up with evaluation metrics based on expert knowledge.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;I have some questions along the way:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Should I parse the data from the screenshots and HTML structure beforehand, or can I put it into storage as it is? Will it help with the quality of RAG?&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/zkid18&quot;&gt; /u/zkid18 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bkhdqe/need_advice_for_structuring_multimodal_data_for/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bkhdqe/need_advice_for_structuring_multimodal_data_for/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bkhdqe</id><link href="https://www.reddit.com/r/LangChain/comments/1bkhdqe/need_advice_for_structuring_multimodal_data_for/" /><updated>2024-03-21T21:02:22+00:00</updated><published>2024-03-21T21:02:22+00:00</published><title>Need advice for structuring multimodal data for RAG</title></entry><entry><author><name>/u/BossHoggHazzard</name><uri>https://www.reddit.com/user/BossHoggHazzard</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;We know 3-large has a 8191 token context window. I have text articles that are anywhere from 2500-4500 tokens each. Is there any advantage to chunking these? Or will I lose some of the context splitting articles into pieces?&lt;/p&gt; &lt;p&gt;Is it better to just get embeddings on whole articles or is it still a good idea to split them up into paragraphs? Or both? Feed it whole articles and paragraphs?&lt;/p&gt; &lt;p&gt;Thanks in advance for your insight.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/BossHoggHazzard&quot;&gt; /u/BossHoggHazzard &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bkh8wq/textembedding3large_chunking_question_for_rag/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bkh8wq/textembedding3large_chunking_question_for_rag/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bkh8wq</id><link href="https://www.reddit.com/r/LangChain/comments/1bkh8wq/textembedding3large_chunking_question_for_rag/" /><updated>2024-03-21T20:57:09+00:00</updated><published>2024-03-21T20:57:09+00:00</published><title>text-embedding-3-large chunking question for RAG</title></entry><entry><author><name>/u/halixness</name><uri>https://www.reddit.com/user/halixness</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi. I’m testing a variety of LLaMa2 7b and 13b (Hermes2Pro, MistralInstruct0.2, Chat, Solar10) as base for the React agent, but I can’t get outputs consistently as I’m encountering these issues:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;After providing the final answer, the LLM keeps running other actions and it gets off track with eg. Non relevant questions or even random programming code.&lt;/li&gt; &lt;li&gt;Sometimes it calls the tools incorrectly, especially if I switch to the structured chat agent (non existing arguments or swapped args between functions).&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;What I’m guessing is that I need larger models. I would appreciate someone else’s experience and takes on this. Thank you very much!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/halixness&quot;&gt; /u/halixness &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bkb1kc/suggestions_on_working_agents_and_base_llms/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bkb1kc/suggestions_on_working_agents_and_base_llms/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bkb1kc</id><link href="https://www.reddit.com/r/LangChain/comments/1bkb1kc/suggestions_on_working_agents_and_base_llms/" /><updated>2024-03-21T16:44:08+00:00</updated><published>2024-03-21T16:44:08+00:00</published><title>Suggestions on working agents and base LLMs?</title></entry><entry><author><name>/u/Obvious-Ad2752</name><uri>https://www.reddit.com/user/Obvious-Ad2752</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Does LangChain for Node.js offer the same level of functionality as its Python counterpart when it comes to functions and features? &lt;/p&gt; &lt;p&gt;Context : I am familiar with Node.js. I am looking to interact with an LLM for text extraction, NER, and coherence. I aim to take the response to create nodes, relationships, and labels in a Neo4j graph database. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Obvious-Ad2752&quot;&gt; /u/Obvious-Ad2752 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bkg3bg/langchain_functionality_in_nodejs_and_python_for/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bkg3bg/langchain_functionality_in_nodejs_and_python_for/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bkg3bg</id><link href="https://www.reddit.com/r/LangChain/comments/1bkg3bg/langchain_functionality_in_nodejs_and_python_for/" /><updated>2024-03-21T20:09:50+00:00</updated><published>2024-03-21T20:09:50+00:00</published><title>LangChain Functionality in Node.js and Python for Text Processing</title></entry><entry><author><name>/u/No_Donut_5349</name><uri>https://www.reddit.com/user/No_Donut_5349</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;m building a software application. I want to use a LLM as the basis. I will finetune the model with about 20,000 pages of legal text. Specifically laws all around the country. I will then use the trained model to answer help companies create compliant products and services. At the moment I am unsure as to the best way to go about it. My initial thought was to use gpt 3.5 and then further train it with the 20,000 pages of text. The text will be broken down by state and federal agencies. This text will be added to as new laws and regulations are passed.&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;The goal is for the model to only answer based on the data set it&amp;#39;s finetuned with. The data will be broken down by state. For example if they want information about loan requirements, they will ask the system and it will return with an outline of the requirements for loans by state. It will respond in a way that&amp;#39;s easy to understand.&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;My thinking is once I have all the data collected, using Langchain to fine tune the LLM. Am I on the right path here?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/No_Donut_5349&quot;&gt; /u/No_Donut_5349 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bk8t5f/need_input_on_software_project/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bk8t5f/need_input_on_software_project/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bk8t5f</id><link href="https://www.reddit.com/r/LangChain/comments/1bk8t5f/need_input_on_software_project/" /><updated>2024-03-21T15:10:34+00:00</updated><published>2024-03-21T15:10:34+00:00</published><title>Need input on Software Project</title></entry><entry><author><name>/u/guidsen15</name><uri>https://www.reddit.com/user/guidsen15</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi folks!&lt;/p&gt; &lt;p&gt;I have been interested in AI for some time, but now the time has come for I want to create a self-browsing agent that answers a certain question.&lt;/p&gt; &lt;p&gt;Couple of examples:&lt;/p&gt; &lt;p&gt;1️⃣ Prompt: what is the company &lt;a href=&quot;https://www.segment.com&quot;&gt;www.segment.com&lt;/a&gt; about?&lt;/p&gt; &lt;p&gt;- Should execute a Google search&lt;br/&gt; - Navigate to the about-us page&lt;br/&gt; - Read the page and return the result&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;2️⃣ Prompt: what is the difference between &lt;a href=&quot;https://www.segment.com&quot;&gt;www.segment.com&lt;/a&gt; and &lt;a href=&quot;https://www.intercom.com&quot;&gt;www.intercom.com&lt;/a&gt;?&lt;/p&gt; &lt;p&gt;- Should split this into multiple tasks, doing something similar to the workflow above.&lt;br/&gt; - Returns a detailed comparison based on the scanned pages&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;What are the best ways of implementing this? Are there any open-source frameworks that I might get inspired by?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/guidsen15&quot;&gt; /u/guidsen15 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bk4v99/best_way_to_create_an_ai_browsing_agent/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bk4v99/best_way_to_create_an_ai_browsing_agent/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bk4v99</id><link href="https://www.reddit.com/r/LangChain/comments/1bk4v99/best_way_to_create_an_ai_browsing_agent/" /><updated>2024-03-21T12:00:36+00:00</updated><published>2024-03-21T12:00:36+00:00</published><title>Best way to create an AI browsing agent</title></entry><entry><author><name>/u/pratikkoti04</name><uri>https://www.reddit.com/user/pratikkoti04</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I want to build a LLM Chatbot that can follow a particular flow the one we build in intent based chatbot frameworks. I want the llm to collect some information from user based on it fetch some data handle fallback queries and it should not deviate from the flow handling multi turn conversations.Any idea or open source framework that does this? Basically I want to use RASA stories and feed it to LLM so that it can follow a particular conversational flow. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/pratikkoti04&quot;&gt; /u/pratikkoti04 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bkcllq/rule_based_llm_chatbot/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bkcllq/rule_based_llm_chatbot/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bkcllq</id><link href="https://www.reddit.com/r/LangChain/comments/1bkcllq/rule_based_llm_chatbot/" /><updated>2024-03-21T17:47:25+00:00</updated><published>2024-03-21T17:47:25+00:00</published><title>Rule Based LLM Chatbot</title></entry><entry><author><name>/u/ninja24x7</name><uri>https://www.reddit.com/user/ninja24x7</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I am implementing RAG and trying to understand what&amp;#39;s the advantage of Overlapping.&lt;br/&gt; Consider this text:&lt;br/&gt; &lt;code&gt;&amp;quot;One of the most important things I didn&amp;#39;t understand about the world when I was a child is the degree to which the returns for performance are superlinear.&amp;quot;&lt;/code&gt;&lt;br/&gt; which is chunked and overlapped as using Naive or any strategy :&lt;br/&gt; &lt;code&gt;chunk 1 : One of the most important things&lt;/code&gt;&lt;br/&gt; &lt;code&gt;Chunk 2 : things I didn&amp;#39;t understand about&lt;/code&gt;&lt;br/&gt; &lt;code&gt;chunk 3: about the world when I was a child&lt;/code&gt;&lt;br/&gt; and so on..&lt;br/&gt; As you can see there is a word overlap with the chunks.&lt;br/&gt; What advantage does LLM get when you feed overlapping &lt;code&gt;chunk2&lt;/code&gt; and &lt;code&gt;chunk3&lt;/code&gt; to execute RAG prompt against a user query. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/ninja24x7&quot;&gt; /u/ninja24x7 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bjxvov/what_is_the_advantage_of_overlapping_in_chunking/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bjxvov/what_is_the_advantage_of_overlapping_in_chunking/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bjxvov</id><link href="https://www.reddit.com/r/LangChain/comments/1bjxvov/what_is_the_advantage_of_overlapping_in_chunking/" /><updated>2024-03-21T04:12:54+00:00</updated><published>2024-03-21T04:12:54+00:00</published><title>what is the advantage of overlapping in chunking strategy</title></entry><entry><author><name>/u/redditforgets</name><uri>https://www.reddit.com/user/redditforgets</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bjlldg/got_the_accuracy_of_gpt4_function_calling_from_35/&quot;&gt; &lt;img src=&quot;https://b.thumbs.redditmedia.com/EVm_c1rJmIBCr6_4BoHQygB8rtJ_kZYrD-LWEoWIYPQ.jpg&quot; alt=&quot;Got the accuracy of GPT4 Function Calling from 35% to 75% by tweaking function definitions.&quot; title=&quot;Got the accuracy of GPT4 Function Calling from 35% to 75% by tweaking function definitions.&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;ul&gt; &lt;li&gt;Adding function definitions in the system prompt of functions (Clickup&amp;#39;s API calls).&lt;/li&gt; &lt;li&gt;Flattening the Schema of the function&lt;/li&gt; &lt;li&gt;Adding system prompts&lt;/li&gt; &lt;li&gt;Adding function definitions in system prompt&lt;/li&gt; &lt;li&gt;Adding individual parameter examples&lt;/li&gt; &lt;li&gt;Adding function examples&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Wrote a nice blog with an &lt;a href=&quot;https://blog.composio.dev/improving-function-calling-accuracy-for-agentic-integrations/&quot;&gt;Indepth explanation&lt;/a&gt; here.&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/rmxgt35zfjpc1.png?width=816&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=934eddf839e17f2324c590157943a92ebbdedffa&quot;&gt;https://preview.redd.it/rmxgt35zfjpc1.png?width=816&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=934eddf839e17f2324c590157943a92ebbdedffa&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/redditforgets&quot;&gt; /u/redditforgets &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bjlldg/got_the_accuracy_of_gpt4_function_calling_from_35/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bjlldg/got_the_accuracy_of_gpt4_function_calling_from_35/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1bjlldg</id><media:thumbnail url="https://b.thumbs.redditmedia.com/EVm_c1rJmIBCr6_4BoHQygB8rtJ_kZYrD-LWEoWIYPQ.jpg" /><link href="https://www.reddit.com/r/LangChain/comments/1bjlldg/got_the_accuracy_of_gpt4_function_calling_from_35/" /><updated>2024-03-20T19:10:44+00:00</updated><published>2024-03-20T19:10:44+00:00</published><title>Got the accuracy of GPT4 Function Calling from 35% to 75% by tweaking function definitions.</title></entry><entry><author><name>/u/XhoniShollaj</name><uri>https://www.reddit.com/user/XhoniShollaj</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Has anyone here succesfully deployed LangChain in production? If yes, what were the main issues enountered and how did you approach them?&lt;/p&gt; &lt;p&gt;If not, what alternatives did you use or considering (e.g. Haystack etc.) ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/XhoniShollaj&quot;&gt; /u/XhoniShollaj &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bjxx32/langchain_in_production_alternatives/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bjxx32/langchain_in_production_alternatives/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bjxx32</id><link href="https://www.reddit.com/r/LangChain/comments/1bjxx32/langchain_in_production_alternatives/" /><updated>2024-03-21T04:15:01+00:00</updated><published>2024-03-21T04:15:01+00:00</published><title>Langchain in Production (&amp; Alternatives)</title></entry><entry><author><name>/u/Livid-Violinist-7652</name><uri>https://www.reddit.com/user/Livid-Violinist-7652</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I have a word doc and an excel file whose information is interconnected? The excel file outlines the process steps and the word file has process specifics. &lt;/p&gt; &lt;p&gt;I want to build a RAG by leveraging these two files to generate a document based on some prompts. What is the best strategy to do it?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Livid-Violinist-7652&quot;&gt; /u/Livid-Violinist-7652 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bk0kyo/how_to_build_a_rag_on_structed_data/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bk0kyo/how_to_build_a_rag_on_structed_data/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bk0kyo</id><link href="https://www.reddit.com/r/LangChain/comments/1bk0kyo/how_to_build_a_rag_on_structed_data/" /><updated>2024-03-21T07:06:27+00:00</updated><published>2024-03-21T07:06:27+00:00</published><title>How to build a RAG on structed data?</title></entry><entry><author><name>/u/Yintorion</name><uri>https://www.reddit.com/user/Yintorion</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I have the biography of a fictional character. It is about 160 pages long. How do I create a chatbot of this character with memory using RAG? I am using Gemini btw. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Yintorion&quot;&gt; /u/Yintorion &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bk0bo3/creating_chatbot_of_characters_using_rag/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bk0bo3/creating_chatbot_of_characters_using_rag/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bk0bo3</id><link href="https://www.reddit.com/r/LangChain/comments/1bk0bo3/creating_chatbot_of_characters_using_rag/" /><updated>2024-03-21T06:48:41+00:00</updated><published>2024-03-21T06:48:41+00:00</published><title>Creating chatbot of characters using RAG</title></entry><entry><author><name>/u/Zealousideal-Fall705</name><uri>https://www.reddit.com/user/Zealousideal-Fall705</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I’m a Ph.D. student who recently try to switch from hugging face to langchain. It feels like huggingface organize their libraries the research way (or the PyTorch way? It just feel like I can use them the same way I use research papers’ code), but langchain is more like something developed by JavaScript engineers and designed with no research user cases. &lt;/p&gt; &lt;p&gt;For example, all the “batch inference “ requirements on GitHub are ignored. The interface for customized functions (e.g., chat history post processing) are ill-designed. &lt;/p&gt; &lt;p&gt;I chose langchain in the beginning because the LLMs hosted by langchain responds faster than my local ones. But it seems that it’s really hard to customize the functionalities for research purposes.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Zealousideal-Fall705&quot;&gt; /u/Zealousideal-Fall705 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bjks1c/do_researchers_like_langchain/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bjks1c/do_researchers_like_langchain/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bjks1c</id><link href="https://www.reddit.com/r/LangChain/comments/1bjks1c/do_researchers_like_langchain/" /><updated>2024-03-20T18:37:14+00:00</updated><published>2024-03-20T18:37:14+00:00</published><title>Do researchers like langchain?</title></entry><entry><author><name>/u/Electronic_Key_8235</name><uri>https://www.reddit.com/user/Electronic_Key_8235</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Electronic_Key_8235&quot;&gt; /u/Electronic_Key_8235 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;/r/LLMDevs/comments/1bjzhja/ideal_toolchain_for_embedding_employee_training/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bjzzx8/ideal_toolchain_for_embedding_employee_training/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bjzzx8</id><link href="https://www.reddit.com/r/LangChain/comments/1bjzzx8/ideal_toolchain_for_embedding_employee_training/" /><updated>2024-03-21T06:25:59+00:00</updated><published>2024-03-21T06:25:59+00:00</published><title>Ideal Toolchain for Embedding Employee Training Documents</title></entry><entry><author><name>/u/Forward-Tip8621</name><uri>https://www.reddit.com/user/Forward-Tip8621</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi all, was going through the search tools available via langchain. Just wanted to check which is the best one to use? What are the key aspects to consider other than cost? If anyone who has used/compared these APIs that would be a great value add to my research &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Forward-Tip8621&quot;&gt; /u/Forward-Tip8621 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bjsx89/best_search_tool_in_langchain/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bjsx89/best_search_tool_in_langchain/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bjsx89</id><link href="https://www.reddit.com/r/LangChain/comments/1bjsx89/best_search_tool_in_langchain/" /><updated>2024-03-21T00:13:03+00:00</updated><published>2024-03-21T00:13:03+00:00</published><title>Best Search Tool in Langchain</title></entry><entry><author><name>/u/tisi3000</name><uri>https://www.reddit.com/user/tisi3000</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;When building LLM workflows with LangChain/LangGraph what&amp;#39;s the best way to build a node in the workflow &lt;strong&gt;where a human can validate/approve/reject&lt;/strong&gt; a flow? I know there is a Human-in-the-loop component in LangGraph that will prompt the user for input. But what if I&amp;#39;m not creating a user-initiated chat conversation, but a flow that reacts to e.g. incoming emails?&lt;/p&gt; &lt;p&gt;I guess I&amp;#39;d have to design my UI so that it&amp;#39;s not only a simple single-threaded chat interface, but some sort of inbox, right? Or is there any standard way that comes to mind?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/tisi3000&quot;&gt; /u/tisi3000 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bjnmu4/human_intervention_in_agent_workflows/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bjnmu4/human_intervention_in_agent_workflows/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bjnmu4</id><link href="https://www.reddit.com/r/LangChain/comments/1bjnmu4/human_intervention_in_agent_workflows/" /><updated>2024-03-20T20:34:23+00:00</updated><published>2024-03-20T20:34:23+00:00</published><title>Human intervention in agent workflows</title></entry><entry><author><name>/u/redfuel2</name><uri>https://www.reddit.com/user/redfuel2</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hello everyone,&lt;/p&gt; &lt;p&gt;I&amp;#39;m embarking on a project that requires a fresh start, and I find myself at a crossroads trying to decide on the optimal technology stack. The core objective is to enable conversations with a database using natural language, aiming for precise outcomes. This involves working with tabular data, applying filters, and conducting semantic searches.&lt;/p&gt; &lt;p&gt;Given the plethora of options out there, from graph databases and SQLCoder models to Retrieval-Augmented Generation (RAG) techniques, making a choice feels overwhelming. Each of these technologies brings something unique to the table, but I&amp;#39;m looking for a solution that balances ease of integration, scalability, and, most importantly, the ability to understand and process natural language queries effectively.&lt;/p&gt; &lt;p&gt;I would greatly appreciate your insights, experiences, or any advice you could share on this matter. Which stack or combination of technologies have you found to be the most effective for interacting with databases through natural language? Any pitfalls or success stories you could share would also be incredibly helpful as I navigate through these options.&lt;/p&gt; &lt;p&gt;Thank you in advance for your time and help!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/redfuel2&quot;&gt; /u/redfuel2 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bjf4xd/seeking_the_ideal_stack_for_natural_language/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bjf4xd/seeking_the_ideal_stack_for_natural_language/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bjf4xd</id><link href="https://www.reddit.com/r/LangChain/comments/1bjf4xd/seeking_the_ideal_stack_for_natural_language/" /><updated>2024-03-20T14:43:52+00:00</updated><published>2024-03-20T14:43:52+00:00</published><title>Seeking the Ideal Stack for Natural Language Database Interactions</title></entry><entry><author><name>/u/HappyDataGuy</name><uri>https://www.reddit.com/user/HappyDataGuy</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/HappyDataGuy&quot;&gt; /u/HappyDataGuy &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;/r/LLMDevs/comments/1bjctuz/has_anyone_used_dspy_for_rag_how_does_it_compare/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bjcun4/has_anyone_used_dspy_for_rag_how_does_it_compare/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bjcun4</id><link href="https://www.reddit.com/r/LangChain/comments/1bjcun4/has_anyone_used_dspy_for_rag_how_does_it_compare/" /><updated>2024-03-20T13:00:30+00:00</updated><published>2024-03-20T13:00:30+00:00</published><title>Has anyone used dspy for RAG? how does it compare to langchain/llama-index? and how does it &quot;train&quot; an LLM?</title></entry><entry><author><name>/u/Mediocre-Card8046</name><uri>https://www.reddit.com/user/Mediocre-Card8046</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;i have built a Langchain RAG app with a local model and now want to be able to run it on a Laptop. I am using a quantized Mixtral Model (Q5_0) and for this I want to conntect 2 GeoForce RTX 4090 to my laptop. As I am a newby (and nooby) in the Hardware topic, is it even possible to connect 2 RTX 4090 to a more or less &amp;quot;normal&amp;quot; Laptop?&lt;/p&gt; &lt;p&gt;The use case would be that the customer tries the (local) application on a standalone device and if he is happy with it he buys more Hardware to host it for production.&lt;/p&gt; &lt;p&gt;At the moment I am running everything on my Macbook with 64GB RAM but I need a solution for a customer with a Windows PC.&lt;/p&gt; &lt;p&gt;One other option would be that the customer just buys a Macbook, but the 2 GeForece RTX 4090 would be a better investment I think because these could further be used for a prodcution setting.&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;Thanks for you suggestions!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Mediocre-Card8046&quot;&gt; /u/Mediocre-Card8046 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bji0np/is_it_possible_to_connect_2_geforce_rtx_4090_to_a/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bji0np/is_it_possible_to_connect_2_geforce_rtx_4090_to_a/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bji0np</id><link href="https://www.reddit.com/r/LangChain/comments/1bji0np/is_it_possible_to_connect_2_geforce_rtx_4090_to_a/" /><updated>2024-03-20T16:44:18+00:00</updated><published>2024-03-20T16:44:18+00:00</published><title>is it possible to connect 2 GeForce RTX 4090 to a Laptop?</title></entry><entry><author><name>/u/fish2079</name><uri>https://www.reddit.com/user/fish2079</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I have built a simple RAG chain with message history using Mistral-7b model with 4bit quantization. &lt;/p&gt; &lt;p&gt;Whenever I build this chain using a model from the dockerized Ollama, everything works fine and I can have a long conversation with the chain. &lt;/p&gt; &lt;p&gt;However, as soon as I switch to HF model, only the first message goes through, everything else gets the OOM memory. In fact, the memory usage seems to increase with each subsequent invoke. &lt;/p&gt; &lt;p&gt;In both cases, I am using the Mistral-7b model with quantization. So I am confused as to where the memory issue comes from. &lt;/p&gt; &lt;p&gt;Here are the code snippets: &lt;/p&gt; &lt;p&gt;Using HF model:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;model_name = &amp;quot;mistralai/Mistral-7B-Instruct-v0.2&amp;quot; bnb_config = BitsAndBytesConfig( load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=&amp;quot;nf4&amp;quot;, bnb_4bit_compute_dtype=torch.bfloat16 ) model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=bnb_config) tokenizer = AutoTokenizer.from_pretrained(model_name) text_generation_pipeline = pipeline( model=model, tokenizer=tokenizer, task=&amp;quot;text-generation&amp;quot;, temperature=0.2, do_sample=True, repetition_penalty=1.1, max_new_tokens=400, ) chat_llm = HuggingFacePipeline(pipeline=text_generation_pipeline) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Using Ollama model:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;chat_llm = ChatOllama(model=&amp;quot;mistral:7b&amp;quot;) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Overall chain setup&lt;/p&gt; &lt;pre&gt;&lt;code&gt;chatbot_conversation_with_context_chain = &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;RunnablePassthrough.assign(standalone_message=standalone_message_chain).assign(context= itemgetter(&amp;#39;standalone_message&amp;#39;) | retriever).assign(output= question_answering_prompt | chat_llm | StrOutputParser())&lt;/p&gt; &lt;pre&gt;&lt;code&gt;chatbot = RunnableWithMessageHistory( chatbot_conversation_with_context_chain, get_session_history=get_session_history, input_messages_key=&amp;quot;messages&amp;quot;, history_messages_key=&amp;quot;chat_history&amp;quot;, history_factory_config=[ ConfigurableFieldSpec( id=&amp;quot;user_id&amp;quot;, annotation=str, name=&amp;quot;User ID&amp;quot;, description=&amp;quot;Unique identifier for the user.&amp;quot;, default=&amp;quot;&amp;quot;, is_shared=True, ), ConfigurableFieldSpec( id=&amp;quot;conversation_id&amp;quot;, annotation=str, name=&amp;quot;Conversation ID&amp;quot;, description=&amp;quot;Unique identifier for the conversation.&amp;quot;, default=&amp;quot;&amp;quot;, is_shared=True, ), ], &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;) response = chatbot.invoke( {&amp;#39;messages&amp;#39;: &amp;quot;Can you give me the basic Java code for reading a CSV file?&amp;quot;}, config={ &amp;quot;configurable&amp;quot;: {&amp;quot;user_id&amp;quot;: &amp;quot;test&amp;quot;, &amp;quot;conversation_id&amp;quot;: &amp;quot;dummy&amp;quot;} }, )&lt;/p&gt; &lt;p&gt;print(response.keys()) for key in response.keys(): print(key+&amp;quot;: &amp;quot;, end=&amp;quot;&amp;quot;) print(response[key])&lt;/p&gt; &lt;p&gt;response = chatbot.invoke( {&amp;#39;messages&amp;#39;: &amp;quot;Can you elaborate on the first function?&amp;quot;}, config={&amp;quot;configurable&amp;quot;: {&amp;quot;user_id&amp;quot;: &amp;quot;test&amp;quot;, &amp;quot;conversation_id&amp;quot;: &amp;quot;dummy&amp;quot;} }, )&lt;/p&gt; &lt;p&gt;print(response.keys()) for key in response.keys(): print(key+&amp;quot;: &amp;quot;, end=&amp;quot;&amp;quot;) print(response[key])&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/fish2079&quot;&gt; /u/fish2079 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bjm5rp/rag_chain_with_hf_model_works_fine_for_first/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bjm5rp/rag_chain_with_hf_model_works_fine_for_first/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bjm5rp</id><link href="https://www.reddit.com/r/LangChain/comments/1bjm5rp/rag_chain_with_hf_model_works_fine_for_first/" /><updated>2024-03-20T19:33:59+00:00</updated><published>2024-03-20T19:33:59+00:00</published><title>RAG chain with HF model works fine for first quest, then OOM for subsequent chain. No OOM issue when using Ollama model instead</title></entry><entry><author><name>/u/VegetableAddendum888</name><uri>https://www.reddit.com/user/VegetableAddendum888</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;If any Colab notebook or github repo available then it will be helpful&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/VegetableAddendum888&quot;&gt; /u/VegetableAddendum888 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bjiabv/can_anyone_suggest_a_idea_to_implement_rag_with/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bjiabv/can_anyone_suggest_a_idea_to_implement_rag_with/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bjiabv</id><link href="https://www.reddit.com/r/LangChain/comments/1bjiabv/can_anyone_suggest_a_idea_to_implement_rag_with/" /><updated>2024-03-20T16:55:26+00:00</updated><published>2024-03-20T16:55:26+00:00</published><title>Can anyone suggest a idea to implement RAG with LLm.Like if the searched query not in RAG data then LLm responses to the query</title></entry><entry><author><name>/u/stargazer1Q84</name><uri>https://www.reddit.com/user/stargazer1Q84</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi everyone, I hope it is fine to post questions here. &lt;/p&gt; &lt;p&gt;I am just getting started with output-parsers and I&amp;#39;m impressed with their usefulness when they work properly. I have, however, run into a case where every now and then, a chain returns an error that seems to be related to the JsonOutputParser that I use, as indicated by the following (condensed) error message:&lt;/p&gt; &lt;p&gt;&lt;code&gt;JSONDecodeError&lt;/code&gt;&lt;br/&gt; &lt;code&gt;JsonOutputParser.parse_result(self, result, partial)&lt;/code&gt;&lt;br/&gt; &lt;code&gt;156 # Parse the JSON string into a Python dictionary&lt;/code&gt;&lt;br/&gt; &lt;code&gt;--&amp;gt; 157 parsed = parser(json_str)&lt;/code&gt;&lt;br/&gt; &lt;code&gt;159 return parsed&lt;/code&gt;&lt;br/&gt; &lt;code&gt;122 # If we got here, we ran out of characters to remove&lt;/code&gt;&lt;br/&gt; &lt;code&gt;123 # and still couldn&amp;#39;t parse the string as JSON, so return the parse error&lt;/code&gt;&lt;br/&gt; &lt;code&gt;124 # for the original string.&lt;/code&gt;&lt;br/&gt; &lt;code&gt;--&amp;gt; 125 return json.loads(s, strict=strict)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;According to &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/17hep0o/comment/k6na6nd/?utm_source=share&amp;amp;utm_medium=web3x&amp;amp;utm_name=web3xcss&amp;amp;utm_term=1&amp;amp;utm_content=share_button&quot;&gt;this post here&lt;/a&gt; this could be related to there not being &amp;quot;enough tokens left to fully generate my output&amp;quot;, which seems to be in line with the error message above:&lt;/p&gt; &lt;p&gt;&amp;gt;&lt;code&gt;122 # If we got here, we ran out of characters to remove&lt;/code&gt; &lt;/p&gt; &lt;p&gt;although I am not fully sure what that means or how it can be fixed. &lt;/p&gt; &lt;p&gt;Has anybody encountered this problem before and could offer some guidance? I must admit that I&amp;#39;m feeling kind of stumped, especially since the error can&amp;#39;t be reproduced reliably and only occurs every other time I run my script. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/stargazer1Q84&quot;&gt; /u/stargazer1Q84 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bjdjk0/understanding_jsondecodeerror_when_using/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bjdjk0/understanding_jsondecodeerror_when_using/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bjdjk0</id><link href="https://www.reddit.com/r/LangChain/comments/1bjdjk0/understanding_jsondecodeerror_when_using/" /><updated>2024-03-20T13:33:06+00:00</updated><published>2024-03-20T13:33:06+00:00</published><title>Understanding JSONDecodeError when using JsonOutputParser</title></entry><entry><author><name>/u/mehul_gupta1997</name><uri>https://www.reddit.com/user/mehul_gupta1997</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/mehul_gupta1997&quot;&gt; /u/mehul_gupta1997 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;/r/ArtificialInteligence/comments/1bjbd36/multiagent_conversation_using_crewai_genai/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bjbe0j/multiagent_conversation_using_crewai_genai/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bjbe0j</id><link href="https://www.reddit.com/r/LangChain/comments/1bjbe0j/multiagent_conversation_using_crewai_genai/" /><updated>2024-03-20T11:40:44+00:00</updated><published>2024-03-20T11:40:44+00:00</published><title>Multi-Agent Conversation using CrewAI (GenAI)</title></entry><entry><author><name>/u/Thegunsmith98</name><uri>https://www.reddit.com/user/Thegunsmith98</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I am trying to build an application that takes templates of things like a cover letter , resume , medical research document. Now based on this template I will upload another document containing information to be used to fill the template. However after the model generates a new document following the template and information , the whole alignment of the document is wrong and it doesnt bold the necessary parts. Is there any way to ensure that a model can follow the format for a template like center allignment , bolding the headers , etc. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Thegunsmith98&quot;&gt; /u/Thegunsmith98 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bj6xl3/langchain_usage_doubt_for_document_generation/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bj6xl3/langchain_usage_doubt_for_document_generation/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bj6xl3</id><link href="https://www.reddit.com/r/LangChain/comments/1bj6xl3/langchain_usage_doubt_for_document_generation/" /><updated>2024-03-20T06:25:51+00:00</updated><published>2024-03-20T06:25:51+00:00</published><title>Langchain Usage doubt for document generation</title></entry><entry><author><name>/u/Mediocre-Card8046</name><uri>https://www.reddit.com/user/Mediocre-Card8046</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;when thinking about RAG evaluation, everybody talks about RAGAS. It is generally nice to have a framework where you can evaluate your RAG workflows. However I tried it with an own local LLM as well as with the gpt-4-turbo model and the results really are not reliable. &lt;/p&gt; &lt;p&gt;I adapted prompts to my language (german) and with my test dataset, the answer_correctness, answer_relevancy scores are often times very low, zero or NaN, even if the answer is completely correct. &lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;Does anyone have similar experiences? &lt;/p&gt; &lt;p&gt;With my experience, I am not feeling comfortable using ragas as results differ heavenly from run to run, so all the evaluation doesn&amp;#39;t really help me. &lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Mediocre-Card8046&quot;&gt; /u/Mediocre-Card8046 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bijg75/why_is_everyone_using_ragas_for_rag_evaluation/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bijg75/why_is_everyone_using_ragas_for_rag_evaluation/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bijg75</id><link href="https://www.reddit.com/r/LangChain/comments/1bijg75/why_is_everyone_using_ragas_for_rag_evaluation/" /><updated>2024-03-19T12:49:43+00:00</updated><published>2024-03-19T12:49:43+00:00</published><title>Why is everyone using RAGAS for RAG evaluation? For me it looks very unreliable</title></entry></feed>