<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/"><category term="LangChain" label="r/LangChain"/><updated>2024-07-11T19:05:51+00:00</updated><icon>https://www.redditstatic.com/icon.png/</icon><id>/r/LangChain.rss</id><link rel="self" href="https://www.reddit.com/r/LangChain.rss" type="application/atom+xml" /><link rel="alternate" href="https://www.reddit.com/r/LangChain" type="text/html" /><subtitle>LangChain is an open-source framework and developer toolkit that helps developers get LLM applications from prototype to production. It is available for Python and Javascript at https://www.langchain.com/.</subtitle><title>LangChain</title><entry><author><name>/u/zchaarm</name><uri>https://www.reddit.com/user/zchaarm</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;A place for members of &lt;a href=&quot;/r/LangChain&quot;&gt;r/LangChain&lt;/a&gt; to chat with each other&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/zchaarm&quot;&gt; /u/zchaarm &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_10ljho9</id><link href="https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/" /><updated>2023-01-26T04:33:05+00:00</updated><published>2023-01-26T04:33:05+00:00</published><title>r/LangChain Lounge</title></entry><entry><author><name>/u/MZuc</name><uri>https://www.reddit.com/user/MZuc</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;ve heard so many AI teams ask this question, I decided to sum up my take on this in a short post. Let me know what you guys think.&lt;/p&gt; &lt;p&gt;The way I see it, the first step is to change how you identify and approach problems. Too often, teams use vague terms like “it &lt;strong&gt;&lt;em&gt;feels&lt;/em&gt;&lt;/strong&gt; like” or “it &lt;strong&gt;&lt;em&gt;seems&lt;/em&gt;&lt;/strong&gt; like” instead of specific metrics, like “the feedback score for this type of request improved by 20%.”&lt;/p&gt; &lt;p&gt;When you&amp;#39;re developing a new AI-driven RAG application, the process tends to be chaotic. There are too many priorities and not enough time to tackle them all. Even if you could, you&amp;#39;re not sure how to enhance your RAG system. You sense that there&amp;#39;s a &amp;quot;right path&amp;quot; – a set of steps that would lead to maximum growth in the shortest time. There are a myriad of great trendy RAG libraries, pipelines, and tools out there but you don&amp;#39;t know which will work on &lt;em&gt;your&lt;/em&gt; documents and &lt;em&gt;your&lt;/em&gt; usecase (&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dr5kki/the_most_important_thing_to_build_great_rag_system/&quot;&gt;as mentioned in another Reddit post that inspired this one&lt;/a&gt;).&lt;/p&gt; &lt;p&gt;I discuss this whole topic more detail in my &lt;a href=&quot;https://pashpashpash.substack.com/p/why-does-my-rag-suck-and-how-do-i&quot;&gt;Substack article&lt;/a&gt; including specific advice for pre-launch and post-launch, but in a nutshell, when starting any RAG system &lt;strong&gt;&lt;em&gt;you need to capture valuable metrics like cosine similarity, user feedback, and reranker scores - for every retrieval, right from the start&lt;/em&gt;&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;Basically, in an ideal scenario, you will end up with an &lt;strong&gt;observability table&lt;/strong&gt; that looks like this:&lt;/p&gt; &lt;p&gt;|| || |&lt;strong&gt;retrieval_id&lt;/strong&gt; (some unique identifier for every piece of retrieved context)|&lt;strong&gt;query_id&lt;/strong&gt; (unique id for the input query/question/message that RAG was used to answer)|&lt;strong&gt;cosine similarity score&lt;/strong&gt; (null for non-vector retrieval e.g. elastic search)|&lt;strong&gt;reranker relevancy score&lt;/strong&gt; (highly recommended for ALL kinds of retrieval, including vector and traditional text search like elastic)|&lt;strong&gt;timestamp&lt;/strong&gt;|&lt;strong&gt;retrieved_context&lt;/strong&gt; (optional, but nice to have for QA purposes)|&lt;strong&gt;user_feedback&lt;/strong&gt;| |1|11|0.77|0.82|12345678|&amp;quot;NYC Media [...]&amp;quot;|false (thumbs down)| |2|11|0.75|0.65|12345679|&amp;quot;The New York City Subway [...]&amp;quot;|true (thumbs up)| |...||...|...|...|...|...|&lt;/p&gt; &lt;p&gt;Once you start collecting and storing these super powerful observability metrics, you can begin analyzing production performance. We can &lt;a href=&quot;https://x.com/jxnlco/status/1803899526723387895&quot;&gt;categorize this analysis into two main areas&lt;/a&gt;:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Topics: This refers to the content and context of the data, which can be represented by the way words are structured or the embeddings used in search queries. You can use topic modeling to better understand the types of responses your system handles. &lt;ul&gt; &lt;li&gt;E.g. People talking about their family, or their hobbies, etc.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Capabilities (Agent Tools/Functions): This pertains to the functional aspects of the queries, such as: &lt;ul&gt; &lt;li&gt;Direct conversation requests (e.g., &lt;em&gt;“Remind me what we talked about when we discussed my neighbor&amp;#39;s dogs barking all the time.”&lt;/em&gt;)&lt;/li&gt; &lt;li&gt;Time-sensitive queries (e.g., &lt;em&gt;“Show me the latest X”&lt;/em&gt; or &lt;em&gt;“Show me the most recent Y.”&lt;/em&gt;)&lt;/li&gt; &lt;li&gt;Metadata-specific inquiries (e.g., &lt;em&gt;“What date was our last conversation?”&lt;/em&gt;), which might require specific filters or keyword matching that go beyond simple text embeddings.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;By applying clustering techniques to these topics and capabilities (I cover this in more depth in my &lt;a href=&quot;https://pashpashpash.substack.com/p/tackling-the-challenge-of-document&quot;&gt;previous article on K-Means clusterization&lt;/a&gt;), you can:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Group similar queries/questions together and categorize them by topic e.g. &lt;em&gt;“Product availability questions”&lt;/em&gt; or capability e.g. &lt;em&gt;“Requests to search previous conversations”&lt;/em&gt;.&lt;/li&gt; &lt;li&gt;Calculate the frequency and distribution of these groups.&lt;/li&gt; &lt;li&gt;Assess the average performance scores for each group.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This data-driven approach allows you to prioritize system enhancements based on actual user needs and system performance. For instance:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;If person-entity-retrieval commands a significant portion of query volume (say 60%) and shows high satisfaction rates (90% thumbs up) with minimal cosine distance, this area may not need further refinement.&lt;/li&gt; &lt;li&gt;Conversely, queries like &amp;quot;What date was our last conversation&amp;quot; might show poor results, indicating a limitation of our current functional capabilities. If such queries constitute a small fraction (e.g., 2%) of total volume, it might be more strategic to temporarily exclude these from the system’s capabilities (&lt;em&gt;“I forget, honestly!”&lt;/em&gt; or &lt;em&gt;“Do you think I&amp;#39;m some kind of calendar!?”&lt;/em&gt;), thus improving overall system performance. &lt;ul&gt; &lt;li&gt;Handling these exclusions gracefully significantly improves user experience. &lt;ul&gt; &lt;li&gt;When appropriate, Use humor and personality to your advantage instead of saying &lt;em&gt;“I cannot answer this right now.”&lt;/em&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;TL;DR:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Getting your RAG system from “sucks” to “good” isn&amp;#39;t about magic solutions or trendy libraries. The first step is to implement strong observability practices to continuously analyze and improve performance. Cluster collected data into topics &amp;amp; capabilities to have a clear picture of how people are using your product and where it falls short. Prioritize enhancements based on real usage and remember, a touch of personality can go a long way in handling limitations.&lt;/p&gt; &lt;p&gt;For a more detailed treatment of this topic, check out my &lt;a href=&quot;https://pashpashpash.substack.com/p/why-does-my-rag-suck-and-how-do-i&quot;&gt;article here&lt;/a&gt;. I&amp;#39;d love to hear your thoughts on this, please let me know if there are any other good metrics or considerations to keep in mind!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/MZuc&quot;&gt; /u/MZuc &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e0rsou/why_does_my_rag_suck_and_how_do_i_make_it_good/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e0rsou/why_does_my_rag_suck_and_how_do_i_make_it_good/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1e0rsou</id><link href="https://www.reddit.com/r/LangChain/comments/1e0rsou/why_does_my_rag_suck_and_how_do_i_make_it_good/" /><updated>2024-07-11T15:33:21+00:00</updated><published>2024-07-11T15:33:21+00:00</published><title>&quot;Why does my RAG suck and how do I make it good&quot;</title></entry><entry><author><name>/u/northwolf56</name><uri>https://www.reddit.com/user/northwolf56</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e0twcd/my_serverless_visual_langgraph_editor/&quot;&gt; &lt;img src=&quot;https://b.thumbs.redditmedia.com/UJ085UairrlLGvO3kv9DnoNb9gVW-_hN6_J8iNlVyME.jpg&quot; alt=&quot;My Serverless Visual LangGraph Editor&quot; title=&quot;My Serverless Visual LangGraph Editor&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/northwolf56&quot;&gt; /u/northwolf56 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/gallery/1e0twcd&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e0twcd/my_serverless_visual_langgraph_editor/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1e0twcd</id><media:thumbnail url="https://b.thumbs.redditmedia.com/UJ085UairrlLGvO3kv9DnoNb9gVW-_hN6_J8iNlVyME.jpg" /><link href="https://www.reddit.com/r/LangChain/comments/1e0twcd/my_serverless_visual_langgraph_editor/" /><updated>2024-07-11T17:01:43+00:00</updated><published>2024-07-11T17:01:43+00:00</published><title>My Serverless Visual LangGraph Editor</title></entry><entry><author><name>/u/burcapaul</name><uri>https://www.reddit.com/user/burcapaul</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I believe that chat interfaces are the worst way to interact with LLMs, but they&amp;#39;re currently our only real option (voice doesn&amp;#39;t count). Here&amp;#39;s my reasoning:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Even though we&amp;#39;re programmed as humans to &amp;quot;prompt&amp;quot; each other in daily interactions, I&amp;#39;ve observed that when it comes to LLMs, people are very deficient at it.&lt;/li&gt; &lt;li&gt;In the past few decades, Microsoft and Apple have trained us to use computers primarily through visual interactions (clicks, taps, buttons, etc.).&lt;/li&gt; &lt;li&gt;Maybe the interface for LLMs should be more visual, rather than relying on text prompting.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;What do you guys think about this? Are we limiting the potential of LLMs by sticking to chat interfaces? Could a more visual approach make these AI tools more accessible and effective for the average user?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/burcapaul&quot;&gt; /u/burcapaul &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e0k7e8/chat_interfaces_are_holding_back_llms_we_need_a/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e0k7e8/chat_interfaces_are_holding_back_llms_we_need_a/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1e0k7e8</id><link href="https://www.reddit.com/r/LangChain/comments/1e0k7e8/chat_interfaces_are_holding_back_llms_we_need_a/" /><updated>2024-07-11T08:49:53+00:00</updated><published>2024-07-11T08:49:53+00:00</published><title>Chat interfaces are holding back LLMs - we need a more visual approach</title></entry><entry><author><name>/u/Gloomy-Traffic4964</name><uri>https://www.reddit.com/user/Gloomy-Traffic4964</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;I have a few questions after watching the generative UI videos by Langchain (fyi my fullstack/frontend knowledge is poor).&lt;/p&gt; &lt;p&gt;What&amp;#39;s the difference between Langchain&amp;#39;s generative UI vs just streaming data to the front end to be made into a react component with websockets.&lt;/p&gt; &lt;p&gt;For example, I am streaming results to my React frontend from my django backend. In my frontend I am using &lt;code&gt;new WebSocket(\${REACT_APP_WS_URL}/ws/chat/\)&lt;/code&gt; and &lt;code&gt;ws.current.onmessage&lt;/code&gt; to check for &lt;code&gt;{&amp;#39;event&amp;#39;: &amp;#39;on_tool_start&amp;#39;, ...}.&lt;/code&gt; If event is &amp;#39;on_tool_start&amp;#39;, it returns a loading component instead of a text message response, and if the message is &lt;code&gt;{&amp;#39;event&amp;#39;: &amp;#39;on_tool_end&amp;#39;, &amp;#39;output&amp;#39;: [{ &amp;#39;key&amp;#39;:&amp;#39;value&amp;#39;, ..]}&lt;/code&gt; it replaces the loading component with a react component that takes &amp;#39;output&amp;#39; as inputs.&lt;/p&gt; &lt;p&gt;Follow up questions&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Why does the Langchain implementation use react server components in the frontend when there is a separate backend server with FastAPI?&lt;/li&gt; &lt;li&gt;What is ai/rsc in &lt;code&gt;(import { createStreamableUI } from &amp;quot;ai/rsc&amp;quot;)&lt;/code&gt; . It looks like it&amp;#39;s next/Vercel specific? Where are the docs for createStreamableUI ?&lt;/li&gt; &lt;li&gt;How much value is there from the extra complexity in Langchain implementation (specifically in the frontend).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/bracesproul/gen-ui-python/tree/main&quot;&gt;Here is the gen ui repo&lt;/a&gt;&lt;br/&gt; &lt;a href=&quot;https://www.youtube.com/watch?v=d3uoLbfBPkw&amp;amp;t=1557s&quot;&gt;Here is the youtube video&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Gloomy-Traffic4964&quot;&gt; /u/Gloomy-Traffic4964 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e0qc2w/generative_ui/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e0qc2w/generative_ui/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1e0qc2w</id><link href="https://www.reddit.com/r/LangChain/comments/1e0qc2w/generative_ui/" /><updated>2024-07-11T14:30:14+00:00</updated><published>2024-07-11T14:30:14+00:00</published><title>Generative UI</title></entry><entry><author><name>/u/MeltingHippos</name><uri>https://www.reddit.com/user/MeltingHippos</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;This post by an AI engineer explains the challenges they encountered when implementing RAG for massive enterprise codebases and how they solved them. They share lots of alpha on strategies for chunking, creating vector embeddings for code chunks, using LLMs to generate natural language descriptions that improve indexing, and using LLMs to rank the chunks that are retrieved from the vector store: &lt;a href=&quot;https://www.codium.ai/blog/rag-for-large-scale-code-repos/&quot;&gt;https://www.codium.ai/blog/rag-for-large-scale-code-repos/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/MeltingHippos&quot;&gt; /u/MeltingHippos &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e0jpkk/rag_techniques_for_a_big_codebase_with_10k_repos/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e0jpkk/rag_techniques_for_a_big_codebase_with_10k_repos/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1e0jpkk</id><link href="https://www.reddit.com/r/LangChain/comments/1e0jpkk/rag_techniques_for_a_big_codebase_with_10k_repos/" /><updated>2024-07-11T08:14:14+00:00</updated><published>2024-07-11T08:14:14+00:00</published><title>RAG Techniques for a Big Codebase with 10k Repos</title></entry><entry><author><name>/u/ApprehensiveCut799</name><uri>https://www.reddit.com/user/ApprehensiveCut799</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I created a quick proof of concept by copy pasting LangChain code from here &lt;a href=&quot;https://js.langchain.com/v0.1/docs/modules/agents/agent_types/tool_calling/&quot;&gt;https://js.langchain.com/v0.1/docs/modules/agents/agent_types/tool_calling/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It basically uses a tool calling agent that uses the google search tool, web browser tool and a retriever tool that allows a user to search for data and also query their own uploaded documents. I ended up using the tool calling agent as the reAct agent was causing a lot of bugs. The issue now is that I read a few research papers on reAct prompting and was planning to use them as sources to beef up my hackathon presentation and make it look more cutting edge. I&amp;#39;m just wondering whether anyone has any similar papers that can make my current POC sound more cutting edge? I know this probably sounds really stupid to you but I really want to win this hackathon and I believe that the presentation is equally as important as the MVP. Thanks a lot.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/ApprehensiveCut799&quot;&gt; /u/ApprehensiveCut799 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e0mgl2/looking_for_research_papers_to_beef_up_my/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e0mgl2/looking_for_research_papers_to_beef_up_my/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1e0mgl2</id><link href="https://www.reddit.com/r/LangChain/comments/1e0mgl2/looking_for_research_papers_to_beef_up_my/" /><updated>2024-07-11T11:17:47+00:00</updated><published>2024-07-11T11:17:47+00:00</published><title>Looking for research papers to beef up my Hackathon project</title></entry><entry><author><name>/u/anehzat</name><uri>https://www.reddit.com/user/anehzat</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e0gqn6/psql_extended_to_support_sql_autocomplete_chat/&quot;&gt; &lt;img src=&quot;https://preview.redd.it/6zcw8ik2mtbd1.gif?width=320&amp;amp;crop=smart&amp;amp;s=0d16d7147456f09c07dfab560fb29bc77d857767&quot; alt=&quot;psql extended to support SQL autocomplete &amp;amp; Chat Assistance with DB context.&quot; title=&quot;psql extended to support SQL autocomplete &amp;amp; Chat Assistance with DB context.&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/anehzat&quot;&gt; /u/anehzat &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://i.redd.it/6zcw8ik2mtbd1.gif&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e0gqn6/psql_extended_to_support_sql_autocomplete_chat/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1e0gqn6</id><media:thumbnail url="https://preview.redd.it/6zcw8ik2mtbd1.gif?width=320&amp;crop=smart&amp;s=0d16d7147456f09c07dfab560fb29bc77d857767" /><link href="https://www.reddit.com/r/LangChain/comments/1e0gqn6/psql_extended_to_support_sql_autocomplete_chat/" /><updated>2024-07-11T04:55:20+00:00</updated><published>2024-07-11T04:55:20+00:00</published><title>psql extended to support SQL autocomplete &amp; Chat Assistance with DB context.</title></entry><entry><author><name>/u/Best_Sail5</name><uri>https://www.reddit.com/user/Best_Sail5</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e0r49j/training_llm_for_langgraph_specific_use/&quot;&gt; &lt;img src=&quot;https://b.thumbs.redditmedia.com/6dbbLK2Jl_HPiGd9EdOuVkLNjlsygojMZ_UcNxJIg8c.jpg&quot; alt=&quot;training LLM for Langgraph specific use&quot; title=&quot;training LLM for Langgraph specific use&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hello everyone,&lt;/p&gt; &lt;p&gt;I wanna use a LLM to create an agent in langgraph with this kind of architecture:&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/uw5c6kozmwbd1.png?width=199&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ca17ef164138c99d913c9d068c59ef5c996ad56c&quot;&gt;https://preview.redd.it/uw5c6kozmwbd1.png?width=199&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ca17ef164138c99d913c9d068c59ef5c996ad56c&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The idea is similar to a React agent where the model has to provide a prompt for my terminal tool and observe if it achieve a given objective.&lt;/p&gt; &lt;p&gt;I have a question about how should i train such model?&lt;/p&gt; &lt;p&gt;Could i use DPO/ORPO procedure to align my model on multi-step feeding him the context each time?&lt;/p&gt; &lt;p&gt;Or is there a smarter way to do that?&lt;/p&gt; &lt;p&gt;Thanks for your suggestions!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Best_Sail5&quot;&gt; /u/Best_Sail5 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e0r49j/training_llm_for_langgraph_specific_use/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e0r49j/training_llm_for_langgraph_specific_use/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1e0r49j</id><media:thumbnail url="https://b.thumbs.redditmedia.com/6dbbLK2Jl_HPiGd9EdOuVkLNjlsygojMZ_UcNxJIg8c.jpg" /><link href="https://www.reddit.com/r/LangChain/comments/1e0r49j/training_llm_for_langgraph_specific_use/" /><updated>2024-07-11T15:04:16+00:00</updated><published>2024-07-11T15:04:16+00:00</published><title>training LLM for Langgraph specific use</title></entry><entry><author><name>/u/Select-Coconut-1161</name><uri>https://www.reddit.com/user/Select-Coconut-1161</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e0iovb/filtering_based_on_metadata/&quot;&gt; &lt;img src=&quot;https://external-preview.redd.it/UallS6eTqMQ6Q_VovzOpEJke6ZgDb6GzqRJtFsGug9s.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=82f0fba791dd0f75899e3dc5480f915fe8c0cafd&quot; alt=&quot;Filtering based on Metadata&quot; title=&quot;Filtering based on Metadata&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey everyone, I am trying to filter the items I retrieve by Metadata fields. I have like 5 metadata fields and I am trying to use 3 filters using 3 of those fields but somehow, only one of them does not work.&lt;/p&gt; &lt;p&gt;I declare the attribute info in metadata_field_info like that:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;AttributeInfo( name=&amp;quot;speaker&amp;quot;, description=&amp;quot;The person who spoke, identified as either &amp;#39;Moderator&amp;#39; or &amp;#39;Participant&amp;#39;.&amp;quot;, type=&amp;quot;string&amp;quot;, ), &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Then, I try to filter it like&lt;/p&gt; &lt;pre&gt;&lt;code&gt; filter=f&amp;quot;and(eq(&amp;#39;speaker&amp;#39;, &amp;#39;Participant&amp;#39;), eq(&amp;#39;participant&amp;#39;, &amp;#39;k6&amp;#39;), ({part_filter}))&amp;quot;, &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;However, this did not work. So I asked Claude and updated it:&lt;/p&gt; &lt;pre&gt;&lt;code&gt; filter_str=f&amp;quot;speaker == &amp;#39;Participant&amp;#39; and participant == &amp;#39;k6&amp;#39; and ({part_filter})&amp;quot;, &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This also did not work. I decided to check LangChain documentations and using &lt;a href=&quot;https://python.langchain.com/v0.1/docs/modules/data_connection/retrievers/self_query/&quot;&gt;this page&lt;/a&gt; as a reference, I changed it to:&lt;/p&gt; &lt;pre&gt;&lt;code&gt; filter=f&amp;quot;and(eq(\&amp;quot;speaker\&amp;quot;, \&amp;quot;Participant\&amp;quot;), eq(\&amp;quot;participant\&amp;quot;, \&amp;quot;k6\&amp;quot;), ({part_filter}))&amp;quot;, &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;When this also did not work. I got suspicious about whether there was a problem with my tagging so I checked my DB and found out it was also correct. The item below is tagged &amp;quot;Moderator&amp;quot; as you can see but it is retrieved when I use the filters above.&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/5x8krgis8ubd1.jpg?width=1080&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=8d193e29a4eef3213b7097c10b2ceab7da664bfa&quot;&gt;https://preview.redd.it/5x8krgis8ubd1.jpg?width=1080&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=8d193e29a4eef3213b7097c10b2ceab7da664bfa&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Weird part is I use exactly same syntax for other filters and they work. I am lost. Any ideas?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Select-Coconut-1161&quot;&gt; /u/Select-Coconut-1161 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e0iovb/filtering_based_on_metadata/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e0iovb/filtering_based_on_metadata/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1e0iovb</id><media:thumbnail url="https://external-preview.redd.it/UallS6eTqMQ6Q_VovzOpEJke6ZgDb6GzqRJtFsGug9s.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=82f0fba791dd0f75899e3dc5480f915fe8c0cafd" /><link href="https://www.reddit.com/r/LangChain/comments/1e0iovb/filtering_based_on_metadata/" /><updated>2024-07-11T07:02:33+00:00</updated><published>2024-07-11T07:02:33+00:00</published><title>Filtering based on Metadata</title></entry><entry><author><name>/u/NoChampionship7630</name><uri>https://www.reddit.com/user/NoChampionship7630</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;ve been having issues training my llm to be able to use tool during certain part of the conversaiton. Prior to fine-tuning the interaction with the agent is good, it calls all the necessary when needed, but I haven&amp;#39;t found a way to incorporate the tool usage in the training data correctly, I&amp;#39;m missing something... after training it simply doesn&amp;#39;t use any tools.... I&amp;#39;m currently using langgraph. Any suggestions woud be highly appreciated.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/NoChampionship7630&quot;&gt; /u/NoChampionship7630 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e0oqol/fine_tuning_llm_with_tool_usage/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e0oqol/fine_tuning_llm_with_tool_usage/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1e0oqol</id><link href="https://www.reddit.com/r/LangChain/comments/1e0oqol/fine_tuning_llm_with_tool_usage/" /><updated>2024-07-11T13:17:56+00:00</updated><published>2024-07-11T13:17:56+00:00</published><title>Fine Tuning LLM with tool Usage</title></entry><entry><author><name>/u/goddamnit_1</name><uri>https://www.reddit.com/user/goddamnit_1</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e03z0y/i_used_langchain_to_build_a_slack_agent_my/&quot;&gt; &lt;img src=&quot;https://external-preview.redd.it/u5rxHXqj1K-44fCKJCxS7ivB42knsXLiW4mFDtHvbdQ.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f4921d56599664bdd7e92e20b29f17d8100f8070&quot; alt=&quot;I used Langchain to build a Slack Agent - My Experience&quot; title=&quot;I used Langchain to build a Slack Agent - My Experience&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;My AI Agent does the following:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Instant answers from the web in any Slack channel&lt;/li&gt; &lt;li&gt;Code interpretation &amp;amp; execution on the fly&lt;/li&gt; &lt;li&gt;Smart web crawling for up-to-date info&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;project link : &lt;a href=&quot;http://git.new/slack-bot-agent-ollama&quot;&gt;git.new/slack-bot-agent-ollama&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;My experience with Langchain&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;One of the key advantages of Langchain is its ability to integrate different LLMs into your applications. This flexibility allows you to experiment with various models and find the one that best suits your needs.&lt;/p&gt; &lt;p&gt;Langchain&amp;#39;s approach is a game-changer. However, I do have one gripe - the documentation could be better. I wasn&amp;#39;t aware that I needed to use the ChatModels instead of the direct models, and this wasn&amp;#39;t specified clearly enough. This kind of information is crucial for users to get up and running quickly.&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/jjtezql9nqbd1.png?width=1283&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5707c453e8a1f1bc0f756b6821780603065dd4fb&quot;&gt;https://preview.redd.it/jjtezql9nqbd1.png?width=1283&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5707c453e8a1f1bc0f756b6821780603065dd4fb&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/goddamnit_1&quot;&gt; /u/goddamnit_1 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e03z0y/i_used_langchain_to_build_a_slack_agent_my/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e03z0y/i_used_langchain_to_build_a_slack_agent_my/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1e03z0y</id><media:thumbnail url="https://external-preview.redd.it/u5rxHXqj1K-44fCKJCxS7ivB42knsXLiW4mFDtHvbdQ.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=f4921d56599664bdd7e92e20b29f17d8100f8070" /><link href="https://www.reddit.com/r/LangChain/comments/1e03z0y/i_used_langchain_to_build_a_slack_agent_my/" /><updated>2024-07-10T19:03:37+00:00</updated><published>2024-07-10T19:03:37+00:00</published><title>I used Langchain to build a Slack Agent - My Experience</title></entry><entry><author><name>/u/New-Cryptographer131</name><uri>https://www.reddit.com/user/New-Cryptographer131</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I have been working with django and langchain for few days. What i have noticed is it’s not possible to save the langchain messages directly with the django ORM. So the integration seems bit confusing when trying to save some details using the django models and others using pydantic models for langchain. Is this a missing support by langchain or am I missing something ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/New-Cryptographer131&quot;&gt; /u/New-Cryptographer131 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e0nqxb/langchain_django_orm_integration/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e0nqxb/langchain_django_orm_integration/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1e0nqxb</id><link href="https://www.reddit.com/r/LangChain/comments/1e0nqxb/langchain_django_orm_integration/" /><updated>2024-07-11T12:28:07+00:00</updated><published>2024-07-11T12:28:07+00:00</published><title>Langchain + Django ORM integration</title></entry><entry><author><name>/u/BellaHi</name><uri>https://www.reddit.com/user/BellaHi</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e0dp2l/from_concept_to_execution_crafting_cuttingedge_ai/&quot;&gt; &lt;img src=&quot;https://external-preview.redd.it/mrFcgMwx0I5hbiBHVY0Y7RFxYzTqpONsp1DNMaJb2bo.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bbd4ab5ae934c2107af3adc3067699bb73f680f8&quot; alt=&quot;From Concept to Execution: Crafting Cutting-Edge AI Agents with LangChain&quot; title=&quot;From Concept to Execution: Crafting Cutting-Edge AI Agents with LangChain&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/BellaHi&quot;&gt; /u/BellaHi &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://myscale.com/blog/build-ai-agent-with-langchain/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e0dp2l/from_concept_to_execution_crafting_cuttingedge_ai/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1e0dp2l</id><media:thumbnail url="https://external-preview.redd.it/mrFcgMwx0I5hbiBHVY0Y7RFxYzTqpONsp1DNMaJb2bo.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=bbd4ab5ae934c2107af3adc3067699bb73f680f8" /><link href="https://www.reddit.com/r/LangChain/comments/1e0dp2l/from_concept_to_execution_crafting_cuttingedge_ai/" /><updated>2024-07-11T02:09:43+00:00</updated><published>2024-07-11T02:09:43+00:00</published><title>From Concept to Execution: Crafting Cutting-Edge AI Agents with LangChain</title></entry><entry><author><name>/u/md1630</name><uri>https://www.reddit.com/user/md1630</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;m trying to create a streaming agent chatbot with streamlit as the frontend. I was able to find an example of this using callbacks, and streamlit even has a special callback class.&lt;/p&gt; &lt;p&gt;However, it looks like things sure change quickly. From langchain&amp;#39;s documentation it looks like callbacks is being deprecated, and there is a new function astream_events. I&amp;#39;m very happy with how simple it is to stream events with astream_events.&lt;/p&gt; &lt;p&gt;However, I&amp;#39;m having some issues with getting this to work with streamlit. It&amp;#39;s mostly working, but for some small details. For example:&lt;/p&gt; &lt;pre&gt;&lt;code&gt; async for event in agent_executor.astream_events( {&amp;quot;input&amp;quot;: user_query}, version=&amp;quot;v2&amp;quot;, config=cfg ): kind = event[&amp;quot;event&amp;quot;] if kind == &amp;quot;on_chat_model_stream&amp;quot;: content = event[&amp;quot;data&amp;quot;][&amp;quot;chunk&amp;quot;].content if content: answer_container.write(content) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;There are other events, but I can illustrate this problem with just &amp;quot;on_chat_model_stream&amp;quot;. &lt;/p&gt; &lt;p&gt;Just focusing on the event &amp;quot;on_chat_model_stream&amp;quot;, this causes the content to be written one word in every line, so the chat message looks like:&lt;/p&gt; &lt;p&gt;Hello&lt;/p&gt; &lt;p&gt;how&lt;/p&gt; &lt;p&gt;may&lt;/p&gt; &lt;p&gt;I&lt;/p&gt; &lt;p&gt;help&lt;/p&gt; &lt;p&gt;you&lt;/p&gt; &lt;p&gt;So, it looks like the content is being written token by token. However, I can&amp;#39;t use write_stream, because I would get the error ``st.write_stream` expects a generator or stream-like object as input not &amp;lt;class &amp;#39;str&amp;#39;&amp;gt;. Please use `st.write` instead for this data type.`&lt;/p&gt; &lt;p&gt;How do I get the content to stream in this case?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/md1630&quot;&gt; /u/md1630 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e0ezhj/how_to_use_agentexecutorstream_events_with/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e0ezhj/how_to_use_agentexecutorstream_events_with/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1e0ezhj</id><link href="https://www.reddit.com/r/LangChain/comments/1e0ezhj/how_to_use_agentexecutorstream_events_with/" /><updated>2024-07-11T03:16:34+00:00</updated><published>2024-07-11T03:16:34+00:00</published><title>how to use agentexecutor.stream_events with streamlit</title></entry><entry><author><name>/u/stoic-AI</name><uri>https://www.reddit.com/user/stoic-AI</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi everyone!&lt;/p&gt; &lt;p&gt;I&amp;#39;ve created a mini series on how to build a real time AI application using Django, LangChain and Celery.&lt;/p&gt; &lt;p&gt;Free knowledge - posting it in here for anyone working on something similar and had the same blockers I had when building.&lt;/p&gt; &lt;p&gt;Let me know what you think on how I could potentially improve this architecture.&lt;/p&gt; &lt;p&gt;Part 1&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://medium.com/towardsdev/how-to-set-up-django-from-scratch-with-celery-channels-redis-docker-real-time-django-601dff7ada79&quot;&gt;https://medium.com/towardsdev/how-to-set-up-django-from-scratch-with-celery-channels-redis-docker-real-time-django-601dff7ada79&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Part 2&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://medium.com/towardsdev/how-to-set-up-django-from-scratch-with-celery-channels-redis-docker-real-time-django-5828a1ea43a3&quot;&gt;https://medium.com/towardsdev/how-to-set-up-django-from-scratch-with-celery-channels-redis-docker-real-time-django-5828a1ea43a3&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Part 3&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://medium.com/towardsdev/how-to-set-up-django-from-scratch-with-celery-channels-redis-docker-real-time-django-8e73c7b6b4c8&quot;&gt;https://medium.com/towardsdev/how-to-set-up-django-from-scratch-with-celery-channels-redis-docker-real-time-django-8e73c7b6b4c8&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Part 4&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://medium.com/towardsdev/how-to-set-up-django-from-scratch-with-celery-channels-redis-docker-real-time-django-c090c300517a&quot;&gt;https://medium.com/towardsdev/how-to-set-up-django-from-scratch-with-celery-channels-redis-docker-real-time-django-c090c300517a&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Part 5&lt;br/&gt; &lt;a href=&quot;https://medium.com/@cubode/how-to-set-up-django-from-scratch-with-celery-channels-redis-docker-real-time-django-0845eb7e083c&quot;&gt;https://medium.com/@cubode/how-to-set-up-django-from-scratch-with-celery-channels-redis-docker-real-time-django-0845eb7e083c&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/stoic-AI&quot;&gt; /u/stoic-AI &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dzs2p4/real_time_ai_workers_using_django_x_langchain/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dzs2p4/real_time_ai_workers_using_django_x_langchain/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dzs2p4</id><link href="https://www.reddit.com/r/LangChain/comments/1dzs2p4/real_time_ai_workers_using_django_x_langchain/" /><updated>2024-07-10T10:07:51+00:00</updated><published>2024-07-10T10:07:51+00:00</published><title>Real Time AI Workers using Django x LangChain</title></entry><entry><author><name>/u/Pure-Exercise-9955</name><uri>https://www.reddit.com/user/Pure-Exercise-9955</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hello everyone,&lt;/p&gt; &lt;p&gt;I have to build a Retrieval-Augmented Generation (RAG) system using a Large Language Model (LLM) &lt;strong&gt;to search for an information about an employee within a vast collection of documents.&lt;/strong&gt; The LLM must return the information with references indicating the specific page and document.&lt;/p&gt; &lt;p&gt;if someone already done a project similar or can help me with the steps.&lt;/p&gt; &lt;p&gt;what i decided to do is to select an open source llm (qwen2), then fine tune it, after that build a RAG&lt;/p&gt; &lt;p&gt;&lt;strong&gt;what is your opinion gays&lt;/strong&gt;&lt;br/&gt; &lt;strong&gt;i need your help&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Pure-Exercise-9955&quot;&gt; /u/Pure-Exercise-9955 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e02kx2/rag_to_search_for_an_information_about_an/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e02kx2/rag_to_search_for_an_information_about_an/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1e02kx2</id><link href="https://www.reddit.com/r/LangChain/comments/1e02kx2/rag_to_search_for_an_information_about_an/" /><updated>2024-07-10T18:07:58+00:00</updated><published>2024-07-10T18:07:58+00:00</published><title>RAG to search for an information about an employee within a vast collection of documents</title></entry><entry><author><name>/u/Typical-Scene-5794</name><uri>https://www.reddit.com/user/Typical-Scene-5794</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi &lt;a href=&quot;/r/langchain&quot;&gt;r/langchain&lt;/a&gt;, I&amp;#39;m sharing an example on building a multi-modal search application using GPT-4o, featuring extraction of metadata and hybrid indexing for accurately retrieving relevant information from presentations.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Repo: &lt;a href=&quot;https://github.com/pathwaycom/llm-app/tree/main/examples/pipelines/slides_ai_search&quot;&gt;~https://github.com/pathwaycom/llm-app/tree/main/examples/pipelines/slides_ai_search~&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Architecture: &lt;a href=&quot;https://github.com/pathwaycom/llm-app/tree/main/examples/pipelines/slides_ai_search#architecture&quot;&gt;~https://github.com/pathwaycom/llm-app/tree/main/examples/pipelines/slides_ai_search#architecture~&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This project also focuses on automatically updating indexes as changes happen in your repository. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Quick details:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Ingestion:&lt;/strong&gt; The application reads slide files (PPTX and PDF) stored locally or on Google Drive or Microsoft SharePoint.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Parsing:&lt;/strong&gt; Utilizes the SlideParser from Pathway, configured with a detailed schema. The app parses images, charts, diagrams, and other visual elements as well, and features automatic unstructured metadata extraction. &lt;/li&gt; &lt;li&gt;&lt;strong&gt;Indexing:&lt;/strong&gt; Parsed slide content is embedded using OpenAI&amp;#39;s embedder and stored in Pathway&amp;#39;s vector store (&lt;a href=&quot;https://python.langchain.com/v0.2/docs/integrations/vectorstores/pathway/&quot;&gt;natively available on LangChain&lt;/a&gt;) that is optimized for incremental indexing.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;How it helps:&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Text in presentations is often limited. This example removes the need to manually sift through countless presentations by recalling keywords.&lt;/li&gt; &lt;li&gt;Organize your slide library by topic or other criteria. Indexes update automatically whenever a slide is added, modified, or removed.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;Preliminary Results:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;This method has proven to be efficient in managing large volumes of slides, ensuring that the most up-to-date and accurate information is available. It significantly enhances productivity by streamlining the search process across PowerPoints, PDFs, and Slides.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Open to your questions and feedback!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Typical-Scene-5794&quot;&gt; /u/Typical-Scene-5794 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dzzrw5/accurate_multimodal_slides_search_with_realtime/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dzzrw5/accurate_multimodal_slides_search_with_realtime/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dzzrw5</id><link href="https://www.reddit.com/r/LangChain/comments/1dzzrw5/accurate_multimodal_slides_search_with_realtime/" /><updated>2024-07-10T16:15:22+00:00</updated><published>2024-07-10T16:15:22+00:00</published><title>Accurate Multimodal Slides Search with Real-Time Updates from SharePoint, Google Drive, and Local Data Sources</title></entry><entry><author><name>/u/neilkatz</name><uri>https://www.reddit.com/user/neilkatz</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey All,&lt;/p&gt; &lt;p&gt;I&amp;#39;m curious what everyone is using to parse complex PDFs, extract the data and turn it into something LLMs can better comprehend.&lt;/p&gt; &lt;p&gt;Is there something that can consistently find tables, forms, charts, graphics that we see in many enterprise documents. It seems without this step, RAG hallucinations are a significant issue. &lt;/p&gt; &lt;p&gt;Much appreciated. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/neilkatz&quot;&gt; /u/neilkatz &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dzj5qx/best_pdf_parser_for_rag/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dzj5qx/best_pdf_parser_for_rag/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dzj5qx</id><link href="https://www.reddit.com/r/LangChain/comments/1dzj5qx/best_pdf_parser_for_rag/" /><updated>2024-07-10T01:13:19+00:00</updated><published>2024-07-10T01:13:19+00:00</published><title>Best PDF Parser for RAG?</title></entry><entry><author><name>/u/No_Entrepreneur7665</name><uri>https://www.reddit.com/user/No_Entrepreneur7665</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;What’s the best tool to make a UI for an agent that interacts with the user as it processes tgrough the graph. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/No_Entrepreneur7665&quot;&gt; /u/No_Entrepreneur7665 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e02blv/ui_for_langgraph_for_agent_with_user_input/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e02blv/ui_for_langgraph_for_agent_with_user_input/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1e02blv</id><link href="https://www.reddit.com/r/LangChain/comments/1e02blv/ui_for_langgraph_for_agent_with_user_input/" /><updated>2024-07-10T17:58:06+00:00</updated><published>2024-07-10T17:58:06+00:00</published><title>UI for langgraph for agent with user input</title></entry><entry><author><name>/u/Bivee2000</name><uri>https://www.reddit.com/user/Bivee2000</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I want to connect my langchain to Snowflake db. But the snowflake I am using have no password but SSO. What is the syntax should be used to connect the snowflake to langchain using the following function SQLDatabase.from_uri?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Bivee2000&quot;&gt; /u/Bivee2000 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e00w6o/connecting_langchain_to_snowflake_db_with_sso/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e00w6o/connecting_langchain_to_snowflake_db_with_sso/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1e00w6o</id><link href="https://www.reddit.com/r/LangChain/comments/1e00w6o/connecting_langchain_to_snowflake_db_with_sso/" /><updated>2024-07-10T17:00:15+00:00</updated><published>2024-07-10T17:00:15+00:00</published><title>Connecting Langchain to Snowflake DB with SSO authentication</title></entry><entry><author><name>/u/External_Ad_11</name><uri>https://www.reddit.com/user/External_Ad_11</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;The video tutorial covers:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;How to use LLM from HuggingFaceHub without even loading it.&lt;br/&gt;&lt;/li&gt; &lt;li&gt;Prompt Template for Open Source LLM&lt;br/&gt;&lt;/li&gt; &lt;li&gt;Converting normal text to Langchain schema&lt;br/&gt;&lt;/li&gt; &lt;li&gt;Vector Database and embedding important functions&lt;br/&gt;&lt;/li&gt; &lt;li&gt;Memory&lt;br/&gt;&lt;/li&gt; &lt;li&gt;LCEL &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Langchain Zero to LCEL: &lt;a href=&quot;https://www.youtube.com/watch?v=TWmV95-dUgQ&quot;&gt;https://www.youtube.com/watch?v=TWmV95-dUgQ&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/External_Ad_11&quot;&gt; /u/External_Ad_11 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dzuxak/langchain_ultimate_guide_using_open_source_llms/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dzuxak/langchain_ultimate_guide_using_open_source_llms/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dzuxak</id><link href="https://www.reddit.com/r/LangChain/comments/1dzuxak/langchain_ultimate_guide_using_open_source_llms/" /><updated>2024-07-10T12:47:48+00:00</updated><published>2024-07-10T12:47:48+00:00</published><title>Langchain Ultimate Guide using Open Source LLMs</title></entry><entry><author><name>/u/coolcloud</name><uri>https://www.reddit.com/user/coolcloud</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dzfp48/agent_retrieval_how_we_almost_always_find_the/&quot;&gt; &lt;img src=&quot;https://b.thumbs.redditmedia.com/Pb2SWSeA1tyEw8SKW3E2DTFD13TWyENVp-4WWyttigA.jpg&quot; alt=&quot;Agent Retrieval - How we almost always find the right vectors. Pt 3&quot; title=&quot;Agent Retrieval - How we almost always find the right vectors. Pt 3&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey all - &lt;/p&gt; &lt;p&gt;Today I wanted to run through how we narrow down our vector space.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Issues with vector search only:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;If you have used vector search over a large corpus of documents, you&amp;#39;ll know vector search doesn&amp;#39;t work well.&lt;/li&gt; &lt;li&gt;Almost 100% of the time someone is using RAG, they are looking for something specific. &lt;ul&gt; &lt;li&gt; Example: If you use vector search on a name, most names will come back, regardless of it&amp;#39;s Bob Smith, or Sally Blu. This is bad if I just want to find Sally Blu.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;W&lt;strong&gt;hat are we doing?&lt;/strong&gt; &lt;/p&gt; &lt;ol&gt; &lt;li&gt;We split each doc up into the smallest possible vector (normally sentences) We&amp;#39;ve found this is the best, and most accurate for vector similarity. &lt;/li&gt; &lt;li&gt;NER/Keyword extraction from the query&lt;/li&gt; &lt;li&gt; Search docs for keywords/NER&lt;/li&gt; &lt;li&gt;Vector search query within the docs that are returned.&lt;/li&gt; &lt;li&gt; Traditionally top 20 results (no similarity score min)&lt;/li&gt; &lt;li&gt;Reconstruct the docs into headers etc.&lt;/li&gt; &lt;li&gt;Reranker Jina - top 10 results (over .x similarity)&lt;/li&gt; &lt;li&gt;Each result sent to an LLM for quotes&lt;/li&gt; &lt;li&gt;combine all into prompt #2&lt;/li&gt; &lt;li&gt;LLM answer&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Today we&amp;#39;ll primarily talk through step 2-7.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;How?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;Example query:&lt;/em&gt; Does Sally blu work at tada?&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Use an LLM to extract named entities and key words/phrases from the query. &lt;/li&gt; &lt;li&gt;Search the entire document to see if &amp;quot;all&amp;quot; keywords/NER match the doc. [&amp;quot;Tada&amp;quot; and &amp;quot;Sally Blu&amp;quot;]&lt;/li&gt; &lt;li&gt;If there&amp;#39;s no matches, do an &amp;quot;or&amp;quot; search for keywords/NER [&amp;quot;Tada&amp;quot; or &amp;quot;Sally Blu&amp;quot;]&lt;/li&gt; &lt;li&gt;If there&amp;#39;s no matches, don&amp;#39;t return a doc.&lt;/li&gt; &lt;li&gt;If there are matches in either step 2 or 3, return those docs only and do vector search within only those documents. &lt;/li&gt; &lt;/ol&gt; &lt;p&gt;This significantly limits the scope of the vector space and based on our experiences almost never filters out the documents that are important to answering the question.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;How are we able to search an entire doc?&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;This process wouldn&amp;#39;t be possible without out our document structure, so here&amp;#39;s a link &amp;amp; a quick overview of how we how we &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dpbc4g/how_we_chunk_turning_pdfs_into_hierarchical/&quot;&gt;chunk docs. &lt;/a&gt;&lt;/p&gt; &lt;p&gt;TLDR: We extract and save the document structure into a hieratical format, headers, sub-headers, list, paragraphs, tables, etc. Because we do this, we can easily search the entire document. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Diagram of our first pass search&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/jczepvzejkbd1.png?width=365&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f7818f84954b2486793cb012f02b7097b988859c&quot;&gt;https://preview.redd.it/jczepvzejkbd1.png?width=365&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f7818f84954b2486793cb012f02b7097b988859c&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Example Query:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;/u/warriorA&quot;&gt;u/warriorA&lt;/a&gt; asked this question a couple of days ago in another post. It&amp;#39;s simple so we&amp;#39;ll re-use it:&lt;/p&gt; &lt;p&gt;______&lt;/p&gt; &lt;p&gt;Consider the query: &amp;quot;How many Presidents did we have in America?&amp;quot;&lt;/p&gt; &lt;p&gt;Now we might have a document chunk with this information:&lt;/p&gt; &lt;ol&gt; &lt;li&gt; doc_1_chunk: &amp;quot;United States has been governed by a total of 46 people&amp;quot;&lt;br/&gt;&lt;/li&gt; &lt;li&gt; doc_2_chunk: &amp;quot;The USA is a country in north america.&amp;quot;&lt;br/&gt;&lt;/li&gt; &lt;li&gt; doc_3_chunk: &amp;quot;We&amp;#39;ve had 1 President in &amp;#39;Random-Country&amp;#39;.&amp;quot;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Wouldn&amp;#39;t your search fail?&lt;/p&gt; &lt;p&gt;Note - (I made a few small edits for example purposes)&lt;/p&gt; &lt;p&gt;______&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Would our system work for this use case?&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;Most likely, yes. &lt;/p&gt; &lt;p&gt;From the query we&amp;#39;d extract: [&amp;quot;Presidents&amp;quot; &amp;amp; &amp;quot;America&amp;quot;] &lt;/p&gt; &lt;p&gt;Again, we search the entire document, not just the chunks to find hits.&lt;/p&gt; &lt;p&gt;✅ Doc_1: It&amp;#39;s very likely that doc_1 would contain both the word president and America, meaning that document would come back. &lt;/p&gt; &lt;p&gt;❌ Doc_2: Isn&amp;#39;t talking about Presidents, thus it wouldn&amp;#39;t come back.&lt;/p&gt; &lt;p&gt;✅or ❌ Doc_3: Would most likely not come back as it&amp;#39;s not talking about America. (If it did come back because America was in the document somewhere, vector search + rerankers would help filter it out.)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;If we extended chunk one:&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;here&amp;#39;s an example of what it would look like and it contains both the word president &amp;amp; america.&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/zwuhtvrdkkbd1.png?width=1047&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9a62623c04a8d1836262d3bd6c0ba3dbe00f91f0&quot;&gt;https://preview.redd.it/zwuhtvrdkkbd1.png?width=1047&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9a62623c04a8d1836262d3bd6c0ba3dbe00f91f0&lt;/a&gt;&lt;/p&gt; &lt;p&gt;If we didn&amp;#39;t do this step, it&amp;#39;s likely all 3 chunks would come back, and doc_3_chunk, would be rated the highest. You could imagine if you had hundreds or thousands of documents the most important vectors to answer the question may not show up at all.&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/cr6jmxuwkkbd1.png?width=405&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=76c747237adb98fb3ba6c78ff491c04ac05bdecc&quot;&gt;https://preview.redd.it/cr6jmxuwkkbd1.png?width=405&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=76c747237adb98fb3ba6c78ff491c04ac05bdecc&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;how do we extract NER?&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;We have found the NER models aren&amp;#39;t consistent enough, you have to use an LLM. If you ask a question like &amp;quot;what are the terms of the wings contract&amp;quot; a NER model may see no named entities, where an LLM would understand the named entity is &amp;quot;wings&amp;quot;.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Prompt:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Respond with valid json. &lt;/p&gt; &lt;p&gt;You will receive text, your goal is to: &lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Identify potential search phrases. Put those in the &amp;quot;P&amp;quot; field. For example, in &amp;quot;When was the Huck Finn contract signed?&amp;quot;, the main concept is &amp;quot;Huck Finn contract&amp;quot; &lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Identify named entities. Put those in the &amp;quot;N&amp;quot; field. For example: &amp;quot;Huck Finn&amp;quot; or &amp;quot;Apple A7&amp;quot; Emit a valid JSON object with a single &amp;quot;N&amp;quot; field and a single &amp;quot;P&amp;quot; field. &lt;/p&gt; &lt;p&gt;Example &lt;/p&gt; &lt;p&gt;Input: When was the Huck Finn contract signed? &lt;/p&gt; &lt;p&gt;Output: &lt;/p&gt; &lt;p&gt;{ &lt;/p&gt; &lt;p&gt;&amp;quot;P&amp;quot;: [&amp;quot;Huck Finn contract&amp;quot;], &lt;/p&gt; &lt;p&gt;&amp;quot;N&amp;quot;: [&amp;quot;Huck Finn&amp;quot;],&lt;/p&gt; &lt;p&gt;} &lt;/p&gt; &lt;p&gt;Input: does share and perform offer a performance engagement tool &lt;/p&gt; &lt;p&gt;Output: &lt;/p&gt; &lt;p&gt;{ &lt;/p&gt; &lt;p&gt;&amp;quot;P&amp;quot;: [&amp;quot;share and perform performance engagement&amp;quot;], &lt;/p&gt; &lt;p&gt;&amp;quot;N&amp;quot;: [&amp;quot;share and perform&amp;quot;],&lt;/p&gt; &lt;p&gt;} &lt;/p&gt; &lt;p&gt;Input: how many processors in the apple a7? &lt;/p&gt; &lt;p&gt;{ &lt;/p&gt; &lt;p&gt;&amp;quot;P&amp;quot;: [&amp;quot;apple a7 processors&amp;quot;], &lt;/p&gt; &lt;p&gt;&amp;quot;N&amp;quot;: [&amp;quot;Apple&amp;quot;, &amp;quot;Apple A7&amp;quot;], &lt;/p&gt; &lt;p&gt;} &lt;/p&gt; &lt;p&gt;Input: How much is the wings contract Output: &lt;/p&gt; &lt;p&gt;{ &amp;quot;P&amp;quot;: [&amp;quot;wings contract&amp;quot;],&lt;/p&gt; &lt;p&gt;&amp;quot;N&amp;quot;: [&amp;quot;wings&amp;quot;], &lt;/p&gt; &lt;p&gt;} &lt;/p&gt; &lt;p&gt;Input: what are the different tiers of wotc? &lt;/p&gt; &lt;p&gt;Output: &lt;/p&gt; &lt;p&gt;{&lt;/p&gt; &lt;p&gt;&amp;quot;P&amp;quot;: [&amp;quot;wotc tiers&amp;quot;], &lt;/p&gt; &lt;p&gt;&amp;quot;N&amp;quot;: [&amp;quot;WOTC&amp;quot;], &lt;/p&gt; &lt;p&gt;} &lt;/p&gt; &lt;p&gt;Input: what is included in the QuickBooks General Journal report Output: &lt;/p&gt; &lt;p&gt;{&lt;/p&gt; &lt;p&gt;&amp;quot;P&amp;quot;: [&amp;quot;QuickBooks General Journal report&amp;quot;], &lt;/p&gt; &lt;p&gt;&amp;quot;N&amp;quot;: [&amp;quot;Quickbooks&amp;quot;], &lt;/p&gt; &lt;p&gt;} &lt;/p&gt; &lt;p&gt;Example &amp;quot;&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Why did we decide on this prompt? &lt;/p&gt; &lt;ul&gt; &lt;li&gt;We use P &amp;amp; N because the less tokens than doing something like named entities &amp;amp; keywords. This mean there&amp;#39;s less tokens the LLM needs to return. (The average response time is 1.1 seconds.)&lt;/li&gt; &lt;li&gt;We found you need a large example set for the LLM to understand what you&amp;#39;re trying to do.&lt;/li&gt; &lt;li&gt;We recommend tuning these prompts to questions that your customer may similarly ask&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Next step:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;After finding the top 20 vectors, we re-construct the document. Because re-rankers tend to work better, and we are giving them additional context, we&amp;#39;ve found that we almost always return the most relevant chunks to answer the question. Here&amp;#39;s our&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dtr49t/agent_rag_parallel_quotes_how_we_built_rag_on/&quot;&gt; article&lt;/a&gt; for going from vectors to search.&lt;/p&gt; &lt;p&gt;Happy to answer any and all questions!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/coolcloud&quot;&gt; /u/coolcloud &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dzfp48/agent_retrieval_how_we_almost_always_find_the/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dzfp48/agent_retrieval_how_we_almost_always_find_the/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1dzfp48</id><media:thumbnail url="https://b.thumbs.redditmedia.com/Pb2SWSeA1tyEw8SKW3E2DTFD13TWyENVp-4WWyttigA.jpg" /><link href="https://www.reddit.com/r/LangChain/comments/1dzfp48/agent_retrieval_how_we_almost_always_find_the/" /><updated>2024-07-09T22:35:02+00:00</updated><published>2024-07-09T22:35:02+00:00</published><title>Agent Retrieval - How we almost always find the right vectors. Pt 3</title></entry><entry><author><name>/u/giagara</name><uri>https://www.reddit.com/user/giagara</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I personally host my app in aws using lambda for compute, s3 for storage and rds (postgres) for vector db. There are some sqs, dynamo, etc but are for statistic purpose.&lt;/p&gt; &lt;p&gt;Edit: i mean for commercial purpose, not just personal &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/giagara&quot;&gt; /u/giagara &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dzord2/where_do_you_host_your_rag/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dzord2/where_do_you_host_your_rag/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dzord2</id><link href="https://www.reddit.com/r/LangChain/comments/1dzord2/where_do_you_host_your_rag/" /><updated>2024-07-10T06:22:28+00:00</updated><published>2024-07-10T06:22:28+00:00</published><title>Where do you host your Rag</title></entry><entry><author><name>/u/Ukpersfidev</name><uri>https://www.reddit.com/user/Ukpersfidev</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;m trying to get Claude to generate content that could be up to 8,000 tokens long.&lt;/p&gt; &lt;p&gt;To overcome the max token output limitations, I have specifically prompted Claude to stop generating when the max token limit is reached, and then continue exactly where you left off when the user responds with a &amp;quot;continue&amp;quot; message.&lt;/p&gt; &lt;p&gt;Example of the prompt:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;const prompt = ` Generate me an adventure story that is between 6000 - 7000 words long If you reach the max token output, the user will send a message saying &amp;quot;continue&amp;quot; Before continuing, you should check the previous outputs and start exactly where you left off so that the output is coherent and consistent. You should not ackowledge the &amp;quot;continue&amp;quot; message in the output, just continue generating the content. We will join the outputs together at the end and they should form a coherent and consistent response. If you have finished generating the content and the user asks you to continue, you should just respond with an empty message. For example: Assistant: mess User: continue Assistant: age ` &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The problem is that it won&amp;#39;t do as I ask, it just keeps shortening the story, I find this weird because when I change the max tokens to something like, 50 and then change the requested story to have a length of 200 words, it works fine for multiple continuations.&lt;/p&gt; &lt;p&gt;Anyone faced this before?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Ukpersfidev&quot;&gt; /u/Ukpersfidev &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dzxzu0/how_are_you_managing_long_form_content/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dzxzu0/how_are_you_managing_long_form_content/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dzxzu0</id><link href="https://www.reddit.com/r/LangChain/comments/1dzxzu0/how_are_you_managing_long_form_content/" /><updated>2024-07-10T15:03:07+00:00</updated><published>2024-07-10T15:03:07+00:00</published><title>How are you managing long form content?</title></entry><entry><author><name>/u/NoDance9749</name><uri>https://www.reddit.com/user/NoDance9749</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hello, what are the most used serveless vector/graph databases for LLM RAG usage, with convenient integration in Python and AWS (Knowledge base in Bedrock)? Ideally which are PAYG. I wanted to use OpenSearch from AWS, but I don&amp;#39;t wanna pay 700 USD/month though and other solutions don&amp;#39;t seem to be serverless at all.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/NoDance9749&quot;&gt; /u/NoDance9749 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dzrcfl/serverless_vectorgraph_database_for_rag/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dzrcfl/serverless_vectorgraph_database_for_rag/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dzrcfl</id><link href="https://www.reddit.com/r/LangChain/comments/1dzrcfl/serverless_vectorgraph_database_for_rag/" /><updated>2024-07-10T09:18:55+00:00</updated><published>2024-07-10T09:18:55+00:00</published><title>Serverless Vector/Graph Database for RAG</title></entry></feed>