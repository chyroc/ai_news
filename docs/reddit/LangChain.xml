<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/"><category term="LangChain" label="r/LangChain"/><updated>2024-04-26T18:19:59+00:00</updated><icon>https://www.redditstatic.com/icon.png/</icon><id>/r/LangChain.rss</id><link rel="self" href="https://www.reddit.com/r/LangChain.rss" type="application/atom+xml" /><link rel="alternate" href="https://www.reddit.com/r/LangChain" type="text/html" /><subtitle>LangChain is an open-source framework and developer toolkit that helps developers get LLM applications from prototype to production. It is available for Python and Javascript at https://www.langchain.com/.</subtitle><title>LangChain</title><entry><author><name>/u/zchaarm</name><uri>https://www.reddit.com/user/zchaarm</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;A place for members of &lt;a href=&quot;/r/LangChain&quot;&gt;r/LangChain&lt;/a&gt; to chat with each other&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/zchaarm&quot;&gt; /u/zchaarm &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_10ljho9</id><link href="https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/" /><updated>2023-01-26T04:33:05+00:00</updated><published>2023-01-26T04:33:05+00:00</published><title>r/LangChain Lounge</title></entry><entry><author><name>/u/QueRoub</name><uri>https://www.reddit.com/user/QueRoub</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Are there any resources about RAG application that uses as knowledge base either excel files or Databases? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/QueRoub&quot;&gt; /u/QueRoub &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cdmqk8/any_resources_for_rag_with_excel_files_or/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cdmqk8/any_resources_for_rag_with_excel_files_or/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cdmqk8</id><link href="https://www.reddit.com/r/LangChain/comments/1cdmqk8/any_resources_for_rag_with_excel_files_or/" /><updated>2024-04-26T14:23:55+00:00</updated><published>2024-04-26T14:23:55+00:00</published><title>Any resources for RAG with excel files or Databases?</title></entry><entry><author><name>/u/Just_Guide7361</name><uri>https://www.reddit.com/user/Just_Guide7361</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey, I am trying to build a RAG Q&amp;amp;A chain, with memory (chat history). While the invoke function works perfectly fine and allows me to extract the answer, the stream does not. I&amp;#39;ve followed the documentation: &lt;a href=&quot;https://python.langchain.com/docs/use_cases/question_answering/chat_history/#tying-it-together&quot;&gt;https://python.langchain.com/docs/use_cases/question_answering/chat_history/#tying-it-together&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;The only change is as follows:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;# This works perfectly fine: conversational_rag_chain.invoke( {&amp;quot;input&amp;quot;: &amp;quot;What is Task Decomposition 2?&amp;quot;}, config={&amp;quot;configurable&amp;quot;: {&amp;quot;session_id&amp;quot;: &amp;quot;abc123&amp;quot;}}, # constructs a key &amp;quot;abc123&amp;quot; in `store`. )[&amp;#39;answer&amp;#39;] &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;# This does not work - it streams back everything and i can not extract the answer for chuck in conversational_rag_chain.stream( {&amp;quot;input&amp;quot;: &amp;quot;What is Task Decomposition 2?&amp;quot;}, config={&amp;quot;configurable&amp;quot;: {&amp;quot;session_id&amp;quot;: &amp;quot;abc123&amp;quot;}}, # constructs a key &amp;quot;abc123&amp;quot; in `store`. ): print(chuck) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;# I have also tried the following but none works; print(chuck[&amp;#39;answer&amp;#39;]) print(chuck.content) print(chuck.content[&amp;#39;answer&amp;#39;]) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Any suggestion or ideas on how to make this work? Seems like very normal behaviour to expect from a stream function?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Just_Guide7361&quot;&gt; /u/Just_Guide7361 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cdmey2/how_to_make_streaming_work_with_a_rag_qa_chain/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cdmey2/how_to_make_streaming_work_with_a_rag_qa_chain/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cdmey2</id><link href="https://www.reddit.com/r/LangChain/comments/1cdmey2/how_to_make_streaming_work_with_a_rag_qa_chain/" /><updated>2024-04-26T14:10:41+00:00</updated><published>2024-04-26T14:10:41+00:00</published><title>How to make streaming work with a RAG Q&amp;A chain with memory</title></entry><entry><author><name>/u/ArcuisAlezanzo</name><uri>https://www.reddit.com/user/ArcuisAlezanzo</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Anyone can provide good explanations or articles of creating custom agent ? I&amp;#39;m looking for Creating agent using agent class so we can control agent finish and agent action .&lt;/p&gt; &lt;p&gt;Sub question:&lt;/p&gt; &lt;p&gt;How tool calling automatically break complex question into subs questions ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/ArcuisAlezanzo&quot;&gt; /u/ArcuisAlezanzo &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cdi80s/agents_guide/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cdi80s/agents_guide/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cdi80s</id><link href="https://www.reddit.com/r/LangChain/comments/1cdi80s/agents_guide/" /><updated>2024-04-26T10:46:57+00:00</updated><published>2024-04-26T10:46:57+00:00</published><title>Agents guide</title></entry><entry><author><name>/u/ava69_open</name><uri>https://www.reddit.com/user/ava69_open</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Is there a way to throw one prompt at all the big LLMs (GPT-3, Bard, you name it) and see their responses side-by-side? I know LangChain might be an option for local development, but I was wondering if there are any existing tools out there.&lt;/p&gt; &lt;p&gt;Imagine the time saved! No more copy-pasting the same prompt across different platforms just to compare answers and check accuracy. Anyone else feeling this struggle?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/ava69_open&quot;&gt; /u/ava69_open &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cdawva/tool_to_compare_llm_outputs/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cdawva/tool_to_compare_llm_outputs/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cdawva</id><link href="https://www.reddit.com/r/LangChain/comments/1cdawva/tool_to_compare_llm_outputs/" /><updated>2024-04-26T03:09:59+00:00</updated><published>2024-04-26T03:09:59+00:00</published><title>Tool to compare LLM Outputs</title></entry><entry><author><name>/u/UpskillingDS17</name><uri>https://www.reddit.com/user/UpskillingDS17</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi, I have a pdf where some Return in % is under 4 categories such as A, B and so on. When I ask question using Llama3 it is returning the correct answer but it is picking the Return from A rather than knowing from which category the Return should be picked from? How can I make LLM return the output saying which category rather than picking the answer from category A ? Thanks&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/UpskillingDS17&quot;&gt; /u/UpskillingDS17 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cdm2tx/how_to_make_llm_return_question_to_be_more/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cdm2tx/how_to_make_llm_return_question_to_be_more/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cdm2tx</id><link href="https://www.reddit.com/r/LangChain/comments/1cdm2tx/how_to_make_llm_return_question_to_be_more/" /><updated>2024-04-26T13:57:01+00:00</updated><published>2024-04-26T13:57:01+00:00</published><title>How to make LLM return question to be more specific rather than throwing output?</title></entry><entry><author><name>/u/UpskillingDS17</name><uri>https://www.reddit.com/user/UpskillingDS17</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hello, I was trying RAG using Llama3 and the input prompt is different as compared to other LLMs. How I can know which kind of prompts work best for a specific LLM as prompt used in LLama3 was very different and uses &amp;lt;eos&amp;gt; and etc etc. Is there any template for different LLMs?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/UpskillingDS17&quot;&gt; /u/UpskillingDS17 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cdisrq/how_to_stay_up_to_date_with_prompts_for_llms/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cdisrq/how_to_stay_up_to_date_with_prompts_for_llms/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cdisrq</id><link href="https://www.reddit.com/r/LangChain/comments/1cdisrq/how_to_stay_up_to_date_with_prompts_for_llms/" /><updated>2024-04-26T11:19:35+00:00</updated><published>2024-04-26T11:19:35+00:00</published><title>How to stay up to date with prompts for LLMs</title></entry><entry><author><name>/u/ravediamond000</name><uri>https://www.reddit.com/user/ravediamond000</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hello everyone, &lt;/p&gt; &lt;p&gt;Just wrote and article on two underestimated (and mostly unknown) features of Langchain to create completely configurable chains while still being production ready. This is actually what I use in my own production chains.&lt;br/&gt; Here&amp;#39;s the link: &lt;a href=&quot;https://www.metadocs.co/2024/04/25/two-underestimated-langchain-features-to-create-production-ready-configurable-chains/&quot;&gt;https://www.metadocs.co/2024/04/25/two-underestimated-langchain-features-to-create-production-ready-configurable-chains/&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Enjoy!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/ravediamond000&quot;&gt; /u/ravediamond000 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ccxg2l/two_underestimated_langchain_features_to_create/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ccxg2l/two_underestimated_langchain_features_to_create/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1ccxg2l</id><link href="https://www.reddit.com/r/LangChain/comments/1ccxg2l/two_underestimated_langchain_features_to_create/" /><updated>2024-04-25T17:47:37+00:00</updated><published>2024-04-25T17:47:37+00:00</published><title>Two underestimated Langchain features to create production-ready configurable chains</title></entry><entry><author><name>/u/sarthak_uchiha</name><uri>https://www.reddit.com/user/sarthak_uchiha</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi guys do anyone know how to convert .msg files to.pdf , msg files may contains IMG in the body , I tried some thing but it was not able to take the IMG in the body, need for a usecase that the whole data gets converted into the pdf including the images &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/sarthak_uchiha&quot;&gt; /u/sarthak_uchiha &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cdf17a/msg_files_to_pdf/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cdf17a/msg_files_to_pdf/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cdf17a</id><link href="https://www.reddit.com/r/LangChain/comments/1cdf17a/msg_files_to_pdf/" /><updated>2024-04-26T07:08:18+00:00</updated><published>2024-04-26T07:08:18+00:00</published><title>.msg files to .pdf</title></entry><entry><author><name>/u/Any-Demand-2928</name><uri>https://www.reddit.com/user/Any-Demand-2928</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;For those of you who build your frontend UI in React, what library are you using to create the actual chat part of the website? For example, displaying messages, being able to send messages using a chat box, etc...&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Any-Demand-2928&quot;&gt; /u/Any-Demand-2928 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cd31v4/what_react_library_do_you_use_to_build_the_actual/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cd31v4/what_react_library_do_you_use_to_build_the_actual/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cd31v4</id><link href="https://www.reddit.com/r/LangChain/comments/1cd31v4/what_react_library_do_you_use_to_build_the_actual/" /><updated>2024-04-25T21:16:41+00:00</updated><published>2024-04-25T21:16:41+00:00</published><title>What React Library do you use to build the actual Chat Interface?</title></entry><entry><author><name>/u/fokke2508</name><uri>https://www.reddit.com/user/fokke2508</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi All,&lt;/p&gt; &lt;p&gt;I work for a startup that is developing a platform to easily build GenAI-infused applications. As part of our platform, we are starting a community-based building blocks library (sort of like an app store).&lt;/p&gt; &lt;p&gt;We are about to release the community-based components and I would love to fill it up a bit more with great building blocks. Wondering if there are people here that would want to contribute?&lt;/p&gt; &lt;p&gt;I can provide you with the resources to build anything you want, including vector stores, LLMs etc.&lt;br/&gt; The idea is that each building block should help you and others build LLM apps more easily. For example, we might have a building block that provides a specific RAG task or one that converts a PDF into vectors. Could be langchain based but does not have to. &lt;/p&gt; &lt;p&gt;I don&amp;#39;t want to turn this too much into a sales pitch so I&amp;#39;ll stop there, would love to hear if anyone is interested in contributing.&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/fokke2508&quot;&gt; /u/fokke2508 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cd1roj/community_created_building_blocks_for_llms/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cd1roj/community_created_building_blocks_for_llms/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cd1roj</id><link href="https://www.reddit.com/r/LangChain/comments/1cd1roj/community_created_building_blocks_for_llms/" /><updated>2024-04-25T20:27:55+00:00</updated><published>2024-04-25T20:27:55+00:00</published><title>Community created building blocks for LLMs</title></entry><entry><author><name>/u/Extreme-Berry7898</name><uri>https://www.reddit.com/user/Extreme-Berry7898</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I want to use a row from a table to retrieve K rows related to it. One element of each row is a vector, so a row is a vector group. So what I need to do is retrieve the vector group using the vector group. I have no idea how to accomplish this task, can you give me some advice?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Extreme-Berry7898&quot;&gt; /u/Extreme-Berry7898 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cd9ywg/vector_group_based_retrieval/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cd9ywg/vector_group_based_retrieval/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cd9ywg</id><link href="https://www.reddit.com/r/LangChain/comments/1cd9ywg/vector_group_based_retrieval/" /><updated>2024-04-26T02:22:19+00:00</updated><published>2024-04-26T02:22:19+00:00</published><title>Vector group based retrieval</title></entry><entry><author><name>/u/Calm_Pea_2428</name><uri>https://www.reddit.com/user/Calm_Pea_2428</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I want to ask natural-language questions to collections. For example: for sales collection, “Whats the average quantity sold in the past 3 months?&amp;quot;. I got about 10 collections. About 100K rows each and 25 columns each and this data is updated daily. Apart from mongo, If you have developed this kind of application using any database please add your suggestions. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Calm_Pea_2428&quot;&gt; /u/Calm_Pea_2428 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ccv7qg/build_a_rag_application_with_large_knowledge_base/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ccv7qg/build_a_rag_application_with_large_knowledge_base/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1ccv7qg</id><link href="https://www.reddit.com/r/LangChain/comments/1ccv7qg/build_a_rag_application_with_large_knowledge_base/" /><updated>2024-04-25T15:43:16+00:00</updated><published>2024-04-25T15:43:16+00:00</published><title>Build a RAG application with large knowledge base</title></entry><entry><author><name>/u/TinyZoro</name><uri>https://www.reddit.com/user/TinyZoro</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Can Langchain chains be imported / exported and therefore easily shared?&lt;/p&gt; &lt;p&gt;What does Langchain really give that you can’t easily do with something like pipedream or buildship?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/TinyZoro&quot;&gt; /u/TinyZoro &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cd1i0j/couple_of_noob_questions/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cd1i0j/couple_of_noob_questions/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cd1i0j</id><link href="https://www.reddit.com/r/LangChain/comments/1cd1i0j/couple_of_noob_questions/" /><updated>2024-04-25T20:17:45+00:00</updated><published>2024-04-25T20:17:45+00:00</published><title>Couple of Noob questions</title></entry><entry><author><name>/u/Minute_Scientist8107</name><uri>https://www.reddit.com/user/Minute_Scientist8107</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;How can LLMs be leveraged in a company that produces thermal products for electric cars ? Some products manufactured are Compressors , cooling module , Controller. I’d like to know how can gen AI be utilized in this ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Minute_Scientist8107&quot;&gt; /u/Minute_Scientist8107 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cdgtc9/how_can_gen_ai_be_utilized/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cdgtc9/how_can_gen_ai_be_utilized/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cdgtc9</id><link href="https://www.reddit.com/r/LangChain/comments/1cdgtc9/how_can_gen_ai_be_utilized/" /><updated>2024-04-26T09:13:25+00:00</updated><published>2024-04-26T09:13:25+00:00</published><title>How can Gen AI be utilized ?</title></entry><entry><author><name>/u/WesEd178</name><uri>https://www.reddit.com/user/WesEd178</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I am building a RAG application from 400+ XML documents, half of the content are tables which I am converting to csv and then extracting all text from the xml tags. A document before being added to the retriever contains both text and csv. Currently I am using an ensemble retriever combining bm25, tfidf and vectorstore (FAISS, chunk_size=2000, overlap=100). I have around 4000 test questions for these documents along with human labeled ground truth for each question and I also have a reference to the document that contains the answer. Right now I am able to get 91 questions out of 100 correctly in a random sample. &lt;/p&gt; &lt;p&gt;model: gpt-4&lt;br/&gt; embeddings: OpenAI text-embedding-3-large retriever: ensemble (bm25, tfidf, FAISS(hunk_size=2000, overlap=100)) additional: -RAPTOR clustering -sort by date then reordered using Long-Context Reorder&lt;/p&gt; &lt;p&gt;Is this a good &amp;quot;accuracy&amp;quot;? How can I improve? Is there such thing as 100% accurate RAG? How are your RAG applications doing?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/WesEd178&quot;&gt; /u/WesEd178 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cclcns/how_accurate_are_your_rag_applications/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cclcns/how_accurate_are_your_rag_applications/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cclcns</id><link href="https://www.reddit.com/r/LangChain/comments/1cclcns/how_accurate_are_your_rag_applications/" /><updated>2024-04-25T06:51:10+00:00</updated><published>2024-04-25T06:51:10+00:00</published><title>How accurate are your RAG applications?</title></entry><entry><author><name>/u/neodyme4</name><uri>https://www.reddit.com/user/neodyme4</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Complete noob in AI, deep learning, machine learning, everything with &amp;quot;intelligent something&amp;quot;.&lt;/p&gt; &lt;p&gt;I would love some advice to start understanding how it works, and understand my mistakes.&lt;/p&gt; &lt;p&gt;I started to write code for a very simple task: &lt;/p&gt; &lt;p&gt;- I have a text file in Spanish (but Spanish is not important), and there is no necessarily a relationship between the lines - meaning by now I do not need to handle the context (maybe later!)&lt;/p&gt; &lt;p&gt;- I read it line by line&lt;/p&gt; &lt;p&gt;- I write a prompt asking to translate it for a towerinstruct model &lt;/p&gt; &lt;p&gt;- Then I print the result.&lt;/p&gt; &lt;p&gt;To be honest, the behavior of the machine seems very strange to me. At first it works (first lines), but after few lines it starts to write text by himself as such as &amp;quot;The translation you entered is as follows: &amp;quot; , &amp;quot;Translation in English&amp;quot; or &amp;quot;Spanish: &amp;quot;. I tried to add some system prompt, without significant success.&lt;/p&gt; &lt;p&gt;Here is my dumb code. Any comment will be so helpful to me! &lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;import sys import os import re from langchain.callbacks.manager import CallbackManager from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler from langchain.chains import LLMChain from langchain.prompts import PromptTemplate from langchain_community.llms import LlamaCpp MODEL=&amp;quot;/home/dani/AI-models/towerinstruct-7b-v0.1.Q8_0.gguf&amp;quot; TEMPLATE = &amp;quot;&amp;quot;&amp;quot; &amp;lt;|im_start|&amp;gt;system {system_message}&amp;lt;|im_end|&amp;gt; &amp;lt;|im_start|&amp;gt;user {prompt}&amp;lt;|im_end|&amp;gt; &amp;lt;|im_start|&amp;gt;assistant &amp;quot;&amp;quot;&amp;quot; PROMPT = PromptTemplate( input_variables=[&amp;quot;prompt&amp;quot;, &amp;quot;system_message&amp;quot;], template=TEMPLATE, ) SYSTEM_MESSAGE = &amp;quot;&amp;quot; CALLBACK_MANAGER = CallbackManager([StreamingStdOutCallbackHandler()]) LLM = LlamaCpp( model_path=MODEL, temperature=0.5, max_tokens=500, top_p=1, callback_manager=CALLBACK_MANAGER, verbose=False, ) def prompt_tr(txt, in_lang=&amp;#39;Spanish&amp;#39;, out_lang=&amp;#39;English&amp;#39;): return &amp;quot;Translate the following text from {lang1} into {lang2}.\n{lang1}: {prompt}\n{lang2}:&amp;quot;.format( lang1=in_lang, lang2=out_lang, prompt=txt ) def translate_sp_en(txt): text = prompt_tr(txt) #print(PROMPT.format(prompt=text, system_message=SYSTEM_MESSAGE)) output = LLM.invoke(PROMPT.format(prompt=text, system_message=SYSTEM_MESSAGE)) print(output) def usage(): print(&amp;quot;Usage: {} @filepath&amp;quot;.format(sys.argv[0])) if __name__ == &amp;#39;__main__&amp;#39;: if len(sys.argv) &amp;lt; 2: usage() sys.exit(1) if not os.path.isfile(sys.argv[1]): print(&amp;quot;Wrong path &amp;#39;{}&amp;#39;&amp;quot;.format(sys.argv[1])) usage() sys.exit(2) with open(sys.argv[1],&amp;#39;r&amp;#39;) as f: for line in f: translate_sp_en(line.rstrip()) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/neodyme4&quot;&gt; /u/neodyme4 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cd4na0/noob_asking_code_review_and_advice_langchain_and/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cd4na0/noob_asking_code_review_and_advice_langchain_and/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cd4na0</id><link href="https://www.reddit.com/r/LangChain/comments/1cd4na0/noob_asking_code_review_and_advice_langchain_and/" /><updated>2024-04-25T22:26:03+00:00</updated><published>2024-04-25T22:26:03+00:00</published><title>Noob asking code review and advice: langchain and translation</title></entry><entry><author><name>/u/svenjacobs3</name><uri>https://www.reddit.com/user/svenjacobs3</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I am trying to wrap my head around Langchain and streaming content from an agent to the frontend token after token (to mitigate long response times). I&amp;#39;m really just looking for something barebones to grasp how best to do this. AsyncIteratorCallbackHandler() looked promising since it appears to create a queue of tokens I should be able to iterate through. I get a &lt;em&gt;&amp;quot;TypeError: &amp;#39;async_generator&amp;#39; object is not iterable&amp;quot;&lt;/em&gt; error however, and I&amp;#39;m not sure how to remedy the problem to bring about the solution I&amp;#39;m looking for: &lt;/p&gt; &lt;pre&gt;&lt;code&gt;@app.route(&amp;#39;/stream&amp;#39;) async def stream_chunks(): CSV_PROMPT_PREFIX = &amp;quot;&amp;quot;&amp;quot; - First set the pandas display options to show all the columns, get the column names, then answer the question. &amp;quot;&amp;quot;&amp;quot; handler = AsyncIteratorCallbackHandler() llm = AzureChatOpenAI(deployment_name=os.environ[&amp;quot;GPT35_DEPLOYMENT_NAME&amp;quot;], temperature=0.1, max_tokens=1000, streaming=True, callbacks=[handler] ) agent_executor = create_csv_agent(llm=llm, path=&amp;quot;static/DemographicCompilation.csv&amp;quot;, prefix=CSV_PROMPT_PREFIX, verbose=True, agent_type=AgentType.OPENAI_FUNCTIONS, return_intermediate_steps=False ) agent = agent_executor(&amp;quot;Tell me a long story&amp;quot;) async def generate(): async for token in handler.aiter(): yield token return Response(generate(), content_type=&amp;quot;text/plain&amp;quot;) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;If anyone could help I&amp;#39;d be much obliged. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/svenjacobs3&quot;&gt; /u/svenjacobs3 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cd3dot/langchain_and_asynciteratorcallbackhandler/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cd3dot/langchain_and_asynciteratorcallbackhandler/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cd3dot</id><link href="https://www.reddit.com/r/LangChain/comments/1cd3dot/langchain_and_asynciteratorcallbackhandler/" /><updated>2024-04-25T21:29:32+00:00</updated><published>2024-04-25T21:29:32+00:00</published><title>Langchain and AsyncIteratorCallbackHandler()</title></entry><entry><author><name>/u/hodl_and_chill</name><uri>https://www.reddit.com/user/hodl_and_chill</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi builders,&lt;/p&gt; &lt;p&gt;Figuring out to set up the most productive code environment. GitHub Co-Pilot seems to outperform Google Duet, but GitHub co-pilot doesn&amp;#39;t have the documentation of LangChain integrated.&lt;/p&gt; &lt;p&gt;Is there a way to do this? And in general how to get external docs as extra context for AI coding co-pilots? Imagine being able to drop any documentation of API&amp;#39;s/external tools you are trying to connect, and quickly leveraging those abstractions to spit out working code.&lt;/p&gt; &lt;p&gt;Using VSCode as IDE but open to switch in case there is a workflow that increases my output and allows me to ship prototypes faster :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/hodl_and_chill&quot;&gt; /u/hodl_and_chill &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ccp13d/feeding_langchain_documentation_in_a_copilot_for/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ccp13d/feeding_langchain_documentation_in_a_copilot_for/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1ccp13d</id><link href="https://www.reddit.com/r/LangChain/comments/1ccp13d/feeding_langchain_documentation_in_a_copilot_for/" /><updated>2024-04-25T10:59:21+00:00</updated><published>2024-04-25T10:59:21+00:00</published><title>Feeding LangChain documentation in a co-pilot for VSCode</title></entry><entry><author><name>/u/HappyDataGuy</name><uri>https://www.reddit.com/user/HappyDataGuy</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Lets say question is - What was my top performing post?&lt;br/&gt; the actual question here is - which post has highest summation of likes,comment and shares?&lt;br/&gt; in second question LLM knows what columns to use, in first question it doesn&amp;#39;t. &lt;/p&gt; &lt;p&gt;do any one of you have experience with using knowledge graph for this? or any other way to solve this? any tutorial or paper would be amazing. Open source solutions are welcome as well. &lt;/p&gt; &lt;p&gt;If you&amp;#39;re working on a similar project im down for sharing ideas. Thanks. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/HappyDataGuy&quot;&gt; /u/HappyDataGuy &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ccovds/how_to_knowledge_graphs_can_be_used_to_improve/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ccovds/how_to_knowledge_graphs_can_be_used_to_improve/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1ccovds</id><link href="https://www.reddit.com/r/LangChain/comments/1ccovds/how_to_knowledge_graphs_can_be_used_to_improve/" /><updated>2024-04-25T10:49:27+00:00</updated><published>2024-04-25T10:49:27+00:00</published><title>How to knowledge graphs can be used to improve SQL generation? [text to sql]</title></entry><entry><author><name>/u/SashaBaych</name><uri>https://www.reddit.com/user/SashaBaych</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Any ideas how to add memory/persistence to a StateGraph when doing a langgraph? There is tutorial on MessageGraph, but what would I do with the StateGraph with multiple chains? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/SashaBaych&quot;&gt; /u/SashaBaych &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cco1yb/stategraph_persistencehistory/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cco1yb/stategraph_persistencehistory/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cco1yb</id><link href="https://www.reddit.com/r/LangChain/comments/1cco1yb/stategraph_persistencehistory/" /><updated>2024-04-25T09:58:11+00:00</updated><published>2024-04-25T09:58:11+00:00</published><title>StateGraph persistence/history</title></entry><entry><author><name>/u/patcher99</name><uri>https://www.reddit.com/user/patcher99</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ccjozm/openlit_preview_opentelemetrynative_llm/&quot;&gt; &lt;img src=&quot;https://b.thumbs.redditmedia.com/jllXvUlBtTTyc1aFLMyhzX7p3v8xZJMp2-Zweq5w5oQ.jpg&quot; alt=&quot;OpenLIT Preview: OpenTelemetry-native LLM Application Observability&quot; title=&quot;OpenLIT Preview: OpenTelemetry-native LLM Application Observability&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey folks! My friend and I were working on an LLM-based legal helper but got really stuck trying to figure out our tweaked GPT-3.5. So, we came up with a tool named Doku to keep an monitor on our LLM apps and make them more trusty. It got stars fairly quickly, but folks found it a bit tricky since they had to set up things before diving into the analysis.&lt;/p&gt; &lt;p&gt;Here&amp;#39;s what we did next:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;We switched our tech to OpenTelemetry for easier tracking.&lt;/li&gt; &lt;li&gt;We made it so you can see costs and how many tokens you&amp;#39;re using straight from your console – no extra Infra needed for basic debugging.&lt;/li&gt; &lt;li&gt;We decided to call it OpenLIT (short for Learning Interpretability Tool, shining a light on model behavior and data visualization, inspired by a term from &lt;a href=&quot;https://developers.google.com/machine-learning/glossary#learning-interpretability-tool-lit&quot;&gt;Google&lt;/a&gt;).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;We&amp;#39;ve just put out our new Python library, OpenLIT, in preview. You can check it out here: &lt;a href=&quot;https://pypi.org/project/openlit/&quot;&gt;https://pypi.org/project/openlit/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This library is provides OpenTelemetry Auto-Instrumentation for LLM Applications, It integrates monitoring for&lt;/p&gt; &lt;ul&gt; &lt;li&gt;LLM providers like OpenAI, Anthropic, HuggingFace, Cohere, and Mistral.&lt;/li&gt; &lt;li&gt;Vector DBs including Pinecone and ChromaDB.&lt;/li&gt; &lt;li&gt;Frameworks like LangChain&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This library will work even if you are using frameworks like LiteLLM!&lt;/p&gt; &lt;p&gt;It&amp;#39;s the first of its kind to align with the OTEL &lt;a href=&quot;https://github.com/open-telemetry/semantic-conventions/tree/main/docs/gen-ai&quot;&gt;Semcov for GenAI Applications&lt;/a&gt;, allowing you to forward all collected metrics to any OTEL-compatible backend.&lt;/p&gt; &lt;p&gt;We&amp;#39;re also working on an open-source, self-hosted UI - Attached a couple pictures for you to get a feel of it.&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/8toxeja56kwc1.png?width=2553&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6704818740f9e76f953e951351c06698a21396c9&quot;&gt;https://preview.redd.it/8toxeja56kwc1.png?width=2553&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6704818740f9e76f953e951351c06698a21396c9&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/k5b3cka56kwc1.png?width=2552&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=266b290b1548ff85ae5730bc7b81b855cfe71fd2&quot;&gt;https://preview.redd.it/k5b3cka56kwc1.png?width=2552&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=266b290b1548ff85ae5730bc7b81b855cfe71fd2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Follow our project here for updates - &lt;a href=&quot;https://github.com/openlit/openlit&quot;&gt;https://github.com/openlit/openlit&lt;/a&gt;. The stable release drops tomorrow for both the SDK and UI. Let me know if there&amp;#39;s something specific you’d love to see!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/patcher99&quot;&gt; /u/patcher99 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ccjozm/openlit_preview_opentelemetrynative_llm/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ccjozm/openlit_preview_opentelemetrynative_llm/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1ccjozm</id><media:thumbnail url="https://b.thumbs.redditmedia.com/jllXvUlBtTTyc1aFLMyhzX7p3v8xZJMp2-Zweq5w5oQ.jpg" /><link href="https://www.reddit.com/r/LangChain/comments/1ccjozm/openlit_preview_opentelemetrynative_llm/" /><updated>2024-04-25T05:06:40+00:00</updated><published>2024-04-25T05:06:40+00:00</published><title>OpenLIT Preview: OpenTelemetry-native LLM Application Observability</title></entry><entry><author><name>/u/ashpreetbedi</name><uri>https://www.reddit.com/user/ashpreetbedi</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Here&amp;#39;s an unedited video testing tools with llama3 running locally (at 1.5x speed). The good, bad and ugly. &lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://reddit.com/link/1ccdexb/video/a47qddncliwc1/player&quot;&gt;https://reddit.com/link/1ccdexb/video/a47qddncliwc1/player&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/ashpreetbedi&quot;&gt; /u/ashpreetbedi &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ccdexb/testing_local_llama3_at_function_calling_and_tool/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ccdexb/testing_local_llama3_at_function_calling_and_tool/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1ccdexb</id><link href="https://www.reddit.com/r/LangChain/comments/1ccdexb/testing_local_llama3_at_function_calling_and_tool/" /><updated>2024-04-24T23:48:18+00:00</updated><published>2024-04-24T23:48:18+00:00</published><title>🧙Testing local llama3 at function calling and tool use.</title></entry><entry><author><name>/u/Travolta1984</name><uri>https://www.reddit.com/user/Travolta1984</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;LangChain recently added Semantic Chunker as an option for splitting documents, and from my experience it performs better than RecursiveCharacterSplitter (although it&amp;#39;s more expensive due to the sentence embeddings). &lt;/p&gt; &lt;p&gt;One thing that I noticed though, is that there&amp;#39;s no pre-defined limit to the size of the result chunks: I have seen chunks that are just a couple of words (i.e. section headers), and also very long chunks (5k+ characters). Which makes total sense, given the logic: if all sentences in that chunk are semantically similar, they should all be grouped together, regardless of how long that chunk will be. But that can lead to issues downstream: document context gets too large for the LLM, or small chunks that add no context at all.&lt;/p&gt; &lt;p&gt;Based on that, I wrote my custom version of the Semantic Chunker that optionally respects the character count limit (both minimum and maximum). The logic I am using is: a chunk split happens when either the semantic distance between the sentences becomes too large and the chunk is at least &amp;lt;MIN\_SIZE&amp;gt; long, or when the chunk becomes larger than &amp;lt;MAX\_SIZE&amp;gt;.&lt;/p&gt; &lt;p&gt;My question to the community is: &lt;/p&gt; &lt;p&gt;- Does the above make sense? I feel like this approach can be useful, but it kind of goes against the idea of chunking your texts semantically.&lt;/p&gt; &lt;p&gt;- I thought about creating a PR to add this option to the official code. Has anyone contributed to LangChain&amp;#39;s repo? What has been your experience doing so?&lt;/p&gt; &lt;p&gt;Thanks.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Travolta1984&quot;&gt; /u/Travolta1984 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cca0h0/question_about_semantic_chunker/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cca0h0/question_about_semantic_chunker/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cca0h0</id><link href="https://www.reddit.com/r/LangChain/comments/1cca0h0/question_about_semantic_chunker/" /><updated>2024-04-24T21:21:33+00:00</updated><published>2024-04-24T21:21:33+00:00</published><title>Question about Semantic Chunker</title></entry><entry><author><name>/u/BuildingLLMTools</name><uri>https://www.reddit.com/user/BuildingLLMTools</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey folks,&lt;/p&gt; &lt;p&gt;We&amp;#39;re building Langtrace, an open-source LLM App observability platform (&lt;a href=&quot;http://www.langtrace.ai&quot;&gt;www.langtrace.ai&lt;/a&gt;) and we recently built support for LlamaIndex, the go-to library for building retrieval-augmented generation (RAG) applications.&lt;/p&gt; &lt;p&gt;As builders, we know how frustrating it can be to optimize RAG apps (e.g. trying to figure out where the bottlenecks are, whether your retrieval strategy is effective, etc.) That&amp;#39;s why we&amp;#39;re building a tool that makes it easy to gain deeper insights and optimize performance, reliability, and user experience for your LLM apps.&lt;/p&gt; &lt;p&gt;With Langtrace and LlamaIndex, you can:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Get one-click observability for LlamaIndex-based RAG applications&lt;/li&gt; &lt;li&gt;Visualize latency breakdowns, context relevance, and resource utilization&lt;/li&gt; &lt;li&gt;Monitor and analyze traces, evals, metrics, and logs with OpenTelemetry&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Feel free to check out our &lt;a href=&quot;https://github.com/Scale3-Labs/langtrace&quot;&gt;repo&lt;/a&gt; for &lt;a href=&quot;https://github.com/Scale3-Labs/langtrace-docs/blob/main/langtrace-examples/llamaindex_essay/starter.py.ipynb&quot;&gt;examples&lt;/a&gt;, contribute, provide feedback, and join our &lt;a href=&quot;https://discord.com/invite/EaSATwtr4t&quot;&gt;community&lt;/a&gt;. More info on the integration with LlamaIndex &lt;a href=&quot;https://langtrace.ai/blog/langtrace-llamaindex-a-game-changing-combo-for-rag-developers&quot;&gt;here&lt;/a&gt; including a video demo. Looking forward to hearing of your feedback! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/BuildingLLMTools&quot;&gt; /u/BuildingLLMTools &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cc8fx4/solve_rag_app_optimization_puzzles_with_langtrace/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cc8fx4/solve_rag_app_optimization_puzzles_with_langtrace/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cc8fx4</id><link href="https://www.reddit.com/r/LangChain/comments/1cc8fx4/solve_rag_app_optimization_puzzles_with_langtrace/" /><updated>2024-04-24T20:18:16+00:00</updated><published>2024-04-24T20:18:16+00:00</published><title>Solve RAG App Optimization Puzzles with Langtrace + LlamaIndex</title></entry><entry><author><name>/u/OfficeSalamander</name><uri>https://www.reddit.com/user/OfficeSalamander</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I am trying to use internet-search enabled bots, and I was wondering how you guys were doing it - I see that Serpdev and Tavily have Langchain integration - which of these two do you guys like? Or do you roll your own?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/OfficeSalamander&quot;&gt; /u/OfficeSalamander &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cc1dyq/how_are_you_guys_doing_internet_search/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cc1dyq/how_are_you_guys_doing_internet_search/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cc1dyq</id><link href="https://www.reddit.com/r/LangChain/comments/1cc1dyq/how_are_you_guys_doing_internet_search/" /><updated>2024-04-24T15:40:46+00:00</updated><published>2024-04-24T15:40:46+00:00</published><title>How are you guys doing internet search?</title></entry></feed>