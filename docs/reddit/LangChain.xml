<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/"><category term="LangChain" label="r/LangChain"/><updated>2024-07-10T06:59:47+00:00</updated><icon>https://www.redditstatic.com/icon.png/</icon><id>/r/LangChain.rss</id><link rel="self" href="https://www.reddit.com/r/LangChain.rss" type="application/atom+xml" /><link rel="alternate" href="https://www.reddit.com/r/LangChain" type="text/html" /><subtitle>LangChain is an open-source framework and developer toolkit that helps developers get LLM applications from prototype to production. It is available for Python and Javascript at https://www.langchain.com/.</subtitle><title>LangChain</title><entry><author><name>/u/zchaarm</name><uri>https://www.reddit.com/user/zchaarm</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;A place for members of &lt;a href=&quot;/r/LangChain&quot;&gt;r/LangChain&lt;/a&gt; to chat with each other&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/zchaarm&quot;&gt; /u/zchaarm &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_10ljho9</id><link href="https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/" /><updated>2023-01-26T04:33:05+00:00</updated><published>2023-01-26T04:33:05+00:00</published><title>r/LangChain Lounge</title></entry><entry><author><name>/u/neilkatz</name><uri>https://www.reddit.com/user/neilkatz</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey All,&lt;/p&gt; &lt;p&gt;I&amp;#39;m curious what everyone is using to parse complex PDFs, extract the data and turn it into something LLMs can better comprehend.&lt;/p&gt; &lt;p&gt;Is there something that can consistently find tables, forms, charts, graphics that we see in many enterprise documents. It seems without this step, RAG hallucinations are a significant issue. &lt;/p&gt; &lt;p&gt;Much appreciated. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/neilkatz&quot;&gt; /u/neilkatz &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dzj5qx/best_pdf_parser_for_rag/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dzj5qx/best_pdf_parser_for_rag/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dzj5qx</id><link href="https://www.reddit.com/r/LangChain/comments/1dzj5qx/best_pdf_parser_for_rag/" /><updated>2024-07-10T01:13:19+00:00</updated><published>2024-07-10T01:13:19+00:00</published><title>Best PDF Parser for RAG?</title></entry><entry><author><name>/u/coolcloud</name><uri>https://www.reddit.com/user/coolcloud</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dzfp48/agent_retrieval_how_we_almost_always_find_the/&quot;&gt; &lt;img src=&quot;https://b.thumbs.redditmedia.com/Pb2SWSeA1tyEw8SKW3E2DTFD13TWyENVp-4WWyttigA.jpg&quot; alt=&quot;Agent Retrieval - How we almost always find the right vectors. Pt 3&quot; title=&quot;Agent Retrieval - How we almost always find the right vectors. Pt 3&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey all - &lt;/p&gt; &lt;p&gt;Today I wanted to run through how we narrow down our vector space.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Issues with vector search only:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;If you have used vector search over a large corpus of documents, you&amp;#39;ll know vector search doesn&amp;#39;t work well.&lt;/li&gt; &lt;li&gt;Almost 100% of the time someone is using RAG, they are looking for something specific. &lt;ul&gt; &lt;li&gt; Example: If you use vector search on a name, most names will come back, regardless of it&amp;#39;s Bob Smith, or Sally Blu. This is bad if I just want to find Sally Blu.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;W&lt;strong&gt;hat are we doing?&lt;/strong&gt; &lt;/p&gt; &lt;ol&gt; &lt;li&gt;We split each doc up into the smallest possible vector (normally sentences) We&amp;#39;ve found this is the best, and most accurate for vector similarity. &lt;/li&gt; &lt;li&gt;NER/Keyword extraction from the query&lt;/li&gt; &lt;li&gt; Search docs for keywords/NER&lt;/li&gt; &lt;li&gt;Vector search query within the docs that are returned.&lt;/li&gt; &lt;li&gt; Traditionally top 20 results (no similarity score min)&lt;/li&gt; &lt;li&gt;Reconstruct the docs into headers etc.&lt;/li&gt; &lt;li&gt;Reranker Jina - top 10 results (over .x similarity)&lt;/li&gt; &lt;li&gt;Each result sent to an LLM for quotes&lt;/li&gt; &lt;li&gt;combine all into prompt #2&lt;/li&gt; &lt;li&gt;LLM answer&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Today we&amp;#39;ll primarily talk through step 2-7.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;How?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;Example query:&lt;/em&gt; Does Sally blu work at tada?&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Use an LLM to extract named entities and key words/phrases from the query. &lt;/li&gt; &lt;li&gt;Search the entire document to see if &amp;quot;all&amp;quot; keywords/NER match the doc. [&amp;quot;Tada&amp;quot; and &amp;quot;Sally Blu&amp;quot;]&lt;/li&gt; &lt;li&gt;If there&amp;#39;s no matches, do an &amp;quot;or&amp;quot; search for keywords/NER [&amp;quot;Tada&amp;quot; or &amp;quot;Sally Blu&amp;quot;]&lt;/li&gt; &lt;li&gt;If there&amp;#39;s no matches, don&amp;#39;t return a doc.&lt;/li&gt; &lt;li&gt;If there are matches in either step 2 or 3, return those docs only and do vector search within only those documents. &lt;/li&gt; &lt;/ol&gt; &lt;p&gt;This significantly limits the scope of the vector space and based on our experiences almost never filters out the documents that are important to answering the question.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;How are we able to search an entire doc?&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;This process wouldn&amp;#39;t be possible without out our document structure, so here&amp;#39;s a link &amp;amp; a quick overview of how we how we &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dpbc4g/how_we_chunk_turning_pdfs_into_hierarchical/&quot;&gt;chunk docs. &lt;/a&gt;&lt;/p&gt; &lt;p&gt;TLDR: We extract and save the document structure into a hieratical format, headers, sub-headers, list, paragraphs, tables, etc. Because we do this, we can easily search the entire document. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Diagram of our first pass search&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/jczepvzejkbd1.png?width=365&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f7818f84954b2486793cb012f02b7097b988859c&quot;&gt;https://preview.redd.it/jczepvzejkbd1.png?width=365&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f7818f84954b2486793cb012f02b7097b988859c&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Example Query:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;/u/warriorA&quot;&gt;u/warriorA&lt;/a&gt; asked this question a couple of days ago in another post. It&amp;#39;s simple so we&amp;#39;ll re-use it:&lt;/p&gt; &lt;p&gt;______&lt;/p&gt; &lt;p&gt;Consider the query: &amp;quot;How many Presidents did we have in America?&amp;quot;&lt;/p&gt; &lt;p&gt;Now we might have a document chunk with this information:&lt;/p&gt; &lt;ol&gt; &lt;li&gt; doc_1_chunk: &amp;quot;United States has been governed by a total of 46 people&amp;quot;&lt;br/&gt;&lt;/li&gt; &lt;li&gt; doc_2_chunk: &amp;quot;The USA is a country in north america.&amp;quot;&lt;br/&gt;&lt;/li&gt; &lt;li&gt; doc_3_chunk: &amp;quot;We&amp;#39;ve had 1 President in &amp;#39;Random-Country&amp;#39;.&amp;quot;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Wouldn&amp;#39;t your search fail?&lt;/p&gt; &lt;p&gt;Note - (I made a few small edits for example purposes)&lt;/p&gt; &lt;p&gt;______&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Would our system work for this use case?&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;Most likely, yes. &lt;/p&gt; &lt;p&gt;From the query we&amp;#39;d extract: [&amp;quot;Presidents&amp;quot; &amp;amp; &amp;quot;America&amp;quot;] &lt;/p&gt; &lt;p&gt;Again, we search the entire document, not just the chunks to find hits.&lt;/p&gt; &lt;p&gt;✅ Doc_1: It&amp;#39;s very likely that doc_1 would contain both the word president and America, meaning that document would come back. &lt;/p&gt; &lt;p&gt;❌ Doc_2: Isn&amp;#39;t talking about Presidents, thus it wouldn&amp;#39;t come back.&lt;/p&gt; &lt;p&gt;✅or ❌ Doc_3: Would most likely not come back as it&amp;#39;s not talking about America. (If it did come back because America was in the document somewhere, vector search + rerankers would help filter it out.)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;If we extended chunk one:&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;here&amp;#39;s an example of what it would look like and it contains both the word president &amp;amp; america.&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/zwuhtvrdkkbd1.png?width=1047&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9a62623c04a8d1836262d3bd6c0ba3dbe00f91f0&quot;&gt;https://preview.redd.it/zwuhtvrdkkbd1.png?width=1047&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9a62623c04a8d1836262d3bd6c0ba3dbe00f91f0&lt;/a&gt;&lt;/p&gt; &lt;p&gt;If we didn&amp;#39;t do this step, it&amp;#39;s likely all 3 chunks would come back, and doc_3_chunk, would be rated the highest. You could imagine if you had hundreds or thousands of documents the most important vectors to answer the question may not show up at all.&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/cr6jmxuwkkbd1.png?width=405&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=76c747237adb98fb3ba6c78ff491c04ac05bdecc&quot;&gt;https://preview.redd.it/cr6jmxuwkkbd1.png?width=405&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=76c747237adb98fb3ba6c78ff491c04ac05bdecc&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;how do we extract NER?&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;We have found the NER models aren&amp;#39;t consistent enough, you have to use an LLM. If you ask a question like &amp;quot;what are the terms of the wings contract&amp;quot; a NER model may see no named entities, where an LLM would understand the named entity is &amp;quot;wings&amp;quot;.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Prompt:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Respond with valid json. &lt;/p&gt; &lt;p&gt;You will receive text, your goal is to: &lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Identify potential search phrases. Put those in the &amp;quot;P&amp;quot; field. For example, in &amp;quot;When was the Huck Finn contract signed?&amp;quot;, the main concept is &amp;quot;Huck Finn contract&amp;quot; &lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Identify named entities. Put those in the &amp;quot;N&amp;quot; field. For example: &amp;quot;Huck Finn&amp;quot; or &amp;quot;Apple A7&amp;quot; Emit a valid JSON object with a single &amp;quot;N&amp;quot; field and a single &amp;quot;P&amp;quot; field. &lt;/p&gt; &lt;p&gt;Example &lt;/p&gt; &lt;p&gt;Input: When was the Huck Finn contract signed? &lt;/p&gt; &lt;p&gt;Output: &lt;/p&gt; &lt;p&gt;{ &lt;/p&gt; &lt;p&gt;&amp;quot;P&amp;quot;: [&amp;quot;Huck Finn contract&amp;quot;], &lt;/p&gt; &lt;p&gt;&amp;quot;N&amp;quot;: [&amp;quot;Huck Finn&amp;quot;],&lt;/p&gt; &lt;p&gt;} &lt;/p&gt; &lt;p&gt;Input: does share and perform offer a performance engagement tool &lt;/p&gt; &lt;p&gt;Output: &lt;/p&gt; &lt;p&gt;{ &lt;/p&gt; &lt;p&gt;&amp;quot;P&amp;quot;: [&amp;quot;share and perform performance engagement&amp;quot;], &lt;/p&gt; &lt;p&gt;&amp;quot;N&amp;quot;: [&amp;quot;share and perform&amp;quot;],&lt;/p&gt; &lt;p&gt;} &lt;/p&gt; &lt;p&gt;Input: how many processors in the apple a7? &lt;/p&gt; &lt;p&gt;{ &lt;/p&gt; &lt;p&gt;&amp;quot;P&amp;quot;: [&amp;quot;apple a7 processors&amp;quot;], &lt;/p&gt; &lt;p&gt;&amp;quot;N&amp;quot;: [&amp;quot;Apple&amp;quot;, &amp;quot;Apple A7&amp;quot;], &lt;/p&gt; &lt;p&gt;} &lt;/p&gt; &lt;p&gt;Input: How much is the wings contract Output: &lt;/p&gt; &lt;p&gt;{ &amp;quot;P&amp;quot;: [&amp;quot;wings contract&amp;quot;],&lt;/p&gt; &lt;p&gt;&amp;quot;N&amp;quot;: [&amp;quot;wings&amp;quot;], &lt;/p&gt; &lt;p&gt;} &lt;/p&gt; &lt;p&gt;Input: what are the different tiers of wotc? &lt;/p&gt; &lt;p&gt;Output: &lt;/p&gt; &lt;p&gt;{&lt;/p&gt; &lt;p&gt;&amp;quot;P&amp;quot;: [&amp;quot;wotc tiers&amp;quot;], &lt;/p&gt; &lt;p&gt;&amp;quot;N&amp;quot;: [&amp;quot;WOTC&amp;quot;], &lt;/p&gt; &lt;p&gt;} &lt;/p&gt; &lt;p&gt;Input: what is included in the QuickBooks General Journal report Output: &lt;/p&gt; &lt;p&gt;{&lt;/p&gt; &lt;p&gt;&amp;quot;P&amp;quot;: [&amp;quot;QuickBooks General Journal report&amp;quot;], &lt;/p&gt; &lt;p&gt;&amp;quot;N&amp;quot;: [&amp;quot;Quickbooks&amp;quot;], &lt;/p&gt; &lt;p&gt;} &lt;/p&gt; &lt;p&gt;Example &amp;quot;&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Why did we decide on this prompt? &lt;/p&gt; &lt;ul&gt; &lt;li&gt;We use P &amp;amp; N because the less tokens than doing something like named entities &amp;amp; keywords. This mean there&amp;#39;s less tokens the LLM needs to return. (The average response time is 1.1 seconds.)&lt;/li&gt; &lt;li&gt;We found you need a large example set for the LLM to understand what you&amp;#39;re trying to do.&lt;/li&gt; &lt;li&gt;We recommend tuning these prompts to questions that your customer may similarly ask&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Next step:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;After finding the top 20 vectors, we re-construct the document. Because re-rankers tend to work better, and we are giving them additional context, we&amp;#39;ve found that we almost always return the most relevant chunks to answer the question. Here&amp;#39;s our&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dtr49t/agent_rag_parallel_quotes_how_we_built_rag_on/&quot;&gt; article&lt;/a&gt; for going from vectors to search.&lt;/p&gt; &lt;p&gt;Happy to answer any and all questions!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/coolcloud&quot;&gt; /u/coolcloud &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dzfp48/agent_retrieval_how_we_almost_always_find_the/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dzfp48/agent_retrieval_how_we_almost_always_find_the/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1dzfp48</id><media:thumbnail url="https://b.thumbs.redditmedia.com/Pb2SWSeA1tyEw8SKW3E2DTFD13TWyENVp-4WWyttigA.jpg" /><link href="https://www.reddit.com/r/LangChain/comments/1dzfp48/agent_retrieval_how_we_almost_always_find_the/" /><updated>2024-07-09T22:35:02+00:00</updated><published>2024-07-09T22:35:02+00:00</published><title>Agent Retrieval - How we almost always find the right vectors. Pt 3</title></entry><entry><author><name>/u/ayiding</name><uri>https://www.reddit.com/user/ayiding</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dz6jni/is_your_langchain_too_slow_learn_c/&quot;&gt; &lt;img src=&quot;https://external-preview.redd.it/L77W14UQfAyAB_qTT6-kfw4lI0KKeCSZOdrzHWit1yg.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8501ce915ce518acc8b72102f34473ea5e7dbe1d&quot; alt=&quot;Is your LangChain too slow? Learn C&quot; title=&quot;Is your LangChain too slow? Learn C&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/ayiding&quot;&gt; /u/ayiding &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://github.com/yisding/libchatty/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dz6jni/is_your_langchain_too_slow_learn_c/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1dz6jni</id><media:thumbnail url="https://external-preview.redd.it/L77W14UQfAyAB_qTT6-kfw4lI0KKeCSZOdrzHWit1yg.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=8501ce915ce518acc8b72102f34473ea5e7dbe1d" /><link href="https://www.reddit.com/r/LangChain/comments/1dz6jni/is_your_langchain_too_slow_learn_c/" /><updated>2024-07-09T16:19:43+00:00</updated><published>2024-07-09T16:19:43+00:00</published><title>Is your LangChain too slow? Learn C</title></entry><entry><author><name>/u/DeepakBhattarai69</name><uri>https://www.reddit.com/user/DeepakBhattarai69</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I am trying to build an app with tool calling , but when ever I change the model to Gemini (gemini-1.5-pro-latest or gemini-1.5-flash) . The model just outputs empty response. It does not have any content nor does it have function call. The response is just empty. &lt;/p&gt; &lt;p&gt;I just have one function/tool i.e the taviliy search tool. &lt;/p&gt; &lt;p&gt;Even when I ask it , latest news or some other question . Its just gives empty response. &lt;/p&gt; &lt;pre&gt;&lt;code&gt;import { env } from &amp;quot;@/env&amp;quot;; import { ChatAnthropic } from &amp;quot;@langchain/anthropic&amp;quot;; import { ChatGoogleGenerativeAI } from &amp;quot;@langchain/google-genai&amp;quot;; import { ChatOpenAI, type ChatOpenAICallOptions } from &amp;quot;@langchain/openai&amp;quot;; import { z } from &amp;quot;zod&amp;quot;; type Model = | ChatOpenAI&amp;lt;ChatOpenAICallOptions&amp;gt; | ChatAnthropic | ChatGoogleGenerativeAI; export const AvailableModels = z.enum([&amp;quot;gpt&amp;quot;, &amp;quot;claude&amp;quot;, &amp;quot;gemini&amp;quot;]); export type AvailableModels = z.infer&amp;lt;typeof AvailableModels&amp;gt;; export function modelPicker( model : z.infer&amp;lt;typeof AvailableModels&amp;gt;, stream ?: boolean, modelName = &amp;quot;gpt-4o&amp;quot;, ) { let modelObject: Model; switch ( model ) { case &amp;quot;gpt&amp;quot;: { modelObject = new ChatOpenAI({ model: modelName , apiKey: env.OPENAI_API_KEY, streaming: stream , modelKwargs: stream ? { parallel_tool_calls: false, } : undefined, }); break ; } case &amp;quot;claude&amp;quot;: { modelObject = new ChatAnthropic({ model: &amp;quot;claude-3-sonnet-20240229&amp;quot;, modelName: &amp;quot;claude-3-sonnet-20240229&amp;quot;, apiKey: env.ANTHROPIC_API_KEY, streaming: true, }); // modelObject.streaming = true; break ; } case &amp;quot;gemini&amp;quot;: { modelObject = new ChatGoogleGenerativeAI({ model: &amp;quot;gemini-pro&amp;quot;, modelName: &amp;quot;gemini-1.5-flash-latest&amp;quot;, apiKey: env.GOOGLE_AI_API_KEY, // streaming: stream, }); break ; } } return modelObject; } &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This is the code for selecting the models. &lt;/p&gt; &lt;p&gt;Is this the problem with langchain or the gemini api&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/DeepakBhattarai69&quot;&gt; /u/DeepakBhattarai69 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dzp16b/tool_calling_function_calling_not_working_with/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dzp16b/tool_calling_function_calling_not_working_with/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dzp16b</id><link href="https://www.reddit.com/r/LangChain/comments/1dzp16b/tool_calling_function_calling_not_working_with/" /><updated>2024-07-10T06:40:20+00:00</updated><published>2024-07-10T06:40:20+00:00</published><title>Tool Calling / Function Calling not working with Gemini</title></entry><entry><author><name>/u/innocent_kittyy</name><uri>https://www.reddit.com/user/innocent_kittyy</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey everyone, Need help on this, During retrieval when I try asking questions, initially I get the correct answers, but as the number of documents whose embeddings I am storing ,increases,i start getting incorrect data.&lt;/p&gt; &lt;p&gt;I have tried multiple chunk sizes with the various embedding models but this issue is persistent.&lt;/p&gt; &lt;p&gt;Any suggestions on it? PS: my pdf data is very much unstructured so I am first extracting data using python libraries and then storing.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/innocent_kittyy&quot;&gt; /u/innocent_kittyy &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dzotw6/facing_issues_in_retrieval/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dzotw6/facing_issues_in_retrieval/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dzotw6</id><link href="https://www.reddit.com/r/LangChain/comments/1dzotw6/facing_issues_in_retrieval/" /><updated>2024-07-10T06:26:59+00:00</updated><published>2024-07-10T06:26:59+00:00</published><title>Facing issues in retrieval</title></entry><entry><author><name>/u/giagara</name><uri>https://www.reddit.com/user/giagara</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I personally host my app in aws using lambda for compute, s3 for storage and rds (postgres) for vector db. There are some sqs, dynamo, etc but are for statistic purpose.&lt;/p&gt; &lt;p&gt;Edit: i mean for commercial purpose, not just personal &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/giagara&quot;&gt; /u/giagara &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dzord2/where_do_you_host_your_rag/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dzord2/where_do_you_host_your_rag/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dzord2</id><link href="https://www.reddit.com/r/LangChain/comments/1dzord2/where_do_you_host_your_rag/" /><updated>2024-07-10T06:22:28+00:00</updated><published>2024-07-10T06:22:28+00:00</published><title>Where do you host your Rag</title></entry><entry><author><name>/u/Stunning_Cat3195</name><uri>https://www.reddit.com/user/Stunning_Cat3195</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey&lt;br/&gt; For context, I am trying to build a RAG application on scale which should support many concurrent user&lt;br/&gt; I am using ConversationalRetrievalChain and openAI gpt-4-turbo as llm, api is built using Fastapi&lt;br/&gt; for invoking chain I am using ainvoke, and for streaming using AsyncIteratorCallbackHandler&lt;/p&gt; &lt;p&gt;flow is I invoke chain in background using asyncio.create_task() and streaming from callback_handler&lt;br/&gt; Now the issue is, when multiple concurrent user are coming to application, this callback is blocking main_thread which causing response to be delayed &lt;/p&gt; &lt;p&gt;my assumption is - even if callback is async, the switching of task in async loop is too much that it is blocking the cpu/thread&lt;br/&gt; Observation: callback function on_llm_token also get&amp;#39;s block so it does not pass values to hander&lt;/p&gt; &lt;p&gt;If you have any suggestion on better profiling of application, those are also welcome, but as per my analysis this is what blocking application and limiting it to scale&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Stunning_Cat3195&quot;&gt; /u/Stunning_Cat3195 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dzoa09/langchain_with_streaming_in_production/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dzoa09/langchain_with_streaming_in_production/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dzoa09</id><link href="https://www.reddit.com/r/LangChain/comments/1dzoa09/langchain_with_streaming_in_production/" /><updated>2024-07-10T05:51:46+00:00</updated><published>2024-07-10T05:51:46+00:00</published><title>LangChain with streaming in production</title></entry><entry><author><name>/u/gr8satvik</name><uri>https://www.reddit.com/user/gr8satvik</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dznzj7/langchain_agent_issue_in_realtime_information/&quot;&gt; &lt;img src=&quot;https://b.thumbs.redditmedia.com/vcDq0yOYTVHvQoU-w8eUV3AcOFZlmoa9PVi03_BvHtM.jpg&quot; alt=&quot;Langchain Agent Issue in real-time information&quot; title=&quot;Langchain Agent Issue in real-time information&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I am using&lt;/p&gt; &lt;pre&gt;&lt;code&gt;search = TavilySearchResults() &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;as one of the tool to create a tool calling agent.&lt;/p&gt; &lt;p&gt;But since it search the web in real-time(if i m not wrong), it is providing me results dating back to 16th Nov &amp;#39;23&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/e4jvvhiunmbd1.png?width=1136&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a10e5d4de6852036b5eee0d3ccfbc3f0cce5ee1b&quot;&gt;https://preview.redd.it/e4jvvhiunmbd1.png?width=1136&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a10e5d4de6852036b5eee0d3ccfbc3f0cce5ee1b&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/757ggiiunmbd1.png?width=1828&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2509d57b7937cb7b3df8434b1a0f1ec4486d2fc4&quot;&gt;https://preview.redd.it/757ggiiunmbd1.png?width=1828&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2509d57b7937cb7b3df8434b1a0f1ec4486d2fc4&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/3b7ejliunmbd1.png?width=1834&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a8e9958b57266770ea7ee08eb9eff811f73fe8ae&quot;&gt;https://preview.redd.it/3b7ejliunmbd1.png?width=1834&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a8e9958b57266770ea7ee08eb9eff811f73fe8ae&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/fqh00jiunmbd1.png?width=1817&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c28148779b6a6efeba3057fae724de906974d691&quot;&gt;https://preview.redd.it/fqh00jiunmbd1.png?width=1817&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c28148779b6a6efeba3057fae724de906974d691&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/9egk1kiunmbd1.png?width=1343&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=abd77cfa5d5599839c68a6f05d632d0046af37aa&quot;&gt;https://preview.redd.it/9egk1kiunmbd1.png?width=1343&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=abd77cfa5d5599839c68a6f05d632d0046af37aa&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Context: I am making a chatbot, that i basically a financial advisor, but also has the capability to tell information about stock in real time&lt;br/&gt; So for financial advisor part I am using RAG and using a tool as retriever to get advise from my knowledge base&lt;br/&gt; But while using Tavely, it is not providing information in real time&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/gr8satvik&quot;&gt; /u/gr8satvik &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dznzj7/langchain_agent_issue_in_realtime_information/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dznzj7/langchain_agent_issue_in_realtime_information/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1dznzj7</id><media:thumbnail url="https://b.thumbs.redditmedia.com/vcDq0yOYTVHvQoU-w8eUV3AcOFZlmoa9PVi03_BvHtM.jpg" /><link href="https://www.reddit.com/r/LangChain/comments/1dznzj7/langchain_agent_issue_in_realtime_information/" /><updated>2024-07-10T05:32:59+00:00</updated><published>2024-07-10T05:32:59+00:00</published><title>Langchain Agent Issue in real-time information</title></entry><entry><author><name>/u/ArtisticDirt1341</name><uri>https://www.reddit.com/user/ArtisticDirt1341</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;How to deploy for scaling, model parallelism on GPUs, industry best practices.&lt;/p&gt; &lt;p&gt;Anything works, ytb videos, blogs, articles,books&lt;/p&gt; &lt;p&gt;I’m all ears. Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/ArtisticDirt1341&quot;&gt; /u/ArtisticDirt1341 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dz1hxg/where_to_learn_ai_system_design/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dz1hxg/where_to_learn_ai_system_design/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dz1hxg</id><link href="https://www.reddit.com/r/LangChain/comments/1dz1hxg/where_to_learn_ai_system_design/" /><updated>2024-07-09T12:45:52+00:00</updated><published>2024-07-09T12:45:52+00:00</published><title>Where to learn AI system design?</title></entry><entry><author><name>/u/Big_Barracuda_6753</name><uri>https://www.reddit.com/user/Big_Barracuda_6753</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I am developing a PDF RAG app .&lt;br/&gt; In the previous version of Langchain ( i.e. v0.1 ) , we had RetrievalQA class , but now it is deprecated ( &lt;a href=&quot;https://api.python.langchain.com/en/latest/chains/langchain.chains.retrieval_qa.base.RetrievalQA.html&quot;&gt;https://api.python.langchain.com/en/latest/chains/langchain.chains.retrieval_qa.base.RetrievalQA.html&lt;/a&gt; ).&lt;/p&gt; &lt;p&gt;In this same doc , it is mentioned to use create_retrieval_chain . &lt;/p&gt; &lt;p&gt;But but but , in the RetrievalQA class we had an option to pass memory as parameter . How to pass memory ( ConversationSummaryMemory ) in this create_retrieval_chain class ?&lt;/p&gt; &lt;pre&gt;&lt;code&gt; rag = RetrievalQA.from_chain_type( llm=ChatOpenAI( temperature=0.5, model=&amp;quot;gpt-3.5-turbo-0125&amp;quot;, ), retriever=compression_retriever, memory=ConversationSummaryMemory( llm=ChatOpenAI( temperature=0.5, model=&amp;quot;gpt-3.5-turbo-0125&amp;quot;, ) ), chain_type_kwargs={&amp;quot;prompt&amp;quot;: pt, &amp;quot;verbose&amp;quot;: True}, ) response = rag.invoke(question) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The above code snippet is for RetrievalQA class . You can see that I pass ConversationSummaryMemory in the memory parameter .&lt;/p&gt; &lt;pre&gt;&lt;code&gt; system_prompt = ( &amp;quot;You are an assistant for question-answering tasks. &amp;quot; &amp;quot;Use the following pieces of retrieved context to answer &amp;quot; &amp;quot;the question. If you don&amp;#39;t know the answer, say that you &amp;quot; &amp;quot;don&amp;#39;t know. Use three sentences maximum and keep the &amp;quot; &amp;quot;answer concise.&amp;quot; &amp;quot;\n\n&amp;quot; &amp;quot;{context}&amp;quot; ) chatPrompt = ChatPromptTemplate.from_messages( [ (&amp;quot;system&amp;quot;, system_prompt), (&amp;quot;human&amp;quot;, &amp;quot;{input}&amp;quot;), ] ) question_answer_chain = create_stuff_documents_chain(llm, chatPrompt) compression_retriever = reRanker() rag_chain = create_retrieval_chain(compression_retriever, question_answer_chain) response = rag_chain.invoke({&amp;quot;input&amp;quot;:prompt }) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The above code is for the create_retrieval_chain I&amp;#39;m using .&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Big_Barracuda_6753&quot;&gt; /u/Big_Barracuda_6753 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dznl59/how_to_use_conversationsummarymemory_with_create/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dznl59/how_to_use_conversationsummarymemory_with_create/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dznl59</id><link href="https://www.reddit.com/r/LangChain/comments/1dznl59/how_to_use_conversationsummarymemory_with_create/" /><updated>2024-07-10T05:08:12+00:00</updated><published>2024-07-10T05:08:12+00:00</published><title>How to use ConversationSummaryMemory with create_retrieval_chain ?</title></entry><entry><author><name>/u/JussiCook</name><uri>https://www.reddit.com/user/JussiCook</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hello!&lt;/p&gt; &lt;p&gt;I&amp;#39;m pretty new to langchain and ML in general, so asking for advice.&lt;/p&gt; &lt;p&gt;I have around 6200 html files from which I would like to create an RAG application. I&amp;#39;m also overwhelmed on the different modules and options which are offered through langchain.. So what should I do with the files? :)&lt;/p&gt; &lt;p&gt;Is it a good practice to use html files, or should the data be in some other format?&lt;/p&gt; &lt;p&gt;For starters, I would be satisfied if I could find relevant documents from query keywords. I&amp;#39;m ok using openAI embeddings if that is needed, but when following one tutorial, it ends up crashing due to rate limits..&lt;/p&gt; &lt;p&gt;Any pointers, advice or whatever is appreciated! Thank you! :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/JussiCook&quot;&gt; /u/JussiCook &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dz7deq/rag_from_html_files/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dz7deq/rag_from_html_files/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dz7deq</id><link href="https://www.reddit.com/r/LangChain/comments/1dz7deq/rag_from_html_files/" /><updated>2024-07-09T16:53:07+00:00</updated><published>2024-07-09T16:53:07+00:00</published><title>RAG from html files</title></entry><entry><author><name>/u/hi87</name><uri>https://www.reddit.com/user/hi87</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;The docs and vids are too basic. Im trying to understand state management and cant find information on how to modify specific instances of the checkpoints/state.&lt;/p&gt; &lt;p&gt;For example. If the user double texts, Id like to stop execution mid-way and delete the last human message and add a new one. The tutorials only show how to modify an existing message but not how to delete certain messages. Im using the sqlite class for checkpointer.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/hi87&quot;&gt; /u/hi87 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dzj6oy/better_documentation_on_state/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dzj6oy/better_documentation_on_state/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dzj6oy</id><link href="https://www.reddit.com/r/LangChain/comments/1dzj6oy/better_documentation_on_state/" /><updated>2024-07-10T01:14:39+00:00</updated><published>2024-07-10T01:14:39+00:00</published><title>Better Documentation on State</title></entry><entry><author><name>/u/Plane_Past129</name><uri>https://www.reddit.com/user/Plane_Past129</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Plane_Past129&quot;&gt; /u/Plane_Past129 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dyxty1/can_you_suggest_me_some_best_llms_for_rag/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dyxty1/can_you_suggest_me_some_best_llms_for_rag/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dyxty1</id><link href="https://www.reddit.com/r/LangChain/comments/1dyxty1/can_you_suggest_me_some_best_llms_for_rag/" /><updated>2024-07-09T09:07:15+00:00</updated><published>2024-07-09T09:07:15+00:00</published><title>Can you suggest me some best LLM's for RAG application. We want to host it for an enterprise in their EC2.</title></entry><entry><author><name>/u/asim-shrestha</name><uri>https://www.reddit.com/user/asim-shrestha</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dzblj0/using_gpt4_to_extract_web_data/&quot;&gt; &lt;img src=&quot;https://external-preview.redd.it/AZnAbecnLFeFfuBog2wytf-VAsi9l8rSN_Wa9H1rf80.jpg?width=216&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e9da828d4de651f1033f9ed27de3c47f87d9ce76&quot; alt=&quot;Using GPT4 to extract web data&quot; title=&quot;Using GPT4 to extract web data&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;&lt;a href=&quot;https://reddit.com/link/1dzblj0/video/1ymwu5okrjbd1/player&quot;&gt;https://reddit.com/link/1dzblj0/video/1ymwu5okrjbd1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Heyo folks, wanting to share a project we&amp;#39;ve been working on with LLM agents. The product itself leverages LLMs to parse and understand web pages to extract structured web data at scale. We&amp;#39;re doing a larger launch and would love your feedback&lt;/p&gt; &lt;p&gt;Open source projects: &lt;a href=&quot;https://github.com/reworkd/&quot;&gt;https://github.com/reworkd/&lt;/a&gt;&lt;br/&gt; Site: &lt;a href=&quot;https://reworkd.ai/&quot;&gt;https://reworkd.ai/&lt;/a&gt;&lt;br/&gt; More info if needed! &lt;a href=&quot;https://x.com/asimdotshrestha/status/1810720478111371581&quot;&gt;https://x.com/asimdotshrestha/status/1810720478111371581&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/asim-shrestha&quot;&gt; /u/asim-shrestha &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dzblj0/using_gpt4_to_extract_web_data/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dzblj0/using_gpt4_to_extract_web_data/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1dzblj0</id><media:thumbnail url="https://external-preview.redd.it/AZnAbecnLFeFfuBog2wytf-VAsi9l8rSN_Wa9H1rf80.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=e9da828d4de651f1033f9ed27de3c47f87d9ce76" /><link href="https://www.reddit.com/r/LangChain/comments/1dzblj0/using_gpt4_to_extract_web_data/" /><updated>2024-07-09T19:46:43+00:00</updated><published>2024-07-09T19:46:43+00:00</published><title>Using GPT4 to extract web data</title></entry><entry><author><name>/u/omarbhl</name><uri>https://www.reddit.com/user/omarbhl</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey guys, im still new to the LLM world now im working on a project using FastAPI as an intermediate between frontend and Ollama local server, im having trouble with integrating it using langchain, I want to tweak some parameters on the Llama model using langchain but cant get it working.&lt;/p&gt; &lt;p&gt;Another issue is RAG, i dont really know how the procedure should go, I want some suggestion and help is pretty much appreciated!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/omarbhl&quot;&gt; /u/omarbhl &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dza22u/need_help_with_ollama_langchain_integration/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dza22u/need_help_with_ollama_langchain_integration/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dza22u</id><link href="https://www.reddit.com/r/LangChain/comments/1dza22u/need_help_with_ollama_langchain_integration/" /><updated>2024-07-09T18:42:46+00:00</updated><published>2024-07-09T18:42:46+00:00</published><title>Need help with Ollama Langchain integration</title></entry><entry><author><name>/u/MelodicHyena5029</name><uri>https://www.reddit.com/user/MelodicHyena5029</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;So here’s the deal, I’m developing a data extraction pipeline from scratch and I’d love to hear your suggestions on different ways to extract images/diagrams within pdf pages. &lt;/p&gt; &lt;p&gt;FYI : 1) I have experimented with pymupdf and pdfplumber, both is excelled at only extracting explicit images. Diagrams are missing.&lt;/p&gt; &lt;p&gt;2) I have a general detection model with trained upon more than 20k labels, using that comes with a limitation that the model could only classifies images based on the labels it’s been trained upon, (so I have to look for some model which does well as zero shot detection)&lt;/p&gt; &lt;p&gt;3) current solution - Unstructured IO seemingly detects all the diagrams and images, which is fulfilling my purpose, but the problem is its kinda bloated and need additional dependencies!&lt;/p&gt; &lt;p&gt;I assume unstructured under the hood uses an onnx yolo model or something to detect, so if you by chance workjng on similar projects, do suggest me some good ways to do it. Thanks in advance !&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/MelodicHyena5029&quot;&gt; /u/MelodicHyena5029 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dywmnv/methods_to_extract_imagesdiagrams_from_pdfs/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dywmnv/methods_to_extract_imagesdiagrams_from_pdfs/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dywmnv</id><link href="https://www.reddit.com/r/LangChain/comments/1dywmnv/methods_to_extract_imagesdiagrams_from_pdfs/" /><updated>2024-07-09T07:42:59+00:00</updated><published>2024-07-09T07:42:59+00:00</published><title>Methods to extract images/diagrams from PDFs</title></entry><entry><author><name>/u/Key-Mortgage-1515</name><uri>https://www.reddit.com/user/Key-Mortgage-1515</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey guys. I&amp;#39;m working on chatbot+ stream lit webapp which is working fine in local but when ever I tried to deploy it on stream lit or gradio it&amp;#39;s give me lib site error. If anyone have some good resources please share or alternative. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Key-Mortgage-1515&quot;&gt; /u/Key-Mortgage-1515 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dz8y0w/mistrial_deployment/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dz8y0w/mistrial_deployment/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dz8y0w</id><link href="https://www.reddit.com/r/LangChain/comments/1dz8y0w/mistrial_deployment/" /><updated>2024-07-09T17:57:07+00:00</updated><published>2024-07-09T17:57:07+00:00</published><title>Mistrial deployment</title></entry><entry><author><name>/u/EducatorSouth6473</name><uri>https://www.reddit.com/user/EducatorSouth6473</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I want to add an extra input to the SQLDatabaseChain method, it’s currently configured to only accept one I.e. question. Plus table_info and dialect it gets from the SQLDatabase connection. I want to pass a precalculated mappings object to the prompt so that it’s used to generate accurate queries. Is there away to achieve this?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/EducatorSouth6473&quot;&gt; /u/EducatorSouth6473 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dz5bp2/adding_additional_input_to_sqldatabasechain/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dz5bp2/adding_additional_input_to_sqldatabasechain/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dz5bp2</id><link href="https://www.reddit.com/r/LangChain/comments/1dz5bp2/adding_additional_input_to_sqldatabasechain/" /><updated>2024-07-09T15:30:29+00:00</updated><published>2024-07-09T15:30:29+00:00</published><title>Adding additional Input to SQLDatabaseChain</title></entry><entry><author><name>/u/mehul_gupta1997</name><uri>https://www.reddit.com/user/mehul_gupta1997</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/mehul_gupta1997&quot;&gt; /u/mehul_gupta1997 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;/r/learnmachinelearning/comments/1dz4n90/how_graphrag_works_explained/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dz4r9f/how_graphrag_works_explained/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dz4r9f</id><link href="https://www.reddit.com/r/LangChain/comments/1dz4r9f/how_graphrag_works_explained/" /><updated>2024-07-09T15:07:44+00:00</updated><published>2024-07-09T15:07:44+00:00</published><title>How GraphRAG works? Explained</title></entry><entry><author><name>/u/qpoToH</name><uri>https://www.reddit.com/user/qpoToH</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hello, community&lt;/p&gt; &lt;p&gt;Could you please help me to cope with streaming of the structured output in Langchain?&lt;br/&gt; The example below has a simple idea: to generate JSON of fake biographies in a stream. I have a simple FastAPI part:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;from dotenv import load_dotenv from fastapi import FastAPI from typing import AsyncIterable from fastapi.responses import StreamingResponse from langchain_openai import ChatOpenAI from langchain_core.prompts import ChatPromptTemplate from langchain_core.pydantic_v1 import BaseModel, Field from langchain_core.output_parsers import JsonOutputParser load_dotenv() app = FastAPI() prompt = ChatPromptTemplate.from_messages((&amp;#39;human&amp;#39;, &amp;quot;Generate biography for {n} persons&amp;quot;)) model = ChatOpenAI(model=&amp;quot;gpt-3.5-turbo-0125&amp;quot;) class Biography(BaseModel): name: str = Field(description=&amp;#39;The first name of the person&amp;#39;) surname: str = Field(description=&amp;#39;The surname of the person&amp;#39;) birth_place: str = Field(description=&amp;#39;The birth place of the person&amp;#39;) biography: str = Field(description=&amp;#39;The biography of the person&amp;#39;) class Biographies(BaseModel): biographies: list[Biography] = Field(description=&amp;#39;The list of biographies of the persons&amp;#39;) model = model.with_structured_output(Biographies) chain = prompt | model | JsonOutputParser() async def send_message(n_persons: int) -&amp;gt; AsyncIterable[str]: async for chunk in chain.astream({&amp;#39;n&amp;#39;: n_persons}): yield chunk u/app.post(&amp;quot;/stream_biographies/&amp;quot;) async def stream_biographies(persons: int): generator = send_message(persons) return StreamingResponse(generator, media_type=&amp;quot;text/event-stream&amp;quot;) if __name__ == &amp;#39;__main__&amp;#39;: import uvicorn uvicorn.run(app)from dotenv import load_dotenv &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;And a snippet, how I test it:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;import httpx url = &amp;quot;http://127.0.0.1:8000/stream_biographies/&amp;quot; data = {&amp;quot;persons&amp;quot;: 2} headers = {&amp;quot;Content-type&amp;quot;: &amp;quot;application/json&amp;quot;, &amp;#39;accept&amp;#39;: &amp;#39;application/json&amp;#39;} with httpx.stream(&amp;#39;POST&amp;#39;, url, params=data, headers=headers) as r: for chunk in r.iter_text(): print(chunk)import httpx url = &amp;quot;http://127.0.0.1:8000/stream_biographies/&amp;quot; data = {&amp;quot;persons&amp;quot;: 2} headers = {&amp;quot;Content-type&amp;quot;: &amp;quot;application/json&amp;quot;, &amp;#39;accept&amp;#39;: &amp;#39;application/json&amp;#39;} with httpx.stream(&amp;#39;POST&amp;#39;, url, params=data, headers=headers) as r: for chunk in r.iter_text(): print(chunk) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;I expect json generation on the fly, like in this example &lt;a href=&quot;https://baml-examples.vercel.app/examples/get-recipe&quot;&gt;https://baml-examples.vercel.app/examples/get-recipe&lt;/a&gt;&lt;/p&gt; &lt;p&gt;However, I get error&lt;/p&gt; &lt;pre&gt;&lt;code&gt;pydantic.v1.error_wrappers.ValidationError: 1 validation error for GenerationChunk text str type expected (type=type_error.str) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;If I remove output parser from the chain, I get &lt;code&gt;AttributeError: &amp;#39;Biographies&amp;#39; object has no attribute &amp;#39;encode&amp;#39;&lt;/code&gt;&lt;/p&gt; &lt;p&gt;So I think, the problem is in returning Biographies object, and not the JSON from the chain, but I don&amp;#39;t know, how to fix it. Any help is appreciated&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/qpoToH&quot;&gt; /u/qpoToH &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dz4ga0/stream_with_structured_output/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dz4ga0/stream_with_structured_output/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dz4ga0</id><link href="https://www.reddit.com/r/LangChain/comments/1dz4ga0/stream_with_structured_output/" /><updated>2024-07-09T14:55:35+00:00</updated><published>2024-07-09T14:55:35+00:00</published><title>Stream with structured output</title></entry><entry><author><name>/u/New-Contribution6302</name><uri>https://www.reddit.com/user/New-Contribution6302</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I wanted to use LLama2 instruct 32k for summarisation task. I tried to load the llm with n_ctx=16384, rope_freq_scale=0.25 and 0.125. But sometimes I get the output empty and sometimes i don&amp;#39;t even get one and the system gets crashed.&lt;/p&gt; &lt;p&gt;I worked this out in college t4 GPU session, kaggle&amp;#39;s 2x t4 GPU session, and my local session with 32GB RAM and rtx 3050 6gb vRAM system. &lt;/p&gt; &lt;p&gt;Any suggestions on how to load the llm and What will be the minimum hardware requirement. Model used: LLama2-instruct-32k-Q4_K_M.gguf by TheBloke&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/New-Contribution6302&quot;&gt; /u/New-Contribution6302 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dyz9eh/guidance_requested_on_llama2_32k_instruct_usage/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dyz9eh/guidance_requested_on_llama2_32k_instruct_usage/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dyz9eh</id><link href="https://www.reddit.com/r/LangChain/comments/1dyz9eh/guidance_requested_on_llama2_32k_instruct_usage/" /><updated>2024-07-09T10:41:45+00:00</updated><published>2024-07-09T10:41:45+00:00</published><title>Guidance requested on Llama2 32k instruct usage and specifications</title></entry><entry><author><name>/u/Beginning_Edge347</name><uri>https://www.reddit.com/user/Beginning_Edge347</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I have two backend services running, one is the application server and the runs the langchain agent service. I have a requirement where i need to ask for input from users from a web interface. The HumanInput tool currently works by getting input from the python terminal.&lt;br/&gt; How do I change it so that I can ask for input from the user in the web interface?&lt;/p&gt; &lt;p&gt;The only approach I could come up with was to use websockets(supabase) and ask for input by sending this message from agent server to the frontend and the human input will be passed back to the agent server via the same socket channel.&lt;/p&gt; &lt;p&gt;Are there any other better ways to accomplish this?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Beginning_Edge347&quot;&gt; /u/Beginning_Edge347 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dyyztx/folks_how_do_i_make_use_of_the_humaninput_tool_in/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dyyztx/folks_how_do_i_make_use_of_the_humaninput_tool_in/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dyyztx</id><link href="https://www.reddit.com/r/LangChain/comments/1dyyztx/folks_how_do_i_make_use_of_the_humaninput_tool_in/" /><updated>2024-07-09T10:25:07+00:00</updated><published>2024-07-09T10:25:07+00:00</published><title>Folks, how do i make use of the HumanInput tool in a web interface application?</title></entry><entry><author><name>/u/jscraft</name><uri>https://www.reddit.com/user/jscraft</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey folks 👋! I&amp;#39;ve wrote a short guide on hot to use R&lt;code&gt;unnableBranch&lt;/code&gt;for route to different prompts in LangChain.js &lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://www.js-craft.io/blog/langchain-runnablebranch-javascript/&quot;&gt;https://www.js-craft.io/blog/langchain-runnablebranch-javascript/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;As a side note keep in mind but in the future, using a &lt;a href=&quot;https://www.js-craft.io/blog/routing-langchain-js-different-prompts-based-on-query-type/&quot;&gt;custom RunnableLambda router function &lt;/a&gt;is the better way to do routing. More details &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dy26hp/will_runnablebranch_be_removed_from_future/&quot;&gt;here&lt;/a&gt;. &lt;/p&gt; &lt;p&gt;Nevertheless, I think it&amp;#39;s good to know about &lt;code&gt;RunnableBranch&lt;/code&gt; in case you see it in some codebase. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/jscraft&quot;&gt; /u/jscraft &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dyxjos/using_runnablebranch_to_route_execution_to/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dyxjos/using_runnablebranch_to_route_execution_to/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dyxjos</id><link href="https://www.reddit.com/r/LangChain/comments/1dyxjos/using_runnablebranch_to_route_execution_to/" /><updated>2024-07-09T08:47:45+00:00</updated><published>2024-07-09T08:47:45+00:00</published><title>Using RunnableBranch to Route Execution to Different Prompts in LangChain.js</title></entry><entry><author><name>/u/Either-Ambassador738</name><uri>https://www.reddit.com/user/Either-Ambassador738</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;m building a graph and I need to create an agent, however this agent is itself the llm, it doesn&amp;#39;t need to call external tools, it just needs to process the user request given a system prompt, I currently have this code:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;def create_agent(llm: ChatOpenAI, tools: list, system_prompt: str): prompt = ChatPromptTemplate.from_messages( [ ( &amp;quot;system&amp;quot;, system_prompt, ), MessagesPlaceholder(variable_name=&amp;quot;messages&amp;quot;), MessagesPlaceholder(variable_name=&amp;quot;agent_scratchpad&amp;quot;) ] ) agent = create_openai_tools_agent(llm, prompt=prompt) executor = AgentExecutor(agent=agent) return executor def agent_node(state: State, agent: AgentExecutor, name: str): result = agent.invoke(state) return {&amp;quot;messages&amp;quot;: [HumanMessage(content=result[&amp;quot;output&amp;quot;], name=name)]} &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Does anyone how can I approach this?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Either-Ambassador738&quot;&gt; /u/Either-Ambassador738 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dyqlux/can_an_llm_be_used_as_a_tool/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dyqlux/can_an_llm_be_used_as_a_tool/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dyqlux</id><link href="https://www.reddit.com/r/LangChain/comments/1dyqlux/can_an_llm_be_used_as_a_tool/" /><updated>2024-07-09T01:53:38+00:00</updated><published>2024-07-09T01:53:38+00:00</published><title>Can an LLM be used as a tool?</title></entry><entry><author><name>/u/bferreira85</name><uri>https://www.reddit.com/user/bferreira85</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi. I managed to create an agent using langgraph. Things work well if I run my code sending my user input to this agent.&lt;br/&gt; However, what I need now is to have this langgraph based agent being used by another agent (my main agent running the system).&lt;br/&gt; Do anybody has any example on how to do that?&lt;br/&gt; I tried creating an structured tool that will call the function run_graph that will invoke the graph but have had no success.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/bferreira85&quot;&gt; /u/bferreira85 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dyy33u/can_a_langgraph_be_used_as_a_langchain_tool/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dyy33u/can_a_langgraph_be_used_as_a_langchain_tool/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dyy33u</id><link href="https://www.reddit.com/r/LangChain/comments/1dyy33u/can_a_langgraph_be_used_as_a_langchain_tool/" /><updated>2024-07-09T09:25:00+00:00</updated><published>2024-07-09T09:25:00+00:00</published><title>Can a langgraph be used as a Langchain Tool?</title></entry></feed>