<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/"><category term="LangChain" label="r/LangChain"/><updated>2024-06-13T21:59:32+00:00</updated><icon>https://www.redditstatic.com/icon.png/</icon><id>/r/LangChain.rss</id><link rel="self" href="https://www.reddit.com/r/LangChain.rss" type="application/atom+xml" /><link rel="alternate" href="https://www.reddit.com/r/LangChain" type="text/html" /><subtitle>LangChain is an open-source framework and developer toolkit that helps developers get LLM applications from prototype to production. It is available for Python and Javascript at https://www.langchain.com/.</subtitle><title>LangChain</title><entry><author><name>/u/zchaarm</name><uri>https://www.reddit.com/user/zchaarm</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;A place for members of &lt;a href=&quot;/r/LangChain&quot;&gt;r/LangChain&lt;/a&gt; to chat with each other&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/zchaarm&quot;&gt; /u/zchaarm &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_10ljho9</id><link href="https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/" /><updated>2023-01-26T04:33:05+00:00</updated><published>2023-01-26T04:33:05+00:00</published><title>r/LangChain Lounge</title></entry><entry><author><name>/u/cryptokaykay</name><uri>https://www.reddit.com/user/cryptokaykay</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dfaquj/run_evaluations_with_langtrace/&quot;&gt; &lt;img src=&quot;https://b.thumbs.redditmedia.com/OlQpvPg6C80CPbeQqL74YpdIgrAULdbnNlYTTliAWPg.jpg&quot; alt=&quot;Run Evaluations with Langtrace&quot; title=&quot;Run Evaluations with Langtrace&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi all,&lt;/p&gt; &lt;p&gt;Its been a while from me, but just wanted to share that we have added support for running automated evals with Langtrace. As a reminder, Langtrace is an open source LLM application observability and evaluations tool. It is open telemetry compatible so no vendor lock-in. You can also self-host and run Langtrace.&lt;/p&gt; &lt;p&gt;We integrated langtrace with inspect AI (&lt;a href=&quot;https://github.com/UKGovernmentBEIS/inspect%5C_ai&quot;&gt;https://github.com/UKGovernmentBEIS/inspect\_ai&lt;/a&gt;). Inspect is an open source evluations tool from the developers of RStudio - you should definitely check it out. I love it.&lt;br/&gt; With langtrace, you can now&lt;/p&gt; &lt;ul&gt; &lt;li&gt;set up tracing in 2 lines of code&lt;/li&gt; &lt;li&gt;annotate and curate datasets&lt;/li&gt; &lt;li&gt;run evaluations against this dataset using Inspect&lt;/li&gt; &lt;li&gt;view results, compare the outputs against models and understand the performance of your app&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;So, you can now establish this feedback loop with langtrace.&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/qrwn7r1kte6d1.png?width=2304&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3c2d7c82abbb329518b35c133c0e7a0e73a6d53d&quot;&gt;https://preview.redd.it/qrwn7r1kte6d1.png?width=2304&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3c2d7c82abbb329518b35c133c0e7a0e73a6d53d&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Shown below are some screenshots:&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/t45vq2xute6d1.png?width=3156&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1c15fc71499ba5c5ccbf0aa566fc78c82730e209&quot;&gt;https://preview.redd.it/t45vq2xute6d1.png?width=3156&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1c15fc71499ba5c5ccbf0aa566fc78c82730e209&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/0gwmyz0xte6d1.png?width=3150&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2713ba619e903d2db227d5922e8e9c7a562fb9b7&quot;&gt;https://preview.redd.it/0gwmyz0xte6d1.png?width=3150&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2713ba619e903d2db227d5922e8e9c7a562fb9b7&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Would love get any feedback. Please do try it out and let me know.&lt;/p&gt; &lt;p&gt;Link: &lt;a href=&quot;https://github.com/Scale3-Labs/langtrace&quot;&gt;https://github.com/Scale3-Labs/langtrace&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/cryptokaykay&quot;&gt; /u/cryptokaykay &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dfaquj/run_evaluations_with_langtrace/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dfaquj/run_evaluations_with_langtrace/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1dfaquj</id><media:thumbnail url="https://b.thumbs.redditmedia.com/OlQpvPg6C80CPbeQqL74YpdIgrAULdbnNlYTTliAWPg.jpg" /><link href="https://www.reddit.com/r/LangChain/comments/1dfaquj/run_evaluations_with_langtrace/" /><updated>2024-06-13T21:50:44+00:00</updated><published>2024-06-13T21:50:44+00:00</published><title>Run Evaluations with Langtrace</title></entry><entry><author><name>/u/vT_Raven</name><uri>https://www.reddit.com/user/vT_Raven</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hello everyone, I mean no disrespect to anyone but I am having trouble seeing the appeal of using the lang chain. In my opinion I&amp;#39;am at best a beginner there for my view coulde be too shalow. I am hoping to find an anweser to where my blind spots are and what use cases the lang chain is useful for. For example, if I want to build a rag chatbot. I would use Ollama with Chromadb without any libery except for chromadb and requests. I have to admit that it is nice to try different things with lang chain. It is also easier to handle complex files like PDF. &lt;/p&gt; &lt;p&gt;If some of you say I don&amp;#39;t have enough experience, that&amp;#39;s why I don&amp;#39;t get it, the answer is fair enough for me to take a agaib a look at Lang Chain.&lt;/p&gt; &lt;p&gt;But I have already tried to work with the framework 3 times and it always seems too complex for what I want to achieve. All those time i build an Chatbot that allows to interact with an modell with some litte custmasation over envs. And the last time was a Rag Chatbot that allows me to index Websites to get answers about their content.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/vT_Raven&quot;&gt; /u/vT_Raven &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1df95xz/why_should_i_use_lang_chain/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1df95xz/why_should_i_use_lang_chain/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1df95xz</id><link href="https://www.reddit.com/r/LangChain/comments/1df95xz/why_should_i_use_lang_chain/" /><updated>2024-06-13T20:41:52+00:00</updated><published>2024-06-13T20:41:52+00:00</published><title>Why should i use lang chain?</title></entry><entry><author><name>/u/chaitu9701</name><uri>https://www.reddit.com/user/chaitu9701</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;To set the context we have 4 environments predev, dev, testing and production. Our RAG uses langchain for PDF extraction, qdrant(self hosted on kubernetes) for vectorstore and gpt-3.5-turbo-16k for the llm.&lt;/p&gt; &lt;p&gt;We have built a RAG, which worked well(gave correct answers from PDF) on predev. When we moved it to dev, in the initial days its performance(correctness) was bad and eventually got good. Then it moved to testing environment where again the same behaviour. Now it&amp;#39;s in prod and again behaves the same. Facing a lot of backlash.&lt;/p&gt; &lt;p&gt;It&amp;#39;s the same document, same gp, but different qdrant hosted different for different environments.&lt;/p&gt; &lt;p&gt;Did anyone experience similar issue? Can anyone explain why the warmup time.&lt;/p&gt; &lt;p&gt;Any help is greatly appreciated.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/chaitu9701&quot;&gt; /u/chaitu9701 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1df1wnb/rag_performs_differently_in_different_environments/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1df1wnb/rag_performs_differently_in_different_environments/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1df1wnb</id><link href="https://www.reddit.com/r/LangChain/comments/1df1wnb/rag_performs_differently_in_different_environments/" /><updated>2024-06-13T15:35:41+00:00</updated><published>2024-06-13T15:35:41+00:00</published><title>RAG performs differently in different environments</title></entry><entry><author><name>/u/ChallengeOk6437</name><uri>https://www.reddit.com/user/ChallengeOk6437</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I am trying to build a model to take in 5-10 PDFs and answer questions based on them.&lt;/p&gt; &lt;p&gt;This is my flow ==&amp;gt; LlamaParse-&amp;gt;OpenAI ada embeddings -&amp;gt; FAISS vector store -&amp;gt; multi query retriever -&amp;gt; cohere reranker -&amp;gt; OpenAI gpt4o -&amp;gt; results&lt;/p&gt; &lt;p&gt;I also have a part in my retriever stage where I get citations and chunking is done page wise&lt;/p&gt; &lt;p&gt;The questions I ask take anywhere between 25-50 seconds to get an answer and also I am missing out on information, I have made the retriever send back all relevant pages, not just the top 3 relevant pages&lt;/p&gt; &lt;p&gt;Is there anyway to get this under 20 seconds and extract all relevant chunks with keeping in mind I need citations?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/ChallengeOk6437&quot;&gt; /u/ChallengeOk6437 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1df0apu/rag_model_too_slow/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1df0apu/rag_model_too_slow/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1df0apu</id><link href="https://www.reddit.com/r/LangChain/comments/1df0apu/rag_model_too_slow/" /><updated>2024-06-13T14:26:24+00:00</updated><published>2024-06-13T14:26:24+00:00</published><title>RAG Model TOO SLOW</title></entry><entry><author><name>/u/Glittering-Bear5748</name><uri>https://www.reddit.com/user/Glittering-Bear5748</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hello guys&lt;/p&gt; &lt;p&gt;i am creating chat bot with QA retrieval using vector DB and i want to add one more feature that is follow up question along with response to current question&lt;br/&gt; can anybody provide me example how implement it ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Glittering-Bear5748&quot;&gt; /u/Glittering-Bear5748 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1df5lc7/suggest_5_followup_question_based_on_previous/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1df5lc7/suggest_5_followup_question_based_on_previous/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1df5lc7</id><link href="https://www.reddit.com/r/LangChain/comments/1df5lc7/suggest_5_followup_question_based_on_previous/" /><updated>2024-06-13T18:11:38+00:00</updated><published>2024-06-13T18:11:38+00:00</published><title>suggest 5 follow-up question based on previous asked</title></entry><entry><author><name>/u/milkomeda22</name><uri>https://www.reddit.com/user/milkomeda22</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;m looking for recommendations on tools for chemists that can be implemented using LLM and LangChain agents. What useful tools or applications do you think can be created with these technologies? I would appreciate any ideas and suggestions.&lt;/p&gt; &lt;p&gt;Which LLMs do you recommend for laboratory automation solutions, and what data processing life cycles can be implemented by agents?&lt;/p&gt; &lt;p&gt;I&amp;#39;m particularly interested in how to work with the Canonical SMILES format using chatbots and modify it through agents.&lt;/p&gt; &lt;p&gt;I&amp;#39;m exploring this topic as a theoretical preparation for a long-term hackathon focused on the automation of chemical laboratories. All solutions will be &lt;strong&gt;published&lt;/strong&gt; and &lt;strong&gt;open source&lt;/strong&gt; after our team‚Äôs presentation.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/milkomeda22&quot;&gt; /u/milkomeda22 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dewg6w/seeking_recommendations_tools_for_chemists_using/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dewg6w/seeking_recommendations_tools_for_chemists_using/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dewg6w</id><link href="https://www.reddit.com/r/LangChain/comments/1dewg6w/seeking_recommendations_tools_for_chemists_using/" /><updated>2024-06-13T11:11:53+00:00</updated><published>2024-06-13T11:11:53+00:00</published><title>Seeking Recommendations: Tools for Chemists Using Large Language Models and Agents</title></entry><entry><author><name>/u/Capital_learner</name><uri>https://www.reddit.com/user/Capital_learner</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I have to make llm chatbit using open ai on flask. Help me to make this. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Capital_learner&quot;&gt; /u/Capital_learner &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1deh52g/need_help_to_make_langchain_chatbot/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1deh52g/need_help_to_make_langchain_chatbot/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1deh52g</id><link href="https://www.reddit.com/r/LangChain/comments/1deh52g/need_help_to_make_langchain_chatbot/" /><updated>2024-06-12T20:48:11+00:00</updated><published>2024-06-12T20:48:11+00:00</published><title>Need Help to make langchain chatbot</title></entry><entry><author><name>/u/Borfecao</name><uri>https://www.reddit.com/user/Borfecao</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I&amp;#39;m working on a Supervisor with LangGraph for a company internship. My mentor has asked me to create three Agents: &amp;quot;Question Agent&amp;quot;, &amp;quot;Answer Agent&amp;quot;, and &amp;quot;Summarizer Agent&amp;quot;. The input is a PDF, which I need to split by page and add each page to a vectorial database for later use. Each agent will also save its outputs in the vectorial POSTGRES database. Here&amp;#39;s a rough idea of the structure:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Questions Table&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;id (Primary Key)&lt;/li&gt; &lt;li&gt;question (Text)&lt;/li&gt; &lt;li&gt;embedding (Vector)&lt;/li&gt; &lt;li&gt;document_id (Integer)&lt;/li&gt; &lt;li&gt;page_number (Integer)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Answers Table&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;id (Primary Key)&lt;/li&gt; &lt;li&gt;answer (Text)&lt;/li&gt; &lt;li&gt;embedding (Vector)&lt;/li&gt; &lt;li&gt;document_id (Integer)&lt;/li&gt; &lt;li&gt;page_number (Integer)&lt;/li&gt; &lt;li&gt;question_id (Foreign Key to Questions table)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Summaries Table&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;id (Primary Key)&lt;/li&gt; &lt;li&gt;summary (Text)&lt;/li&gt; &lt;li&gt;embedding (Vector)&lt;/li&gt; &lt;li&gt;document_id (Integer)&lt;/li&gt; &lt;li&gt;page_number (Integer)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Documents Table&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;id (Primary Key)&lt;/li&gt; &lt;li&gt;summary (Text)&lt;/li&gt; &lt;li&gt;document_id (Integer)&lt;/li&gt; &lt;li&gt;number_of_pages (Integer)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The workflow is something like this:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Load the document (sanitize the text, embed it, save in &amp;quot;Documents&amp;quot;)&lt;/li&gt; &lt;li&gt;Make a summary of each page (save in &amp;quot;Summaries&amp;quot;)&lt;/li&gt; &lt;li&gt;Generate questions for each page (save in &amp;quot;Questions&amp;quot;)&lt;/li&gt; &lt;li&gt;Answer all the questions generated by the Question Agent, considering the context of the page (save in &amp;quot;Answers&amp;quot;)&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;My biggest question is:&lt;/strong&gt; what tools and agents should I implement for this? Most resources I&amp;#39;ve found online use tools like Tavily Search and Python REPL, which aren&amp;#39;t really helpful for my case. I need to use the Supervisor since it&amp;#39;s a project requirement, and I&amp;#39;m a bit confused about the implementation details, since this would be very easy to implement with simple chains, and the only solution I could come up with is tooless agents...?&lt;/p&gt; &lt;p&gt;Any advice or pointers would be greatly appreciated! Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Borfecao&quot;&gt; /u/Borfecao &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1deaviw/need_help_implementing_supervisor_with_langgraph/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1deaviw/need_help_implementing_supervisor_with_langgraph/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1deaviw</id><link href="https://www.reddit.com/r/LangChain/comments/1deaviw/need_help_implementing_supervisor_with_langgraph/" /><updated>2024-06-12T16:28:31+00:00</updated><published>2024-06-12T16:28:31+00:00</published><title>Need Help Implementing Supervisor with LangGraph</title></entry><entry><author><name>/u/huseyinbabal</name><uri>https://www.reddit.com/user/huseyinbabal</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dec8ls/building_devops_ai_assistant_with_langchain/&quot;&gt; &lt;img src=&quot;https://external-preview.redd.it/3eIYPb9fvv3T8yM1RCQU_MSfK9DpDf_d-D2eAbkLPPE.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c229377b890822b72774120bb4ab72f62d353aa2&quot; alt=&quot;Building Devops AI Assistant with Langchain, Ollama, and PostgreSQL&quot; title=&quot;Building Devops AI Assistant with Langchain, Ollama, and PostgreSQL&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/huseyinbabal&quot;&gt; /u/huseyinbabal &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://docs.rapidapp.io/blog/building-devops-ai-assistant-with-langchain-ollama-and-postgresql&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dec8ls/building_devops_ai_assistant_with_langchain/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1dec8ls</id><media:thumbnail url="https://external-preview.redd.it/3eIYPb9fvv3T8yM1RCQU_MSfK9DpDf_d-D2eAbkLPPE.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=c229377b890822b72774120bb4ab72f62d353aa2" /><link href="https://www.reddit.com/r/LangChain/comments/1dec8ls/building_devops_ai_assistant_with_langchain/" /><updated>2024-06-12T17:25:33+00:00</updated><published>2024-06-12T17:25:33+00:00</published><title>Building Devops AI Assistant with Langchain, Ollama, and PostgreSQL</title></entry><entry><author><name>/u/These-Butterfly8819</name><uri>https://www.reddit.com/user/These-Butterfly8819</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I have been given a requirement from my company to look into and try to comeup with a chatbot that would be integrated into the web application. Specifically, we have a list of Companies and their details like name, what they do, their revenues, etc. and some uploaded pdf files that contain more information regarding the company. So the chatbot will be integrated into the details page of the companies. User could then ask any question regarding the company and the chatbot should provide a relevant answer for that company.&lt;/p&gt; &lt;p&gt;I am fairly new to this, but was able to find out that we can use RAG for achieving this, wherein we take all the data and embed it in a vector database. Then fetch relevant vectors per the question asked and provide it as context to the LLM for answer.&lt;/p&gt; &lt;p&gt;However the issue is that some of the data of the company can change with time.&lt;/p&gt; &lt;p&gt;Is there a way to do it so that the pdf data can use vector store, but the rest of the data can be obtained from API calls? That way, we will always have the most recent data of the company, but also have the additional data from the pdf docs?&lt;/p&gt; &lt;p&gt;How would all these things fit? How would the decision be made when to use data from vector database or when to fetch data from API?&lt;/p&gt; &lt;p&gt;Do you guys have any experience with something like this or any recommendations or resources where I can look into for this project?&lt;/p&gt; &lt;p&gt;That would be very helpful.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/These-Butterfly8819&quot;&gt; /u/These-Butterfly8819 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1de6133/help_regarding_application_specific_chatbot/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1de6133/help_regarding_application_specific_chatbot/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1de6133</id><link href="https://www.reddit.com/r/LangChain/comments/1de6133/help_regarding_application_specific_chatbot/" /><updated>2024-06-12T12:58:22+00:00</updated><published>2024-06-12T12:58:22+00:00</published><title>Help regarding application specific chatbot</title></entry><entry><author><name>/u/thumbsdrivesmecrazy</name><uri>https://www.reddit.com/user/thumbsdrivesmecrazy</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;In Feb 2024, Meta published a paper introducing TestGen-LLM, a tool for automated unit test generation using LLMs, but didn‚Äôt release the TestGen-LLM code.The following blog shows how CodiumAI created the first open-source implementation - Cover-Agent, based on Meta&amp;#39;s approach: &lt;a href=&quot;https://www.codium.ai/blog/we-created-the-first-open-source-implementation-of-metas-testgen-llm/&quot;&gt;We created the first open-source implementation of Meta‚Äôs TestGen‚ÄìLLM&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The tool is implemented as follows:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Receive the following user inputs (Source File for code under test, Existing Test Suite to enhance, Coverage Report, Build/Test Command Code coverage target and maximum iterations to run, Additional context and prompting options)&lt;/li&gt; &lt;li&gt;Generate more tests in the same style&lt;/li&gt; &lt;li&gt;Validate those tests using your runtime environment - Do they build and pass?&lt;/li&gt; &lt;li&gt;Ensure that the tests add value by reviewing metrics such as increased code coverage&lt;/li&gt; &lt;li&gt;Update existing Test Suite and Coverage Report&lt;/li&gt; &lt;li&gt;Repeat until code reaches criteria: either code coverage threshold met, or reached the maximum number of iterations&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/thumbsdrivesmecrazy&quot;&gt; /u/thumbsdrivesmecrazy &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1deals9/opensource_implementation_of_metas_testgenllm/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1deals9/opensource_implementation_of_metas_testgenllm/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1deals9</id><link href="https://www.reddit.com/r/LangChain/comments/1deals9/opensource_implementation_of_metas_testgenllm/" /><updated>2024-06-12T16:17:22+00:00</updated><published>2024-06-12T16:17:22+00:00</published><title>Open-source implementation of Meta‚Äôs TestGen‚ÄìLLM - CodiumAI</title></entry><entry><author><name>/u/migkapa</name><uri>https://www.reddit.com/user/migkapa</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi all, &lt;/p&gt; &lt;p&gt;I get an &amp;quot;An error occurred: Multiple function calls are not currently supported&amp;quot; while using Gemini Pro .&lt;br/&gt; Anyone had the same issue?&lt;/p&gt; &lt;p&gt;Using:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;llm = ChatGoogleGenerativeAI(temperature=0, model=&amp;quot;gemini-pro&amp;quot;) llm.bind_tools(tools) &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/migkapa&quot;&gt; /u/migkapa &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1de99jx/error_with_tool_calling_while_using_gemini_pro/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1de99jx/error_with_tool_calling_while_using_gemini_pro/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1de99jx</id><link href="https://www.reddit.com/r/LangChain/comments/1de99jx/error_with_tool_calling_while_using_gemini_pro/" /><updated>2024-06-12T15:20:37+00:00</updated><published>2024-06-12T15:20:37+00:00</published><title>Error with tool calling while using Gemini Pro</title></entry><entry><author><name>/u/Disneyskidney</name><uri>https://www.reddit.com/user/Disneyskidney</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;m working on a use case that relies on very robust knowledge graph construction and I wanted to know if any startups/companies/open-source have built either free or paid production ready solutions for the unstructured text to knowledge graph pipeline.&lt;/p&gt; &lt;p&gt;UPDATE:&lt;/p&gt; &lt;p&gt;Diffbot seems to have a pretty good API that is compatiable with Llama Index and Langchain&lt;/p&gt; &lt;p&gt;this tutorial for Llama Index was released the same day I posted this and looks promising: &lt;a href=&quot;https://www.llamaindex.ai/blog/customizing-property-graph-index-in-llamaindex&quot;&gt;https://www.llamaindex.ai/blog/customizing-property-graph-index-in-llamaindex&lt;/a&gt;‚Äã&lt;/p&gt; &lt;p&gt;And Here is one for Langchain &lt;a href=&quot;https://python.langchain.com/v0.1/docs/integrations/graphs/diffbot/&quot;&gt;Diffbot | ü¶úÔ∏èüîó LangChain&lt;/a&gt;‚Äã&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Disneyskidney&quot;&gt; /u/Disneyskidney &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ddvywe/production_ready_unstructured_text_to_knowledge/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ddvywe/production_ready_unstructured_text_to_knowledge/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1ddvywe</id><link href="https://www.reddit.com/r/LangChain/comments/1ddvywe/production_ready_unstructured_text_to_knowledge/" /><updated>2024-06-12T02:29:12+00:00</updated><published>2024-06-12T02:29:12+00:00</published><title>Production Ready Unstructured Text to Knowledge Graph</title></entry><entry><author><name>/u/emersounds</name><uri>https://www.reddit.com/user/emersounds</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1de6u3j/deploy_langgraph_in_google_cloud/&quot;&gt; &lt;img src=&quot;https://b.thumbs.redditmedia.com/dl6NGxPnMwN1WkcIeQnqc9aC-oujGT8_wgAtEnVpwxA.jpg&quot; alt=&quot;Deploy Langgraph in Google Cloud?&quot; title=&quot;Deploy Langgraph in Google Cloud?&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I have been searching extensively but haven&amp;#39;t found any guide on deploying a Langgraph runnable with Google Cloud. &lt;/p&gt; &lt;p&gt;Currently, I am using an Reasoning Engine (Vertex AI) with the LangchainAgent template (from Google Cloud documentation) &lt;/p&gt; &lt;p&gt;Now, I tried to deploy my custom Reasoning Engine agent based on Langgraph and I can&amp;#39;t. &lt;/p&gt; &lt;p&gt;I would greatly appreciate any kind of help. &lt;/p&gt; &lt;p&gt;Regards. &lt;/p&gt; &lt;p&gt;PD: Langchain image to bait.&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/03fqi0fm856d1.jpg?width=1024&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=0bab82582504720db2d45a2d82cbec2aad0981a6&quot;&gt;https://preview.redd.it/03fqi0fm856d1.jpg?width=1024&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=0bab82582504720db2d45a2d82cbec2aad0981a6&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/emersounds&quot;&gt; /u/emersounds &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1de6u3j/deploy_langgraph_in_google_cloud/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1de6u3j/deploy_langgraph_in_google_cloud/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1de6u3j</id><media:thumbnail url="https://b.thumbs.redditmedia.com/dl6NGxPnMwN1WkcIeQnqc9aC-oujGT8_wgAtEnVpwxA.jpg" /><link href="https://www.reddit.com/r/LangChain/comments/1de6u3j/deploy_langgraph_in_google_cloud/" /><updated>2024-06-12T13:36:16+00:00</updated><published>2024-06-12T13:36:16+00:00</published><title>Deploy Langgraph in Google Cloud?</title></entry><entry><author><name>/u/sarthakai</name><uri>https://www.reddit.com/user/sarthakai</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;They prepare a QA task to observe hallucinations, on both Known examples (training instances similar to the info that the model has seen during its initial training) and Unknown examples (that introduce new info that the model hasn&amp;#39;t been exposed to before).&lt;/p&gt; &lt;p&gt;They see that:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Unknown examples in the fine-tuning dataset bring down performance, the more you train, because of overfitting. They lead to hallucinations and reduce accuracy. Known examples positively impact performance.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Early stopping helps avoid this, which might mean that Unknown examples are neutral in shorter training.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;The slower fitting of Unknown examples also indicates that models struggle to acquire new knowledge through fine-tuning.&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Paper: &lt;a href=&quot;https://arxiv.org/pdf/2405.05904&quot;&gt;https://arxiv.org/pdf/2405.05904&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I share high quality AI updates and tutorials daily.&lt;/p&gt; &lt;p&gt;If you like this post and want to stay updated on latest AI research, you can check out: &lt;a href=&quot;https://linktr.ee/sarthakrastogi&quot;&gt;https://linktr.ee/sarthakrastogi&lt;/a&gt; or my Twitter: &lt;a href=&quot;https://x.com/sarthakai&quot;&gt;https://x.com/sarthakai&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/sarthakai&quot;&gt; /u/sarthakai &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1de5ury/google_study_says_finetuning_an_llm_linearly/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1de5ury/google_study_says_finetuning_an_llm_linearly/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1de5ury</id><link href="https://www.reddit.com/r/LangChain/comments/1de5ury/google_study_says_finetuning_an_llm_linearly/" /><updated>2024-06-12T12:49:29+00:00</updated><published>2024-06-12T12:49:29+00:00</published><title>Google study says fine-tuning an LLM linearly increases hallucinations? üòê</title></entry><entry><author><name>/u/Highlight-Content</name><uri>https://www.reddit.com/user/Highlight-Content</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;In this &lt;a href=&quot;https://medium.com/ama-tech-blog/combining-langchain-and-llamaindex-to-build-your-first-agentic-rag-system-6e8e2e7825e7&quot;&gt;Medium article&lt;/a&gt;, the agent has three tools:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;&amp;quot;lyft_10k&amp;quot;: &amp;quot;Provides information about Lyft financials for year 2021. &amp;quot;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&amp;quot;uber_10k&amp;quot;: &amp;quot;Provides information about Uber financials for year 2021. &amp;quot;&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;and&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&amp;#39;DuckDuckGoSearch&amp;#39;: &amp;#39;Use for when you need to perform an internet search to find information that another tool can not provide.&amp;#39;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;In one of the test cases, the author queries the agent&lt;/p&gt; &lt;pre&gt;&lt;code&gt;&amp;quot;List me the names of Uber&amp;#39;s board of directors.&amp;quot; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Intuitively, one would assume the agent will invoke the &amp;quot;uber_10k&amp;quot; tool. However, the agent invokes &amp;quot;DuckDuckGoSearch&amp;quot;.&lt;/p&gt; &lt;p&gt;The author explains that:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Since this information is out-of-scope for any of the retriever tools, the agent correctly decided to invoke the external search tool.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;How does the agent know that question is out-of-scope for the &amp;quot;uber_10k&amp;quot; retriever?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Highlight-Content&quot;&gt; /u/Highlight-Content &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ddr9hj/how_does_this_langchain_agent_correctly_identify/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ddr9hj/how_does_this_langchain_agent_correctly_identify/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1ddr9hj</id><link href="https://www.reddit.com/r/LangChain/comments/1ddr9hj/how_does_this_langchain_agent_correctly_identify/" /><updated>2024-06-11T22:36:45+00:00</updated><published>2024-06-11T22:36:45+00:00</published><title>How does this LangChain agent correctly identify the tool to use?</title></entry><entry><author><name>/u/AnxiousEmu3480</name><uri>https://www.reddit.com/user/AnxiousEmu3480</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;m building a chatbot using &lt;code&gt;LangChain&lt;/code&gt;, &lt;code&gt;Next.js&lt;/code&gt;,and &lt;code&gt;CosmosDB&lt;/code&gt; (vector store). My implementation is based on &lt;a href=&quot;https://github.com/langchain-ai/langchain-nextjs-template/blob/main/app/api/chat/retrieval/route.ts&quot;&gt;this&lt;/a&gt;. I&amp;#39;m trying to display the source documents used by the LLM in my UI, but I&amp;#39;m facing two issues:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Source documents not displaying: Despite using a &lt;strong&gt;StreamingTextResponse&lt;/strong&gt; to send the source information in the headers as JSON chunks (see code snippet below), they don&amp;#39;t show up in my UI. There are no console errors.&lt;/li&gt; &lt;li&gt;Incorrect sources: When some source documents do appear, they are not the ones actually used by the LLM or contain unrelated information.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Here&amp;#39;s the part supposed to return the sources:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;import { NextRequest, NextResponse } from &amp;quot;next/server&amp;quot;; import { Message as VercelChatMessage, StreamingTextResponse } from &amp;quot;ai&amp;quot;;import { AzureCosmosDBVectorStore } from &amp;quot;@langchain/community/vectorstores/azure_cosmosdb&amp;quot;; import { AzureOpenAIEmbeddings, AzureChatOpenAI, } from &amp;quot;@langchain/azure-openai&amp;quot;; import { PromptTemplate } from &amp;quot;@langchain/core/prompts&amp;quot;; import { Document } from &amp;quot;@langchain/core/documents&amp;quot;; import { RunnableSequence } from &amp;quot;@langchain/core/runnables&amp;quot;; import { BytesOutputParser, StringOutputParser, } from &amp;quot;@langchain/core/output_parsers&amp;quot;; const combineDocumentsFn = (docs: Document[]) =&amp;gt; { const serializedDocs = docs.map((doc) =&amp;gt; doc.pageContent); return serializedDocs.join(&amp;quot;\n\n&amp;quot;); }; const formatVercelMessages = (chatHistory: VercelChatMessage[]) =&amp;gt; { const formattedDialogueTurns = chatHistory.map((message) =&amp;gt; { if (message.role === &amp;quot;user&amp;quot;) { return `Human: ${message.content}`; } else if (message.role === &amp;quot;assistant&amp;quot;) { return `Assistant: ${message.content}`; } else { return `${message.role}: ${message.content}`; } }); return formattedDialogueTurns.join(&amp;quot;\n&amp;quot;); }; const CONDENSE_QUESTION_TEMPLATE = `Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.&amp;lt;chat_history&amp;gt; {chat_history} &amp;lt;/chat_history&amp;gt; Follow Up Input: {question} Standalone question:`; const condenseQuestionPrompt = PromptTemplate.fromTemplate( CONDENSE_QUESTION_TEMPLATE ); const ANSWER_TEMPLATE = `Answer the question based only on the following context and chat history: &amp;lt;context&amp;gt; {context} &amp;lt;/context&amp;gt; &amp;lt;chat_history&amp;gt; {chat_history} &amp;lt;/chat_history&amp;gt; Question: {question} `; const answerPrompt = PromptTemplate.fromTemplate(ANSWER_TEMPLATE); export async function POST(req: NextRequest) { try { const body = await req.json(); const messages = body.messages ?? []; const previousMessages = messages.slice(0, -1); const currentMessageContent = messages[messages.length - 1].content; const vectorstore = new AzureCosmosDBVectorStore( new AzureOpenAIEmbeddings(), { databaseName: process.env.DB_NAME, collectionName: process.env.DB_COLLECTION_NAME, } ); const model = new AzureChatOpenAI({ azureOpenAIEndpoint: process.env.AZURE_OPENAI_API_ENDPOINT, azureOpenAIApiKey: process.env.AZURE_OPENAI_API_KEY, azureOpenAIApiDeploymentName: process.env.AZURE_OPENAI_API_DEPLOYMENT_NAME, modelName: process.env.AZURE_OPENAI_MODEL_NAME, }); const standaloneQuestionChain = RunnableSequence.from([ condenseQuestionPrompt, model, new StringOutputParser(), ]); let resolveWithDocuments: (value: Document[]) =&amp;gt; void; const documentPromise = new Promise&amp;lt;Document[]&amp;gt;((resolve) =&amp;gt; { resolveWithDocuments = resolve; }); const retriever = vectorstore.asRetriever({ callbacks: [ { handleRetrieverEnd(documents) { resolveWithDocuments(documents); }, }, ], }); const retrievalChain = retriever.pipe(combineDocumentsFn); const answerChain = RunnableSequence.from([ { context: RunnableSequence.from([ (input) =&amp;gt; input.question, retrievalChain, ]), chat_history: (input) =&amp;gt; input.chat_history, question: (input) =&amp;gt; input.question, }, answerPrompt, model, ]); const conversationalRetrievalQAChain = RunnableSequence.from([ { question: standaloneQuestionChain, chat_history: (input) =&amp;gt; input.chat_history, }, answerChain, new BytesOutputParser(), ]); const stream = await conversationalRetrievalQAChain.stream({ question: currentMessageContent, chat_history: formatVercelMessages(previousMessages), }); const documents = await documentPromise; console.log(&amp;quot;documents &amp;quot;, documents.length); const serializedSources = Buffer.from( JSON.stringify( documents.map((doc) =&amp;gt; { return { pageContent: doc.pageContent.slice(0, 50) + &amp;quot;...&amp;quot;, metadata: doc.metadata, }; }) ) ).toString(&amp;quot;base64&amp;quot;); const sourceMetadata = documents.map((doc) =&amp;gt; ({ title: doc.metadata.title, // Or whatever metadata you want url: doc.metadata.url, })); return new StreamingTextResponse(stream, { headers: { &amp;quot;x-message-index&amp;quot;: (previousMessages.length + 1).toString(), &amp;quot;x-message-sources&amp;quot;: serializedSources, }, }); } catch (e: any) { return NextResponse.json({ error: e.message }, { status: e.status ?? 500 }); } } &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;So my questions:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Is my approach to streaming source documents via &lt;strong&gt;StreamingTextResponse&lt;/strong&gt; wrong?&lt;/li&gt; &lt;li&gt;How can I ensure I&amp;#39;m associating the correct source documents with each LLM response?&lt;/li&gt; &lt;li&gt;What debugging techniques can I use to pinpoint where the source information is getting lost or mismatched?&lt;br/&gt;&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/AnxiousEmu3480&quot;&gt; /u/AnxiousEmu3480 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1de50ah/langchainnextjs_chatbot_displaying_incorrect/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1de50ah/langchainnextjs_chatbot_displaying_incorrect/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1de50ah</id><link href="https://www.reddit.com/r/LangChain/comments/1de50ah/langchainnextjs_chatbot_displaying_incorrect/" /><updated>2024-06-12T12:05:33+00:00</updated><published>2024-06-12T12:05:33+00:00</published><title>LangChain/Next.js chatbot displaying incorrect sources</title></entry><entry><author><name>/u/curious-airesearcher</name><uri>https://www.reddit.com/user/curious-airesearcher</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;m currently working on a local codebase and was using Cody to ask questions from my local code base context. But was wondering if there is an open-source project that&amp;#39;s similar. Ideally, the tool would:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Index all project files&lt;/li&gt; &lt;li&gt;Retrieve specific code snippets from all files or tell about specific local variables&lt;/li&gt; &lt;li&gt;Use as Chatbot?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Document loaders and file embeddings could work but I&amp;#39;m not sure on how to handle function interdependencies. Do I need to also additionally pass AST for it to organize it better? Not really sure on what direction to take?&lt;/p&gt; &lt;p&gt;Anyone has tried something similar? What approach did you take? and how was the result?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/curious-airesearcher&quot;&gt; /u/curious-airesearcher &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1de4yz8/local_source_code_indexing_querying_with_rag/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1de4yz8/local_source_code_indexing_querying_with_rag/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1de4yz8</id><link href="https://www.reddit.com/r/LangChain/comments/1de4yz8/local_source_code_indexing_querying_with_rag/" /><updated>2024-06-12T12:03:35+00:00</updated><published>2024-06-12T12:03:35+00:00</published><title>Local Source Code Indexing &amp; Querying with RAG</title></entry><entry><author><name>/u/thewanitz</name><uri>https://www.reddit.com/user/thewanitz</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey guys!&lt;/p&gt; &lt;p&gt;So I‚Äôve been looking at a lot of tutorials to build a basic RAG search which does the following:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Takes the user query and put it into the state ‚Äúuser_query‚Äù&lt;/li&gt; &lt;li&gt;Searches the internet for results. These results are then populated as text in the state ‚Äúinternet_search_results‚Äù field with the url and title of the text&lt;/li&gt; &lt;li&gt; Does the same but searches the local database and populates the state ‚Äúlocal_search_results‚Äù field with the post ID and title of the search results. &lt;/li&gt; &lt;li&gt;Then passes the state with the information above into a summariser function which uses GPT 3.5 to return structured output with the following fields: (i) the text response, (ii) an array of the sources which include the title, the type (web search or local post), and either the url or the post ID. &lt;/li&gt; &lt;/ol&gt; &lt;p&gt;I‚Äôm at a loss on this as can‚Äôt find any good tutorials for this. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/thewanitz&quot;&gt; /u/thewanitz &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1de095x/good_tutorials_for_rag_with_structured_state_and/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1de095x/good_tutorials_for_rag_with_structured_state_and/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1de095x</id><link href="https://www.reddit.com/r/LangChain/comments/1de095x/good_tutorials_for_rag_with_structured_state_and/" /><updated>2024-06-12T06:48:15+00:00</updated><published>2024-06-12T06:48:15+00:00</published><title>Good Tutorials For RAG with Structured State and Output?</title></entry><entry><author><name>/u/Outrageous-Tap-6665</name><uri>https://www.reddit.com/user/Outrageous-Tap-6665</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Last week we came across a serious security issue with GPT4o. In both ChatGPT and OpenAI APIs. Until OpenAI fixes it, we manage to fix from our side. We would like to share it with the community. We implemented it in LlamaIndex but should be easy to implement using Langchain as well.&lt;br/&gt; This is the medium article about the fix - &lt;a href=&quot;https://medium.com/@deltaaruna/fixing-a-serious-security-issue-in-gpt4o-api-via-llamaindex-4aa1368b5d2f&quot;&gt;https://medium.com/@deltaaruna/fixing-a-serious-security-issue-in-gpt4o-api-via-llamaindex-4aa1368b5d2f&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This is the medium article about the issue - &lt;a href=&quot;https://medium.com/@deltaaruna/how-anyone-can-hack-chatgpt-aa7959684ef0&quot;&gt;https://medium.com/@deltaaruna/how-anyone-can-hack-chatgpt-aa7959684ef0&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Outrageous-Tap-6665&quot;&gt; /u/Outrageous-Tap-6665 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1de7ar1/a_serious_security_issue_with_gpt4o/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1de7ar1/a_serious_security_issue_with_gpt4o/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1de7ar1</id><link href="https://www.reddit.com/r/LangChain/comments/1de7ar1/a_serious_security_issue_with_gpt4o/" /><updated>2024-06-12T13:57:22+00:00</updated><published>2024-06-12T13:57:22+00:00</published><title>A serious security issue with GPT4o</title></entry><entry><author><name>/u/Medium_Eggplant795</name><uri>https://www.reddit.com/user/Medium_Eggplant795</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I am not a software engineer but an enthusiast of RAG and LLM agents. I wanted to know where is the real bottleneck in building an agent who would build documents based on chat that I am currently having with an LLM based chat interface and embed the chat text using embedding models and store it in vector db for the user to search in later? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Medium_Eggplant795&quot;&gt; /u/Medium_Eggplant795 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ddyhkq/question_regarding_limitation_of_agent_use/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ddyhkq/question_regarding_limitation_of_agent_use/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1ddyhkq</id><link href="https://www.reddit.com/r/LangChain/comments/1ddyhkq/question_regarding_limitation_of_agent_use/" /><updated>2024-06-12T04:51:46+00:00</updated><published>2024-06-12T04:51:46+00:00</published><title>Question regarding limitation of agent use</title></entry><entry><author><name>/u/diptanuc</name><uri>https://www.reddit.com/user/diptanuc</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi folks, we built a TypeScript library to improve search results in RAG Applications. If you are building a RAG application on top of vector indexes, re-ranking search results will always improve LLM&amp;#39;s response synthesis. We implemented two commonly used re-ranking techniques - Reciprocal Rank Fusion(RRF) and LLM Based Re-Ranking(using Llama3 from Groq and GPT-4). Hope this is useful to folks building LLM Applications in React/NextJS.&lt;/p&gt; &lt;p&gt;Code - &lt;a href=&quot;https://github.com/tensorlakeai/rerank-ts&quot;&gt;https://github.com/tensorlakeai/rerank-ts&lt;/a&gt; &lt;/p&gt; &lt;p&gt;We were building a consumer application with our open source data framework &lt;a href=&quot;https://github.com/tensorlakeai/indexify&quot;&gt;https://github.com/tensorlakeai/indexify&lt;/a&gt; and were not able to find a good re-ranking library in TypeScript. So we decided to build one, and it works really well to re-rank ~100 results. We get latency of around 1 second with Llama3/Groq. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/diptanuc&quot;&gt; /u/diptanuc &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ddhd9t/rerankts_typescript_library_for_improving_search/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ddhd9t/rerankts_typescript_library_for_improving_search/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1ddhd9t</id><link href="https://www.reddit.com/r/LangChain/comments/1ddhd9t/rerankts_typescript_library_for_improving_search/" /><updated>2024-06-11T15:46:38+00:00</updated><published>2024-06-11T15:46:38+00:00</published><title>rerank-ts: TypeScript Library for Improving Search Results in RAG Applications</title></entry><entry><author><name>/u/Fit_Influence_1576</name><uri>https://www.reddit.com/user/Fit_Influence_1576</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I‚Äôm looking for a chatbot frontend with citations, that utilizes a fast api/langserve backend. Anyone have good suggestions? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Fit_Influence_1576&quot;&gt; /u/Fit_Influence_1576 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ddm4a1/full_stack_starter/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ddm4a1/full_stack_starter/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1ddm4a1</id><link href="https://www.reddit.com/r/LangChain/comments/1ddm4a1/full_stack_starter/" /><updated>2024-06-11T19:01:34+00:00</updated><published>2024-06-11T19:01:34+00:00</published><title>Full stack starter</title></entry><entry><author><name>/u/youniss_k</name><uri>https://www.reddit.com/user/youniss_k</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Using langchain sometimes feels like gambling with costs to me. I never really know how much my requests would actually cost when I send it. I know there are detailed charts which we should read, but who really does? Instead I wanted to ask if anybody knows of an automated way to calculate costs before sending the requests? For my use case, specifically for OpenAI, but maybe there is another way.&lt;/p&gt; &lt;p&gt;And if there isnt anything like that, maybe this would be an interessting project... Like a package which calculates your LLM costs before the requests, depending on the specific platform you use&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/youniss_k&quot;&gt; /u/youniss_k &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ddft47/calculating_llm_costs_before_sending_requests/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ddft47/calculating_llm_costs_before_sending_requests/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1ddft47</id><link href="https://www.reddit.com/r/LangChain/comments/1ddft47/calculating_llm_costs_before_sending_requests/" /><updated>2024-06-11T14:41:50+00:00</updated><published>2024-06-11T14:41:50+00:00</published><title>Calculating LLM costs before sending requests?</title></entry><entry><author><name>/u/Gvascons</name><uri>https://www.reddit.com/user/Gvascons</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;So, I&amp;#39;m building this simple rag pipeline with langchain and ollama that takes in a PDF document and returns it&amp;#39;s summary as bulletpoints.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;file_path = &amp;quot;paper.pdf&amp;quot; loader = PyPDFLoader(file_path) docs = loader.load() embeddings = (OllamaEmbeddings(model=&amp;#39;llama3&amp;#39;)) text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200) splits = text_splitter.split_documents(docs) vectorstore = Chroma.from_documents(documents=splits, embedding=embeddings, persist_directory=&amp;quot;emb&amp;quot;) retriever = as_retriever( embeddings=embeddings, chroma=vectorstore ) prompt_template = &amp;quot;&amp;quot;&amp;quot;Based on the following information and being really specific about it&amp;#39;s data: &amp;#39;{text}&amp;#39;.\n\n Here are the goals, methodology, and conclusions/achievements of the paper, written as bullet points:&amp;quot;&amp;quot;&amp;quot; prompt = PromptTemplate.from_template(prompt_template) llm = Ollama(model=&amp;quot;llama3&amp;quot;) chain = ( retriever | prompt | llm ) result = chain.invoke({}) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The calling of the chain just seems too much like a workaround, since I didn&amp;#39;t have a specific question about the reference document, therefore I just had to use the prompt_template as the instruction to treat the pdf. It just seems like there are a lot of way to get to this same result. Whether to call the llm by it&amp;#39;s default completion object or through it&amp;#39;s chat variation. Whether to use a LLMChain(), a RetrievalQA.from_chain_type() or a simple chain() specifying it&amp;#39;s common parameters etc. Ins&amp;#39;t there a way to standardize this workflow according to your needs?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Gvascons&quot;&gt; /u/Gvascons &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dddjew/multiple_ways_to_get_to_the_same_result_w_rag/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dddjew/multiple_ways_to_get_to_the_same_result_w_rag/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dddjew</id><link href="https://www.reddit.com/r/LangChain/comments/1dddjew/multiple_ways_to_get_to_the_same_result_w_rag/" /><updated>2024-06-11T13:00:58+00:00</updated><published>2024-06-11T13:00:58+00:00</published><title>Multiple ways to get to the same result w/ RAG</title></entry></feed>