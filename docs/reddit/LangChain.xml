<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/"><category term="LangChain" label="r/LangChain"/><updated>2024-05-25T23:02:56+00:00</updated><icon>https://www.redditstatic.com/icon.png/</icon><id>/r/LangChain.rss</id><link rel="self" href="https://www.reddit.com/r/LangChain.rss" type="application/atom+xml" /><link rel="alternate" href="https://www.reddit.com/r/LangChain" type="text/html" /><subtitle>LangChain is an open-source framework and developer toolkit that helps developers get LLM applications from prototype to production. It is available for Python and Javascript at https://www.langchain.com/.</subtitle><title>LangChain</title><entry><author><name>/u/zchaarm</name><uri>https://www.reddit.com/user/zchaarm</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;A place for members of &lt;a href=&quot;/r/LangChain&quot;&gt;r/LangChain&lt;/a&gt; to chat with each other&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/zchaarm&quot;&gt; /u/zchaarm &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_10ljho9</id><link href="https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/" /><updated>2023-01-26T04:33:05+00:00</updated><published>2023-01-26T04:33:05+00:00</published><title>r/LangChain Lounge</title></entry><entry><author><name>/u/Puzzleheaded_Move35</name><uri>https://www.reddit.com/user/Puzzleheaded_Move35</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hello, I am trying to create a chatbot using Langchain where I am using RetrievalQA and OpenAI API. I need to create chains where if the user asks a question which is unrelated to the context, basically retrieve from a document provided, the chatbot should bypass the retrieval steps and just answer the query directly. And if it asks related questions it should apply RAG and retrieve the relevant info to answer the questions. I am totally stuck here and don’t know how to move forward. Any help will be appreciated. &lt;/p&gt; &lt;p&gt;Code:&lt;/p&gt; &lt;p&gt;llm = ChatOpenAI( api_key= api_key,&lt;br/&gt; # openai_api_key= os.environ[&amp;quot;OPENAI_API_KEY&amp;quot;],&lt;br/&gt; model_name=&amp;#39;gpt-4o&amp;#39; &lt;/p&gt; &lt;p&gt;) template = &amp;quot;&amp;quot;&amp;quot; Use the following context provided (delimited by &amp;lt;ctx&amp;gt;&amp;lt;/ctx&amp;gt;), answer the questions properly and the chat history (delimited by &amp;lt;hs&amp;gt;&amp;lt;/hs&amp;gt;) to answer the questions from the user. &lt;/p&gt; &lt;h2&gt;If they are asking questions not related to the context, skip performing RAG and just straight up answer their query&amp;quot;:&lt;/h2&gt; &lt;p&gt;&amp;lt;ctx&amp;gt; {context}&lt;/p&gt; &lt;h2&gt;&amp;lt;/ctx&amp;gt;&lt;/h2&gt; &lt;p&gt;&amp;lt;hs&amp;gt; {history}&lt;/p&gt; &lt;h2&gt;&amp;lt;/hs&amp;gt;&lt;/h2&gt; &lt;p&gt;{question} Answer: &amp;quot;&amp;quot;&amp;quot; prompt = PromptTemplate( input_variables=[&amp;quot;history&amp;quot;, &amp;quot;context&amp;quot;, &amp;quot;question&amp;quot;], template=template, )&lt;/p&gt; &lt;p&gt;memory = ConversationBufferMemory( memory_key=&amp;quot;history&amp;quot;, input_key=&amp;quot;question&amp;quot; )&lt;/p&gt; &lt;p&gt;qa = RetrievalQA.from_chain_type( llm=llm, chain_type=&amp;#39;stuff&amp;#39;, retriever=vectorstore.as_retriever(search_type=&amp;quot;similarity&amp;quot;, search_kwargs={&amp;#39;k&amp;#39;: 20}), verbose=True, chain_type_kwargs={ &amp;quot;verbose&amp;quot;: True, &amp;quot;prompt&amp;quot;: prompt, &amp;quot;memory&amp;quot;: memory, } )&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Puzzleheaded_Move35&quot;&gt; /u/Puzzleheaded_Move35 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d0e7ov/how_to_ignore_retrieval_step_rag_when_it_is_not/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d0e7ov/how_to_ignore_retrieval_step_rag_when_it_is_not/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1d0e7ov</id><link href="https://www.reddit.com/r/LangChain/comments/1d0e7ov/how_to_ignore_retrieval_step_rag_when_it_is_not/" /><updated>2024-05-25T15:15:49+00:00</updated><published>2024-05-25T15:15:49+00:00</published><title>How to ignore retrieval step (RAG) when it is not necessary</title></entry><entry><author><name>/u/mehul_gupta1997</name><uri>https://www.reddit.com/user/mehul_gupta1997</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d00vla/my_langchain_book_now_available_on_packt_and/&quot;&gt; &lt;img src=&quot;https://b.thumbs.redditmedia.com/6qJv0lGS_OUWt49TIW0VLit2wRHq59aNal2DwSpSfgU.jpg&quot; alt=&quot;My LangChain book now available on Packt and O'Reilly&quot; title=&quot;My LangChain book now available on Packt and O'Reilly&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;m glad to share that my debut book, &amp;quot;&lt;strong&gt;LangChain in your Pocket: Beginner&amp;#39;s Guide to Building Generative AI Applications using LLMs,&lt;/strong&gt;&amp;quot; has been republished by Packt and is now available on their official website and partner publications like O&amp;#39;Reilly, Barnes &amp;amp; Noble, etc. A big thanks for the support! The first version is still available on Amazon&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/5b0trmcl7h2d1.png?width=1080&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6f12126f846d5fc174768628ebc42c9921017687&quot;&gt;https://preview.redd.it/5b0trmcl7h2d1.png?width=1080&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6f12126f846d5fc174768628ebc42c9921017687&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/4xdgzk9l7h2d1.png?width=1080&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bfe4aac06ce89bff475a415b8c0091f830ba10e3&quot;&gt;https://preview.redd.it/4xdgzk9l7h2d1.png?width=1080&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bfe4aac06ce89bff475a415b8c0091f830ba10e3&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/mehul_gupta1997&quot;&gt; /u/mehul_gupta1997 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d00vla/my_langchain_book_now_available_on_packt_and/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d00vla/my_langchain_book_now_available_on_packt_and/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1d00vla</id><media:thumbnail url="https://b.thumbs.redditmedia.com/6qJv0lGS_OUWt49TIW0VLit2wRHq59aNal2DwSpSfgU.jpg" /><link href="https://www.reddit.com/r/LangChain/comments/1d00vla/my_langchain_book_now_available_on_packt_and/" /><updated>2024-05-25T01:32:55+00:00</updated><published>2024-05-25T01:32:55+00:00</published><title>My LangChain book now available on Packt and O'Reilly</title></entry><entry><author><name>/u/throwaway0134hdj</name><uri>https://www.reddit.com/user/throwaway0134hdj</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;When you are making RAG chatbots using Graph and Vectors how are you storing the internal data? What’s the general approach?&lt;/p&gt; &lt;p&gt;For example, say you are asked to ingest all your companies files, like word docs PDFs and everything in between. If you use RAG with Graph and Vector embeddedings where are you storing the data from the documents? I’m curious what the general approach is to chunking, tokenizing, and embedding are?&lt;/p&gt; &lt;p&gt;If you had to ingest your companies documents using a RAG, Graph, and vector approach how would you set this up? What would the schema be of the Graph, where would the vectors be stored?&lt;/p&gt; &lt;p&gt;Thanks&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/throwaway0134hdj&quot;&gt; /u/throwaway0134hdj &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d0j0hc/how_do_you_go_about_creating_a_rag_chatbot_using/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d0j0hc/how_do_you_go_about_creating_a_rag_chatbot_using/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1d0j0hc</id><link href="https://www.reddit.com/r/LangChain/comments/1d0j0hc/how_do_you_go_about_creating_a_rag_chatbot_using/" /><updated>2024-05-25T19:03:06+00:00</updated><published>2024-05-25T19:03:06+00:00</published><title>How do you go about creating a RAG chatbot using Graph and Vector on internal documents?</title></entry><entry><author><name>/u/AustinChanKL</name><uri>https://www.reddit.com/user/AustinChanKL</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hello everyone, need some pointer here. Is there a way to exclude intermediate steps when streaming the events with AgentExecutor? I only want the agent to stream the final output without streaming the intermediate steps (Observations).&lt;/p&gt; &lt;p&gt;So I&amp;#39;m using create_tool_calling_agent() to create an agent and passing multiple tools. And one of the tool itself actually is using langchain csv agent. When the agent using this particular tool, it start to stream the intermediate observation steps in my chat application, which is weird for my user point of view. I want to exclude all the intermediate steps from this tool.&lt;/p&gt; &lt;p&gt;When I was using langchain==0.1.16, it behave exactly what I wanted, which it won&amp;#39;t stream any output or intermediate steps from the tool that using langchain csv agent. After I upgrade it to 0.2.1, it started to stream intermediate steps.&lt;/p&gt; &lt;p&gt;Here&amp;#39;s some example code I have:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;async def chat_stream(...) -&amp;gt; str: prompt = ChatPromptTemplate.from_messages(...) tools = [query_data_from_csv] llm = ChatOpenAI(...) agent = create_tool_calling_agent(llm, tools, prompt) agent_executor = AgentExecutor(agent=agent, tools=tools, handle_parsing_errors=True) return agent_executor.astream_events(..., version=&amp;quot;v2&amp;quot;) @ tool def query_data_from_csv(question: str, csv_url: str) -&amp;gt; str: &amp;quot;&amp;quot;&amp;quot;Tool to query and interact with data in CSV format.&amp;quot;&amp;quot;&amp;quot; ai_agent = create_csv_agent( ChatOpenAI(temperature=0, model=&amp;quot;gpt-4-turbo&amp;quot;), csv_url, agent_type=AgentType.ZERO_SHOT_REACT_DESCRIPTION, return_intermediate_steps=False ) return ai_agent.invoke(question) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Your help is greatly appreciated. Thank you.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/AustinChanKL&quot;&gt; /u/AustinChanKL &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d0b3zs/is_there_anyway_to_prevent_agentexecutorastream/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d0b3zs/is_there_anyway_to_prevent_agentexecutorastream/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1d0b3zs</id><link href="https://www.reddit.com/r/LangChain/comments/1d0b3zs/is_there_anyway_to_prevent_agentexecutorastream/" /><updated>2024-05-25T12:37:48+00:00</updated><published>2024-05-25T12:37:48+00:00</published><title>Is there anyway to prevent AgentExecutor.astream_events() streaming intermediate steps?</title></entry><entry><author><name>/u/Pokedrive123</name><uri>https://www.reddit.com/user/Pokedrive123</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I wanted to know if the chain method and ChatOpenAI from langchain_openai supports gpt4o image inputs and if there are any guides out there showing us how to use it? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Pokedrive123&quot;&gt; /u/Pokedrive123 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d03y7b/has_langchain_been_updated_with_gpt4o/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d03y7b/has_langchain_been_updated_with_gpt4o/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1d03y7b</id><link href="https://www.reddit.com/r/LangChain/comments/1d03y7b/has_langchain_been_updated_with_gpt4o/" /><updated>2024-05-25T04:30:41+00:00</updated><published>2024-05-25T04:30:41+00:00</published><title>Has langchain been updated with Gpt4o?</title></entry><entry><author><name>/u/UpvoteBeast</name><uri>https://www.reddit.com/user/UpvoteBeast</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1czfu8v/understanding_the_magic_deconstructing_langchains/&quot;&gt; &lt;img src=&quot;https://external-preview.redd.it/PuvfwiESasEaQDZvjllJMtQHd1LLNsCK92LgBTOvras.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=dc7f8a79051cfb7f73b1c8cdc014ce2e73509532&quot; alt=&quot;Understanding the Magic: Deconstructing Langchain’s SQL Agent &quot; title=&quot;Understanding the Magic: Deconstructing Langchain’s SQL Agent &quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/UpvoteBeast&quot;&gt; /u/UpvoteBeast &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://dly.to/K6CJFoxPldx&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1czfu8v/understanding_the_magic_deconstructing_langchains/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1czfu8v</id><media:thumbnail url="https://external-preview.redd.it/PuvfwiESasEaQDZvjllJMtQHd1LLNsCK92LgBTOvras.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=dc7f8a79051cfb7f73b1c8cdc014ce2e73509532" /><link href="https://www.reddit.com/r/LangChain/comments/1czfu8v/understanding_the_magic_deconstructing_langchains/" /><updated>2024-05-24T08:20:52+00:00</updated><published>2024-05-24T08:20:52+00:00</published><title>Understanding the Magic: Deconstructing Langchain’s SQL Agent</title></entry><entry><author><name>/u/nik0-bellic</name><uri>https://www.reddit.com/user/nik0-bellic</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Im planning to do an endpoint that given a user question it makes the underlying work to get the query and i would like to only receive the final answer as im going to show it on a Streamlit chat app. Any idea on how to extract only that?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/nik0-bellic&quot;&gt; /u/nik0-bellic &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1czv6k8/how_could_i_just_return_the_final_answer_from_sql/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1czv6k8/how_could_i_just_return_the_final_answer_from_sql/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1czv6k8</id><link href="https://www.reddit.com/r/LangChain/comments/1czv6k8/how_could_i_just_return_the_final_answer_from_sql/" /><updated>2024-05-24T20:59:31+00:00</updated><published>2024-05-24T20:59:31+00:00</published><title>How could I just return the final answer from SQL Agent?</title></entry><entry><author><name>/u/newpeak</name><uri>https://www.reddit.com/user/newpeak</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1czk90g/implementing_a_longcontext_rag_based_on_raptor/&quot;&gt; &lt;img src=&quot;https://external-preview.redd.it/0cS0XK03rFRxZqaK6QPMgfSagZE4Vn9YYciJnzYH9mk.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=77af57d9f34ce52533ece926c923557a6f272dbb&quot; alt=&quot; Implementing a long-context RAG based on RAPTOR &quot; title=&quot; Implementing a long-context RAG based on RAPTOR &quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/newpeak&quot;&gt; /u/newpeak &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://medium.com/@infiniflowai/implementing-a-long-context-rag-based-on-raptor-0538a354ada3&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1czk90g/implementing_a_longcontext_rag_based_on_raptor/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1czk90g</id><media:thumbnail url="https://external-preview.redd.it/0cS0XK03rFRxZqaK6QPMgfSagZE4Vn9YYciJnzYH9mk.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=77af57d9f34ce52533ece926c923557a6f272dbb" /><link href="https://www.reddit.com/r/LangChain/comments/1czk90g/implementing_a_longcontext_rag_based_on_raptor/" /><updated>2024-05-24T13:04:48+00:00</updated><published>2024-05-24T13:04:48+00:00</published><title>Implementing a long-context RAG based on RAPTOR</title></entry><entry><author><name>/u/Ok_Comfort_4103</name><uri>https://www.reddit.com/user/Ok_Comfort_4103</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;m new to LLMs, but I&amp;#39;m planning to build an application that answers technical questions about my API using a RAG system based on my tech docs (e.g., &amp;quot;how can I configure the API request to wait for the response and to retry if the request times out?&amp;quot;). To summarize ...Goal: answer technical questions (found in my /docs/ subdirectory) so the user doesn&amp;#39;t have to search and dig for it. My Plan: &lt;/p&gt; &lt;ol&gt; &lt;li&gt;Setup LangChain (are there any tips or tricks I should keep in mind?)&lt;/li&gt; &lt;li&gt;Connect to Weaviate (if there&amp;#39;s a better VectorDB to use, I&amp;#39;d love recommendations &amp;amp; rationale)&lt;/li&gt; &lt;li&gt;Connect to ChatGPT 3.5 (pls let me know if there&amp;#39;s a better model to use)&lt;/li&gt; &lt;li&gt;Vectorize my /docs/ directory (I&amp;#39;ve never done this before, but it looks like I need a chunking strategy &amp;amp; embedding model)&lt;/li&gt; &lt;li&gt;Create &amp;amp; style a simple modal UI for the chatbot in Airtable (again, if there&amp;#39;s a better/quicker way to do this, I&amp;#39;m all ears)&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;I also want to make it not simply a Q&amp;amp;A bot so that if the answer returned is not valid to the user&amp;#39;s use case, there is a feedback query used to improve the process, giving the tool either more context or showing where the documentation should be expanded/refined.&lt;/p&gt; &lt;p&gt;Thanks in advance for any guidance and/or pitfalls to avoid :-)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Ok_Comfort_4103&quot;&gt; /u/Ok_Comfort_4103 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1czronv/attempt_to_be_forwardlooking_on_a_new_project/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1czronv/attempt_to_be_forwardlooking_on_a_new_project/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1czronv</id><link href="https://www.reddit.com/r/LangChain/comments/1czronv/attempt_to_be_forwardlooking_on_a_new_project/" /><updated>2024-05-24T18:28:41+00:00</updated><published>2024-05-24T18:28:41+00:00</published><title>Attempt to be Forward-looking on a New Project</title></entry><entry><author><name>/u/cryptokaykay</name><uri>https://www.reddit.com/user/cryptokaykay</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Looking to understand what evaluation tools/methods people like and use the most?&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://www.reddit.com/poll/1czpsmq&quot;&gt;View Poll&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/cryptokaykay&quot;&gt; /u/cryptokaykay &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1czpsmq/what_evaluation_toolsmethods_do_you_use/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1czpsmq/what_evaluation_toolsmethods_do_you_use/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1czpsmq</id><link href="https://www.reddit.com/r/LangChain/comments/1czpsmq/what_evaluation_toolsmethods_do_you_use/" /><updated>2024-05-24T17:08:15+00:00</updated><published>2024-05-24T17:08:15+00:00</published><title>What evaluation tools/methods do you use?</title></entry><entry><author><name>/u/Lablade04</name><uri>https://www.reddit.com/user/Lablade04</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey folks! My team recently worked on building this bot to help orgs monitor and even forecast costs on the Snowflake Data Warehouse. We used LangChain, Streamlit, Snowflake Arctic + Cortex and GPT 4 Turbo for this. &lt;/p&gt; &lt;p&gt;We just open sourced this Agent and even wrote a guide on how to create one yourself, check it out here: &lt;a href=&quot;https://medium.com/snowflake/crystalcosts-building-an-ai-agent-for-cost-monitoring-on-snowflake-c9d49645f5c4&quot;&gt;https://medium.com/snowflake/crystalcosts-building-an-ai-agent-for-cost-monitoring-on-snowflake-c9d49645f5c4&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Would love to get inputs on this! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Lablade04&quot;&gt; /u/Lablade04 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1czfe4s/ai_agent_for_monitoring_snowflake_costs/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1czfe4s/ai_agent_for_monitoring_snowflake_costs/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1czfe4s</id><link href="https://www.reddit.com/r/LangChain/comments/1czfe4s/ai_agent_for_monitoring_snowflake_costs/" /><updated>2024-05-24T07:47:01+00:00</updated><published>2024-05-24T07:47:01+00:00</published><title>AI Agent for monitoring Snowflake Costs!</title></entry><entry><author><name>/u/business24_ai</name><uri>https://www.reddit.com/user/business24_ai</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;&lt;a href=&quot;https://youtu.be/gflsu_6R_8g&quot;&gt;https://youtu.be/gflsu_6R_8g&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/business24_ai&quot;&gt; /u/business24_ai &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1czdcpz/langgraph_essentials_create_your_first_graph_with/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1czdcpz/langgraph_essentials_create_your_first_graph_with/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1czdcpz</id><link href="https://www.reddit.com/r/LangChain/comments/1czdcpz/langgraph_essentials_create_your_first_graph_with/" /><updated>2024-05-24T05:25:50+00:00</updated><published>2024-05-24T05:25:50+00:00</published><title>LangGraph Essentials: Create Your First Graph with Ease!</title></entry><entry><author><name>/u/Ok_Comfort_4103</name><uri>https://www.reddit.com/user/Ok_Comfort_4103</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey all, looking to learn :)&lt;/p&gt; &lt;p&gt;What are some good robust platforms that I can use to vectorize various types of data and implement RAG to generate results from LLMs? I have a few projects I am working on each with their own types of data and models to use. Wondering if there are any all-in-one platforms you know of that would limit my time spent learning new technologies that will have to be updated as the methods progress. Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Ok_Comfort_4103&quot;&gt; /u/Ok_Comfort_4103 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cz9zga/vector_embedding_and_rag_platforms/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cz9zga/vector_embedding_and_rag_platforms/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cz9zga</id><link href="https://www.reddit.com/r/LangChain/comments/1cz9zga/vector_embedding_and_rag_platforms/" /><updated>2024-05-24T02:09:28+00:00</updated><published>2024-05-24T02:09:28+00:00</published><title>Vector Embedding and RAG Platforms</title></entry><entry><author><name>/u/ZuckyFox</name><uri>https://www.reddit.com/user/ZuckyFox</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Which is your favourite caching technique in LLM Applications. Is it in memory or something else. Which caching integration you like the most and why for a scalable and reliable application.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/ZuckyFox&quot;&gt; /u/ZuckyFox &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cz3ls8/caching_in_llm_apps/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cz3ls8/caching_in_llm_apps/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cz3ls8</id><link href="https://www.reddit.com/r/LangChain/comments/1cz3ls8/caching_in_llm_apps/" /><updated>2024-05-23T21:01:41+00:00</updated><published>2024-05-23T21:01:41+00:00</published><title>Caching in LLM Apps</title></entry><entry><author><name>/u/Confident-Addendum-2</name><uri>https://www.reddit.com/user/Confident-Addendum-2</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Been struggling with parsing pdf with complex layout, table, imagines.&lt;/p&gt; &lt;p&gt;The option that I am testing is multi modal vector, based on unstructured library for pdf extraction. &lt;/p&gt; &lt;p&gt;I recently discovered llamaparse proprietary solution. Excluding the facts that isn&amp;#39;t open source and limited for commercial use. Would it perform better then the unstructured approach for parsing?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Confident-Addendum-2&quot;&gt; /u/Confident-Addendum-2 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cyplp8/parsing_solutions_for_pdf/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cyplp8/parsing_solutions_for_pdf/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cyplp8</id><link href="https://www.reddit.com/r/LangChain/comments/1cyplp8/parsing_solutions_for_pdf/" /><updated>2024-05-23T10:31:22+00:00</updated><published>2024-05-23T10:31:22+00:00</published><title>Parsing solutions for PDF</title></entry><entry><author><name>/u/link2ani</name><uri>https://www.reddit.com/user/link2ani</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;We’re building a RAG based application which works on internal documents. We’re experimenting with OpenAI for embedding models, Milvus (Zilliz cloud) for embedding storage and similarity search, Postgres for all other data and AWS for hosting.&lt;/p&gt; &lt;p&gt;Our main priorities are: - being fast to market - above average performance - costs that don’t scale exponentially with scale - being scalable so we don’t have to refactor all of the code, if we achieve any scale&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/link2ani&quot;&gt; /u/link2ani &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cyjfap/best_stack_for_rag/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cyjfap/best_stack_for_rag/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cyjfap</id><link href="https://www.reddit.com/r/LangChain/comments/1cyjfap/best_stack_for_rag/" /><updated>2024-05-23T03:39:37+00:00</updated><published>2024-05-23T03:39:37+00:00</published><title>Best stack for RAG?</title></entry><entry><author><name>/u/AndrewShf</name><uri>https://www.reddit.com/user/AndrewShf</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;langchain_core.messages.human.HumanMessage&lt;/p&gt; &lt;p&gt;langchain.schema.messages.HumanMessage&lt;/p&gt; &lt;p&gt;I got unsupported HumanMessage error when using langchain and found out two kinds of messages. Why?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/AndrewShf&quot;&gt; /u/AndrewShf &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cyz7kw/why_two_different_kinds_of_messages/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cyz7kw/why_two_different_kinds_of_messages/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cyz7kw</id><link href="https://www.reddit.com/r/LangChain/comments/1cyz7kw/why_two_different_kinds_of_messages/" /><updated>2024-05-23T17:59:49+00:00</updated><published>2024-05-23T17:59:49+00:00</published><title>why two different kinds of messages?</title></entry><entry><author><name>/u/Mean-Night6324</name><uri>https://www.reddit.com/user/Mean-Night6324</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi!&lt;/p&gt; &lt;p&gt;I&amp;#39;m currently facing this issue of trying to get an XML out of a model and I use that XML structure to extract and format a document that I generate but, no matter how I prompt the model, or even, using different calls generate the answer and to structure it into the required format, sometimes going through different stages of structuring (like first just bullet points, then try to only put stuff into a basic XML format before going into nested.), it still sometimes generate an answer that&amp;#39;s not structured.&lt;/p&gt; &lt;p&gt;I included retries on those calls hoping that the model in its second generation would structure the output correctly but often this doesn&amp;#39;t work.&lt;/p&gt; &lt;p&gt;I was wondering how the community handles this issue or if there are creative ways you stumbled upon that deal well with it.&lt;/p&gt; &lt;p&gt;I have seen in the past some libraries that force the generation in some kind of way like the grammars from llama-cpp, or outlines. Maybe there was guidance as well. But I don&amp;#39;t think they work with LLMs from providers. I&amp;#39;m facing this problem with mistral-large.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Mean-Night6324&quot;&gt; /u/Mean-Night6324 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cyp7ij/what_are_some_ways_to_enforce_structured_outputs/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cyp7ij/what_are_some_ways_to_enforce_structured_outputs/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cyp7ij</id><link href="https://www.reddit.com/r/LangChain/comments/1cyp7ij/what_are_some_ways_to_enforce_structured_outputs/" /><updated>2024-05-23T10:04:32+00:00</updated><published>2024-05-23T10:04:32+00:00</published><title>What are some ways to enforce structured outputs from LLMs not in your control beyond basic prompting?</title></entry><entry><author><name>/u/AnEdgeLordWeeb</name><uri>https://www.reddit.com/user/AnEdgeLordWeeb</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey guys! I&amp;#39;m trying to develop a chatbot that offers video games recommendations based on user input.&lt;br/&gt; Problem is, I&amp;#39;m stuck at the chain which objective is to use Tavily API tool to search for video games&amp;#39; titles that fit the user&amp;#39;s criteria.&lt;/p&gt; &lt;p&gt;Here&amp;#39;s what I&amp;#39;ve tried:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;# Game Title Search prompt = PromptTemplate( template=&amp;quot;&amp;quot;&amp;quot;You are part of a chatbot that provides personalized video game recommendations based on user preferences. \n Your task is to search for the top 5 video games that match the user query. \n Only return the titles of the games. \n\n User Query: {query}&amp;quot;&amp;quot;&amp;quot;, input_variables=[&amp;quot;query&amp;quot;], ) game_title_search = prompt | llm.bind_tools(tools) QUERY = &amp;quot;&amp;quot;&amp;quot;What games are similar to Skyrim?&amp;quot;&amp;quot;&amp;quot; result = game_title_search.invoke({&amp;quot;query&amp;quot;: QUERY}) print(result) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Problem is, when I print result it gives me this instead of the response that I&amp;#39;m expecting (which are the video games&amp;#39; titles:&lt;/p&gt; &lt;p&gt;&lt;code&gt;content=&amp;#39;&amp;#39; additional_kwargs={&amp;#39;tool_calls&amp;#39;: [{&amp;#39;id&amp;#39;: &amp;#39;call_xJGybVhCtBAYGHyNkEE04U1c&amp;#39;, &amp;#39;function&amp;#39;: {&amp;#39;arguments&amp;#39;: &amp;#39;{&amp;quot;query&amp;quot;:&amp;quot;games similar to Skyrim&amp;quot;}&amp;#39;, &amp;#39;name&amp;#39;: &amp;#39;tavily_search_results_json&amp;#39;}, &amp;#39;type&amp;#39;: &amp;#39;function&amp;#39;}]} response_metadata={&amp;#39;token_usage&amp;#39;: {&amp;#39;completion_tokens&amp;#39;: 21, &amp;#39;prompt_tokens&amp;#39;: 141, &amp;#39;total_tokens&amp;#39;: 162}, &amp;#39;model_name&amp;#39;: &amp;#39;gpt-3.5-turbo-1106&amp;#39;, &amp;#39;system_fingerprint&amp;#39;: None, &amp;#39;finish_reason&amp;#39;: &amp;#39;tool_calls&amp;#39;, &amp;#39;logprobs&amp;#39;: None} id=&amp;#39;run-c7c33094-2173-43d8-9e9a-319c80265f57-0&amp;#39; tool_calls=[{&amp;#39;name&amp;#39;: &amp;#39;tavily_search_results_json&amp;#39;, &amp;#39;args&amp;#39;: {&amp;#39;query&amp;#39;: &amp;#39;games similar to Skyrim&amp;#39;}, &amp;#39;id&amp;#39;: &amp;#39;call_xJGybVhCtBAYGHyNkEE04U1c&amp;#39;}]&lt;/code&gt;&lt;/p&gt; &lt;p&gt;How can I solve this and use the tools alongside the ChatModel and the PromptTemplate to achieve what I want?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/AnEdgeLordWeeb&quot;&gt; /u/AnEdgeLordWeeb &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cyt7uf/how_can_i_properly_use_tools_within_a_chain_in/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cyt7uf/how_can_i_properly_use_tools_within_a_chain_in/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cyt7uf</id><link href="https://www.reddit.com/r/LangChain/comments/1cyt7uf/how_can_i_properly_use_tools_within_a_chain_in/" /><updated>2024-05-23T13:48:44+00:00</updated><published>2024-05-23T13:48:44+00:00</published><title>How can I properly use tools within a chain in LangGraph?</title></entry><entry><author><name>/u/Mission_Tip4316</name><uri>https://www.reddit.com/user/Mission_Tip4316</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;What worked for me was to create small modular functions out of one big function with different parameters. I broke down my API for the bot to use into smaller, modular endpoints with maximum of two parameters each. &lt;/p&gt; &lt;p&gt;I have been able to use gpt-3.5 to get satisfactory outputs without fails. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Mission_Tip4316&quot;&gt; /u/Mission_Tip4316 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cyn34y/for_those_struggling_with_api_function_calls/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cyn34y/for_those_struggling_with_api_function_calls/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cyn34y</id><link href="https://www.reddit.com/r/LangChain/comments/1cyn34y/for_those_struggling_with_api_function_calls/" /><updated>2024-05-23T07:31:44+00:00</updated><published>2024-05-23T07:31:44+00:00</published><title>For those struggling with API function calls</title></entry><entry><author><name>/u/mehulgupta7991</name><uri>https://www.reddit.com/user/mehulgupta7991</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/mehulgupta7991&quot;&gt; /u/mehulgupta7991 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;/r/ArtificialInteligence/comments/1cytvn5/generative_ai_for_time_series/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cyu0sy/timegpt_generative_ai_for_time_series/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cyu0sy</id><link href="https://www.reddit.com/r/LangChain/comments/1cyu0sy/timegpt_generative_ai_for_time_series/" /><updated>2024-05-23T14:23:42+00:00</updated><published>2024-05-23T14:23:42+00:00</published><title>TimeGPT: Generative AI for Time Series</title></entry><entry><author><name>/u/TripleCheeeze</name><uri>https://www.reddit.com/user/TripleCheeeze</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;pre&gt;&lt;code&gt;from langchain.embeddings import OpenAIEmbeddings from langchain.retrievers import ParentDocumentRetriever from langchain.schema import Document from langchain.storage import InMemoryStore from langchain.text_splitter import RecursiveCharacterTextSplitter from langchain.vectorstores.chroma import Chroma vectorstore = Chroma( collection_name=&amp;quot;full_documents&amp;quot;, embedding_function=OpenAIEmbeddings() ) store = InMemoryStore() docs = [Document(page_content=txt, metadata={&amp;quot;id&amp;quot;: id}) for txt, id in [(&amp;quot;aaaaaa&amp;quot;, 1), (&amp;quot;bbbbbb&amp;quot;, 2)]] ParentDocumentRetriever( vectorstore=vectorstore, docstore=store, id_key=&amp;quot;id&amp;quot;, parent_splitter=RecursiveCharacterTextSplitter( chunk_size = 2, chunk_overlap = 0, length_function = len, add_start_index = True, ), child_splitter=RecursiveCharacterTextSplitter( chunk_size = 1, chunk_overlap = 0, length_function = len, add_start_index = True, ), ).add_documents(docs,ids=[doc.metadata[&amp;quot;id&amp;quot;] for doc in docs])from langchain.embeddings import OpenAIEmbeddings from langchain.retrievers import ParentDocumentRetriever from langchain.schema import Document from langchain.storage import InMemoryStore from langchain.text_splitter import RecursiveCharacterTextSplitter from langchain.vectorstores.chroma import Chroma vectorstore = Chroma( collection_name=&amp;quot;full_documents&amp;quot;, embedding_function=OpenAIEmbeddings() ) store = InMemoryStore() docs = [Document(page_content=txt, metadata={&amp;quot;id&amp;quot;: id}) for txt, id in [(&amp;quot;aaaaaa&amp;quot;, 1), (&amp;quot;bbbbbb&amp;quot;, 2)]] ParentDocumentRetriever( vectorstore=vectorstore, docstore=store, id_key=&amp;quot;id&amp;quot;, parent_splitter=RecursiveCharacterTextSplitter( chunk_size = 2, chunk_overlap = 0, length_function = len, add_start_index = True, ), child_splitter=RecursiveCharacterTextSplitter( chunk_size = 1, chunk_overlap = 0, length_function = len, add_start_index = True, ), ).add_documents(docs,ids=[doc.metadata[&amp;quot;id&amp;quot;] for doc in docs]) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The error :&lt;/p&gt; &lt;pre&gt;&lt;code&gt;ValueError: Got uneven list of documents and ids. If `ids` is provided, should be same length as `documents`. &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The size of documents list and ids list are nevertheless equal, i don&amp;#39;t understand this error &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/TripleCheeeze&quot;&gt; /u/TripleCheeeze &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cytwsx/parentdocumentretrieveradd_document_function_with/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cytwsx/parentdocumentretrieveradd_document_function_with/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cytwsx</id><link href="https://www.reddit.com/r/LangChain/comments/1cytwsx/parentdocumentretrieveradd_document_function_with/" /><updated>2024-05-23T14:18:57+00:00</updated><published>2024-05-23T14:18:57+00:00</published><title>ParentDocumentRetriever.add_document function with &quot;ids&quot; parameter - can't fix an error</title></entry><entry><author><name>/u/Lost-Most-487</name><uri>https://www.reddit.com/user/Lost-Most-487</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;m using create_csv_agent to get a csv parsing agent to analyze and return a list of items that meets the criteria. The agent handles the questions fine and I can see the correct results printed out in its Observations. However it doesn&amp;#39;t include the list of items in the final output. How can I get around this?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Lost-Most-487&quot;&gt; /u/Lost-Most-487 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cyoeho/how_can_i_get_the_csv_agent_to_return_the/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cyoeho/how_can_i_get_the_csv_agent_to_return_the/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cyoeho</id><link href="https://www.reddit.com/r/LangChain/comments/1cyoeho/how_can_i_get_the_csv_agent_to_return_the/" /><updated>2024-05-23T09:07:00+00:00</updated><published>2024-05-23T09:07:00+00:00</published><title>How can I get the csv_agent to return the complete results from its Observation?</title></entry><entry><author><name>/u/juansm2001</name><uri>https://www.reddit.com/user/juansm2001</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey I am doing an internship and my boss asked me to build a RAG that can read financial documents (pdf) and create a LLM that, with a query, answers based on these documents. I was using BGE as the embedding model and ollama with llama2 for the LLM. My problem is that I was using google collab with the free GPU but once it reaches the limit, I can&amp;#39;t keep creating the embeddings. Is there any FREE solution for this? Thank you and sorry for my inexperience.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/juansm2001&quot;&gt; /u/juansm2001 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cynhl3/im_new_to_this_and_i_need_help_for_my_rag/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cynhl3/im_new_to_this_and_i_need_help_for_my_rag/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cynhl3</id><link href="https://www.reddit.com/r/LangChain/comments/1cynhl3/im_new_to_this_and_i_need_help_for_my_rag/" /><updated>2024-05-23T08:01:05+00:00</updated><published>2024-05-23T08:01:05+00:00</published><title>I'm new to this and I need help for my RAG</title></entry><entry><author><name>/u/MangjoseKlyne</name><uri>https://www.reddit.com/user/MangjoseKlyne</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;m working on a project that involves using a language model chain to process questions and generate responses. However, I&amp;#39;ve encountered an issue where the chain seems to get stuck at the invocation stage without producing any results.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Background:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;I&amp;#39;m using a Python script that involves various components such as document loaders, embeddings, text splitters, vector stores, retrievers, prompts, parsers, and language models.&lt;/li&gt; &lt;li&gt;The script is designed to load a PDF document, split it into chunks, add the chunks to a vector database, initialize a language model, and then retrieve relevant information based on input questions.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Problem:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Despite setting up the chain correctly and providing a question to the system, it seems to get stuck at the invocation stage without producing any results.&lt;/li&gt; &lt;li&gt;I&amp;#39;ve checked the logs, and everything seems to be initialized and processed correctly up to the invocation step.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Code:&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;from langchain_community.document_loaders import UnstructuredPDFLoader from langchain_community.document_loaders import OnlinePDFLoader from langchain_community.embeddings import OllamaEmbeddings from langchain_text_splitters import RecursiveCharacterTextSplitter from langchain_community.vectorstores import Chroma from langchain.prompts import ChatPromptTemplate, PromptTemplate from langchain_core.output_parsers import StrOutputParser from langchain_community.chat_models import ChatOllama from langchain_core.runnables import RunnablePassthrough from langchain_core.tracers import ConsoleCallbackHandler from langchain.retrievers.multi_query import MultiQueryRetriever import asyncio # Use raw string notation for the file path local_path = r&amp;quot;C:/Users/User/zven/WEF_The_Global_Cooperation_Barometer_2024.pdf&amp;quot; # Load local PDF file if local_path: try: loader = UnstructuredPDFLoader(file_path=local_path) data = loader.load() print(&amp;quot;PDF loaded successfully.&amp;quot;) except Exception as e: print(f&amp;quot;Error loading PDF: {e}&amp;quot;) data = None else: print(&amp;quot;Upload a PDF file&amp;quot;) data = None if data: # Split and chunk text text_splitter = RecursiveCharacterTextSplitter(chunk_size=7500, chunk_overlap=100) chunks = text_splitter.split_documents(data) print(f&amp;quot;Document split into {len(chunks)} chunks.&amp;quot;) print(data[0].page_content) # Add to vector database try: vector_db = Chroma.from_documents( documents=chunks, embedding=OllamaEmbeddings(model=&amp;quot;nomic-embed-text&amp;quot;, show_progress=True), collection_name=&amp;quot;local-rag&amp;quot; ) print(&amp;quot;Chunks added to vector database.&amp;quot;) except Exception as e: print(f&amp;quot;Error adding chunks to vector database: {e}&amp;quot;) # Initialize LLM from Ollama local_model = &amp;quot;Mistral&amp;quot; try: llm = ChatOllama(model=local_model) print(&amp;quot;LLM initialized successfully.&amp;quot;) except Exception as e: print(f&amp;quot;Error initializing LLM: {e}&amp;quot;) # Define query prompt template QUERY_PROMPT = PromptTemplate( input_variables=[&amp;quot;question&amp;quot;], template=&amp;quot;&amp;quot;&amp;quot;You are an AI language model assistant. Your task is to generate five different versions of the given user question to retrieve relevant documents from a vector database. By generating multiple perspectives on the user question, your goal is to help the user overcome some of the limitations of the distance-based similarity search. Provide these alternative questions separated by newlines. Original question: {question}&amp;quot;&amp;quot;&amp;quot; ) # Initialize retriever try: retriever = MultiQueryRetriever.from_llm( vector_db.as_retriever(), llm, prompt=QUERY_PROMPT ) print(&amp;quot;Retriever initialized successfully.&amp;quot;) except Exception as e: print(f&amp;quot;Error initializing retriever: {e}&amp;quot;) # Define RAG prompt template template = &amp;quot;&amp;quot;&amp;quot;Answer the question based ONLY on the following context: {context} Question: {question} &amp;quot;&amp;quot;&amp;quot; print(&amp;quot;Template: &amp;quot;, template) prompt = ChatPromptTemplate.from_template(template) print(&amp;quot;Prompt: &amp;quot;, prompt) chain = ( {&amp;quot;context&amp;quot;: retriever, &amp;quot;question&amp;quot;: RunnablePassthrough()} | prompt | llm | StrOutputParser() ) print(chain) # Print the chain setup print(&amp;quot;Chain setup completed.&amp;quot;) async def run_chain(): try: print(&amp;quot;Invoking chain...&amp;quot;) # Invoke the chain with a question result = await chain.ainvoke( {&amp;quot;question&amp;quot;: &amp;quot;What are the 5 pillars of global cooperation?&amp;quot;}, config={&amp;#39;callbacks&amp;#39;: [ConsoleCallbackHandler()]} # 30 seconds timeout ) print(&amp;quot;Chain invoked successfully.&amp;quot;) print(&amp;quot;Result:&amp;quot;, result) # Print the answer if it exists if &amp;quot;answer&amp;quot; in result: print(&amp;quot;Answer:&amp;quot;, result[&amp;quot;answer&amp;quot;]) except asyncio.TimeoutError: print(&amp;quot;Chain invocation timed out.&amp;quot;) except Exception as e: print(f&amp;quot;Error invoking chain: {e}&amp;quot;) # Run the chain asyncio.run(run_chain()) # Delete all collections in the db vector_db.delete_collection() &lt;/code&gt;&lt;/pre&gt; &lt;blockquote&gt; &lt;/blockquote&gt; &lt;p&gt;Output when I Run it:&lt;br/&gt; OllamaEmbeddings: 100%|████████████████████████████████████████████████████████████████| 11/11 [05:16&amp;lt;00:00, 28.77s/it]&lt;/p&gt; &lt;p&gt;Chunks added to vector database.&lt;/p&gt; &lt;p&gt;LLM initialized successfully.&lt;/p&gt; &lt;p&gt;Retriever initialized successfully.&lt;/p&gt; &lt;p&gt;Template: Answer the question based ONLY on the following context:&lt;/p&gt; &lt;p&gt;{context}&lt;/p&gt; &lt;p&gt;Question: {question}&lt;/p&gt; &lt;p&gt;Prompt: input_variables=[&amp;#39;context&amp;#39;, &amp;#39;question&amp;#39;] messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=[&amp;#39;context&amp;#39;, &amp;#39;question&amp;#39;], template=&amp;#39;Answer the question based ONLY on the following context:\n {context}\n Question: {question}\n &amp;#39;))]&lt;/p&gt; &lt;p&gt;first={&lt;/p&gt; &lt;p&gt;context: MultiQueryRetriever(retriever=VectorStoreRetriever(tags=[&amp;#39;Chroma&amp;#39;, &amp;#39;OllamaEmbeddings&amp;#39;], vectorstore=&amp;lt;langchain\_community.vectorstores.chroma.Chroma object at 0x0000016DB7450390&amp;gt;), llm_chain=LLMChain(prompt=PromptTemplate(input_variables=[&amp;#39;question&amp;#39;], template=&amp;#39;You are an AI language model assistant. Your task is to generate five\n different versions of the given user question to retrieve relevant documents from\n a vector database. By generating multiple perspectives on the user question, your\n goal is to help the user overcome some of the limitations of the distance-based\n similarity search. Provide these alternative questions separated by newlines.\n Original question: {question}&amp;#39;), llm=ChatOllama(model=&amp;#39;Mistral&amp;#39;), output_parser=LineListOutputParser())),&lt;/p&gt; &lt;p&gt;question: RunnablePassthrough()&lt;/p&gt; &lt;p&gt;} middle=[ChatPromptTemplate(input_variables=[&amp;#39;context&amp;#39;, &amp;#39;question&amp;#39;], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=[&amp;#39;context&amp;#39;, &amp;#39;question&amp;#39;], template=&amp;#39;Answer the question based ONLY on the following context:\n {context}\n Question: {question}\n &amp;#39;))]), ChatOllama(model=&amp;#39;Mistral&amp;#39;)] last=StrOutputParser()&lt;/p&gt; &lt;p&gt;Chain setup completed.&lt;/p&gt; &lt;p&gt;Invoking chain...&lt;/p&gt; &lt;p&gt;[chain/start] [chain:RunnableSequence] Entering Chain run with input:&lt;/p&gt; &lt;p&gt;{&lt;/p&gt; &lt;p&gt;&amp;quot;question&amp;quot;: &amp;quot;What are the 5 pillars of global cooperation?&amp;quot;&lt;/p&gt; &lt;p&gt;}&lt;/p&gt; &lt;p&gt;[chain/start] [chain:RunnableSequence &amp;gt; chain:RunnableParallel&amp;lt;context,question&amp;gt;] Entering Chain run with input:&lt;/p&gt; &lt;p&gt;{&lt;/p&gt; &lt;p&gt;&amp;quot;question&amp;quot;: &amp;quot;What are the 5 pillars of global cooperation?&amp;quot;&lt;/p&gt; &lt;p&gt;}&lt;/p&gt; &lt;p&gt;[chain/start] [chain:RunnableSequence &amp;gt; chain:RunnableParallel&amp;lt;context,question&amp;gt; &amp;gt; chain:RunnablePassthrough] Entering Chain run with input:&lt;/p&gt; &lt;p&gt;{&lt;/p&gt; &lt;p&gt;&amp;quot;question&amp;quot;: &amp;quot;What are the 5 pillars of global cooperation?&amp;quot;&lt;/p&gt; &lt;p&gt;}&lt;/p&gt; &lt;p&gt;[chain/start] [chain:RunnableSequence &amp;gt; chain:RunnableParallel&amp;lt;context,question&amp;gt; &amp;gt; retriever:Retriever &amp;gt; chain:LLMChain] Entering Chain run with input:&lt;/p&gt; &lt;p&gt;{&lt;/p&gt; &lt;p&gt;&amp;quot;question&amp;quot;: {&lt;/p&gt; &lt;p&gt;&amp;quot;question&amp;quot;: &amp;quot;What are the 5 pillars of global cooperation?&amp;quot;&lt;/p&gt; &lt;p&gt;}&lt;/p&gt; &lt;p&gt;}[chain/end] [chain:RunnableSequence &amp;gt; chain:RunnableParallel&amp;lt;context,question&amp;gt; &amp;gt; chain:RunnablePassthrough] [16ms] Exiting Chain run with output:&lt;/p&gt; &lt;p&gt;{&lt;/p&gt; &lt;p&gt;&amp;quot;question&amp;quot;: &amp;quot;What are the 5 pillars of global cooperation?&amp;quot;&lt;/p&gt; &lt;p&gt;}&lt;/p&gt; &lt;p&gt;[llm/start] [chain:RunnableSequence &amp;gt; chain:RunnableParallel&amp;lt;context,question&amp;gt; &amp;gt; retriever:Retriever &amp;gt; chain:LLMChain &amp;gt; llm:ChatOllama] Entering LLM run with input:&lt;/p&gt; &lt;p&gt;{&lt;/p&gt; &lt;p&gt;&amp;quot;prompts&amp;quot;: [&lt;/p&gt; &lt;p&gt;&amp;quot;Human: You are an AI language model assistant. Your task is to generate five\n different versions of the given user question to retrieve relevant documents from\n a vector database. By generating multiple perspectives on the user question, your\n goal is to help the user overcome some of the limitations of the distance-based\n similarity search. Provide these alternative questions separated by newlines.\n Original question: {&amp;#39;question&amp;#39;: &amp;#39;What are the 5 pillars of global cooperation?&amp;#39;}&amp;quot;&lt;/p&gt; &lt;p&gt;]&lt;/p&gt; &lt;p&gt;}&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/MangjoseKlyne&quot;&gt; /u/MangjoseKlyne &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cyhm4y/need_help_understanding_why_my_language_model/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cyhm4y/need_help_understanding_why_my_language_model/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cyhm4y</id><link href="https://www.reddit.com/r/LangChain/comments/1cyhm4y/need_help_understanding_why_my_language_model/" /><updated>2024-05-23T02:01:37+00:00</updated><published>2024-05-23T02:01:37+00:00</published><title>Need Help Understanding Why My Language Model Chain Isn't Producing Results</title></entry></feed>