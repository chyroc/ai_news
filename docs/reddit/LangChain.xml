<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/"><category term="LangChain" label="r/LangChain"/><updated>2024-04-23T19:18:24+00:00</updated><icon>https://www.redditstatic.com/icon.png/</icon><id>/r/LangChain.rss</id><link rel="self" href="https://www.reddit.com/r/LangChain.rss" type="application/atom+xml" /><link rel="alternate" href="https://www.reddit.com/r/LangChain" type="text/html" /><subtitle>LangChain is an open-source framework and developer toolkit that helps developers get LLM applications from prototype to production. It is available for Python and Javascript at https://www.langchain.com/.</subtitle><title>LangChain</title><entry><author><name>/u/zchaarm</name><uri>https://www.reddit.com/user/zchaarm</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;A place for members of &lt;a href=&quot;/r/LangChain&quot;&gt;r/LangChain&lt;/a&gt; to chat with each other&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/zchaarm&quot;&gt; /u/zchaarm &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_10ljho9</id><link href="https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/" /><updated>2023-01-26T04:33:05+00:00</updated><published>2023-01-26T04:33:05+00:00</published><title>r/LangChain Lounge</title></entry><entry><author><name>/u/ravediamond000</name><uri>https://www.reddit.com/user/ravediamond000</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hello guys,&lt;/p&gt; &lt;p&gt;Just wrote a new blog post explaining Langchain LCEL in a easier manner: &lt;a href=&quot;https://www.metadocs.co/&quot;&gt;link&lt;/a&gt;.&lt;br/&gt; I really love LCEL (feels a little like functional programing right !?) and wanted to try to explain it in a simpler way.&lt;br/&gt; Enjoy.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/ravediamond000&quot;&gt; /u/ravediamond000 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cb01ch/langchain_lcel_explained_the_easy_way/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cb01ch/langchain_lcel_explained_the_easy_way/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cb01ch</id><link href="https://www.reddit.com/r/LangChain/comments/1cb01ch/langchain_lcel_explained_the_easy_way/" /><updated>2024-04-23T09:12:48+00:00</updated><published>2024-04-23T09:12:48+00:00</published><title>Langchain LCEL explained the easy way</title></entry><entry><author><name>/u/phantom69_ftw</name><uri>https://www.reddit.com/user/phantom69_ftw</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Basically if I have a pdf of 100 pages and to answer my question I need 30 diff chunks across diff pages. Now if my top_k is set to 20. How will this ever be possible?&lt;/p&gt; &lt;p&gt;Like in general, isn&amp;#39;t this a issue with RAGs? How can I know how many chunks are needed to answer a question? If it&amp;#39;s less than whatever topk I set, it&amp;#39;s fine. But what if there are more?&lt;/p&gt; &lt;p&gt;Is this a limitation of RAG? If no, how to solve for this? If yes, what other ways can I explore?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/phantom69_ftw&quot;&gt; /u/phantom69_ftw &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cbcln9/how_to_solve_if_relevant_docs_max_top_k/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cbcln9/how_to_solve_if_relevant_docs_max_top_k/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cbcln9</id><link href="https://www.reddit.com/r/LangChain/comments/1cbcln9/how_to_solve_if_relevant_docs_max_top_k/" /><updated>2024-04-23T18:52:55+00:00</updated><published>2024-04-23T18:52:55+00:00</published><title>How to solve if relevant docs &gt; max top_k ?</title></entry><entry><author><name>/u/Money_Mycologist4939</name><uri>https://www.reddit.com/user/Money_Mycologist4939</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cb68b9/langsmith_render_of_retrieved_documents/&quot;&gt; &lt;img src=&quot;https://b.thumbs.redditmedia.com/JmafAij99N282f2vvy_q1p_h8FB_SzZHa5Ipo8uMwPg.jpg&quot; alt=&quot;Langsmith render of retrieved documents&quot; title=&quot;Langsmith render of retrieved documents&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I have been using langsmith for controling the retrieving step of my rag application. And it&amp;#39;s nice because it has a render that format the raw langchain docs in a more readable format. &lt;/p&gt; &lt;p&gt;The problem is that since I changed the format of my langchain docs, adding more metadata, this feature does not work anymore. Do anyone has got any advice on what&amp;#39;s the right format compatible to the render??&lt;/p&gt; &lt;p&gt;now: &lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/7leugfcup8wc1.png?width=1266&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f7738461ab2add7e587aeb9aed415a5b2e1f6c20&quot;&gt;https://preview.redd.it/7leugfcup8wc1.png?width=1266&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f7738461ab2add7e587aeb9aed415a5b2e1f6c20&lt;/a&gt;&lt;/p&gt; &lt;p&gt;BEFORE:&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/8s86iy6xp8wc1.png?width=1287&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=302f1f3b2923f428288c59c29f31327d5d9b6db0&quot;&gt;https://preview.redd.it/8s86iy6xp8wc1.png?width=1287&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=302f1f3b2923f428288c59c29f31327d5d9b6db0&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Money_Mycologist4939&quot;&gt; /u/Money_Mycologist4939 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cb68b9/langsmith_render_of_retrieved_documents/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cb68b9/langsmith_render_of_retrieved_documents/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1cb68b9</id><media:thumbnail url="https://b.thumbs.redditmedia.com/JmafAij99N282f2vvy_q1p_h8FB_SzZHa5Ipo8uMwPg.jpg" /><link href="https://www.reddit.com/r/LangChain/comments/1cb68b9/langsmith_render_of_retrieved_documents/" /><updated>2024-04-23T14:35:42+00:00</updated><published>2024-04-23T14:35:42+00:00</published><title>Langsmith render of retrieved documents</title></entry><entry><author><name>/u/furyacer</name><uri>https://www.reddit.com/user/furyacer</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I am trying to build a chatbot using RAG and LangChain that will update the PDFs based on the user prompt and the pdfs will be stored in a db (chromedb) that will be connected to the chatbot. I&amp;#39;m planning to use OpenAI for chunking and indexing information that will be analyzed by the bot. &lt;/p&gt; &lt;p&gt;It will be helpful if anyone can tell me how to proceed further with this. I have only found projects and repos which focus on QA chatbots so I just want to extend this project to include this functionality.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/furyacer&quot;&gt; /u/furyacer &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cb35fa/chatbot_using_rag_to_update_pdfs/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cb35fa/chatbot_using_rag_to_update_pdfs/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cb35fa</id><link href="https://www.reddit.com/r/LangChain/comments/1cb35fa/chatbot_using_rag_to_update_pdfs/" /><updated>2024-04-23T12:18:55+00:00</updated><published>2024-04-23T12:18:55+00:00</published><title>Chat-bot using RAG to update PDFs</title></entry><entry><author><name>/u/phenobarbital_</name><uri>https://www.reddit.com/user/phenobarbital_</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hello guys, I&amp;#39;m new on this of NPL and Langchain, I&amp;#39;m currently working on a chatbot to &amp;quot;talk&amp;quot; to my data, converting pandas dataframes into JSON and every row in dataframe is a document saved into Vector Store (I&amp;#39;m using Milvus as Vector Database). &lt;/p&gt; &lt;p&gt;For questions related to 1 to N (getting one row from many), the similarity search is working as expected and I am achieving good results.&lt;/p&gt; &lt;p&gt;For example, if I asking &amp;quot;where this store is located?&amp;quot; or &amp;quot;how many displays has Store A?&amp;quot; is working, but if I ask something about the entire dataset as &amp;quot;how many displays are overall in US?&amp;quot;, or &amp;quot;how many displays are in California?&amp;quot;, the totalization is related to the &amp;quot;k&amp;quot; passed to the vector retriever:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;retriever = VectorStoreRetriever( vectorstore=vector, search_type=&amp;#39;similarity&amp;#39;, search_kwargs={&amp;quot;k&amp;quot;: 10} ) chain = RetrievalQA.from_chain_type( llm=llm, retriever=retriever, chain_type=&amp;#39;stuff&amp;#39;, verbose=True ) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;I cannot pass a bigger &amp;quot;K&amp;quot; because my LLM rejects it (I&amp;#39;m using Google Gemini-Pro).&lt;/p&gt; &lt;p&gt;There is a way to:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Check if the user&amp;#39;s question involves a quantification.&lt;/li&gt; &lt;li&gt;Executing something like a &amp;quot;Map Reduce&amp;quot; over the entire dataset to return the reduced version of the documents (or documents with question applied).&lt;/li&gt; &lt;li&gt;Passing the reduction to the LLM for getting the final result.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Or if there is a way to making this in Langchain using another type on Chain.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/phenobarbital_&quot;&gt; /u/phenobarbital_ &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cb9mab/getting_totals_and_counts_based_on_the_entire/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cb9mab/getting_totals_and_counts_based_on_the_entire/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cb9mab</id><link href="https://www.reddit.com/r/LangChain/comments/1cb9mab/getting_totals_and_counts_based_on_the_entire/" /><updated>2024-04-23T16:54:06+00:00</updated><published>2024-04-23T16:54:06+00:00</published><title>Getting totals and counts based on the entire dataset with RetrievalQA</title></entry><entry><author><name>/u/ErnteSkunkFest</name><uri>https://www.reddit.com/user/ErnteSkunkFest</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;me and my company are currently building a(nother) RAG system for our customers in the legal sector. &lt;/p&gt; &lt;p&gt;We are performing a keyword (BM25) + vector search and then using Reciprocal Rank Fusion (RRF) algorithm fuse the combined 10 top_k results and feed them into our LLM. &lt;/p&gt; &lt;p&gt;We have also implemented HyDE (hypothetical document embeddings) &lt;a href=&quot;https://github.com/texttron/hyde&quot;&gt;https://github.com/texttron/hyde&lt;/a&gt; to further improve retrieval quality. &lt;/p&gt; &lt;p&gt;Now I read through a few articles on Medium and Reddit and saw a lot of recommendations to use reranking to further improve results. &lt;/p&gt; &lt;p&gt;Does it make sense to again rerank the results of the hybrid search, even though strictly speaking we already are reranking them based on the output of BM25 + vector search? I specifically thought about Cohere rerank here. &lt;/p&gt; &lt;p&gt;Thanks and greets!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/ErnteSkunkFest&quot;&gt; /u/ErnteSkunkFest &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cazrxf/reranking_after_rrfhybrid_search/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cazrxf/reranking_after_rrfhybrid_search/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cazrxf</id><link href="https://www.reddit.com/r/LangChain/comments/1cazrxf/reranking_after_rrfhybrid_search/" /><updated>2024-04-23T08:54:38+00:00</updated><published>2024-04-23T08:54:38+00:00</published><title>Reranking after RRF-Hybrid Search?</title></entry><entry><author><name>/u/BellaHi</name><uri>https://www.reddit.com/user/BellaHi</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cayehu/how_to_summarize_large_documents_with_langchain/&quot;&gt; &lt;img src=&quot;https://external-preview.redd.it/CAQRWbTPWtJv7yFV9pZ0ie87SyVlKHo4X3u_ASI_PdE.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4c5c3f7d9a488a164dd2cee201910ba6f442b02e&quot; alt=&quot;How to Summarize Large Documents with LangChain and OpenAI&quot; title=&quot;How to Summarize Large Documents with LangChain and OpenAI&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/BellaHi&quot;&gt; /u/BellaHi &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://thenewstack.io/how-to-summarize-large-documents-with-langchain-and-openai/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cayehu/how_to_summarize_large_documents_with_langchain/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1cayehu</id><media:thumbnail url="https://external-preview.redd.it/CAQRWbTPWtJv7yFV9pZ0ie87SyVlKHo4X3u_ASI_PdE.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=4c5c3f7d9a488a164dd2cee201910ba6f442b02e" /><link href="https://www.reddit.com/r/LangChain/comments/1cayehu/how_to_summarize_large_documents_with_langchain/" /><updated>2024-04-23T07:17:31+00:00</updated><published>2024-04-23T07:17:31+00:00</published><title>How to Summarize Large Documents with LangChain and OpenAI</title></entry><entry><author><name>/u/AchillesFirstStand</name><uri>https://www.reddit.com/user/AchillesFirstStand</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;This is the excerpt from my code:&lt;br/&gt; &lt;code&gt;chain = prompt_template | ollama | output_parser&lt;/code&gt;&lt;/p&gt; &lt;p&gt;How do I store the output from &lt;code&gt;ollama&lt;/code&gt; as a variable and then pass that output to &lt;code&gt;output_parser&lt;/code&gt;? &lt;/p&gt; &lt;p&gt;I don&amp;#39;t understand how the pipe operator | works. I am asking the ollama model to give me a structured output and I need to be able to debug and see what the output was when the output_parser gives an error.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/AchillesFirstStand&quot;&gt; /u/AchillesFirstStand &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cb7rx7/how_can_i_see_the_input_that_is_passed_to_the/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cb7rx7/how_can_i_see_the_input_that_is_passed_to_the/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cb7rx7</id><link href="https://www.reddit.com/r/LangChain/comments/1cb7rx7/how_can_i_see_the_input_that_is_passed_to_the/" /><updated>2024-04-23T15:40:48+00:00</updated><published>2024-04-23T15:40:48+00:00</published><title>How can I see the input that is passed to the output parser when the commands are chained?</title></entry><entry><author><name>/u/Desperate-Energy2694</name><uri>https://www.reddit.com/user/Desperate-Energy2694</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cb4yjd/how_does_chunk_size_relate_to_an_embedding_models/&quot;&gt; &lt;img src=&quot;https://b.thumbs.redditmedia.com/9QB0QmRhJMwVuBBSw9iKfRPwWGMk-YQugUlRLhWvp3c.jpg&quot; alt=&quot;How does chunk size relate to an embedding model's dimension of vectors and max token lenght?&quot; title=&quot;How does chunk size relate to an embedding model's dimension of vectors and max token lenght?&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi, everyone! I&amp;#39;m fairly new to NLP tasks, and I&amp;#39;m currently building a langchain RAG app, and for that I need to do some testings with different chunks sizes.&lt;/p&gt; &lt;p&gt;I&amp;#39;m currently using a large BERT model, and what I&amp;#39;ve been confused about is: What is the relationship between the chunk size chosen to chunk my documents, and the embedding model&amp;#39;s vector dimension (if there is any) or even the max token limit?&lt;/p&gt; &lt;p&gt;I&amp;#39;ve seen a wide range of articles from people testings out chunk sizes from 128 to 2048, but I&amp;#39;ve also read in some places that the original BERT models take 512 tokens max. What does that influence on the way I&amp;#39;m doing things?&lt;/p&gt; &lt;p&gt;I&amp;#39;m creating embeddings like this: Using RecursiveCharacterSplitter with different chunks sizes and len function using my bert model&amp;#39;s tokenizer and finally creating embeddings with HuggingFaceBgeEmbeddings (arbitrary choice, I couldn&amp;#39;t figure out which class to choose) and storing on a vector store.&lt;/p&gt; &lt;p&gt;I iterated over my chunked documents (splitted using the same flow mentioned above) and their sizes nor the amount of tokens gotten from tokenizing it directly are equal to my chunk_size:&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/s9txv1rej8wc1.png?width=1015&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a11725cf9967ec9a04ddad46f8c2bedc587d7ed8&quot;&gt;https://preview.redd.it/s9txv1rej8wc1.png?width=1015&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a11725cf9967ec9a04ddad46f8c2bedc587d7ed8&lt;/a&gt;&lt;/p&gt; &lt;p&gt;But the actual embeddings are generated with dimension 1024 (which is the correct value for vector dimensions for large bert models):&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/9dta4kymj8wc1.png?width=1210&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=31c1787b790d1ecabe5a91c54735b5f0dded62de&quot;&gt;https://preview.redd.it/9dta4kymj8wc1.png?width=1210&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=31c1787b790d1ecabe5a91c54735b5f0dded62de&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The reason I&amp;#39;m asking this is because I&amp;#39;ve been gettting some weird results analyzing results with chunks bigger than 512 (this graph is a comparison of the chunk overlap with fixed chunk size = 1024, where the results don&amp;#39;t really vary even though I&amp;#39;m varying other parameters, like chunk overlap here, which does affect other chunk sizes comparisons)&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/x7qwsfo2g8wc1.png?width=1015&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a3c2273dbe60d07c6974d64802b1b6391398e3d0&quot;&gt;https://preview.redd.it/x7qwsfo2g8wc1.png?width=1015&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a3c2273dbe60d07c6974d64802b1b6391398e3d0&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Desperate-Energy2694&quot;&gt; /u/Desperate-Energy2694 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cb4yjd/how_does_chunk_size_relate_to_an_embedding_models/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cb4yjd/how_does_chunk_size_relate_to_an_embedding_models/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1cb4yjd</id><media:thumbnail url="https://b.thumbs.redditmedia.com/9QB0QmRhJMwVuBBSw9iKfRPwWGMk-YQugUlRLhWvp3c.jpg" /><link href="https://www.reddit.com/r/LangChain/comments/1cb4yjd/how_does_chunk_size_relate_to_an_embedding_models/" /><updated>2024-04-23T13:42:03+00:00</updated><published>2024-04-23T13:42:03+00:00</published><title>How does chunk size relate to an embedding model's dimension of vectors and max token lenght?</title></entry><entry><author><name>/u/kedu16</name><uri>https://www.reddit.com/user/kedu16</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I have a requirement to create parent-child retrieval mechanism for texts. On langchain&amp;#39;s official docs I can see the below code:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;#This text splitter is used to create the parent documents parent_splitter = RecursiveCharacterTextSplitter(chunk_size=2000) #This text splitter is used to create the child documents #It should create documents smaller than the parent child_splitter = RecursiveCharacterTextSplitter(chunk_size=400) #The vectorstore to use to index the child chunks vectorstore = Chroma( collection_name=&amp;quot;split_parents&amp;quot;,embedding_function=OpenAIEmbeddings() ) #The storage layer for the parent documents store = InMemoryStore() retriever = ParentDocumentRetriever( vectorstore=vectorstore, docstore=store, child_splitter=child_splitter, parent_splitter=parent_splitter, ) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;My question is why there is no &lt;strong&gt;split_text()&lt;/strong&gt; for each parent and child chunks that actually splits te document?&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/kedu16&quot;&gt; /u/kedu16 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cb4xpb/query_with_langchains_parentchild_retriever/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cb4xpb/query_with_langchains_parentchild_retriever/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cb4xpb</id><link href="https://www.reddit.com/r/LangChain/comments/1cb4xpb/query_with_langchains_parentchild_retriever/" /><updated>2024-04-23T13:41:03+00:00</updated><published>2024-04-23T13:41:03+00:00</published><title>Query with langchain's parent-child retriever</title></entry><entry><author><name>/u/Spiritual-Taro9889</name><uri>https://www.reddit.com/user/Spiritual-Taro9889</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;The docs contain pages and pages of ramblings that I find very difficult to follow &lt;/p&gt; &lt;p&gt;I&amp;#39;m not very bright so please explain this to a 5 year old with a real world example?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Spiritual-Taro9889&quot;&gt; /u/Spiritual-Taro9889 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1carmfv/explain_to_a_5_year_old_langchain_and_crewai_and/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1carmfv/explain_to_a_5_year_old_langchain_and_crewai_and/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1carmfv</id><link href="https://www.reddit.com/r/LangChain/comments/1carmfv/explain_to_a_5_year_old_langchain_and_crewai_and/" /><updated>2024-04-23T01:03:20+00:00</updated><published>2024-04-23T01:03:20+00:00</published><title>explain to a 5 year old langchain and crewai and its dependency on langchain?</title></entry><entry><author><name>/u/loczngo</name><uri>https://www.reddit.com/user/loczngo</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey guys, i&amp;#39;m fairly new to this and i was searching for a way to use my own locally installed LLMs from HuggingFace. I looked through the official documentation and this is the normal way to invoke and get a response using JS&lt;/p&gt; &lt;p&gt;--------------------------------------------------------------------------------&lt;/p&gt; &lt;p&gt;const chatModel = new ChatOpenAI({});&lt;/p&gt; &lt;p&gt;const response = await chatModel.invoke()&lt;/p&gt; &lt;p&gt;--------------------------------------------------------------------------------&lt;/p&gt; &lt;p&gt;So yea, referring back to my question. Can you guys provide me some documentations or some ways to use my own LLMs from HuggingFace and not having to specify out using the ChatOpenAI class in a NodeJS project i&amp;#39;m working on? Thanks a bunch&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/loczngo&quot;&gt; /u/loczngo &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cb0vo3/custom_llmchat_models_instead_of_llms_such_as/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cb0vo3/custom_llmchat_models_instead_of_llms_such_as/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cb0vo3</id><link href="https://www.reddit.com/r/LangChain/comments/1cb0vo3/custom_llmchat_models_instead_of_llms_such_as/" /><updated>2024-04-23T10:09:41+00:00</updated><published>2024-04-23T10:09:41+00:00</published><title>Custom LLM,Chat models instead of LLMs such as OpenAI using JS</title></entry><entry><author><name>/u/No_Barnacle_8251</name><uri>https://www.reddit.com/user/No_Barnacle_8251</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I am getting a lot of errors and it particularly because the FAISS index expects a particular shape for embeddings and even after I resolved that. I cannot get the right response to my query or some or the other errors are popping up? NOTE: I canâ€™t use langchain &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/No_Barnacle_8251&quot;&gt; /u/No_Barnacle_8251 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cb0p5d/what_embedding_model_to_use_to_convert_textual/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cb0p5d/what_embedding_model_to_use_to_convert_textual/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cb0p5d</id><link href="https://www.reddit.com/r/LangChain/comments/1cb0p5d/what_embedding_model_to_use_to_convert_textual/" /><updated>2024-04-23T09:58:11+00:00</updated><published>2024-04-23T09:58:11+00:00</published><title>What embedding model to use to convert textual data in PDF when using FAISS</title></entry><entry><author><name>/u/No_Barnacle_8251</name><uri>https://www.reddit.com/user/No_Barnacle_8251</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;What to use for converting files greater than 20MB into embeddings and storing in the vector store?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/No_Barnacle_8251&quot;&gt; /u/No_Barnacle_8251 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cau2lk/what_embedding_model_open_source_to_use_for/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cau2lk/what_embedding_model_open_source_to_use_for/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cau2lk</id><link href="https://www.reddit.com/r/LangChain/comments/1cau2lk/what_embedding_model_open_source_to_use_for/" /><updated>2024-04-23T03:01:21+00:00</updated><published>2024-04-23T03:01:21+00:00</published><title>What embedding model (open Source) to use for converting &gt;20MB PDF files?</title></entry><entry><author><name>/u/eyemeroll90</name><uri>https://www.reddit.com/user/eyemeroll90</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;ve been thinking about this a while. How do you ensure the security of your SQL Agent when running in production?&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Is it safe to allow SQL Agent to query db directly? Will it affect db performance if it goes haywire.&lt;/li&gt; &lt;li&gt;We may give SQL Agent read only access. But how to add a more fine-grain access - limiting what it can read?&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/eyemeroll90&quot;&gt; /u/eyemeroll90 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1catddf/how_do_you_handle_langchain_sql_agent_in/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1catddf/how_do_you_handle_langchain_sql_agent_in/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1catddf</id><link href="https://www.reddit.com/r/LangChain/comments/1catddf/how_do_you_handle_langchain_sql_agent_in/" /><updated>2024-04-23T02:26:40+00:00</updated><published>2024-04-23T02:26:40+00:00</published><title>How do you handle Langchain SQL Agent in production</title></entry><entry><author><name>/u/Less_Transition_4827</name><uri>https://www.reddit.com/user/Less_Transition_4827</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I want a chatgpt 3 or 4 assistant which will have access to my calendar Also able to create daily todo list Help me brain stom ideas and save it somewhere like a document. It should remember all old conversations It should have contextual awareness of the document files and also able to write to it &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Less_Transition_4827&quot;&gt; /u/Less_Transition_4827 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cazcu3/i_want_a_chatgpt_3_or_4_assistant_is_there_any/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cazcu3/i_want_a_chatgpt_3_or_4_assistant_is_there_any/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cazcu3</id><link href="https://www.reddit.com/r/LangChain/comments/1cazcu3/i_want_a_chatgpt_3_or_4_assistant_is_there_any/" /><updated>2024-04-23T08:24:23+00:00</updated><published>2024-04-23T08:24:23+00:00</published><title>I want a chatgpt 3 or 4 assistant , is there any solution exists for below requirements? If not how can I create it</title></entry><entry><author><name>/u/Top_Raccoon_1493</name><uri>https://www.reddit.com/user/Top_Raccoon_1493</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I am trying to fetch URLs using below code but not getting whole content extracted from it, and I am getting :&lt;/p&gt; &lt;p&gt;&lt;strong&gt;[Document(page_content=&amp;#39;Enable JavaScript and cookies to continue&amp;#39;, metadata={&amp;#39;source&amp;#39;: &amp;#39;&lt;a href=&quot;https://medium.com/@woyera/how-to-chat-with-microsoft-word-documents-using-llama-2-b30faa053284&amp;#x27;%7D)%5C&quot;&gt;https://medium.com/@woyera/how-to-chat-with-microsoft-word-documents-using-llama-2-b30faa053284&amp;#39;})\&lt;/a&gt;]&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Below is my code:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;loader = UnstructuredURLLoader(urls=all_urls) urlDocument = loader.load() &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Top_Raccoon_1493&quot;&gt; /u/Top_Raccoon_1493 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1caxvfh/not_able_to_get_all_the_content_from_the_url_by/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1caxvfh/not_able_to_get_all_the_content_from_the_url_by/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1caxvfh</id><link href="https://www.reddit.com/r/LangChain/comments/1caxvfh/not_able_to_get_all_the_content_from_the_url_by/" /><updated>2024-04-23T06:42:31+00:00</updated><published>2024-04-23T06:42:31+00:00</published><title>Not able to get all the content from the URL by using UnstructuredURLLoader</title></entry><entry><author><name>/u/darkziosj</name><uri>https://www.reddit.com/user/darkziosj</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hello, I would greatly appreciate it if someone could give me a little help on how to approach this problem. I already have an API endpoint. My goal is to interact with the API endpoint using Langchain. If I tell the language model to send me the result via email, then it should do so; otherwise, it should not send anything, what can i use to achieve the email send intention? thank you for all the help.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/darkziosj&quot;&gt; /u/darkziosj &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1caxhtq/how_to_aproach_this_api_call_then_email_send/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1caxhtq/how_to_aproach_this_api_call_then_email_send/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1caxhtq</id><link href="https://www.reddit.com/r/LangChain/comments/1caxhtq/how_to_aproach_this_api_call_then_email_send/" /><updated>2024-04-23T06:17:04+00:00</updated><published>2024-04-23T06:17:04+00:00</published><title>How to aproach this - APi call then email send</title></entry><entry><author><name>/u/trojans10</name><uri>https://www.reddit.com/user/trojans10</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Lets say I&amp;#39;m using Pinecone as a vector database and my content is courses, should I organize my indexes by separating modules, videos, audio, titles, etc., into different indexes/tables, or should I put them all into one index with metadata even though that index/table would get quite large over time?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/trojans10&quot;&gt; /u/trojans10 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1capafo/vector_database_separate_indexestables/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1capafo/vector_database_separate_indexestables/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1capafo</id><link href="https://www.reddit.com/r/LangChain/comments/1capafo/vector_database_separate_indexestables/" /><updated>2024-04-22T23:17:01+00:00</updated><published>2024-04-22T23:17:01+00:00</published><title>Vector database - separate indexes/tables?</title></entry><entry><author><name>/u/complexrexton</name><uri>https://www.reddit.com/user/complexrexton</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I have predefined clusters of roughly ~800, each cluster has been built based on items which could be grouped together. My task is to put the unclassified items in the relevant clusters. &lt;/p&gt; &lt;p&gt;Each Cluster takes a row of CSV file with the following attributes : cluster id, cluster label and the items in the cluster. I built the Vector Database using the CSV file and ran the unclassified items through it using the map-rerank query method. In the prompt, I asked to respond me just the cluster id if there is a relevant cluster otherwise give -1. I did some fine-tuning of the prompt to get just the cluster id but it seems like I always run into some kind of issue because LLM (using GPT4) doesn&amp;#39;t respond the cluster id in the right format. Is there a way to get the index of the classified vector database row which would inturn be the corresponding row of the cluster CSV file?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/complexrexton&quot;&gt; /u/complexrexton &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1calwq1/supervised_clustering_using_rag/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1calwq1/supervised_clustering_using_rag/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1calwq1</id><link href="https://www.reddit.com/r/LangChain/comments/1calwq1/supervised_clustering_using_rag/" /><updated>2024-04-22T21:03:49+00:00</updated><published>2024-04-22T21:03:49+00:00</published><title>Supervised Clustering using RAG</title></entry><entry><author><name>/u/gugavieira</name><uri>https://www.reddit.com/user/gugavieira</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Iâ€™m looking for an app/saas where I could upload data such as pdfs, text files maybe even links and be able to summarize, extract structured data and do general research based on the data provided.&lt;/p&gt; &lt;p&gt;Essentially a RAG as a service. RaaS? &lt;/p&gt; &lt;p&gt;Update: To be clear, iâ€™m looking for a finished and ready app I can use. Iâ€™m not trying to implement it myself. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/gugavieira&quot;&gt; /u/gugavieira &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cagxf4/whats_the_best_chat_with_your_data_app_out_there/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cagxf4/whats_the_best_chat_with_your_data_app_out_there/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cagxf4</id><link href="https://www.reddit.com/r/LangChain/comments/1cagxf4/whats_the_best_chat_with_your_data_app_out_there/" /><updated>2024-04-22T17:36:49+00:00</updated><published>2024-04-22T17:36:49+00:00</published><title>Whatâ€™s the best chat with your data app out there?</title></entry><entry><author><name>/u/DueHearing1315</name><uri>https://www.reddit.com/user/DueHearing1315</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1caersh/466_prs_awaiting_review/&quot;&gt; &lt;img src=&quot;https://preview.redd.it/watfccf122wc1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6fc9e787234c71fb3b373fc92eccadc784ee1088&quot; alt=&quot;466 PRs awaiting review ðŸ˜³&quot; title=&quot;466 PRs awaiting review ðŸ˜³&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/DueHearing1315&quot;&gt; /u/DueHearing1315 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://i.redd.it/watfccf122wc1.jpeg&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1caersh/466_prs_awaiting_review/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1caersh</id><media:thumbnail url="https://preview.redd.it/watfccf122wc1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=6fc9e787234c71fb3b373fc92eccadc784ee1088" /><link href="https://www.reddit.com/r/LangChain/comments/1caersh/466_prs_awaiting_review/" /><updated>2024-04-22T16:11:11+00:00</updated><published>2024-04-22T16:11:11+00:00</published><title>466 PRs awaiting review ðŸ˜³</title></entry><entry><author><name>/u/hoozr4ace</name><uri>https://www.reddit.com/user/hoozr4ace</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi there!&lt;/p&gt; &lt;p&gt;I want to implement such thing like use RAG prompt if user question match RAG semantic and regular not RAG prompt, if it doesnâ€™t match.&lt;/p&gt; &lt;p&gt;Iâ€™ve came in thoughts with three approaches: 1. Before retrieval, use semantic router on user question and decide which prompt/chain to use based on this&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;After retrieval, when docs count is 0 or their relevancy score &amp;lt; some threshold use simple regular prompt &lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;After Iâ€™ve got LLM answer I evaluate it and based on evaluation metrics if they are poor, I call LLM again with regular prompt&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;What do you think about these?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/hoozr4ace&quot;&gt; /u/hoozr4ace &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1caic8s/pipeline_for_rag_and_not_rag_based_on_question/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1caic8s/pipeline_for_rag_and_not_rag_based_on_question/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1caic8s</id><link href="https://www.reddit.com/r/LangChain/comments/1caic8s/pipeline_for_rag_and_not_rag_based_on_question/" /><updated>2024-04-22T18:32:12+00:00</updated><published>2024-04-22T18:32:12+00:00</published><title>Pipeline for RAG and not RAG based on question and retrieved docs</title></entry><entry><author><name>/u/eriksmith1990</name><uri>https://www.reddit.com/user/eriksmith1990</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I am currently trying to build a few tools and have run into not being able to use async functions. Does Langchain have an answer for this?&lt;/p&gt; &lt;p&gt;I get this error &amp;quot;NotImplementedError: Tool does not support sync&amp;quot; when calling the tool with the agent. I have tried a few different ways classifying the tool to no avail.&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/eriksmith1990&quot;&gt; /u/eriksmith1990 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cae5ta/building_tools_that_utilize_async/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cae5ta/building_tools_that_utilize_async/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cae5ta</id><link href="https://www.reddit.com/r/LangChain/comments/1cae5ta/building_tools_that_utilize_async/" /><updated>2024-04-22T15:46:39+00:00</updated><published>2024-04-22T15:46:39+00:00</published><title>Building tools that utilize async</title></entry><entry><author><name>/u/Ornery-Interaction63</name><uri>https://www.reddit.com/user/Ornery-Interaction63</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I want to identify the exact property address for online properties eg on Rightmove. &lt;/p&gt; &lt;p&gt;Currently online UK property URL listings provide the Road Name and some further info but NOT the house number or the full postcode.&lt;/p&gt; &lt;p&gt;As a human you can find the house number by using Google Streetview and searching for a property match by using the front image of the house.&lt;/p&gt; &lt;p&gt;I suspect automating this process will require a research team of AI Agents using visual AI but open to other solutions.&lt;/p&gt; &lt;p&gt;Please note, there are some other ways to identify the property number (they are not always possible). This project is specifically about automating the process of finding a specific property on Google Streetview.&lt;/p&gt; &lt;p&gt;See this property as an example: &lt;a href=&quot;https://www.rightmove.co.uk/properties/144815291&quot;&gt;https://www.rightmove.co.uk/properties/144815291&lt;/a&gt; Using Streetview, its number 46. I can share the manual process I use.&lt;/p&gt; &lt;p&gt;Any help or advice would be greatly appreciated. If you know someone who could do this work, please let me know.&lt;/p&gt; &lt;p&gt;Thank you.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Ornery-Interaction63&quot;&gt; /u/Ornery-Interaction63 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1caamav/ai_multiagent_agentic_python_vision_web_scraping/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1caamav/ai_multiagent_agentic_python_vision_web_scraping/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1caamav</id><link href="https://www.reddit.com/r/LangChain/comments/1caamav/ai_multiagent_agentic_python_vision_web_scraping/" /><updated>2024-04-22T13:21:01+00:00</updated><published>2024-04-22T13:21:01+00:00</published><title>AI, Multiagent, Agentic, Python, Vision, Web scraping help needed.</title></entry></feed>