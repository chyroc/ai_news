<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/"><category term="LangChain" label="r/LangChain"/><updated>2024-08-07T20:29:39+00:00</updated><icon>https://www.redditstatic.com/icon.png/</icon><id>/r/LangChain.rss</id><link rel="self" href="https://www.reddit.com/r/LangChain.rss" type="application/atom+xml" /><link rel="alternate" href="https://www.reddit.com/r/LangChain" type="text/html" /><subtitle>LangChain is an open-source framework and developer toolkit that helps developers get LLM applications from prototype to production. It is available for Python and Javascript at https://www.langchain.com/.</subtitle><title>LangChain</title><entry><author><name>/u/zchaarm</name><uri>https://www.reddit.com/user/zchaarm</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;A place for members of &lt;a href=&quot;/r/LangChain&quot;&gt;r/LangChain&lt;/a&gt; to chat with each other&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/zchaarm&quot;&gt; /u/zchaarm &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_10ljho9</id><link href="https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/" /><updated>2023-01-26T04:33:05+00:00</updated><published>2023-01-26T04:33:05+00:00</published><title>r/LangChain Lounge</title></entry><entry><author><name>/u/abhinavkimothi</name><uri>https://www.reddit.com/user/abhinavkimothi</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1em6m7e/embeddings_the_blueprint_of_contextual_ai/&quot;&gt; &lt;img src=&quot;https://b.thumbs.redditmedia.com/QKVGsXwodAGF0k5YNnyDIcsDJJeG9DZ2QpyUcA904NE.jpg&quot; alt=&quot;Embeddings : The blueprint of Contextual AI&quot; title=&quot;Embeddings : The blueprint of Contextual AI&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/abhinavkimothi&quot;&gt; /u/abhinavkimothi &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/gallery/1em6m7e&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1em6m7e/embeddings_the_blueprint_of_contextual_ai/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1em6m7e</id><media:thumbnail url="https://b.thumbs.redditmedia.com/QKVGsXwodAGF0k5YNnyDIcsDJJeG9DZ2QpyUcA904NE.jpg" /><link href="https://www.reddit.com/r/LangChain/comments/1em6m7e/embeddings_the_blueprint_of_contextual_ai/" /><updated>2024-08-07T08:34:09+00:00</updated><published>2024-08-07T08:34:09+00:00</published><title>Embeddings : The blueprint of Contextual AI</title></entry><entry><author><name>/u/jiraiya1729</name><uri>https://www.reddit.com/user/jiraiya1729</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Other than chatbot what are the other use cases the LLM or open source hugging face models are used for? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/jiraiya1729&quot;&gt; /u/jiraiya1729 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1emgi8p/discussion_llm_use_cases/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1emgi8p/discussion_llm_use_cases/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1emgi8p</id><link href="https://www.reddit.com/r/LangChain/comments/1emgi8p/discussion_llm_use_cases/" /><updated>2024-08-07T16:35:10+00:00</updated><published>2024-08-07T16:35:10+00:00</published><title>[Discussion] LLM use cases</title></entry><entry><author><name>/u/jakezegil</name><uri>https://www.reddit.com/user/jakezegil</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey everybody,&lt;/p&gt; &lt;p&gt;I&amp;#39;ve been really frustrated with the developer experience of Langchain in Typescript, particularly around structured extraction from image and text and agent workflows. I have started building out a dev toolkit to solve that with some DX inspiration from dev tools like vercel and prisma: &lt;a href=&quot;https://github.com/forge-ml/forge-ml&quot;&gt;https://github.com/forge-ml/forge-ml&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;I&amp;#39;d love feedback on the current product, but I&amp;#39;d also love to know what else I can incorporate - what are the big pain points people are having? &lt;/p&gt; &lt;p&gt;Some of the current things on the roadmap are, in no particular order:&lt;br/&gt; - structured extraction from video&lt;br/&gt; - structured extraction from audio&lt;br/&gt; - Anthropic/Groq support&lt;br/&gt; - Semantic Search over Documents&lt;br/&gt; - Semantic Search over Databases&lt;br/&gt; - Fine tuning&lt;br/&gt; - Workflows&lt;br/&gt; - Model routing&lt;/p&gt; &lt;p&gt;What are the biggest issues that you&amp;#39;re facing when building AI products?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/jakezegil&quot;&gt; /u/jakezegil &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1emfs9w/how_can_i_help_you_build_more_reliable_ai/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1emfs9w/how_can_i_help_you_build_more_reliable_ai/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1emfs9w</id><link href="https://www.reddit.com/r/LangChain/comments/1emfs9w/how_can_i_help_you_build_more_reliable_ai/" /><updated>2024-08-07T16:07:25+00:00</updated><published>2024-08-07T16:07:25+00:00</published><title>How can I help you build more reliable AI products faster?</title></entry><entry><author><name>/u/MountainBlock</name><uri>https://www.reddit.com/user/MountainBlock</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hello,&lt;/p&gt; &lt;p&gt;I&amp;#39;ve developed a script using Langchain and Agents that I plan to launch internally within our company. Before moving it to production, I&amp;#39;d appreciate a second pair of eyes to review the code for any potential redundancies or areas for improvement.&lt;/p&gt; &lt;p&gt;I’ve noticed that some posts sharing code here don’t always get much traction. I didn’t see any rules against sharing code for review, but I wanted to check if it&amp;#39;s appropriate to ask for feedback here. &lt;/p&gt; &lt;p&gt;If not, could you suggest any other platforms where I might get my code reviewed?&lt;/p&gt; &lt;p&gt;Thanks in advance.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/MountainBlock&quot;&gt; /u/MountainBlock &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1emiqo5/where_can_i_get_my_code_reviewed/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1emiqo5/where_can_i_get_my_code_reviewed/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1emiqo5</id><link href="https://www.reddit.com/r/LangChain/comments/1emiqo5/where_can_i_get_my_code_reviewed/" /><updated>2024-08-07T18:01:57+00:00</updated><published>2024-08-07T18:01:57+00:00</published><title>Where can I get my code reviewed?</title></entry><entry><author><name>/u/Wise-Dog-9930</name><uri>https://www.reddit.com/user/Wise-Dog-9930</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi Folks, I am really new here. I am working on a multi-agent project where each agent would look up information on a set of select predefined websites. I am hoping to use web search to do it. Is there a way for me search articles related to a specific topic on a specific website using web search?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Wise-Dog-9930&quot;&gt; /u/Wise-Dog-9930 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1emdose/specialized_web_search/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1emdose/specialized_web_search/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1emdose</id><link href="https://www.reddit.com/r/LangChain/comments/1emdose/specialized_web_search/" /><updated>2024-08-07T14:47:07+00:00</updated><published>2024-08-07T14:47:07+00:00</published><title>Specialized Web Search</title></entry><entry><author><name>/u/ekkoogod</name><uri>https://www.reddit.com/user/ekkoogod</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;i&amp;#39;ve developped a rag application using langchain with the retrieval chain that combines history retriever and documents chain, and it performs pretty good .&lt;br/&gt; i have been tasked to add summarization and some other tools , so i tought about using agent and adding tools for such tasks and still use the rag chain as a tool . &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Is there a way to use the same rag chain as a tool for the agent ?&lt;/strong&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/ekkoogod&quot;&gt; /u/ekkoogod &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1emd7m3/using_retrieval_chain_as_a_tool_for_an_agent/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1emd7m3/using_retrieval_chain_as_a_tool_for_an_agent/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1emd7m3</id><link href="https://www.reddit.com/r/LangChain/comments/1emd7m3/using_retrieval_chain_as_a_tool_for_an_agent/" /><updated>2024-08-07T14:27:30+00:00</updated><published>2024-08-07T14:27:30+00:00</published><title>using retrieval_chain as a tool for an agent</title></entry><entry><author><name>/u/Suitable-Ad-8598</name><uri>https://www.reddit.com/user/Suitable-Ad-8598</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Based on some advice I got, I started using AWS Textract ($$$) for PDFs and unstructured (local/free) for all other doc types such as docx and html. &lt;/p&gt; &lt;p&gt;My textract bill is getting a bit out of hand and I was wondering if there are any better services out there that can interpret things like tables and stuff from PDFs and other docs well?&lt;/p&gt; &lt;p&gt;Quality is my number one concern but cost is also important.&lt;/p&gt; &lt;p&gt;Looking to replace textract but also wanted to check to see if unstructured is still considered the best for other doc types.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Suitable-Ad-8598&quot;&gt; /u/Suitable-Ad-8598 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1elr7sr/what_is_the_best_document_loader_for_pdfs_and/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1elr7sr/what_is_the_best_document_loader_for_pdfs_and/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1elr7sr</id><link href="https://www.reddit.com/r/LangChain/comments/1elr7sr/what_is_the_best_document_loader_for_pdfs_and/" /><updated>2024-08-06T19:46:01+00:00</updated><published>2024-08-06T19:46:01+00:00</published><title>What is the best document loader for PDFs? And other docs in general?</title></entry><entry><author><name>/u/phan_ngt</name><uri>https://www.reddit.com/user/phan_ngt</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I have very simple code &lt;strong&gt;Langserve&lt;/strong&gt; with &lt;strong&gt;langfuse_handler&lt;/strong&gt;.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;add_routes(app, normal_agent.with_config(RunnableConfig(callbacks=[langfuse_handler])), per_req_config_modifier=per_request_config_modifier, path=&amp;quot;/normal&amp;quot;) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;When I run my code: &lt;/p&gt; &lt;pre&gt;&lt;code&gt;import asyncio from langserve import RemoteRunnable remote_runnable = RemoteRunnable(&amp;quot;http://localhost:8001/normal&amp;quot;) async def main(): async for chunk in remote_runnable.astream({&amp;#39;input&amp;#39;: &amp;#39;tell me about ronaldo&amp;#39;}): print(chunk, end=&amp;#39;|\n&amp;#39;, flush=True) asyncio.run(main()) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;I want return to client my trace_id of Langfuse. Do you have any ideas to do that? I stuck here for 2 days. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/phan_ngt&quot;&gt; /u/phan_ngt &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1em7aik/how_to_return_langfuse_trace_id_when_using/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1em7aik/how_to_return_langfuse_trace_id_when_using/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1em7aik</id><link href="https://www.reddit.com/r/LangChain/comments/1em7aik/how_to_return_langfuse_trace_id_when_using/" /><updated>2024-08-07T09:20:38+00:00</updated><published>2024-08-07T09:20:38+00:00</published><title>How to return langfuse trace_id when using Langserve stream?</title></entry><entry><author><name>/u/anujtomar_17</name><uri>https://www.reddit.com/user/anujtomar_17</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1emasec/how_is_artificial_intelligence_transforming_every/&quot;&gt; &lt;img src=&quot;https://external-preview.redd.it/c7vrszPtN15vT-OCeI7U4uVmTYXabFtpBRSWRUYD0m8.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=be3b9e23e90b82f3f7f7b7de289007d6bb5361ca&quot; alt=&quot;How is Artificial Intelligence Transforming Every Industry?&quot; title=&quot;How is Artificial Intelligence Transforming Every Industry?&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/anujtomar_17&quot;&gt; /u/anujtomar_17 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.quickwayinfosystems.com/blog/how-artificial-intelligence-transforming/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1emasec/how_is_artificial_intelligence_transforming_every/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1emasec</id><media:thumbnail url="https://external-preview.redd.it/c7vrszPtN15vT-OCeI7U4uVmTYXabFtpBRSWRUYD0m8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=be3b9e23e90b82f3f7f7b7de289007d6bb5361ca" /><link href="https://www.reddit.com/r/LangChain/comments/1emasec/how_is_artificial_intelligence_transforming_every/" /><updated>2024-08-07T12:43:20+00:00</updated><published>2024-08-07T12:43:20+00:00</published><title>How is Artificial Intelligence Transforming Every Industry?</title></entry><entry><author><name>/u/BellaHi</name><uri>https://www.reddit.com/user/BellaHi</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1em0qmz/langchain_vs_llamaindex_choose_the_best_framework/&quot;&gt; &lt;img src=&quot;https://external-preview.redd.it/xdRw2A4E1tAFHQRy07JwUiPTZWn466MaEpJKn-gWRf8.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9161ceea81d99df6557d0af71471532e71f230d8&quot; alt=&quot;LangChain vs LlamaIndex: Choose the Best Framework for Your AI Applications&quot; title=&quot;LangChain vs LlamaIndex: Choose the Best Framework for Your AI Applications&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/BellaHi&quot;&gt; /u/BellaHi &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://myscale.com/blog/llamaindex-vs-langchain-detailed-comparison/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1em0qmz/langchain_vs_llamaindex_choose_the_best_framework/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1em0qmz</id><media:thumbnail url="https://external-preview.redd.it/xdRw2A4E1tAFHQRy07JwUiPTZWn466MaEpJKn-gWRf8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=9161ceea81d99df6557d0af71471532e71f230d8" /><link href="https://www.reddit.com/r/LangChain/comments/1em0qmz/langchain_vs_llamaindex_choose_the_best_framework/" /><updated>2024-08-07T02:41:36+00:00</updated><published>2024-08-07T02:41:36+00:00</published><title>LangChain vs LlamaIndex: Choose the Best Framework for Your AI Applications</title></entry><entry><author><name>/u/mehul_gupta1997</name><uri>https://www.reddit.com/user/mehul_gupta1997</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/mehul_gupta1997&quot;&gt; /u/mehul_gupta1997 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;/r/ArtificialInteligence/comments/1em61b0/free_llm_apis_to_know/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1em61t8/free_llm_apis_to_know/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1em61t8</id><link href="https://www.reddit.com/r/LangChain/comments/1em61t8/free_llm_apis_to_know/" /><updated>2024-08-07T07:55:35+00:00</updated><published>2024-08-07T07:55:35+00:00</published><title>Free LLM APIs to know</title></entry><entry><author><name>/u/notknot0</name><uri>https://www.reddit.com/user/notknot0</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I need to build a simple chatbot for my documentation pages. I need advice on where to host it, currently I adjusted it for Supabase edge function and Supabase vector store because it&amp;#39;s free and deployed all over the world, but if I exceed the requests limit, I will have to self-host Supabase and then it&amp;#39;s just one location.&lt;/p&gt; &lt;p&gt;Does anyone already have chatbots for small documentations and can you share which vector store you use and where you deployed your code? my docs pages are served from AWS S3. If I use AWS lambda to host the edge function, I&amp;#39;m afraid of a cost increase if there will be many requests.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/notknot0&quot;&gt; /u/notknot0 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1em997z/advice_on_where_to_host_langchainjs_rest_api/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1em997z/advice_on_where_to_host_langchainjs_rest_api/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1em997z</id><link href="https://www.reddit.com/r/LangChain/comments/1em997z/advice_on_where_to_host_langchainjs_rest_api/" /><updated>2024-08-07T11:24:10+00:00</updated><published>2024-08-07T11:24:10+00:00</published><title>Advice on where to host LangChain.js REST API function for Q&amp;A chatbot for my docs</title></entry><entry><author><name>/u/GPT-Claude-Gemini</name><uri>https://www.reddit.com/user/GPT-Claude-Gemini</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey everyone I want to share a Langchain-based project that I have been working on for the last few months — &lt;a href=&quot;https://www.jenova.ai/&quot;&gt;JENOVA&lt;/a&gt;, an AI (similar to ChatGPT) that integrates the best foundation models and tools into one seamless experience.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;AI is advancing too fast for most people to follow.&lt;/strong&gt; New state-of-the-art models emerge constantly, each with unique strengths and specialties. Currently:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Claude 3.5 Sonnet is the best at reasoning, math, and coding.&lt;/li&gt; &lt;li&gt;Gemini 1.5 Pro excels in business/financial analysis and language translations.&lt;/li&gt; &lt;li&gt;Llama 3.1 405B is most performative in roleplaying and creativity.&lt;/li&gt; &lt;li&gt;GPT-4o is most knowledgeable in areas such as art, entertainment, and travel.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This rapidly changing and fragmenting AI landscape is leading to the following problems for consumers:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Awareness Gap:&lt;/strong&gt; Most people are unaware of the latest models and their specific strengths, and are often paying for AI (e.g. ChatGPT) that is suboptimal for their tasks.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Constant Switching:&lt;/strong&gt; Due to constant changes in SOTA models, consumers have to frequently switch their preferred AI and subscription.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;User Friction:&lt;/strong&gt; Switching AI results in significant user experience disruptions, such as losing chat histories or critical features such as web browsing.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;JENOVA is built to solve this.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;When you ask JENOVA a question, it automatically routes your query to the model that can provide the optimal answer (built on top of Langchain).&lt;/strong&gt; For example, if your first question is about coding, then Claude 3.5 Sonnet will respond. If your second question is about tourist spots in Tokyo, then GPT-4o will respond. All this happens seamlessly in the background.&lt;/p&gt; &lt;p&gt;JENOVA&amp;#39;s model ranking is continuously updated to incorporate the latest AI models and performance benchmarks, ensuring you are always using the best models for your specific needs.&lt;/p&gt; &lt;p&gt;In addition to the best AI models, JENOVA also provides you with an expanding suite of the most useful tools, starting with:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Web browsing&lt;/strong&gt; for real-time information (performs surprisingly well, nearly on par with Perplexity)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Multi-format document analysis&lt;/strong&gt; including PDF, Word, Excel, PowerPoint, and more&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Image interpretation&lt;/strong&gt; for visual tasks&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Your privacy is very important to us. Your conversations and data are never used for training, either by us or by third-party AI providers.&lt;/p&gt; &lt;p&gt;Try it out at &lt;a href=&quot;https://www.jenova.ai/&quot;&gt;&lt;strong&gt;www.jenova.ai&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Update:&lt;/strong&gt; JENOVA might be running into some issues with web search/browsing right now due to very high demand.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/GPT-Claude-Gemini&quot;&gt; /u/GPT-Claude-Gemini &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ellpk9/sharing_my_project_that_was_built_on_langchain_an/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ellpk9/sharing_my_project_that_was_built_on_langchain_an/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1ellpk9</id><link href="https://www.reddit.com/r/LangChain/comments/1ellpk9/sharing_my_project_that_was_built_on_langchain_an/" /><updated>2024-08-06T16:07:14+00:00</updated><published>2024-08-06T16:07:14+00:00</published><title>Sharing my project that was built on Langchain: An all-in-one AI that integrates the best foundation models (GPT, Claude, Gemini, Llama) and tools into one seamless experience.</title></entry><entry><author><name>/u/maniac_runner</name><uri>https://www.reddit.com/user/maniac_runner</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/maniac_runner&quot;&gt; /u/maniac_runner &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://openai.com/index/introducing-structured-outputs-in-the-api/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1em0d5n/introducing_structured_outputs_in_the_api/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1em0d5n</id><link href="https://www.reddit.com/r/LangChain/comments/1em0d5n/introducing_structured_outputs_in_the_api/" /><updated>2024-08-07T02:22:55+00:00</updated><published>2024-08-07T02:22:55+00:00</published><title>Introducing Structured Outputs in the API</title></entry><entry><author><name>/u/TimeTravellingCat</name><uri>https://www.reddit.com/user/TimeTravellingCat</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1elkjcz/building_multiagent_workflows_with_open_and/&quot;&gt; &lt;img src=&quot;https://external-preview.redd.it/emRxMDJpOTA1MmhkMQOeLRWbXZLLqGF7C1pE6u8v4bV4Eov2GX0h5P2dipJi.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=efb7632c3d308cdeef92241156ced31db3fcbf57&quot; alt=&quot;Building multi-agent workflows with open and closed models using an open-source low-code platform&quot; title=&quot;Building multi-agent workflows with open and closed models using an open-source low-code platform&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/TimeTravellingCat&quot;&gt; /u/TimeTravellingCat &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://v.redd.it/nzs7bi9052hd1&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1elkjcz/building_multiagent_workflows_with_open_and/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1elkjcz</id><media:thumbnail url="https://external-preview.redd.it/emRxMDJpOTA1MmhkMQOeLRWbXZLLqGF7C1pE6u8v4bV4Eov2GX0h5P2dipJi.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=efb7632c3d308cdeef92241156ced31db3fcbf57" /><link href="https://www.reddit.com/r/LangChain/comments/1elkjcz/building_multiagent_workflows_with_open_and/" /><updated>2024-08-06T15:20:54+00:00</updated><published>2024-08-06T15:20:54+00:00</published><title>Building multi-agent workflows with open and closed models using an open-source low-code platform</title></entry><entry><author><name>/u/ProfessionalBig9431</name><uri>https://www.reddit.com/user/ProfessionalBig9431</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;hi there i am learning about rag with knowledge graph any proper documentation from where i can get the reference any thing would be helpful &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/ProfessionalBig9431&quot;&gt; /u/ProfessionalBig9431 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1em3b9p/rag_and_knowledge_graph/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1em3b9p/rag_and_knowledge_graph/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1em3b9p</id><link href="https://www.reddit.com/r/LangChain/comments/1em3b9p/rag_and_knowledge_graph/" /><updated>2024-08-07T04:58:39+00:00</updated><published>2024-08-07T04:58:39+00:00</published><title>RAG and KNOWLEDGE GRAPH</title></entry><entry><author><name>/u/phuzul</name><uri>https://www.reddit.com/user/phuzul</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I am fairly new to Langchain and wanted to seek out some advice for free methods of using LLMs to just do some basic prompting for a web application. The prompting is just providing some music artists and asking the LLM to think of potential ideas for those artists. I have a node.js backend that I am attempting to integrate Langchain.js into. &lt;/p&gt; &lt;p&gt;I first wanted to ask are there any decent models for my task that can be used with Langchain.js for free and what would a very basic code snippet look like to run it. &lt;/p&gt; &lt;p&gt;In addition, I wanted to ask if Ollama is applicable here or is that strictly for local applications?&lt;/p&gt; &lt;p&gt;Thanks !&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/phuzul&quot;&gt; /u/phuzul &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1em3muc/support_with_langchainjs_for_free_inference/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1em3muc/support_with_langchainjs_for_free_inference/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1em3muc</id><link href="https://www.reddit.com/r/LangChain/comments/1em3muc/support_with_langchainjs_for_free_inference/" /><updated>2024-08-07T05:17:36+00:00</updated><published>2024-08-07T05:17:36+00:00</published><title>Support with Langchain.js for free inference</title></entry><entry><author><name>/u/Standard-Factor-9408</name><uri>https://www.reddit.com/user/Standard-Factor-9408</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Has anyone come across an issue with chroma as a vector store where it can’t handle a large K unless it’s “warmed up”?&lt;/p&gt; &lt;p&gt;Have a db with about 30k docs in it each about a paragraph long. I need to return the top 100 or so as part of a larger ranking process. When I first load the db though if I don’t start with a k size of 5 and work my way up it consistently errors out with “cannot return the results in a contiguous 2d array. Probably ef or M is too small”. I’ve tried changing the hnsw parameters when creating the collection but nothing seems to give. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Standard-Factor-9408&quot;&gt; /u/Standard-Factor-9408 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ely226/chromadb_issues/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ely226/chromadb_issues/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1ely226</id><link href="https://www.reddit.com/r/LangChain/comments/1ely226/chromadb_issues/" /><updated>2024-08-07T00:32:49+00:00</updated><published>2024-08-07T00:32:49+00:00</published><title>Chromadb issues</title></entry><entry><author><name>/u/moonbunR</name><uri>https://www.reddit.com/user/moonbunR</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1elcgt3/customer_review_analysis_ai_agent/&quot;&gt; &lt;img src=&quot;https://a.thumbs.redditmedia.com/pZgg5gsUcAK_aEdnnGnjE5DIsQs1HHdDTNiAo1mfyf8.jpg&quot; alt=&quot;Customer review analysis Ai Agent&quot; title=&quot;Customer review analysis Ai Agent&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;&lt;a href=&quot;https://preview.redd.it/bb6kdv6oh0hd1.jpg?width=974&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=078ba4f7babb1c827c9cb413fa236fc9d1cd5c2f&quot;&gt;https://preview.redd.it/bb6kdv6oh0hd1.jpg?width=974&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=078ba4f7babb1c827c9cb413fa236fc9d1cd5c2f&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This &lt;a href=&quot;https://app.smythos.com/builder?templateId=customer-review-analysis-lxxqii7231.smyth&quot;&gt;~agent~&lt;/a&gt; uses sentiment analysis and an LLM to automatically review and give responses to customer reviews. When the agent receives the customer review, it is first taken through sentiment analysis to understand the sentiment of the feedback. Together with the sentiment for context, the review is sent to the LLM which crafts an appropriate response for the review.&lt;/p&gt; &lt;p&gt;Personally, I think the secret lies in the prompting of the LLM, in my case I set up the LLM to do 3 things mainly, &lt;/p&gt; &lt;ol&gt; &lt;li&gt;Empathize with the customer&lt;/li&gt; &lt;li&gt;Offer a solution&lt;/li&gt; &lt;li&gt;Provide additional information&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;These three gave me some pretty decent results, you can try your own or just use these on the template, but I still think that with more robust prompting, the agent can produce more natural and efficient responses that can help out your brand in a huge way. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/moonbunR&quot;&gt; /u/moonbunR &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1elcgt3/customer_review_analysis_ai_agent/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1elcgt3/customer_review_analysis_ai_agent/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1elcgt3</id><media:thumbnail url="https://a.thumbs.redditmedia.com/pZgg5gsUcAK_aEdnnGnjE5DIsQs1HHdDTNiAo1mfyf8.jpg" /><link href="https://www.reddit.com/r/LangChain/comments/1elcgt3/customer_review_analysis_ai_agent/" /><updated>2024-08-06T08:28:24+00:00</updated><published>2024-08-06T08:28:24+00:00</published><title>Customer review analysis Ai Agent</title></entry><entry><author><name>/u/BigYesterday2785</name><uri>https://www.reddit.com/user/BigYesterday2785</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I wanted to ask, if I want to run local LLMs only on CPU.&lt;/p&gt; &lt;p&gt;I do not have access to GPUs and wanted to ask how much slower CPU would be, compared to GPU.&lt;/p&gt; &lt;p&gt;I would love to run a small Open Source LLM only on CPUs to read 500 pages PDFs and be able to ask it questions.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/BigYesterday2785&quot;&gt; /u/BigYesterday2785 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1eli67w/run_local_llm_on_cpu_how_bad_would_would_it_be/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1eli67w/run_local_llm_on_cpu_how_bad_would_would_it_be/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1eli67w</id><link href="https://www.reddit.com/r/LangChain/comments/1eli67w/run_local_llm_on_cpu_how_bad_would_would_it_be/" /><updated>2024-08-06T13:44:24+00:00</updated><published>2024-08-06T13:44:24+00:00</published><title>Run local LLM on CPU. how Bad would would it be compared to GPUs</title></entry><entry><author><name>/u/Relevant_Ebb_3633</name><uri>https://www.reddit.com/user/Relevant_Ebb_3633</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hello everyone, I&amp;#39;m looking for a low-code platform to implement RAG in business processes. I&amp;#39;ve tested tools like Dify, RAGflow, Flowise, and langflow, but none of them seem to be well-optimized for RAG. Does anyone know of any low-code platforms that offer better RAG parameter optimization? Thanks in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Relevant_Ebb_3633&quot;&gt; /u/Relevant_Ebb_3633 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1elarmg/looking_for_lowcode_tools_for_building_and/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1elarmg/looking_for_lowcode_tools_for_building_and/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1elarmg</id><link href="https://www.reddit.com/r/LangChain/comments/1elarmg/looking_for_lowcode_tools_for_building_and/" /><updated>2024-08-06T06:34:58+00:00</updated><published>2024-08-06T06:34:58+00:00</published><title>Looking for low-code tools for building and optimizing RAG</title></entry><entry><author><name>/u/mehul_gupta1997</name><uri>https://www.reddit.com/user/mehul_gupta1997</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/mehul_gupta1997&quot;&gt; /u/mehul_gupta1997 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;/r/ArtificialInteligence/comments/1eli3xb/ragflow_ui_for_rag_framework/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1eli4qr/ragflow_ui_for_rag_framework/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1eli4qr</id><link href="https://www.reddit.com/r/LangChain/comments/1eli4qr/ragflow_ui_for_rag_framework/" /><updated>2024-08-06T13:42:38+00:00</updated><published>2024-08-06T13:42:38+00:00</published><title>RAGflow : UI for RAG framework</title></entry><entry><author><name>/u/mehul_gupta1997</name><uri>https://www.reddit.com/user/mehul_gupta1997</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1el7jc9/langchain_in_your_pocket_completes_6_months/&quot;&gt; &lt;img src=&quot;https://b.thumbs.redditmedia.com/aEiKKct7MORRNB8Sihet1ABmGQk9Ug9_q_P-0jOItBQ.jpg&quot; alt=&quot;LangChain in your Pocket completes 6 months !!&quot; title=&quot;LangChain in your Pocket completes 6 months !!&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;m glad to share that my debut book, &lt;strong&gt;&amp;quot;LangChain in your Pocket: Beginner&amp;#39;s Guide to Building Generative AI Applications using LLMs&lt;/strong&gt;&amp;quot; completed 6 months last week and what a dream run it has been.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;The book has been &lt;strong&gt;republished by Packt.&lt;/strong&gt; And is now available with all major publishers including O&amp;#39;Reilly.&lt;/li&gt; &lt;li&gt;So far, the book has sold over &lt;strong&gt;500 copies&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;It is the &lt;strong&gt;highest-rated book on LangChain&lt;/strong&gt; on Amazon (Amazon.in: 4.7; Amazon.com: 4.3 ).&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;The best part is that the book hasn&amp;#39;t received a bad review regarding the content from anyone, making this even more special for me&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;&lt;em&gt;A big thanks to the community for all the support.&lt;/em&gt;&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/7wtmrl2nnygd1.png?width=901&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=da3d9b2ea43d771ee738bcbb611c6331a36ef580&quot;&gt;https://preview.redd.it/7wtmrl2nnygd1.png?width=901&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=da3d9b2ea43d771ee738bcbb611c6331a36ef580&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/mehul_gupta1997&quot;&gt; /u/mehul_gupta1997 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1el7jc9/langchain_in_your_pocket_completes_6_months/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1el7jc9/langchain_in_your_pocket_completes_6_months/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1el7jc9</id><media:thumbnail url="https://b.thumbs.redditmedia.com/aEiKKct7MORRNB8Sihet1ABmGQk9Ug9_q_P-0jOItBQ.jpg" /><link href="https://www.reddit.com/r/LangChain/comments/1el7jc9/langchain_in_your_pocket_completes_6_months/" /><updated>2024-08-06T03:24:06+00:00</updated><published>2024-08-06T03:24:06+00:00</published><title>LangChain in your Pocket completes 6 months !!</title></entry><entry><author><name>/u/Just_Guide7361</name><uri>https://www.reddit.com/user/Just_Guide7361</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I am currently building a Q&amp;amp;A interface with Streamlit and Langchain. Our initial vector database was in Pinecone. We have documents about the same topic, but different industries. Pure embedding search is not optimal, as it will match the same concepts across industries. So, we build a simple selector option where users pick their industry, and then ask the question. In pinecone each industry had their own namespace, we then simply filter on this:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;vectorstore = PineconeVectorStore(index_name=index_name, embedding=embeddings, namespace=namespace) retriever = vectorstore.as_retriever(search_type=&amp;quot;similarity&amp;quot;, search_kwargs={&amp;quot;k&amp;quot;: 3}) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Hybrid search with pinecone is not as convenient as with Weaviate, and since we noticed beter performance with hybrid search we are switching to Weaviate. The downside is that filters are not so clear for the Weaviate retriever. &lt;/p&gt; &lt;pre&gt;&lt;code&gt;retriever = WeaviateHybridSearchRetriever( client=client, index_name=WEAVIATE_INDEX_NAME, text_key=&amp;quot;page_content&amp;quot;, k=5, alpha=0.75, attributes=[&amp;quot;file_name&amp;quot;, &amp;quot;industry], create_schema_if_missing=False, ) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Our Langchain Chain looks similar to this ( &lt;a href=&quot;https://github.com/langchain-ai/langchain/blob/master/templates/hybrid-search-weaviate/hybrid_search_weaviate/chain.py&quot;&gt;https://github.com/langchain-ai/langchain/blob/master/templates/hybrid-search-weaviate/hybrid_search_weaviate/chain.py&lt;/a&gt; ):&lt;/p&gt; &lt;pre&gt;&lt;code&gt;# RAG prompt template = &amp;quot;&amp;quot;&amp;quot;Answer the question based only on the following context: {context} Question: {question} &amp;quot;&amp;quot;&amp;quot; prompt = ChatPromptTemplate.from_template(template) # RAG model = ChatOpenAI() chain = ( RunnableParallel({&amp;quot;context&amp;quot;: retriever, &amp;quot;question&amp;quot;: RunnablePassthrough()}) | prompt | model | StrOutputParser() ) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The docs do show this:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;retriever.invoke( &amp;quot;AI integration in society&amp;quot;, where_filter={ &amp;quot;path&amp;quot;: [&amp;quot;author&amp;quot;], &amp;quot;operator&amp;quot;: &amp;quot;Equal&amp;quot;, &amp;quot;valueString&amp;quot;: &amp;quot;Prof. Jonathan K. Sterling&amp;quot;, }, ) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;a href=&quot;https://python.langchain.com/v0.2/docs/integrations/retrievers/weaviate-hybrid/&quot;&gt;https://python.langchain.com/v0.2/docs/integrations/retrievers/weaviate-hybrid/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Does anyone know how/where to add the &lt;code&gt;where_filter&lt;/code&gt; parameter for Weaviate hybrid search in the Chain?&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/langchain-ai/langchain/blob/master/templates/hybrid-search-weaviate/hybrid_search_weaviate/chain.py&quot;&gt;&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Just_Guide7361&quot;&gt; /u/Just_Guide7361 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1elk3g2/weaviatehybridsearchretriever_with_filters/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1elk3g2/weaviatehybridsearchretriever_with_filters/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1elk3g2</id><link href="https://www.reddit.com/r/LangChain/comments/1elk3g2/weaviatehybridsearchretriever_with_filters/" /><updated>2024-08-06T15:03:27+00:00</updated><published>2024-08-06T15:03:27+00:00</published><title>WeaviateHybridSearchRetriever with filters?</title></entry><entry><author><name>/u/Sad-Anywhere-2204</name><uri>https://www.reddit.com/user/Sad-Anywhere-2204</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;m working on a ReAct agent(created with langchain.agents.create_react_agent) and we are creating an evaluation and monitoring tool for it to analyze and understand agent behavior, performance, latency, bottlenecks, etc. We have already some metrics, but I&amp;#39;m looking for the best practices or recommended techniques to calculate:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Number of LLM calls: given the agent will do it&amp;#39;s reasoning/acting cycle doing LLM calls internally we want to measure the number of calls.&lt;/li&gt; &lt;li&gt;Duration per LLM call: we also want to measure the duration per call.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;One of the goals is to understand where to focus when doing improvements, for example: &lt;/p&gt; &lt;ol&gt; &lt;li&gt;improving tools descriptions, inputs etc to reduce the number of LLM calls.&lt;/li&gt; &lt;li&gt;Performing model optimization and endpoint tuning to reduce latency per call.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; due to internal constraints we are not using langsmith and preferably the solution should be independent of the model used(given we will evaluate different models and we want the metrics for all of them).&lt;/p&gt; &lt;p&gt;Any other suggested metrics are appreciated.&lt;/p&gt; &lt;p&gt;The question is: &lt;strong&gt;what is the recommended way to achieve this?&lt;/strong&gt; (I have in mind thing like callbacks like the &lt;strong&gt;on_llm_end&lt;/strong&gt; and adding a counter but not sure if this is the recommended way or maybe there is something out of the box).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Sad-Anywhere-2204&quot;&gt; /u/Sad-Anywhere-2204 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1elpms2/how_to_track_number_of_llm_calls_in_a_react_agent/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1elpms2/how_to_track_number_of_llm_calls_in_a_react_agent/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1elpms2</id><link href="https://www.reddit.com/r/LangChain/comments/1elpms2/how_to_track_number_of_llm_calls_in_a_react_agent/" /><updated>2024-08-06T18:42:34+00:00</updated><published>2024-08-06T18:42:34+00:00</published><title>How to track number of LLM calls in a ReAct agent</title></entry></feed>