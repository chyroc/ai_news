<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/"><category term="LangChain" label="r/LangChain"/><updated>2024-05-28T07:19:59+00:00</updated><icon>https://www.redditstatic.com/icon.png/</icon><id>/r/LangChain.rss</id><link rel="self" href="https://www.reddit.com/r/LangChain.rss" type="application/atom+xml" /><link rel="alternate" href="https://www.reddit.com/r/LangChain" type="text/html" /><subtitle>LangChain is an open-source framework and developer toolkit that helps developers get LLM applications from prototype to production. It is available for Python and Javascript at https://www.langchain.com/.</subtitle><title>LangChain</title><entry><author><name>/u/zchaarm</name><uri>https://www.reddit.com/user/zchaarm</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;A place for members of &lt;a href=&quot;/r/LangChain&quot;&gt;r/LangChain&lt;/a&gt; to chat with each other&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/zchaarm&quot;&gt; /u/zchaarm &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_10ljho9</id><link href="https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/" /><updated>2023-01-26T04:33:05+00:00</updated><published>2023-01-26T04:33:05+00:00</published><title>r/LangChain Lounge</title></entry><entry><author><name>/u/jy2k</name><uri>https://www.reddit.com/user/jy2k</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;What do you use to handle agents permissin to retrieve access certain data? Let&amp;#39;s say your building an agent in a large org. How do you make sure it can access finance question if you asked it a question about finance. It needs to validate if you are permitted to access that kind of data.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/jy2k&quot;&gt; /u/jy2k &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d2cezb/agent_permission/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d2cezb/agent_permission/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1d2cezb</id><link href="https://www.reddit.com/r/LangChain/comments/1d2cezb/agent_permission/" /><updated>2024-05-28T05:56:42+00:00</updated><published>2024-05-28T05:56:42+00:00</published><title>Agent permission</title></entry><entry><author><name>/u/Puzzleheaded_Bee5489</name><uri>https://www.reddit.com/user/Puzzleheaded_Bee5489</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;m using OpenAI GPT 3.5 turbo for summarising data from sensitive documents, which contains some of my personal information. Currently, I&amp;#39;m manually removing some of the sensitive data from the inputs. I want to know if LangChain or any other tool/library handles this automatically without me getting involved?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Puzzleheaded_Bee5489&quot;&gt; /u/Puzzleheaded_Bee5489 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d1v710/hashingmasking_sensitive_data_before_sending_out/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d1v710/hashingmasking_sensitive_data_before_sending_out/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1d1v710</id><link href="https://www.reddit.com/r/LangChain/comments/1d1v710/hashingmasking_sensitive_data_before_sending_out/" /><updated>2024-05-27T16:13:41+00:00</updated><published>2024-05-27T16:13:41+00:00</published><title>Hashing/Masking sensitive data before sending out to OpenAI</title></entry><entry><author><name>/u/AnEdgeLordWeeb</name><uri>https://www.reddit.com/user/AnEdgeLordWeeb</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey guys! I&amp;#39;m trying to build a chatbot that offer video games recommendations using LangGraph.&lt;/p&gt; &lt;p&gt;Right now, I was working on the agent called &amp;quot;Game Search Agent&amp;quot; which objective is to search the web for the best results to the user query.&lt;/p&gt; &lt;p&gt;Problem is, the execution never moves from this node to the __end__ one. It will always be stuck in a loop, where the tool function is called without providing a concrete final answer in the end.&lt;/p&gt; &lt;p&gt;Can somebody please take a look at this code snippet and tell me please what&amp;#39;s wrong? Thank you!&lt;/p&gt; &lt;pre&gt;&lt;code&gt;from typing import Annotated, List from langchain_openai import ChatOpenAI from langchain_community.tools.tavily_search import TavilySearchResults from langchain_core.prompts import PromptTemplate from langchain_core.messages import BaseMessage from typing_extensions import TypedDict from langgraph.graph import StateGraph from langgraph.graph.message import add_messages from langgraph.prebuilt import ToolNode, tools_condition os.environ[&amp;quot;TAVILY_API_KEY&amp;quot;] = &amp;quot;your_api_key&amp;quot; os.environ[&amp;quot;OPEN_AI_KEY&amp;quot;] = &amp;quot;your_api_key&amp;quot; class AgentState(TypedDict): messages: Annotated[List[BaseMessage], add_messages] query: str games: List[str] tool = TavilySearchResults(max_results=2) tools = [tool] llm = ChatOpenAI(model=&amp;quot;gpt-4o&amp;quot;, temperature=0) llm_with_tools = llm.bind_tools(tools) def game_title_search(state: AgentState): game_search_prompt = PromptTemplate( template=&amp;quot;&amp;quot;&amp;quot;You are part of a chatbot that provides personalized video game recommendations based on user preferences. \n Your task is to search for video games that match the user query, using the Tavily API. \n Only return the titles of the games. \n The number of games to return is limited to 5. \n\n The results provided will STRICTLY look as follows (Python list): \n [&amp;quot;game_title_1&amp;quot;, &amp;quot;game_title_2&amp;quot;, &amp;quot;game_title_3&amp;quot;, ...] \n\n User Query: {query}&amp;quot;&amp;quot;&amp;quot;, input_variables=[&amp;quot;query&amp;quot;], ) game_search = game_search_prompt | llm_with_tools return {&amp;quot;messages&amp;quot;: [game_search.invoke({&amp;quot;query&amp;quot;: state[&amp;quot;query&amp;quot;]})]} graph_builder = StateGraph(AgentState) graph_builder.add_node(&amp;quot;game_search&amp;quot;, game_title_search) tool_node = ToolNode(tools=[tool]) graph_builder.add_node(&amp;quot;tools&amp;quot;, tool_node) graph_builder.add_conditional_edges( &amp;quot;game_search&amp;quot;, tools_condition, ) graph_builder.add_edge(&amp;quot;tools&amp;quot;, &amp;quot;game_search&amp;quot;) graph_builder.set_entry_point(&amp;quot;game_search&amp;quot;) graph = graph_builder.compile() while True: user_input = input(&amp;quot;User: &amp;quot;) if user_input.lower() in [&amp;quot;quit&amp;quot;, &amp;quot;exit&amp;quot;, &amp;quot;q&amp;quot;]: print(&amp;quot;Goodbye!&amp;quot;) break for event in graph.stream({&amp;quot;messages&amp;quot;: [user_input], &amp;quot;query&amp;quot;: user_input}, {&amp;quot;recursion_limit&amp;quot;: 150}): for value in event.values(): if isinstance(value[&amp;quot;messages&amp;quot;][-1], BaseMessage): print(&amp;quot;Assistant:&amp;quot;, value[&amp;quot;messages&amp;quot;][-1].content) &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/AnEdgeLordWeeb&quot;&gt; /u/AnEdgeLordWeeb &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d24j6j/agent_enters_a_loop_of_continuous_tool_calling/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d24j6j/agent_enters_a_loop_of_continuous_tool_calling/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1d24j6j</id><link href="https://www.reddit.com/r/LangChain/comments/1d24j6j/agent_enters_a_loop_of_continuous_tool_calling/" /><updated>2024-05-27T22:45:47+00:00</updated><published>2024-05-27T22:45:47+00:00</published><title>Agent enters a loop of continuous tool calling without exiting and providing a final answer</title></entry><entry><author><name>/u/Minute_Scientist8107</name><uri>https://www.reddit.com/user/Minute_Scientist8107</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey all, I have a dataset in Azure and I have a SQL LLM. Now I want to generate visuals when a user gives a prompt. How can I implement this ??? Any help would be great &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Minute_Scientist8107&quot;&gt; /u/Minute_Scientist8107 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d1qll4/llm_to_generate_dashboard/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d1qll4/llm_to_generate_dashboard/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1d1qll4</id><link href="https://www.reddit.com/r/LangChain/comments/1d1qll4/llm_to_generate_dashboard/" /><updated>2024-05-27T12:40:21+00:00</updated><published>2024-05-27T12:40:21+00:00</published><title>LLM to generate dashboard</title></entry><entry><author><name>/u/cryptokaykay</name><uri>https://www.reddit.com/user/cryptokaykay</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d1afqa/awesome_prompting_techniques/&quot;&gt; &lt;img src=&quot;https://preview.redd.it/qpe806ybzt2d1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=573b65d4e7c2d59df66440f3a8ad24cea007627c&quot; alt=&quot;Awesome prompting techniques&quot; title=&quot;Awesome prompting techniques&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2312.16171v2&quot;&gt;https://arxiv.org/pdf/2312.16171v2&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/cryptokaykay&quot;&gt; /u/cryptokaykay &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://i.redd.it/qpe806ybzt2d1.jpeg&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d1afqa/awesome_prompting_techniques/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1d1afqa</id><media:thumbnail url="https://preview.redd.it/qpe806ybzt2d1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=573b65d4e7c2d59df66440f3a8ad24cea007627c" /><link href="https://www.reddit.com/r/LangChain/comments/1d1afqa/awesome_prompting_techniques/" /><updated>2024-05-26T20:30:43+00:00</updated><published>2024-05-26T20:30:43+00:00</published><title>Awesome prompting techniques</title></entry><entry><author><name>/u/Fireche</name><uri>https://www.reddit.com/user/Fireche</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Looking at the following tool: &lt;a href=&quot;https://python.langchain.com/v0.1/docs/integrations/tools/google_drive/&quot;&gt;https://python.langchain.com/v0.1/docs/integrations/tools/google_drive/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;What happens under the hood when this tool is called? For queries to find relevant results, does langchain simply make use of the public API which is based on a fullText search?&lt;/p&gt; &lt;p&gt;Any way to retrieve documents with a semantic search with langchain? I think this would actually be a quite neat feature, so we could pass an embedding model, db and it would create embeddings for all the documents.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Fireche&quot;&gt; /u/Fireche &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d1spe6/how_do_the_langchain_integrations_retrieve/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d1spe6/how_do_the_langchain_integrations_retrieve/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1d1spe6</id><link href="https://www.reddit.com/r/LangChain/comments/1d1spe6/how_do_the_langchain_integrations_retrieve/" /><updated>2024-05-27T14:24:28+00:00</updated><published>2024-05-27T14:24:28+00:00</published><title>How do the langchain integrations retrieve relevant documents?</title></entry><entry><author><name>/u/cr33dcode</name><uri>https://www.reddit.com/user/cr33dcode</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I need to build a tool for my fin research where if i ask in NLP to a rag i need the output of those 100PDFs i have uploaded to the RAG. it needs to be able to build charts, graphs and make sense of 100 other things. any OS tool like that&amp;gt; or any suggestions on the stack i should use? please advice. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/cr33dcode&quot;&gt; /u/cr33dcode &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d1mexn/need_some_advice_on_rag/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d1mexn/need_some_advice_on_rag/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1d1mexn</id><link href="https://www.reddit.com/r/LangChain/comments/1d1mexn/need_some_advice_on_rag/" /><updated>2024-05-27T07:56:05+00:00</updated><published>2024-05-27T07:56:05+00:00</published><title>need some advice on rag</title></entry><entry><author><name>/u/Relative_Winner_4588</name><uri>https://www.reddit.com/user/Relative_Winner_4588</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Newbie question here, How do I make a conversational LLM assistant in Streamlit that remembers all the chats but does not have to give a system prompt for each inference?&lt;br/&gt; I know I can use the conversational buffer memory of langchain for chat memory, but I do not want to waste my tokens on system prompts for each inference.&lt;/p&gt; &lt;p&gt;Generally, for each inference, my app takes system prompt + chat context as input for each output. I was wondering if there is a way to reduce the token consumption by sending the system prompt once and making the model remember it for the entire session.&lt;/p&gt; &lt;p&gt;Thank you.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Relative_Winner_4588&quot;&gt; /u/Relative_Winner_4588 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d1nduq/help_creating_a_conversational_assistant/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d1nduq/help_creating_a_conversational_assistant/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1d1nduq</id><link href="https://www.reddit.com/r/LangChain/comments/1d1nduq/help_creating_a_conversational_assistant/" /><updated>2024-05-27T09:09:11+00:00</updated><published>2024-05-27T09:09:11+00:00</published><title>Help creating a conversational assistant</title></entry><entry><author><name>/u/Rock-star-007</name><uri>https://www.reddit.com/user/Rock-star-007</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi Community,&lt;br/&gt; I am building a chatbot app for a specific domain. I am leveraging aws bedrock for storing documents and creating embeddings in pinecone vector db.&lt;/p&gt; &lt;p&gt;But I fee like I am conceptually stuck in how to maintain the conversation context and retrieved documents when trying to create a response for a new query. How to decide when to use the context and when to make a fresh retrieval? Appreciate any help here.&lt;/p&gt; &lt;p&gt;Please help answer in the setup of Django for rest, bedrock for s3 and embedding model, pinecone for vector db.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Rock-star-007&quot;&gt; /u/Rock-star-007 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d1attd/how_to_integrate_conversation_context_and/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d1attd/how_to_integrate_conversation_context_and/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1d1attd</id><link href="https://www.reddit.com/r/LangChain/comments/1d1attd/how_to_integrate_conversation_context_and/" /><updated>2024-05-26T20:48:46+00:00</updated><published>2024-05-26T20:48:46+00:00</published><title>How to integrate conversation context and retrieved documents for a new query for a RAG LLM chatbot app?</title></entry><entry><author><name>/u/sebpeterson</name><uri>https://www.reddit.com/user/sebpeterson</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi ,&lt;/p&gt; &lt;p&gt;I am using Langchain/Gemini1.5/Google Documents AI to OCR, to parse and ask questions to a set of documents. Working pretty sweet. Actually just published my side project here: &lt;a href=&quot;https://zdocs.ai/&quot;&gt;https://zdocs.ai/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I would like to be able to show where the answers to a prompt that is restricted to an uploaded set of documents are coming from?&lt;/p&gt; &lt;p&gt;Google Documents has the whole document structure availabe in JSON. However, I am not sure if the LLM (gemini in my case) can actually provide insights opn where the answer came from ?&lt;/p&gt; &lt;p&gt;Any tips would be welcome !&lt;/p&gt; &lt;p&gt;Cheers, &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/sebpeterson&quot;&gt; /u/sebpeterson &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d18agq/how_can_i_keep_track_of_document_source_being/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d18agq/how_can_i_keep_track_of_document_source_being/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1d18agq</id><link href="https://www.reddit.com/r/LangChain/comments/1d18agq/how_can_i_keep_track_of_document_source_being/" /><updated>2024-05-26T18:52:50+00:00</updated><published>2024-05-26T18:52:50+00:00</published><title>How can I keep track of document source being used in a prompt ?</title></entry><entry><author><name>/u/gordicaleksa</name><uri>https://www.reddit.com/user/gordicaleksa</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d0zm1w/building_llm_apps_in_production_with_hamel_husain/&quot;&gt; &lt;img src=&quot;https://external-preview.redd.it/1OyXglDey73O5mWYiw-lqoPAiX5PEI3CTsy0tnXvnvk.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=774a2d9f486266a3ca2c39236da3b979ca61f3be&quot; alt=&quot;Building LLM Apps in Production with Hamel Husain &quot; title=&quot;Building LLM Apps in Production with Hamel Husain &quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/gordicaleksa&quot;&gt; /u/gordicaleksa &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=MFSd-_pMExI&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d0zm1w/building_llm_apps_in_production_with_hamel_husain/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1d0zm1w</id><media:thumbnail url="https://external-preview.redd.it/1OyXglDey73O5mWYiw-lqoPAiX5PEI3CTsy0tnXvnvk.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=774a2d9f486266a3ca2c39236da3b979ca61f3be" /><link href="https://www.reddit.com/r/LangChain/comments/1d0zm1w/building_llm_apps_in_production_with_hamel_husain/" /><updated>2024-05-26T11:47:55+00:00</updated><published>2024-05-26T11:47:55+00:00</published><title>Building LLM Apps in Production with Hamel Husain</title></entry><entry><author><name>/u/mehul_gupta1997</name><uri>https://www.reddit.com/user/mehul_gupta1997</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/mehul_gupta1997&quot;&gt; /u/mehul_gupta1997 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;/r/learnmachinelearning/comments/1d0ul53/pandasai_generative_ai_for_pandas_dataframe/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d0uoni/pandasai_generative_ai_for_pandas_dataframe/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1d0uoni</id><link href="https://www.reddit.com/r/LangChain/comments/1d0uoni/pandasai_generative_ai_for_pandas_dataframe/" /><updated>2024-05-26T05:46:44+00:00</updated><published>2024-05-26T05:46:44+00:00</published><title>PandasAI: Generative AI for pandas dataframe</title></entry><entry><author><name>/u/Oliver_691</name><uri>https://www.reddit.com/user/Oliver_691</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I want the chat to be able to access all the information from the memory without it passing all the memory to the prompt. Is there a way to only pass the memories from the database to the prompt that are important for answering the query or to summarize the memory according to each prompt once again? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Oliver_691&quot;&gt; /u/Oliver_691 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d0xhn5/how_to_only_take_query_relevant_memory_from/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d0xhn5/how_to_only_take_query_relevant_memory_from/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1d0xhn5</id><link href="https://www.reddit.com/r/LangChain/comments/1d0xhn5/how_to_only_take_query_relevant_memory_from/" /><updated>2024-05-26T09:13:13+00:00</updated><published>2024-05-26T09:13:13+00:00</published><title>How to only take query relevant memory from Upstash?</title></entry><entry><author><name>/u/NoAssumption999</name><uri>https://www.reddit.com/user/NoAssumption999</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hello,&lt;/p&gt; &lt;p&gt;I want to extract paragraphs, title etc from a PDF while.maintaining the separation boundaries. What library to use?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/NoAssumption999&quot;&gt; /u/NoAssumption999 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d0ryqx/extract_text_from_pdf_maintaining_partitions/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d0ryqx/extract_text_from_pdf_maintaining_partitions/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1d0ryqx</id><link href="https://www.reddit.com/r/LangChain/comments/1d0ryqx/extract_text_from_pdf_maintaining_partitions/" /><updated>2024-05-26T02:51:43+00:00</updated><published>2024-05-26T02:51:43+00:00</published><title>Extract text from PDF maintaining partitions</title></entry><entry><author><name>/u/learning-machine1964</name><uri>https://www.reddit.com/user/learning-machine1964</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey guys, I am using langchain to generate structured output, but for some reason, the output is not properly formatted json so when I run json.loads(), it gives me an error. I tried to make a clean_response function but there are too many edge cases to consider. Am I using the wrong function to get the LLM output? Thanks in advance!&lt;/p&gt; &lt;p&gt;Here is my code:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;from abc import ABC import os import pydantic from langchain_groq import ChatGroq from langchain.pydantic_v1 import validator from langchain.output_parsers import PydanticOutputParser from langchain.prompts import PromptTemplate from langchain.pydantic_v1 import BaseModel import json from dotenv import load_dotenv load_dotenv() tags_req = [&amp;quot;HS Seniors Only&amp;quot;, &amp;quot;Need Based&amp;quot;, &amp;quot;Merit Based&amp;quot;, &amp;quot;Essay Required&amp;quot;, &amp;quot;US Citizen&amp;quot;, &amp;quot;Arts&amp;quot;, &amp;quot;STEM&amp;quot;, &amp;quot;Community Service&amp;quot;, &amp;quot;Leadership&amp;quot;] class Tags(BaseModel): tags: list[str] @validator(&amp;#39;tags&amp;#39;) def must_use_only_these_values(cls, v): for tag in v: if tag not in tags_req: raise ValueError(f&amp;quot;tag {tag} is not in {tags_req}&amp;quot;) return v class Description(BaseModel): description: str class Generator(ABC): def __init__(self ) -&amp;gt; None: pass def generate(self, *args): pass class GroqGenerator(Generator): def __init__( self, ): super().__init__() self.llm = ChatGroq( temperature=0.1, groq_api_key=os.getenv(&amp;quot;GROQ_API_KEY&amp;quot;), model_name=&amp;quot;mixtral-8x7b-32768&amp;quot;, ) def generate( self, name, available, opens, closes, details, description, need_based, merit_based, ): msg1 = ( f&amp;quot;Name: {name}\nAvailable: {available}\nOpens: {opens}\nCloses: {closes}\nDetails: {details}\nDescription: {description}\nNeed Based: {need_based}\nMerit Based: {merit_based}\n\n Generate Below: &amp;quot; ) msg2 = ( f&amp;quot;\nName: {name}\nAvailable: {available}\nOpens: {opens}\nCloses: {closes}\nDetails: {details}\nDescription: {description}\nNeed Based: {need_based}\nMerit Based: {merit_based}\n\n Generate Below: &amp;quot;, ) tags = self.inference(msg1, Tags)[&amp;quot;tags&amp;quot;] desc = self.inference(msg2, Description)[&amp;quot;description&amp;quot;] return tags, desc def inference(self, msg, pydantic_object): parser = PydanticOutputParser(pydantic_object=pydantic_object) if pydantic_object.__class__.__name__ == &amp;quot;Tags&amp;quot;: prompt = PromptTemplate( template=&amp;quot;Please follow the instructions of the following user query.\n{format_instructions}\n{query}\n&amp;quot;, input_variables=[&amp;quot;query&amp;quot;], partial_variables={&amp;quot;format_instructions&amp;quot;: parser.get_format_instructions() + f&amp;quot; Make sure only generate the array from this bank of tags and no other tags: {tags_req}&amp;quot;}, ) else: prompt = PromptTemplate( template=&amp;quot;Please follow the instructions of the following user query.\n{format_instructions}\n{query}\n&amp;quot;, input_variables=[&amp;quot;query&amp;quot;], partial_variables={&amp;quot;format_instructions&amp;quot;: parser.get_format_instructions()}, ) _input = prompt.format_prompt(query=msg) response = self.llm.invoke(_input.to_string()).content cleaned_response = self.clean_response(response) print(cleaned_response) return json.loads(cleaned_response) def clean_response(self, response): # Replace curly quotes and other problematic characters response = response.replace(&amp;#39;“&amp;#39;, &amp;#39;&amp;quot;&amp;#39;).replace(&amp;#39;”&amp;#39;, &amp;#39;&amp;quot;&amp;#39;) response = response.replace(&amp;quot;‘&amp;quot;, &amp;quot;&amp;#39;&amp;quot;).replace(&amp;quot;’&amp;quot;, &amp;quot;&amp;#39;&amp;quot;) return response def test(self, msg): return self.inference(msg) if __name__ == &amp;quot;__main__&amp;quot;: groq = GroqGenerator([&amp;quot;scholarship&amp;quot;]) print(groq.test(&amp;quot;Q: What is a scholarship? A:&amp;quot;)) &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/learning-machine1964&quot;&gt; /u/learning-machine1964 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d0q4g6/am_i_doing_something_wrong_in_my_code/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d0q4g6/am_i_doing_something_wrong_in_my_code/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1d0q4g6</id><link href="https://www.reddit.com/r/LangChain/comments/1d0q4g6/am_i_doing_something_wrong_in_my_code/" /><updated>2024-05-26T01:02:40+00:00</updated><published>2024-05-26T01:02:40+00:00</published><title>Am I doing something wrong in my code?</title></entry><entry><author><name>/u/throwaway0134hdj</name><uri>https://www.reddit.com/user/throwaway0134hdj</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;When you are making RAG chatbots using Graph and Vectors how are you storing the internal data? What’s the general approach?&lt;/p&gt; &lt;p&gt;For example, say you are asked to ingest all your companies files, like word docs PDFs and everything in between. If you use RAG with Graph and Vector embeddedings where are you storing the data from the documents? I’m curious what the general approach is to chunking, tokenizing, and embedding are?&lt;/p&gt; &lt;p&gt;If you had to ingest your companies documents using a RAG, Graph, and vector approach how would you set this up? What would the schema be of the Graph, where would the vectors be stored?&lt;/p&gt; &lt;p&gt;Thanks&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/throwaway0134hdj&quot;&gt; /u/throwaway0134hdj &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d0j0hc/how_do_you_go_about_creating_a_rag_chatbot_using/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d0j0hc/how_do_you_go_about_creating_a_rag_chatbot_using/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1d0j0hc</id><link href="https://www.reddit.com/r/LangChain/comments/1d0j0hc/how_do_you_go_about_creating_a_rag_chatbot_using/" /><updated>2024-05-25T19:03:06+00:00</updated><published>2024-05-25T19:03:06+00:00</published><title>How do you go about creating a RAG chatbot using Graph and Vector on internal documents?</title></entry><entry><author><name>/u/adi-dk</name><uri>https://www.reddit.com/user/adi-dk</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi guys,&lt;br/&gt; I am new to Python, so there might be a straightforward solution for this.&lt;/p&gt; &lt;p&gt;I am building a digital assistant chatbot that communicates with customers via WhatsApp. The chatbot processes messages using a series of functions implemented with Crewai and Langchain that may take several seconds to minutes to complete. While the processing is ongoing, new messages from the customer may arrive, requiring the chatbot to:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Pause the current processing&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Incorporate the new message&lt;/strong&gt; into the ongoing process.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Restart the processing&lt;/strong&gt; from a specific function within the sequence, using updated information.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;I will use Flask to receive webhooks with new messages from WhatsApp.&lt;/p&gt; &lt;p&gt;My problem is that I don&amp;#39;t know the exact way to build this with Python. Specifically, I need to determine how to stop and restart the script at a particular point. I will use a large language model (LLM) to decide the exact point to restart the process, (a crewai task, or maybe an entire crew), incorporating the new buyer message.&lt;/p&gt; &lt;p&gt;I don&amp;#39;t have the code yet as I am currently working on the architecture of the system.&lt;/p&gt; &lt;p&gt;Any suggestions will be of great help.&lt;/p&gt; &lt;p&gt;Thank you!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/adi-dk&quot;&gt; /u/adi-dk &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d0tsk2/restarting_crewai_langchain_flow_at_a_particular/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d0tsk2/restarting_crewai_langchain_flow_at_a_particular/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1d0tsk2</id><link href="https://www.reddit.com/r/LangChain/comments/1d0tsk2/restarting_crewai_langchain_flow_at_a_particular/" /><updated>2024-05-26T04:46:36+00:00</updated><published>2024-05-26T04:46:36+00:00</published><title>Restarting Crewai / Langchain flow at a particular point with new variables.</title></entry><entry><author><name>/u/Puzzleheaded_Move35</name><uri>https://www.reddit.com/user/Puzzleheaded_Move35</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hello, I am trying to create a chatbot using Langchain where I am using RetrievalQA and OpenAI API. I need to create chains where if the user asks a question which is unrelated to the context, basically retrieve from a document provided, the chatbot should bypass the retrieval steps and just answer the query directly. And if it asks related questions it should apply RAG and retrieve the relevant info to answer the questions. I am totally stuck here and don’t know how to move forward. Any help will be appreciated. &lt;/p&gt; &lt;p&gt;Code:&lt;/p&gt; &lt;p&gt;llm = ChatOpenAI( api_key= api_key,&lt;br/&gt; # openai_api_key= os.environ[&amp;quot;OPENAI_API_KEY&amp;quot;],&lt;br/&gt; model_name=&amp;#39;gpt-4o&amp;#39; &lt;/p&gt; &lt;p&gt;) template = &amp;quot;&amp;quot;&amp;quot; Use the following context provided (delimited by &amp;lt;ctx&amp;gt;&amp;lt;/ctx&amp;gt;), answer the questions properly and the chat history (delimited by &amp;lt;hs&amp;gt;&amp;lt;/hs&amp;gt;) to answer the questions from the user. &lt;/p&gt; &lt;h2&gt;If they are asking questions not related to the context, skip performing RAG and just straight up answer their query&amp;quot;:&lt;/h2&gt; &lt;p&gt;&amp;lt;ctx&amp;gt; {context}&lt;/p&gt; &lt;h2&gt;&amp;lt;/ctx&amp;gt;&lt;/h2&gt; &lt;p&gt;&amp;lt;hs&amp;gt; {history}&lt;/p&gt; &lt;h2&gt;&amp;lt;/hs&amp;gt;&lt;/h2&gt; &lt;p&gt;{question} Answer: &amp;quot;&amp;quot;&amp;quot; prompt = PromptTemplate( input_variables=[&amp;quot;history&amp;quot;, &amp;quot;context&amp;quot;, &amp;quot;question&amp;quot;], template=template, )&lt;/p&gt; &lt;p&gt;memory = ConversationBufferMemory( memory_key=&amp;quot;history&amp;quot;, input_key=&amp;quot;question&amp;quot; )&lt;/p&gt; &lt;p&gt;qa = RetrievalQA.from_chain_type( llm=llm, chain_type=&amp;#39;stuff&amp;#39;, retriever=vectorstore.as_retriever(search_type=&amp;quot;similarity&amp;quot;, search_kwargs={&amp;#39;k&amp;#39;: 20}), verbose=True, chain_type_kwargs={ &amp;quot;verbose&amp;quot;: True, &amp;quot;prompt&amp;quot;: prompt, &amp;quot;memory&amp;quot;: memory, } )&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Puzzleheaded_Move35&quot;&gt; /u/Puzzleheaded_Move35 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d0e7ov/how_to_ignore_retrieval_step_rag_when_it_is_not/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d0e7ov/how_to_ignore_retrieval_step_rag_when_it_is_not/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1d0e7ov</id><link href="https://www.reddit.com/r/LangChain/comments/1d0e7ov/how_to_ignore_retrieval_step_rag_when_it_is_not/" /><updated>2024-05-25T15:15:49+00:00</updated><published>2024-05-25T15:15:49+00:00</published><title>How to ignore retrieval step (RAG) when it is not necessary</title></entry><entry><author><name>/u/AnEdgeLordWeeb</name><uri>https://www.reddit.com/user/AnEdgeLordWeeb</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey guys. I&amp;#39;m trying to get comfortable with LangGraph in an attempt to then develop a chatbot based on this framework.&lt;br/&gt; When trying to test the idea of a node in my chatbot, I found myself faced with this error.&lt;br/&gt; Could somebody please help me understand what&amp;#39;s wrong with my code and how can I solve this problem?&lt;br/&gt; I would be truly thankful!&lt;/p&gt; &lt;p&gt;The code:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;from typing import Annotated, List from langchain_openai import ChatOpenAI from langchain_community.tools.tavily_search import TavilySearchResults from langchain_core.prompts import ChatPromptTemplate, PromptTemplate from langchain_core.messages import BaseMessage from typing_extensions import TypedDict from langgraph.graph import StateGraph, END from langgraph.graph.message import add_messages from langgraph.prebuilt import ToolNode, tools_condition # Define AgentState class with the proper typing class AgentState(TypedDict): messages: Annotated[List[BaseMessage], add_messages] query: str games: List[str] # Initialize the tool and LLM tool = TavilySearchResults(max_results=3) tools = [tool] llm = ChatOpenAI(model=&amp;quot;gpt-3.5-turbo&amp;quot;, temperature=0) llm_with_tools = llm.bind_tools(tools) # Define the function for the game title search def game_title_search(state: AgentState): game_search_prompt = PromptTemplate( template=&amp;quot;&amp;quot;&amp;quot;You are part of a chatbot that provides personalized video game recommendations based on user preferences. \n Your task is to search for video games that match the user query, using the Tavily API. \n Only return the titles of the games. \n The number of games to return is limited to 5. \n\n The results provided will look as follows (Python list): \n [&amp;#39;game_title_1&amp;#39;, &amp;#39;game_title_2&amp;#39;, &amp;#39;game_title_3&amp;#39;, ...] User Query: {query}&amp;quot;&amp;quot;&amp;quot;, input_variables=[&amp;quot;query&amp;quot;], ) game_search = game_search_prompt | llm_with_tools game_search_result = game_search.invoke({&amp;quot;query&amp;quot;: state[&amp;quot;query&amp;quot;]}) return {&amp;quot;messages&amp;quot;: [game_search_result]} # Also, I need to extract the game titles from the tool&amp;#39;s results and update the state attribute &amp;quot;games&amp;quot; - how can I do this? # Build the graph graph_builder = StateGraph(AgentState) graph_builder.add_node(&amp;quot;game_search&amp;quot;, game_title_search) tool_node = ToolNode(tools=[tool]) graph_builder.add_node(&amp;quot;tools&amp;quot;, tool_node) graph_builder.add_conditional_edges( &amp;quot;game_search&amp;quot;, tools_condition, ) graph_builder.add_edge(&amp;quot;tools&amp;quot;, &amp;quot;game_search&amp;quot;) graph_builder.set_entry_point(&amp;quot;game_search&amp;quot;) graph = graph_builder.compile() # Define the initial state input_state = { &amp;quot;messages&amp;quot;: [], &amp;quot;query&amp;quot;: &amp;quot;&amp;quot;, &amp;quot;games&amp;quot;: [] } user_input = &amp;quot;What games are similar to The Witcher 3?&amp;quot; input_state[&amp;quot;query&amp;quot;] = user_input input_state[&amp;quot;messages&amp;quot;] = [(&amp;quot;user&amp;quot;, user_input)] output = graph.invoke(input_state, config={&amp;quot;recursion_limit&amp;quot;: 50}) print(output) &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/AnEdgeLordWeeb&quot;&gt; /u/AnEdgeLordWeeb &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d0om4f/help_recursion_limit_when_trying_to_use_chain/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d0om4f/help_recursion_limit_when_trying_to_use_chain/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1d0om4f</id><link href="https://www.reddit.com/r/LangChain/comments/1d0om4f/help_recursion_limit_when_trying_to_use_chain/" /><updated>2024-05-25T23:38:32+00:00</updated><published>2024-05-25T23:38:32+00:00</published><title>Help! &quot;Recursion limit&quot; when trying to use chain with &quot;llm.bind_tools&quot; - LangGraph</title></entry><entry><author><name>/u/mehul_gupta1997</name><uri>https://www.reddit.com/user/mehul_gupta1997</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d00vla/my_langchain_book_now_available_on_packt_and/&quot;&gt; &lt;img src=&quot;https://b.thumbs.redditmedia.com/6qJv0lGS_OUWt49TIW0VLit2wRHq59aNal2DwSpSfgU.jpg&quot; alt=&quot;My LangChain book now available on Packt and O'Reilly&quot; title=&quot;My LangChain book now available on Packt and O'Reilly&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;m glad to share that my debut book, &amp;quot;&lt;strong&gt;LangChain in your Pocket: Beginner&amp;#39;s Guide to Building Generative AI Applications using LLMs,&lt;/strong&gt;&amp;quot; has been republished by Packt and is now available on their official website and partner publications like O&amp;#39;Reilly, Barnes &amp;amp; Noble, etc. A big thanks for the support! The first version is still available on Amazon&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/5b0trmcl7h2d1.png?width=1080&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6f12126f846d5fc174768628ebc42c9921017687&quot;&gt;https://preview.redd.it/5b0trmcl7h2d1.png?width=1080&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6f12126f846d5fc174768628ebc42c9921017687&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/4xdgzk9l7h2d1.png?width=1080&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bfe4aac06ce89bff475a415b8c0091f830ba10e3&quot;&gt;https://preview.redd.it/4xdgzk9l7h2d1.png?width=1080&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bfe4aac06ce89bff475a415b8c0091f830ba10e3&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/mehul_gupta1997&quot;&gt; /u/mehul_gupta1997 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d00vla/my_langchain_book_now_available_on_packt_and/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d00vla/my_langchain_book_now_available_on_packt_and/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1d00vla</id><media:thumbnail url="https://b.thumbs.redditmedia.com/6qJv0lGS_OUWt49TIW0VLit2wRHq59aNal2DwSpSfgU.jpg" /><link href="https://www.reddit.com/r/LangChain/comments/1d00vla/my_langchain_book_now_available_on_packt_and/" /><updated>2024-05-25T01:32:55+00:00</updated><published>2024-05-25T01:32:55+00:00</published><title>My LangChain book now available on Packt and O'Reilly</title></entry><entry><author><name>/u/AustinChanKL</name><uri>https://www.reddit.com/user/AustinChanKL</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hello everyone, need some pointer here. Is there a way to exclude intermediate steps when streaming the events with AgentExecutor? I only want the agent to stream the final output without streaming the intermediate steps (Observations).&lt;/p&gt; &lt;p&gt;So I&amp;#39;m using create_tool_calling_agent() to create an agent and passing multiple tools. And one of the tool itself actually is using langchain csv agent. When the agent using this particular tool, it start to stream the intermediate observation steps in my chat application, which is weird for my user point of view. I want to exclude all the intermediate steps from this tool.&lt;/p&gt; &lt;p&gt;When I was using langchain==0.1.16, it behave exactly what I wanted, which it won&amp;#39;t stream any output or intermediate steps from the tool that using langchain csv agent. After I upgrade it to 0.2.1, it started to stream intermediate steps.&lt;/p&gt; &lt;p&gt;Here&amp;#39;s some example code I have:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;async def chat_stream(...) -&amp;gt; str: prompt = ChatPromptTemplate.from_messages(...) tools = [query_data_from_csv] llm = ChatOpenAI(...) agent = create_tool_calling_agent(llm, tools, prompt) agent_executor = AgentExecutor(agent=agent, tools=tools, handle_parsing_errors=True) return agent_executor.astream_events(..., version=&amp;quot;v2&amp;quot;) @ tool def query_data_from_csv(question: str, csv_url: str) -&amp;gt; str: &amp;quot;&amp;quot;&amp;quot;Tool to query and interact with data in CSV format.&amp;quot;&amp;quot;&amp;quot; ai_agent = create_csv_agent( ChatOpenAI(temperature=0, model=&amp;quot;gpt-4-turbo&amp;quot;), csv_url, agent_type=AgentType.ZERO_SHOT_REACT_DESCRIPTION, return_intermediate_steps=False ) return ai_agent.invoke(question) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Your help is greatly appreciated. Thank you.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/AustinChanKL&quot;&gt; /u/AustinChanKL &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d0b3zs/is_there_anyway_to_prevent_agentexecutorastream/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d0b3zs/is_there_anyway_to_prevent_agentexecutorastream/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1d0b3zs</id><link href="https://www.reddit.com/r/LangChain/comments/1d0b3zs/is_there_anyway_to_prevent_agentexecutorastream/" /><updated>2024-05-25T12:37:48+00:00</updated><published>2024-05-25T12:37:48+00:00</published><title>Is there anyway to prevent AgentExecutor.astream_events() streaming intermediate steps?</title></entry><entry><author><name>/u/Pokedrive123</name><uri>https://www.reddit.com/user/Pokedrive123</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I wanted to know if the chain method and ChatOpenAI from langchain_openai supports gpt4o image inputs and if there are any guides out there showing us how to use it? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Pokedrive123&quot;&gt; /u/Pokedrive123 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d03y7b/has_langchain_been_updated_with_gpt4o/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d03y7b/has_langchain_been_updated_with_gpt4o/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1d03y7b</id><link href="https://www.reddit.com/r/LangChain/comments/1d03y7b/has_langchain_been_updated_with_gpt4o/" /><updated>2024-05-25T04:30:41+00:00</updated><published>2024-05-25T04:30:41+00:00</published><title>Has langchain been updated with Gpt4o?</title></entry><entry><author><name>/u/UpvoteBeast</name><uri>https://www.reddit.com/user/UpvoteBeast</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1czfu8v/understanding_the_magic_deconstructing_langchains/&quot;&gt; &lt;img src=&quot;https://external-preview.redd.it/PuvfwiESasEaQDZvjllJMtQHd1LLNsCK92LgBTOvras.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=dc7f8a79051cfb7f73b1c8cdc014ce2e73509532&quot; alt=&quot;Understanding the Magic: Deconstructing Langchain’s SQL Agent &quot; title=&quot;Understanding the Magic: Deconstructing Langchain’s SQL Agent &quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/UpvoteBeast&quot;&gt; /u/UpvoteBeast &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://dly.to/K6CJFoxPldx&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1czfu8v/understanding_the_magic_deconstructing_langchains/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1czfu8v</id><media:thumbnail url="https://external-preview.redd.it/PuvfwiESasEaQDZvjllJMtQHd1LLNsCK92LgBTOvras.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=dc7f8a79051cfb7f73b1c8cdc014ce2e73509532" /><link href="https://www.reddit.com/r/LangChain/comments/1czfu8v/understanding_the_magic_deconstructing_langchains/" /><updated>2024-05-24T08:20:52+00:00</updated><published>2024-05-24T08:20:52+00:00</published><title>Understanding the Magic: Deconstructing Langchain’s SQL Agent</title></entry><entry><author><name>/u/nik0-bellic</name><uri>https://www.reddit.com/user/nik0-bellic</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Im planning to do an endpoint that given a user question it makes the underlying work to get the query and i would like to only receive the final answer as im going to show it on a Streamlit chat app. Any idea on how to extract only that?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/nik0-bellic&quot;&gt; /u/nik0-bellic &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1czv6k8/how_could_i_just_return_the_final_answer_from_sql/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1czv6k8/how_could_i_just_return_the_final_answer_from_sql/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1czv6k8</id><link href="https://www.reddit.com/r/LangChain/comments/1czv6k8/how_could_i_just_return_the_final_answer_from_sql/" /><updated>2024-05-24T20:59:31+00:00</updated><published>2024-05-24T20:59:31+00:00</published><title>How could I just return the final answer from SQL Agent?</title></entry><entry><author><name>/u/newpeak</name><uri>https://www.reddit.com/user/newpeak</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1czk90g/implementing_a_longcontext_rag_based_on_raptor/&quot;&gt; &lt;img src=&quot;https://external-preview.redd.it/0cS0XK03rFRxZqaK6QPMgfSagZE4Vn9YYciJnzYH9mk.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=77af57d9f34ce52533ece926c923557a6f272dbb&quot; alt=&quot; Implementing a long-context RAG based on RAPTOR &quot; title=&quot; Implementing a long-context RAG based on RAPTOR &quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/newpeak&quot;&gt; /u/newpeak &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://medium.com/@infiniflowai/implementing-a-long-context-rag-based-on-raptor-0538a354ada3&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1czk90g/implementing_a_longcontext_rag_based_on_raptor/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1czk90g</id><media:thumbnail url="https://external-preview.redd.it/0cS0XK03rFRxZqaK6QPMgfSagZE4Vn9YYciJnzYH9mk.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=77af57d9f34ce52533ece926c923557a6f272dbb" /><link href="https://www.reddit.com/r/LangChain/comments/1czk90g/implementing_a_longcontext_rag_based_on_raptor/" /><updated>2024-05-24T13:04:48+00:00</updated><published>2024-05-24T13:04:48+00:00</published><title>Implementing a long-context RAG based on RAPTOR</title></entry></feed>