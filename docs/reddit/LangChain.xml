<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/"><category term="LangChain" label="r/LangChain"/><updated>2024-07-13T04:37:34+00:00</updated><icon>https://www.redditstatic.com/icon.png/</icon><id>/r/LangChain.rss</id><link rel="self" href="https://www.reddit.com/r/LangChain.rss" type="application/atom+xml" /><link rel="alternate" href="https://www.reddit.com/r/LangChain" type="text/html" /><subtitle>LangChain is an open-source framework and developer toolkit that helps developers get LLM applications from prototype to production. It is available for Python and Javascript at https://www.langchain.com/.</subtitle><title>LangChain</title><entry><author><name>/u/zchaarm</name><uri>https://www.reddit.com/user/zchaarm</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;A place for members of &lt;a href=&quot;/r/LangChain&quot;&gt;r/LangChain&lt;/a&gt; to chat with each other&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/zchaarm&quot;&gt; /u/zchaarm &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_10ljho9</id><link href="https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/" /><updated>2023-01-26T04:33:05+00:00</updated><published>2023-01-26T04:33:05+00:00</published><title>r/LangChain Lounge</title></entry><entry><author><name>/u/Altruistic-Box5900</name><uri>https://www.reddit.com/user/Altruistic-Box5900</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi, I am building an AI agent with Claude 3.5. My functions are not being invoked by the agent. My understanding was that Langchain automatically calls the agent, but it is not being called. I do, however, am receiving as a model response that function needs to be called. Here&amp;#39;s the model response and my code:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;&amp;gt; Entering new AgentExecutor chain... content: Certainly content: , content: ****. I content: &amp;#39;ll create content: a new I content: **** for you content: using content: the available content: function content: . content: toolu_01G44ahCcrqSaXgiUWrvX85L content: None [{&amp;#39;text&amp;#39;: &amp;quot;Certainly, *****. I&amp;#39;ll create ******* for you using the available function.&amp;quot;, &amp;#39;type&amp;#39;: &amp;#39;text&amp;#39;, &amp;#39;index&amp;#39;: 0}, {&amp;#39;id&amp;#39;: &amp;#39;toolu_01G44ahCcrqSaXgiUWrvX85L&amp;#39;, &amp;#39;input&amp;#39;: {}, &amp;#39;name&amp;#39;: &amp;#39;*******&amp;#39;, &amp;#39;type&amp;#39;: &amp;#39;tool_use&amp;#39;, &amp;#39;index&amp;#39;: 1, &amp;#39;partial_json&amp;#39;: &amp;#39;&amp;#39;}] &amp;gt; Finished chain. llm_with_tools = llm.bind_tools(tools) agent = ( { &amp;quot;input&amp;quot;: lambda x: x[&amp;quot;input&amp;quot;], &amp;quot;agent_scratchpad&amp;quot;: lambda x: format_to_openai_tool_messages( x[&amp;quot;intermediate_steps&amp;quot;] ), &amp;quot;chat_history&amp;quot;: lambda x: x[&amp;quot;chat_history&amp;quot;], } | prompt | llm_with_tools | OpenAIToolsAgentOutputParser() ) agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True) &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Altruistic-Box5900&quot;&gt; /u/Altruistic-Box5900 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e1r8rk/tool_calling_with_claude_35_not_working/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e1r8rk/tool_calling_with_claude_35_not_working/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1e1r8rk</id><link href="https://www.reddit.com/r/LangChain/comments/1e1r8rk/tool_calling_with_claude_35_not_working/" /><updated>2024-07-12T20:10:02+00:00</updated><published>2024-07-12T20:10:02+00:00</published><title>Tool calling with Claude 3.5 not working</title></entry><entry><author><name>/u/WarriorA</name><uri>https://www.reddit.com/user/WarriorA</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hello dear swarm intelligence practitioner! I have come to you in a time of great need.&lt;/p&gt; &lt;p&gt;Imagine you have a document about some topic. Lets say the GoldenGate Bridge. And this document has multiple wonderfully written paragraphs about this Bridge. This includes materials used, the timeframe of when it was built, and much much more.&lt;br/&gt; This document then goes on writing about California and San Francisco.&lt;/p&gt; &lt;p&gt;A few paragraphs down maybe a few sentences about the pacific ocean.&lt;/p&gt; &lt;p&gt;And last but not least this: &amp;quot;It was designed by Joseph Strauss with the help of architect Irving Morrow and engineers Charles Alton Ellis and Leon Moisseiff. Construction was carried out by the McClintic-Marshall Construction Company, a subsidiary of Bethlehem Steel Corporation.&amp;quot;&lt;/p&gt; &lt;p&gt;This article is chunked using common techniques and embedded into a vector index. Now, this RAG System will be used to answer Questions. We retrieve k docs from this index that are similar to the query, and rerank them to top_k documents to generate an answer. This pipeline works with almost all queries and documents.&lt;/p&gt; &lt;p&gt;The problem here would be to answer the question &amp;quot;&lt;strong&gt;Who built the Golden Gate Bridge?&amp;quot;&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;The retrieval process would find these chunks, and confidently (and correctly) rank them as the most relevant for our question. However, the chunk with the information we are actually looking for will not be retrieved, as it is not at all similar to the query.&lt;/p&gt; &lt;p&gt;How can this question be answered, while maintaining performance?&lt;/p&gt; &lt;ol&gt; &lt;li&gt;When finding a high scoring chunk (e.g: Golden Gate Bridge chunk from the top of the article) we fetch the document in its entirety, and &lt;strong&gt;provide the entire article&lt;/strong&gt; to the generator LLM. Problem here is, that we have to either make a decision to do that based on the query, or do it every time. Both are compromises I want to avoid.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Bigger chunks&lt;/strong&gt; would include the information. Problem here is, that we can&amp;#39;t really generalize. What if there is more content in between? This won&amp;#39;t work as reliably as I would like.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Summarize chunks&lt;/strong&gt; to make them smaller and be able to fit all of the context into the generator LLM. This has the same problem as Approach 1, about making a decision to instead of just going with the chunk to answer the query, we do the extra step of retrieving the entire article instead of(or its summary) before returning from the retrieval pipeline.&lt;/li&gt; &lt;li&gt;Use a &lt;strong&gt;LLM as a Reranker&lt;/strong&gt;. This LLM-Reranker would (hopefully) understand to return the relevant chunk (Joseph Strauss), instead of the similar chunks (Golden Gate Bridge). This would again imply, that we have this decision making process based on which Reranker should be applied.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Things that could help, but also don&amp;#39;t really work in a more general approach:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Hybrid Retrieval&lt;/strong&gt;: Embedding and Keywordbased retrieval will still most likely not find the relevant chunk. And even if, the Reranking process would drop it again.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Using NER/POS/TF-IDF keywords&lt;/strong&gt; alongside the text chunks seems do run into similar problems&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Query Expansion and Reformulation&lt;/strong&gt;: Similar Problem as above. Also I have tried this approach and including the Augmented Queries in the Rerank step results in other QA-Tasks to perform worse. IMO only the original query should be involved in reranking. I use LLM-Generated Queries for SimilaritySearch already, but then rerank those results with the original query.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Two Stage Retrieval&lt;/strong&gt;: Retrieve GoldenGate chunks -&amp;gt; get all chunks of this article -&amp;gt; find relevant chunks from this preselection of article-chunks. How can we do the last step without involving a slow LLM and also deciding to go into this TwoStage retrieval in the first place?&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Different Chunk Size&lt;/strong&gt;? Would probably work, but what if the document is just a little bit longer, and again we have the information in another chunk that the Bridge-Chunk? Doesn&amp;#39;t generalize well.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;In the current pipeline, even if the relevant chunk (Joseph Strauss) would appear in the retrieved chunks, lets say via hybrid-search using BM25, keyword matching or other strategies, this chunk would be dropped by the Reranker, because it is not similar to the query.&lt;/p&gt; &lt;p&gt;In conclusion, all approaches I can think of, include some sort of LLM to make a decision before returning the chunks to the generatorLLM and/or another decision based on all chunks of this article.&lt;/p&gt; &lt;p&gt;What are other approaches one could apply here? What is a best practice? How do others handle this problem?&lt;/p&gt; &lt;p&gt;I am looking for an approach that is robust and fast enough to work in a production grade system. Ideally some sort of transformer model similar to a reranker, that instead of returning similar matches it returns relevant matches.&lt;/p&gt; &lt;p&gt;No matter how similar all our chunks are to the Golden Gate Bridge and SanFrancisco and the pacific, the actual desired chunk is not similar. How can we handle this?&lt;/p&gt; &lt;p&gt;&lt;strong&gt;This approach seems the most promising to me, but also seems like a really daunting task?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Train a model to distinguish between &amp;quot;similar&amp;quot; and &amp;quot;relevant&amp;quot; content using contrastive learning techniques.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Create training pairs that include similar but irrelevant chunks as negative examples, and dissimilar but relevant chunks as positive examples.&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/WarriorA&quot;&gt; /u/WarriorA &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e1cdhp/retrieve_chunks_which_are_not_similar_but_still/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e1cdhp/retrieve_chunks_which_are_not_similar_but_still/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1e1cdhp</id><link href="https://www.reddit.com/r/LangChain/comments/1e1cdhp/retrieve_chunks_which_are_not_similar_but_still/" /><updated>2024-07-12T08:18:23+00:00</updated><published>2024-07-12T08:18:23+00:00</published><title>Retrieve Chunks which are not similar but still relevant</title></entry><entry><author><name>/u/Able_Scholar_2420</name><uri>https://www.reddit.com/user/Able_Scholar_2420</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;The model is about to extract the data from the pdf based bunch of questions. I tried different quantized model. But every time I tried i failed to make an output. The model giving me schema it self (jsonoutputparser) &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Able_Scholar_2420&quot;&gt; /u/Able_Scholar_2420 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e1jutc/please_suggest_me_better_open_source_model_for/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e1jutc/please_suggest_me_better_open_source_model_for/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1e1jutc</id><link href="https://www.reddit.com/r/LangChain/comments/1e1jutc/please_suggest_me_better_open_source_model_for/" /><updated>2024-07-12T15:02:46+00:00</updated><published>2024-07-12T15:02:46+00:00</published><title>Please Suggest me better open source model for getting json output (Rag operation).</title></entry><entry><author><name>/u/MZuc</name><uri>https://www.reddit.com/user/MZuc</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;ve heard so many AI teams ask this question, I decided to sum up my take on this in a short post. Let me know what you guys think.&lt;/p&gt; &lt;p&gt;The way I see it, the first step is to change how you identify and approach problems. Too often, teams use vague terms like “it &lt;strong&gt;&lt;em&gt;feels&lt;/em&gt;&lt;/strong&gt; like” or “it &lt;strong&gt;&lt;em&gt;seems&lt;/em&gt;&lt;/strong&gt; like” instead of specific metrics, like “the feedback score for this type of request improved by 20%.”&lt;/p&gt; &lt;p&gt;When you&amp;#39;re developing a new AI-driven RAG application, the process tends to be chaotic. There are too many priorities and not enough time to tackle them all. Even if you could, you&amp;#39;re not sure how to enhance your RAG system. You sense that there&amp;#39;s a &amp;quot;right path&amp;quot; – a set of steps that would lead to maximum growth in the shortest time. There are a myriad of great trendy RAG libraries, pipelines, and tools out there but you don&amp;#39;t know which will work on &lt;em&gt;your&lt;/em&gt; documents and &lt;em&gt;your&lt;/em&gt; usecase (&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dr5kki/the_most_important_thing_to_build_great_rag_system/&quot;&gt;as mentioned in another Reddit post that inspired this one&lt;/a&gt;).&lt;/p&gt; &lt;p&gt;I discuss this whole topic in more detail in my &lt;a href=&quot;https://pashpashpash.substack.com/p/why-does-my-rag-suck-and-how-do-i&quot;&gt;Substack article&lt;/a&gt; including specific advice for pre-launch and post-launch, but in a nutshell, when starting any RAG system &lt;strong&gt;&lt;em&gt;you need to capture valuable metrics like cosine similarity, user feedback, and reranker scores - for every retrieval, right from the start&lt;/em&gt;&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;Basically, in an ideal scenario, you will end up with an &lt;strong&gt;observability table&lt;/strong&gt; that looks like this:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;retrieval_id&lt;/strong&gt; (some unique identifier for every piece of retrieved context)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;query_id&lt;/strong&gt; (unique id for the input query/question/message that RAG was used to answer)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;cosine similarity score&lt;/strong&gt; (null for non-vector retrieval e.g. elastic search)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;reranker relevancy score&lt;/strong&gt; (highly recommended for ALL kinds of retrieval, including vector and traditional text search like elastic)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;timestamp&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;retrieved_context&lt;/strong&gt; (optional, but nice to have for QA purposes) &lt;ul&gt; &lt;li&gt;e.g. &lt;code&gt;&amp;quot;The New York City Subway [...]&amp;quot;&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;user_feedback&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;e.g. &lt;code&gt;false (thumbs down)&lt;/code&gt; or &lt;code&gt;true (thumbs up)&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Once you start collecting and storing these super powerful observability metrics, you can begin analyzing production performance. We can &lt;a href=&quot;https://x.com/jxnlco/status/1803899526723387895&quot;&gt;categorize this analysis into two main areas&lt;/a&gt;:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Topics: This refers to the content and context of the data, which can be represented by the way words are structured or the embeddings used in search queries. You can use topic modeling to better understand the types of responses your system handles. &lt;ul&gt; &lt;li&gt;E.g. People talking about their family, or their hobbies, etc.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Capabilities (Agent Tools/Functions): This pertains to the functional aspects of the queries, such as: &lt;ul&gt; &lt;li&gt;Direct conversation requests (e.g., &lt;em&gt;“Remind me what we talked about when we discussed my neighbor&amp;#39;s dogs barking all the time.”&lt;/em&gt;)&lt;/li&gt; &lt;li&gt;Time-sensitive queries (e.g., &lt;em&gt;“Show me the latest X”&lt;/em&gt; or &lt;em&gt;“Show me the most recent Y.”&lt;/em&gt;)&lt;/li&gt; &lt;li&gt;Metadata-specific inquiries (e.g., &lt;em&gt;“What date was our last conversation?”&lt;/em&gt;), which might require specific filters or keyword matching that go beyond simple text embeddings.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;By applying clustering techniques to these topics and capabilities (I cover this in more depth in my &lt;a href=&quot;https://pashpashpash.substack.com/p/tackling-the-challenge-of-document&quot;&gt;previous article on K-Means clusterization&lt;/a&gt;), you can:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Group similar queries/questions together and categorize them by topic e.g. &lt;em&gt;“Product availability questions”&lt;/em&gt; or capability e.g. &lt;em&gt;“Requests to search previous conversations”&lt;/em&gt;.&lt;/li&gt; &lt;li&gt;Calculate the frequency and distribution of these groups.&lt;/li&gt; &lt;li&gt;Assess the average performance scores for each group.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This data-driven approach allows you to prioritize system enhancements based on actual user needs and system performance. For instance:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;If person-entity-retrieval commands a significant portion of query volume (say 60%) and shows high satisfaction rates (90% thumbs up) with minimal cosine distance, this area may not need further refinement.&lt;/li&gt; &lt;li&gt;Conversely, queries like &amp;quot;What date was our last conversation&amp;quot; might show poor results, indicating a limitation of our current functional capabilities. If such queries constitute a small fraction (e.g., 2%) of total volume, it might be more strategic to temporarily exclude these from the system’s capabilities (&lt;em&gt;“I forget, honestly!”&lt;/em&gt; or &lt;em&gt;“Do you think I&amp;#39;m some kind of calendar!?”&lt;/em&gt;), thus improving overall system performance. &lt;ul&gt; &lt;li&gt;Handling these exclusions gracefully significantly improves user experience. &lt;ul&gt; &lt;li&gt;When appropriate, Use humor and personality to your advantage instead of saying &lt;em&gt;“I cannot answer this right now.”&lt;/em&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;TL;DR:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Getting your RAG system from “sucks” to “good” isn&amp;#39;t about magic solutions or trendy libraries. The first step is to implement strong observability practices to continuously analyze and improve performance. Cluster collected data into topics &amp;amp; capabilities to have a clear picture of how people are using your product and where it falls short. Prioritize enhancements based on real usage and remember, a touch of personality can go a long way in handling limitations.&lt;/p&gt; &lt;p&gt;For a more detailed treatment of this topic, check out my &lt;a href=&quot;https://pashpashpash.substack.com/p/why-does-my-rag-suck-and-how-do-i&quot;&gt;article here&lt;/a&gt;. I&amp;#39;d love to hear your thoughts on this, please let me know if there are any other good metrics or considerations to keep in mind!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/MZuc&quot;&gt; /u/MZuc &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e0rsou/why_does_my_rag_suck_and_how_do_i_make_it_good/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e0rsou/why_does_my_rag_suck_and_how_do_i_make_it_good/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1e0rsou</id><link href="https://www.reddit.com/r/LangChain/comments/1e0rsou/why_does_my_rag_suck_and_how_do_i_make_it_good/" /><updated>2024-07-11T15:33:21+00:00</updated><published>2024-07-11T15:33:21+00:00</published><title>&quot;Why does my RAG suck and how do I make it good&quot;</title></entry><entry><author><name>/u/The404Dude</name><uri>https://www.reddit.com/user/The404Dude</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I need to create an agent that can query a sql database to answer questions. I started playing with agents and function calling in langchain. Following the documentation, I put together a very simple code (see below) but it is definitely not working as intended. &lt;/p&gt; &lt;p&gt;I ask a very direct question&lt;br/&gt; &lt;code&gt;what is my resource consumption for the past 30 days?&lt;/code&gt;&lt;/p&gt; &lt;p&gt;and it invokes my tool with the wrong parameters - see date ranges&lt;/p&gt; &lt;p&gt;&lt;code&gt;Invoking: \&lt;/code&gt;get_resource_consumption` with `{&amp;#39;customer_id&amp;#39;: 1, &amp;#39;start_date&amp;#39;: &amp;#39;2023-09-21&amp;#39;, &amp;#39;end_date&amp;#39;: &amp;#39;2023-10-21&amp;#39;}``&lt;/p&gt; &lt;p&gt;and each time I run my app, it comes up with a different date range.&lt;/p&gt; &lt;p&gt;what am I doing wrong?&lt;/p&gt; &lt;pre&gt;&lt;code&gt;from langchain_core.tools import tool from langchain import hub from langchain_openai import ChatOpenAI from langchain.agents import create_tool_calling_agent from langchain_core.prompts import ChatPromptTemplate llm = ChatOpenAI(model=&amp;quot;gpt-4o&amp;quot;) @tool def get_resource_consumption(customer_id, start_date, end_date): &amp;quot;&amp;quot;&amp;quot;Get the resource consumption for a specific customer between two dates. Args: customer_id: The unique identifier of the customer. start_date: The start date of the period. end_date: The end date of the period. &amp;quot;&amp;quot;&amp;quot; return 100 tools = [get_resource_consumption] prompt = ChatPromptTemplate.from_messages( [ (&amp;quot;system&amp;quot;, &amp;quot;You are a helpful assistant that helps users with their questions about resource consumption.&amp;quot;), (&amp;quot;placeholder&amp;quot;, &amp;quot;{chat_history}&amp;quot;), (&amp;quot;human&amp;quot;, &amp;quot;{input}&amp;quot;), (&amp;quot;placeholder&amp;quot;, &amp;quot;{agent_scratchpad}&amp;quot;), ] ) prompt.pretty_print() agent = create_tool_calling_agent(llm, tools, prompt) from langchain.agents import AgentExecutor agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True) agent_executor.invoke({&amp;quot;input&amp;quot;: &amp;quot;what is my resource consumption for the past 30 days? customer_id = 1&amp;quot;}) &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/The404Dude&quot;&gt; /u/The404Dude &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e1h5c6/asking_for_specific_date_range_and_getting/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e1h5c6/asking_for_specific_date_range_and_getting/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1e1h5c6</id><link href="https://www.reddit.com/r/LangChain/comments/1e1h5c6/asking_for_specific_date_range_and_getting/" /><updated>2024-07-12T13:02:40+00:00</updated><published>2024-07-12T13:02:40+00:00</published><title>Asking for specific date range and getting something totally different. What am I doing wrong?</title></entry><entry><author><name>/u/gr8satvik</name><uri>https://www.reddit.com/user/gr8satvik</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e1av5p/react_prompting/&quot;&gt; &lt;img src=&quot;https://b.thumbs.redditmedia.com/BYIL8aSbC2_LeHXd4Q6UQIZ8vRUQHvhKZK-CA6vLNFY.jpg&quot; alt=&quot;ReAct Prompting&quot; title=&quot;ReAct Prompting&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;My LLM is hallaucinating when I ask it a very simple question&lt;br/&gt; using Langchain tools, here wikipedia.&lt;br/&gt; I am using gemini-1.5-flash&lt;br/&gt; tried llama 70b and gemini pro&lt;br/&gt; but still it hallucinates if not same but easy questions.&lt;br/&gt; I am using React Prompt Template&lt;br/&gt; which goes like (2nd image)&lt;/p&gt; &lt;p&gt;My LLM doesn&amp;#39;t even reach the observation step it starts hallucinating after action Input&lt;br/&gt; I tried even changing params of my tools by changing the max_retrieval etc...&lt;br/&gt; Please help...&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/gmrydfxw81cd1.png?width=1614&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9df61fc7f740dab11233da781e3740a871274a9a&quot;&gt;https://preview.redd.it/gmrydfxw81cd1.png?width=1614&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9df61fc7f740dab11233da781e3740a871274a9a&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/ohj4e34a91cd1.png?width=1145&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=966fb6707cf3e9183aa52b093f29f42c0c2192b4&quot;&gt;https://preview.redd.it/ohj4e34a91cd1.png?width=1145&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=966fb6707cf3e9183aa52b093f29f42c0c2192b4&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/gr8satvik&quot;&gt; /u/gr8satvik &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e1av5p/react_prompting/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e1av5p/react_prompting/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1e1av5p</id><media:thumbnail url="https://b.thumbs.redditmedia.com/BYIL8aSbC2_LeHXd4Q6UQIZ8vRUQHvhKZK-CA6vLNFY.jpg" /><link href="https://www.reddit.com/r/LangChain/comments/1e1av5p/react_prompting/" /><updated>2024-07-12T06:38:10+00:00</updated><published>2024-07-12T06:38:10+00:00</published><title>ReAct Prompting</title></entry><entry><author><name>/u/adlx</name><uri>https://www.reddit.com/user/adlx</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;m interested in adding reranking in our RAG application (rerank the chunks retrieved from a pinecone index similarity search).&lt;/p&gt; &lt;p&gt;We are using models deployed in Azure, mostly OpenAI. Ive seen Cohere offers a reranking model, but it&amp;#39;s not available in Azure not AWS Bedrock.&lt;/p&gt; &lt;p&gt;Do you know any approach that would be possible with models we could consume as service (API) on Azure, AWS bedrock or Google Cloud? (we don&amp;#39;t have any entreprise relationship with Cohere yet so I think any alternative would be faster to deploy). &lt;/p&gt; &lt;p&gt;Thanks in advance for any pointer. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/adlx&quot;&gt; /u/adlx &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e1i30i/reranker_models_experience/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e1i30i/reranker_models_experience/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1e1i30i</id><link href="https://www.reddit.com/r/LangChain/comments/1e1i30i/reranker_models_experience/" /><updated>2024-07-12T13:45:58+00:00</updated><published>2024-07-12T13:45:58+00:00</published><title>Reranker models experience</title></entry><entry><author><name>/u/Plane_Past129</name><uri>https://www.reddit.com/user/Plane_Past129</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Is there any chance of building RAG for real-time data that was stored in SQL databases. Specifically, can we use Unstructured there?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Plane_Past129&quot;&gt; /u/Plane_Past129 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e1ba2l/rag_for_realtime_data_sql/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e1ba2l/rag_for_realtime_data_sql/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1e1ba2l</id><link href="https://www.reddit.com/r/LangChain/comments/1e1ba2l/rag_for_realtime_data_sql/" /><updated>2024-07-12T07:04:45+00:00</updated><published>2024-07-12T07:04:45+00:00</published><title>RAG for real-time data (SQL)</title></entry><entry><author><name>/u/northwolf56</name><uri>https://www.reddit.com/user/northwolf56</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e0twcd/my_serverless_visual_langgraph_editor/&quot;&gt; &lt;img src=&quot;https://b.thumbs.redditmedia.com/UJ085UairrlLGvO3kv9DnoNb9gVW-_hN6_J8iNlVyME.jpg&quot; alt=&quot;My Serverless Visual LangGraph Editor&quot; title=&quot;My Serverless Visual LangGraph Editor&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/northwolf56&quot;&gt; /u/northwolf56 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/gallery/1e0twcd&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e0twcd/my_serverless_visual_langgraph_editor/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1e0twcd</id><media:thumbnail url="https://b.thumbs.redditmedia.com/UJ085UairrlLGvO3kv9DnoNb9gVW-_hN6_J8iNlVyME.jpg" /><link href="https://www.reddit.com/r/LangChain/comments/1e0twcd/my_serverless_visual_langgraph_editor/" /><updated>2024-07-11T17:01:43+00:00</updated><published>2024-07-11T17:01:43+00:00</published><title>My Serverless Visual LangGraph Editor</title></entry><entry><author><name>/u/Informal-Victory8655</name><uri>https://www.reddit.com/user/Informal-Victory8655</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;m having error when using checkpointer in graph.&lt;/p&gt; &lt;p&gt;| ValueError: Checkpointer requires one or more of the following &amp;#39;configurable&amp;#39; keys: [&amp;#39;thread_id&amp;#39;, &amp;#39;thread_ts&amp;#39;]&lt;/p&gt; &lt;p&gt;.&lt;/p&gt; &lt;p&gt;How to pass configuration with thread_id here to graph?&lt;/p&gt; &lt;p&gt;add_routes(&lt;/p&gt; &lt;p&gt;app,&lt;/p&gt; &lt;p&gt;(graph | RunnableLambda(output_parsing_for_playground)).with_types(input_type=InputChat, output_type=str),&lt;/p&gt; &lt;p&gt;playground_type=&amp;quot;chat&amp;quot;&lt;/p&gt; &lt;p&gt;)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Informal-Victory8655&quot;&gt; /u/Informal-Victory8655 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e1d466/how_to_pass_thread_id_using_config_when_serving/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e1d466/how_to_pass_thread_id_using_config_when_serving/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1e1d466</id><link href="https://www.reddit.com/r/LangChain/comments/1e1d466/how_to_pass_thread_id_using_config_when_serving/" /><updated>2024-07-12T09:08:43+00:00</updated><published>2024-07-12T09:08:43+00:00</published><title>how to pass thread_id using config when serving langgraph(with checkpointer) through langgserve chain. Please help</title></entry><entry><author><name>/u/mehul_gupta1997</name><uri>https://www.reddit.com/user/mehul_gupta1997</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/mehul_gupta1997&quot;&gt; /u/mehul_gupta1997 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;/r/ArtificialInteligence/comments/1e18y5b/localgemma_for_loading_gemma2_models_locally/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e190m1/localgemma_for_loading_gemma2_models_locally/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1e190m1</id><link href="https://www.reddit.com/r/LangChain/comments/1e190m1/localgemma_for_loading_gemma2_models_locally/" /><updated>2024-07-12T04:45:29+00:00</updated><published>2024-07-12T04:45:29+00:00</published><title>Local-Gemma for loading Gemma2 models locally</title></entry><entry><author><name>/u/Odd-Bonus-4075</name><uri>https://www.reddit.com/user/Odd-Bonus-4075</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I am working on a project where I need to extract information of a very long document.&lt;br/&gt; Using a local LLM (gemma2) through ollama.&lt;/p&gt; &lt;p&gt;If using a vector store, Is there a way I can make sure my LLM query searches all pages or chunks of the vector store?&lt;/p&gt; &lt;p&gt;Is it perhaps, better to use brute force and do chunks of the text to pass the LLM the compile the results? &lt;/p&gt; &lt;p&gt;Any relevant information on this would be much appreciated. Many thanks&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Odd-Bonus-4075&quot;&gt; /u/Odd-Bonus-4075 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e1aziw/search_a_long_document/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e1aziw/search_a_long_document/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1e1aziw</id><link href="https://www.reddit.com/r/LangChain/comments/1e1aziw/search_a_long_document/" /><updated>2024-07-12T06:45:57+00:00</updated><published>2024-07-12T06:45:57+00:00</published><title>Search a long document</title></entry><entry><author><name>/u/burcapaul</name><uri>https://www.reddit.com/user/burcapaul</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I believe that chat interfaces are the worst way to interact with LLMs, but they&amp;#39;re currently our only real option (voice doesn&amp;#39;t count). Here&amp;#39;s my reasoning:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Even though we&amp;#39;re programmed as humans to &amp;quot;prompt&amp;quot; each other in daily interactions, I&amp;#39;ve observed that when it comes to LLMs, people are very deficient at it.&lt;/li&gt; &lt;li&gt;In the past few decades, Microsoft and Apple have trained us to use computers primarily through visual interactions (clicks, taps, buttons, etc.).&lt;/li&gt; &lt;li&gt;Maybe the interface for LLMs should be more visual, rather than relying on text prompting.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;What do you guys think about this? Are we limiting the potential of LLMs by sticking to chat interfaces? Could a more visual approach make these AI tools more accessible and effective for the average user?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/burcapaul&quot;&gt; /u/burcapaul &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e0k7e8/chat_interfaces_are_holding_back_llms_we_need_a/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e0k7e8/chat_interfaces_are_holding_back_llms_we_need_a/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1e0k7e8</id><link href="https://www.reddit.com/r/LangChain/comments/1e0k7e8/chat_interfaces_are_holding_back_llms_we_need_a/" /><updated>2024-07-11T08:49:53+00:00</updated><published>2024-07-11T08:49:53+00:00</published><title>Chat interfaces are holding back LLMs - we need a more visual approach</title></entry><entry><author><name>/u/HotRepresentative325</name><uri>https://www.reddit.com/user/HotRepresentative325</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I was wondering what embedding model people have had success with for their vector store? I might be wrong but on langchain it simply implies everyone uses the openai one. Are there options for this?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/HotRepresentative325&quot;&gt; /u/HotRepresentative325 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e11luo/what_embedding_model_is_everyone_using/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e11luo/what_embedding_model_is_everyone_using/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1e11luo</id><link href="https://www.reddit.com/r/LangChain/comments/1e11luo/what_embedding_model_is_everyone_using/" /><updated>2024-07-11T22:28:32+00:00</updated><published>2024-07-11T22:28:32+00:00</published><title>What embedding model is everyone using?</title></entry><entry><author><name>/u/MeltingHippos</name><uri>https://www.reddit.com/user/MeltingHippos</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;This post by an AI engineer explains the challenges they encountered when implementing RAG for massive enterprise codebases and how they solved them. They share lots of alpha on strategies for chunking, creating vector embeddings for code chunks, using LLMs to generate natural language descriptions that improve indexing, and using LLMs to rank the chunks that are retrieved from the vector store: &lt;a href=&quot;https://www.codium.ai/blog/rag-for-large-scale-code-repos/&quot;&gt;https://www.codium.ai/blog/rag-for-large-scale-code-repos/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/MeltingHippos&quot;&gt; /u/MeltingHippos &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e0jpkk/rag_techniques_for_a_big_codebase_with_10k_repos/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e0jpkk/rag_techniques_for_a_big_codebase_with_10k_repos/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1e0jpkk</id><link href="https://www.reddit.com/r/LangChain/comments/1e0jpkk/rag_techniques_for_a_big_codebase_with_10k_repos/" /><updated>2024-07-11T08:14:14+00:00</updated><published>2024-07-11T08:14:14+00:00</published><title>RAG Techniques for a Big Codebase with 10k Repos</title></entry><entry><author><name>/u/Gloomy-Traffic4964</name><uri>https://www.reddit.com/user/Gloomy-Traffic4964</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;I have a few questions after watching the generative UI videos by Langchain (fyi my fullstack/frontend knowledge is poor).&lt;/p&gt; &lt;p&gt;What&amp;#39;s the difference between Langchain&amp;#39;s generative UI vs just streaming data to the front end to be made into a react component with websockets.&lt;/p&gt; &lt;p&gt;For example, I am streaming results to my React frontend from my django backend. In my frontend I am using &lt;code&gt;new WebSocket(\${REACT_APP_WS_URL}/ws/chat/\)&lt;/code&gt; and &lt;code&gt;ws.current.onmessage&lt;/code&gt; to check for &lt;code&gt;{&amp;#39;event&amp;#39;: &amp;#39;on_tool_start&amp;#39;, ...}.&lt;/code&gt; If event is &amp;#39;on_tool_start&amp;#39;, it returns a loading component instead of a text message response, and if the message is &lt;code&gt;{&amp;#39;event&amp;#39;: &amp;#39;on_tool_end&amp;#39;, &amp;#39;output&amp;#39;: [{ &amp;#39;key&amp;#39;:&amp;#39;value&amp;#39;, ..]}&lt;/code&gt; it replaces the loading component with a react component that takes &amp;#39;output&amp;#39; as inputs.&lt;/p&gt; &lt;p&gt;Follow up questions&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Why does the Langchain implementation use react server components in the frontend when there is a separate backend server with FastAPI?&lt;/li&gt; &lt;li&gt;What is ai/rsc in &lt;code&gt;(import { createStreamableUI } from &amp;quot;ai/rsc&amp;quot;)&lt;/code&gt; . It looks like it&amp;#39;s next/Vercel specific? Where are the docs for createStreamableUI ?&lt;/li&gt; &lt;li&gt;How much value is there from the extra complexity in Langchain implementation (specifically in the frontend).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/bracesproul/gen-ui-python/tree/main&quot;&gt;Here is the gen ui repo&lt;/a&gt;&lt;br/&gt; &lt;a href=&quot;https://www.youtube.com/watch?v=d3uoLbfBPkw&amp;amp;t=1557s&quot;&gt;Here is the youtube video&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Gloomy-Traffic4964&quot;&gt; /u/Gloomy-Traffic4964 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e0qc2w/generative_ui/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e0qc2w/generative_ui/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1e0qc2w</id><link href="https://www.reddit.com/r/LangChain/comments/1e0qc2w/generative_ui/" /><updated>2024-07-11T14:30:14+00:00</updated><published>2024-07-11T14:30:14+00:00</published><title>Generative UI</title></entry><entry><author><name>/u/Best_Sail5</name><uri>https://www.reddit.com/user/Best_Sail5</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e0r49j/training_llm_for_langgraph_specific_use/&quot;&gt; &lt;img src=&quot;https://b.thumbs.redditmedia.com/6dbbLK2Jl_HPiGd9EdOuVkLNjlsygojMZ_UcNxJIg8c.jpg&quot; alt=&quot;training LLM for Langgraph specific use&quot; title=&quot;training LLM for Langgraph specific use&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hello everyone,&lt;/p&gt; &lt;p&gt;I wanna use a LLM to create an agent in langgraph with this kind of architecture:&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/uw5c6kozmwbd1.png?width=199&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ca17ef164138c99d913c9d068c59ef5c996ad56c&quot;&gt;https://preview.redd.it/uw5c6kozmwbd1.png?width=199&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ca17ef164138c99d913c9d068c59ef5c996ad56c&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The idea is similar to a React agent where the model has to provide a prompt for my terminal tool and observe if it achieve a given objective.&lt;/p&gt; &lt;p&gt;I have a question about how should i train such model?&lt;/p&gt; &lt;p&gt;Could i use DPO/ORPO procedure to align my model on multi-step feeding him the context each time?&lt;/p&gt; &lt;p&gt;Or is there a smarter way to do that?&lt;/p&gt; &lt;p&gt;Thanks for your suggestions!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Best_Sail5&quot;&gt; /u/Best_Sail5 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e0r49j/training_llm_for_langgraph_specific_use/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e0r49j/training_llm_for_langgraph_specific_use/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1e0r49j</id><media:thumbnail url="https://b.thumbs.redditmedia.com/6dbbLK2Jl_HPiGd9EdOuVkLNjlsygojMZ_UcNxJIg8c.jpg" /><link href="https://www.reddit.com/r/LangChain/comments/1e0r49j/training_llm_for_langgraph_specific_use/" /><updated>2024-07-11T15:04:16+00:00</updated><published>2024-07-11T15:04:16+00:00</published><title>training LLM for Langgraph specific use</title></entry><entry><author><name>/u/ApprehensiveCut799</name><uri>https://www.reddit.com/user/ApprehensiveCut799</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I created a quick proof of concept by copy pasting LangChain code from here &lt;a href=&quot;https://js.langchain.com/v0.1/docs/modules/agents/agent_types/tool_calling/&quot;&gt;https://js.langchain.com/v0.1/docs/modules/agents/agent_types/tool_calling/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It basically uses a tool calling agent that uses the google search tool, web browser tool and a retriever tool that allows a user to search for data and also query their own uploaded documents. I ended up using the tool calling agent as the reAct agent was causing a lot of bugs. The issue now is that I read a few research papers on reAct prompting and was planning to use them as sources to beef up my hackathon presentation and make it look more cutting edge. I&amp;#39;m just wondering whether anyone has any similar papers that can make my current POC sound more cutting edge? I know this probably sounds really stupid to you but I really want to win this hackathon and I believe that the presentation is equally as important as the MVP. Thanks a lot.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/ApprehensiveCut799&quot;&gt; /u/ApprehensiveCut799 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e0mgl2/looking_for_research_papers_to_beef_up_my/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e0mgl2/looking_for_research_papers_to_beef_up_my/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1e0mgl2</id><link href="https://www.reddit.com/r/LangChain/comments/1e0mgl2/looking_for_research_papers_to_beef_up_my/" /><updated>2024-07-11T11:17:47+00:00</updated><published>2024-07-11T11:17:47+00:00</published><title>Looking for research papers to beef up my Hackathon project</title></entry><entry><author><name>/u/anehzat</name><uri>https://www.reddit.com/user/anehzat</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e0gqn6/psql_extended_to_support_sql_autocomplete_chat/&quot;&gt; &lt;img src=&quot;https://preview.redd.it/6zcw8ik2mtbd1.gif?width=320&amp;amp;crop=smart&amp;amp;s=0d16d7147456f09c07dfab560fb29bc77d857767&quot; alt=&quot;psql extended to support SQL autocomplete &amp;amp; Chat Assistance with DB context.&quot; title=&quot;psql extended to support SQL autocomplete &amp;amp; Chat Assistance with DB context.&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/anehzat&quot;&gt; /u/anehzat &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://i.redd.it/6zcw8ik2mtbd1.gif&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e0gqn6/psql_extended_to_support_sql_autocomplete_chat/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1e0gqn6</id><media:thumbnail url="https://preview.redd.it/6zcw8ik2mtbd1.gif?width=320&amp;crop=smart&amp;s=0d16d7147456f09c07dfab560fb29bc77d857767" /><link href="https://www.reddit.com/r/LangChain/comments/1e0gqn6/psql_extended_to_support_sql_autocomplete_chat/" /><updated>2024-07-11T04:55:20+00:00</updated><published>2024-07-11T04:55:20+00:00</published><title>psql extended to support SQL autocomplete &amp; Chat Assistance with DB context.</title></entry><entry><author><name>/u/Select-Coconut-1161</name><uri>https://www.reddit.com/user/Select-Coconut-1161</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e0iovb/filtering_based_on_metadata/&quot;&gt; &lt;img src=&quot;https://external-preview.redd.it/UallS6eTqMQ6Q_VovzOpEJke6ZgDb6GzqRJtFsGug9s.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=82f0fba791dd0f75899e3dc5480f915fe8c0cafd&quot; alt=&quot;Filtering based on Metadata&quot; title=&quot;Filtering based on Metadata&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey everyone, I am trying to filter the items I retrieve by Metadata fields. I have like 5 metadata fields and I am trying to use 3 filters using 3 of those fields but somehow, only one of them does not work.&lt;/p&gt; &lt;p&gt;I declare the attribute info in metadata_field_info like that:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;AttributeInfo( name=&amp;quot;speaker&amp;quot;, description=&amp;quot;The person who spoke, identified as either &amp;#39;Moderator&amp;#39; or &amp;#39;Participant&amp;#39;.&amp;quot;, type=&amp;quot;string&amp;quot;, ), &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Then, I try to filter it like&lt;/p&gt; &lt;pre&gt;&lt;code&gt; filter=f&amp;quot;and(eq(&amp;#39;speaker&amp;#39;, &amp;#39;Participant&amp;#39;), eq(&amp;#39;participant&amp;#39;, &amp;#39;k6&amp;#39;), ({part_filter}))&amp;quot;, &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;However, this did not work. So I asked Claude and updated it:&lt;/p&gt; &lt;pre&gt;&lt;code&gt; filter_str=f&amp;quot;speaker == &amp;#39;Participant&amp;#39; and participant == &amp;#39;k6&amp;#39; and ({part_filter})&amp;quot;, &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This also did not work. I decided to check LangChain documentations and using &lt;a href=&quot;https://python.langchain.com/v0.1/docs/modules/data_connection/retrievers/self_query/&quot;&gt;this page&lt;/a&gt; as a reference, I changed it to:&lt;/p&gt; &lt;pre&gt;&lt;code&gt; filter=f&amp;quot;and(eq(\&amp;quot;speaker\&amp;quot;, \&amp;quot;Participant\&amp;quot;), eq(\&amp;quot;participant\&amp;quot;, \&amp;quot;k6\&amp;quot;), ({part_filter}))&amp;quot;, &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;When this also did not work. I got suspicious about whether there was a problem with my tagging so I checked my DB and found out it was also correct. The item below is tagged &amp;quot;Moderator&amp;quot; as you can see but it is retrieved when I use the filters above.&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/5x8krgis8ubd1.jpg?width=1080&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=8d193e29a4eef3213b7097c10b2ceab7da664bfa&quot;&gt;https://preview.redd.it/5x8krgis8ubd1.jpg?width=1080&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=8d193e29a4eef3213b7097c10b2ceab7da664bfa&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Weird part is I use exactly same syntax for other filters and they work. I am lost. Any ideas?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Select-Coconut-1161&quot;&gt; /u/Select-Coconut-1161 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e0iovb/filtering_based_on_metadata/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e0iovb/filtering_based_on_metadata/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1e0iovb</id><media:thumbnail url="https://external-preview.redd.it/UallS6eTqMQ6Q_VovzOpEJke6ZgDb6GzqRJtFsGug9s.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=82f0fba791dd0f75899e3dc5480f915fe8c0cafd" /><link href="https://www.reddit.com/r/LangChain/comments/1e0iovb/filtering_based_on_metadata/" /><updated>2024-07-11T07:02:33+00:00</updated><published>2024-07-11T07:02:33+00:00</published><title>Filtering based on Metadata</title></entry><entry><author><name>/u/NoChampionship7630</name><uri>https://www.reddit.com/user/NoChampionship7630</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;ve been having issues training my llm to be able to use tool during certain part of the conversaiton. Prior to fine-tuning the interaction with the agent is good, it calls all the necessary when needed, but I haven&amp;#39;t found a way to incorporate the tool usage in the training data correctly, I&amp;#39;m missing something... after training it simply doesn&amp;#39;t use any tools.... I&amp;#39;m currently using langgraph. Any suggestions woud be highly appreciated.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/NoChampionship7630&quot;&gt; /u/NoChampionship7630 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e0oqol/fine_tuning_llm_with_tool_usage/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e0oqol/fine_tuning_llm_with_tool_usage/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1e0oqol</id><link href="https://www.reddit.com/r/LangChain/comments/1e0oqol/fine_tuning_llm_with_tool_usage/" /><updated>2024-07-11T13:17:56+00:00</updated><published>2024-07-11T13:17:56+00:00</published><title>Fine Tuning LLM with tool Usage</title></entry><entry><author><name>/u/goddamnit_1</name><uri>https://www.reddit.com/user/goddamnit_1</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e03z0y/i_used_langchain_to_build_a_slack_agent_my/&quot;&gt; &lt;img src=&quot;https://external-preview.redd.it/u5rxHXqj1K-44fCKJCxS7ivB42knsXLiW4mFDtHvbdQ.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f4921d56599664bdd7e92e20b29f17d8100f8070&quot; alt=&quot;I used Langchain to build a Slack Agent - My Experience&quot; title=&quot;I used Langchain to build a Slack Agent - My Experience&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;My AI Agent does the following:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Instant answers from the web in any Slack channel&lt;/li&gt; &lt;li&gt;Code interpretation &amp;amp; execution on the fly&lt;/li&gt; &lt;li&gt;Smart web crawling for up-to-date info&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;project link : &lt;a href=&quot;http://git.new/slack-bot-agent-ollama&quot;&gt;git.new/slack-bot-agent-ollama&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;My experience with Langchain&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;One of the key advantages of Langchain is its ability to integrate different LLMs into your applications. This flexibility allows you to experiment with various models and find the one that best suits your needs.&lt;/p&gt; &lt;p&gt;Langchain&amp;#39;s approach is a game-changer. However, I do have one gripe - the documentation could be better. I wasn&amp;#39;t aware that I needed to use the ChatModels instead of the direct models, and this wasn&amp;#39;t specified clearly enough. This kind of information is crucial for users to get up and running quickly.&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/jjtezql9nqbd1.png?width=1283&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5707c453e8a1f1bc0f756b6821780603065dd4fb&quot;&gt;https://preview.redd.it/jjtezql9nqbd1.png?width=1283&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5707c453e8a1f1bc0f756b6821780603065dd4fb&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/goddamnit_1&quot;&gt; /u/goddamnit_1 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e03z0y/i_used_langchain_to_build_a_slack_agent_my/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e03z0y/i_used_langchain_to_build_a_slack_agent_my/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1e03z0y</id><media:thumbnail url="https://external-preview.redd.it/u5rxHXqj1K-44fCKJCxS7ivB42knsXLiW4mFDtHvbdQ.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=f4921d56599664bdd7e92e20b29f17d8100f8070" /><link href="https://www.reddit.com/r/LangChain/comments/1e03z0y/i_used_langchain_to_build_a_slack_agent_my/" /><updated>2024-07-10T19:03:37+00:00</updated><published>2024-07-10T19:03:37+00:00</published><title>I used Langchain to build a Slack Agent - My Experience</title></entry><entry><author><name>/u/New-Cryptographer131</name><uri>https://www.reddit.com/user/New-Cryptographer131</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I have been working with django and langchain for few days. What i have noticed is it’s not possible to save the langchain messages directly with the django ORM. So the integration seems bit confusing when trying to save some details using the django models and others using pydantic models for langchain. Is this a missing support by langchain or am I missing something ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/New-Cryptographer131&quot;&gt; /u/New-Cryptographer131 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e0nqxb/langchain_django_orm_integration/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e0nqxb/langchain_django_orm_integration/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1e0nqxb</id><link href="https://www.reddit.com/r/LangChain/comments/1e0nqxb/langchain_django_orm_integration/" /><updated>2024-07-11T12:28:07+00:00</updated><published>2024-07-11T12:28:07+00:00</published><title>Langchain + Django ORM integration</title></entry><entry><author><name>/u/FunSession1164</name><uri>https://www.reddit.com/user/FunSession1164</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;On one hand I really want to build from the ground up and see what happens every step of the way, on the other hand, I’m trying to embrace abstraction, l would just go for a no-code drag and drop if the final product works fine. I have been looking into &lt;a href=&quot;https://smythos.com/&quot;&gt;~SmythOS~&lt;/a&gt;, a no code platform for creating AI agents and I think I am sold. But I still can’t shake the feeling that it’s too good to be true. What do you think? Would you go for a no code option or would you rather just do everything yourself?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/FunSession1164&quot;&gt; /u/FunSession1164 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e0xdgw/ai_agents_are_nocode_platforms_better/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e0xdgw/ai_agents_are_nocode_platforms_better/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1e0xdgw</id><link href="https://www.reddit.com/r/LangChain/comments/1e0xdgw/ai_agents_are_nocode_platforms_better/" /><updated>2024-07-11T19:28:20+00:00</updated><published>2024-07-11T19:28:20+00:00</published><title>AI agents. Are no-code platforms better?</title></entry><entry><author><name>/u/BellaHi</name><uri>https://www.reddit.com/user/BellaHi</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e0dp2l/from_concept_to_execution_crafting_cuttingedge_ai/&quot;&gt; &lt;img src=&quot;https://external-preview.redd.it/mrFcgMwx0I5hbiBHVY0Y7RFxYzTqpONsp1DNMaJb2bo.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bbd4ab5ae934c2107af3adc3067699bb73f680f8&quot; alt=&quot;From Concept to Execution: Crafting Cutting-Edge AI Agents with LangChain&quot; title=&quot;From Concept to Execution: Crafting Cutting-Edge AI Agents with LangChain&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/BellaHi&quot;&gt; /u/BellaHi &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://myscale.com/blog/build-ai-agent-with-langchain/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e0dp2l/from_concept_to_execution_crafting_cuttingedge_ai/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1e0dp2l</id><media:thumbnail url="https://external-preview.redd.it/mrFcgMwx0I5hbiBHVY0Y7RFxYzTqpONsp1DNMaJb2bo.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=bbd4ab5ae934c2107af3adc3067699bb73f680f8" /><link href="https://www.reddit.com/r/LangChain/comments/1e0dp2l/from_concept_to_execution_crafting_cuttingedge_ai/" /><updated>2024-07-11T02:09:43+00:00</updated><published>2024-07-11T02:09:43+00:00</published><title>From Concept to Execution: Crafting Cutting-Edge AI Agents with LangChain</title></entry></feed>