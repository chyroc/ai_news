<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/"><category term="LangChain" label="r/LangChain"/><updated>2024-06-12T23:04:18+00:00</updated><icon>https://www.redditstatic.com/icon.png/</icon><id>/r/LangChain.rss</id><link rel="self" href="https://www.reddit.com/r/LangChain.rss" type="application/atom+xml" /><link rel="alternate" href="https://www.reddit.com/r/LangChain" type="text/html" /><subtitle>LangChain is an open-source framework and developer toolkit that helps developers get LLM applications from prototype to production. It is available for Python and Javascript at https://www.langchain.com/.</subtitle><title>LangChain</title><entry><author><name>/u/zchaarm</name><uri>https://www.reddit.com/user/zchaarm</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;A place for members of &lt;a href=&quot;/r/LangChain&quot;&gt;r/LangChain&lt;/a&gt; to chat with each other&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/zchaarm&quot;&gt; /u/zchaarm &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_10ljho9</id><link href="https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/" /><updated>2023-01-26T04:33:05+00:00</updated><published>2023-01-26T04:33:05+00:00</published><title>r/LangChain Lounge</title></entry><entry><author><name>/u/Borfecao</name><uri>https://www.reddit.com/user/Borfecao</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I&amp;#39;m working on a Supervisor with LangGraph for a company internship. My mentor has asked me to create three Agents: &amp;quot;Question Agent&amp;quot;, &amp;quot;Answer Agent&amp;quot;, and &amp;quot;Summarizer Agent&amp;quot;. The input is a PDF, which I need to split by page and add each page to a vectorial database for later use. Each agent will also save its outputs in the vectorial POSTGRES database. Here&amp;#39;s a rough idea of the structure:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Questions Table&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;id (Primary Key)&lt;/li&gt; &lt;li&gt;question (Text)&lt;/li&gt; &lt;li&gt;embedding (Vector)&lt;/li&gt; &lt;li&gt;document_id (Integer)&lt;/li&gt; &lt;li&gt;page_number (Integer)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Answers Table&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;id (Primary Key)&lt;/li&gt; &lt;li&gt;answer (Text)&lt;/li&gt; &lt;li&gt;embedding (Vector)&lt;/li&gt; &lt;li&gt;document_id (Integer)&lt;/li&gt; &lt;li&gt;page_number (Integer)&lt;/li&gt; &lt;li&gt;question_id (Foreign Key to Questions table)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Summaries Table&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;id (Primary Key)&lt;/li&gt; &lt;li&gt;summary (Text)&lt;/li&gt; &lt;li&gt;embedding (Vector)&lt;/li&gt; &lt;li&gt;document_id (Integer)&lt;/li&gt; &lt;li&gt;page_number (Integer)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Documents Table&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;id (Primary Key)&lt;/li&gt; &lt;li&gt;summary (Text)&lt;/li&gt; &lt;li&gt;document_id (Integer)&lt;/li&gt; &lt;li&gt;number_of_pages (Integer)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The workflow is something like this:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Load the document (sanitize the text, embed it, save in &amp;quot;Documents&amp;quot;)&lt;/li&gt; &lt;li&gt;Make a summary of each page (save in &amp;quot;Summaries&amp;quot;)&lt;/li&gt; &lt;li&gt;Generate questions for each page (save in &amp;quot;Questions&amp;quot;)&lt;/li&gt; &lt;li&gt;Answer all the questions generated by the Question Agent, considering the context of the page (save in &amp;quot;Answers&amp;quot;)&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;My biggest question is:&lt;/strong&gt; what tools and agents should I implement for this? Most resources I&amp;#39;ve found online use tools like Tavily Search and Python REPL, which aren&amp;#39;t really helpful for my case. I need to use the Supervisor since it&amp;#39;s a project requirement, and I&amp;#39;m a bit confused about the implementation details, since this would be very easy to implement with simple chains, and the only solution I could come up with is tooless agents...?&lt;/p&gt; &lt;p&gt;Any advice or pointers would be greatly appreciated! Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Borfecao&quot;&gt; /u/Borfecao &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1deaviw/need_help_implementing_supervisor_with_langgraph/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1deaviw/need_help_implementing_supervisor_with_langgraph/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1deaviw</id><link href="https://www.reddit.com/r/LangChain/comments/1deaviw/need_help_implementing_supervisor_with_langgraph/" /><updated>2024-06-12T16:28:31+00:00</updated><published>2024-06-12T16:28:31+00:00</published><title>Need Help Implementing Supervisor with LangGraph</title></entry><entry><author><name>/u/Capital_learner</name><uri>https://www.reddit.com/user/Capital_learner</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I have to make llm chatbit using open ai on flask. Help me to make this. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Capital_learner&quot;&gt; /u/Capital_learner &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1deh52g/need_help_to_make_langchain_chatbot/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1deh52g/need_help_to_make_langchain_chatbot/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1deh52g</id><link href="https://www.reddit.com/r/LangChain/comments/1deh52g/need_help_to_make_langchain_chatbot/" /><updated>2024-06-12T20:48:11+00:00</updated><published>2024-06-12T20:48:11+00:00</published><title>Need Help to make langchain chatbot</title></entry><entry><author><name>/u/huseyinbabal</name><uri>https://www.reddit.com/user/huseyinbabal</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dec8ls/building_devops_ai_assistant_with_langchain/&quot;&gt; &lt;img src=&quot;https://external-preview.redd.it/3eIYPb9fvv3T8yM1RCQU_MSfK9DpDf_d-D2eAbkLPPE.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c229377b890822b72774120bb4ab72f62d353aa2&quot; alt=&quot;Building Devops AI Assistant with Langchain, Ollama, and PostgreSQL&quot; title=&quot;Building Devops AI Assistant with Langchain, Ollama, and PostgreSQL&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/huseyinbabal&quot;&gt; /u/huseyinbabal &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://docs.rapidapp.io/blog/building-devops-ai-assistant-with-langchain-ollama-and-postgresql&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dec8ls/building_devops_ai_assistant_with_langchain/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1dec8ls</id><media:thumbnail url="https://external-preview.redd.it/3eIYPb9fvv3T8yM1RCQU_MSfK9DpDf_d-D2eAbkLPPE.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=c229377b890822b72774120bb4ab72f62d353aa2" /><link href="https://www.reddit.com/r/LangChain/comments/1dec8ls/building_devops_ai_assistant_with_langchain/" /><updated>2024-06-12T17:25:33+00:00</updated><published>2024-06-12T17:25:33+00:00</published><title>Building Devops AI Assistant with Langchain, Ollama, and PostgreSQL</title></entry><entry><author><name>/u/thumbsdrivesmecrazy</name><uri>https://www.reddit.com/user/thumbsdrivesmecrazy</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;In Feb 2024, Meta published a paper introducing TestGen-LLM, a tool for automated unit test generation using LLMs, but didn’t release the TestGen-LLM code.The following blog shows how CodiumAI created the first open-source implementation - Cover-Agent, based on Meta&amp;#39;s approach: &lt;a href=&quot;https://www.codium.ai/blog/we-created-the-first-open-source-implementation-of-metas-testgen-llm/&quot;&gt;We created the first open-source implementation of Meta’s TestGen–LLM&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The tool is implemented as follows:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Receive the following user inputs (Source File for code under test, Existing Test Suite to enhance, Coverage Report, Build/Test Command Code coverage target and maximum iterations to run, Additional context and prompting options)&lt;/li&gt; &lt;li&gt;Generate more tests in the same style&lt;/li&gt; &lt;li&gt;Validate those tests using your runtime environment - Do they build and pass?&lt;/li&gt; &lt;li&gt;Ensure that the tests add value by reviewing metrics such as increased code coverage&lt;/li&gt; &lt;li&gt;Update existing Test Suite and Coverage Report&lt;/li&gt; &lt;li&gt;Repeat until code reaches criteria: either code coverage threshold met, or reached the maximum number of iterations&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/thumbsdrivesmecrazy&quot;&gt; /u/thumbsdrivesmecrazy &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1deals9/opensource_implementation_of_metas_testgenllm/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1deals9/opensource_implementation_of_metas_testgenllm/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1deals9</id><link href="https://www.reddit.com/r/LangChain/comments/1deals9/opensource_implementation_of_metas_testgenllm/" /><updated>2024-06-12T16:17:22+00:00</updated><published>2024-06-12T16:17:22+00:00</published><title>Open-source implementation of Meta’s TestGen–LLM - CodiumAI</title></entry><entry><author><name>/u/migkapa</name><uri>https://www.reddit.com/user/migkapa</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi all, &lt;/p&gt; &lt;p&gt;I get an &amp;quot;An error occurred: Multiple function calls are not currently supported&amp;quot; while using Gemini Pro .&lt;br/&gt; Anyone had the same issue?&lt;/p&gt; &lt;p&gt;Using:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;llm = ChatGoogleGenerativeAI(temperature=0, model=&amp;quot;gemini-pro&amp;quot;) llm.bind_tools(tools) &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/migkapa&quot;&gt; /u/migkapa &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1de99jx/error_with_tool_calling_while_using_gemini_pro/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1de99jx/error_with_tool_calling_while_using_gemini_pro/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1de99jx</id><link href="https://www.reddit.com/r/LangChain/comments/1de99jx/error_with_tool_calling_while_using_gemini_pro/" /><updated>2024-06-12T15:20:37+00:00</updated><published>2024-06-12T15:20:37+00:00</published><title>Error with tool calling while using Gemini Pro</title></entry><entry><author><name>/u/Disneyskidney</name><uri>https://www.reddit.com/user/Disneyskidney</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;m working on a use case that relies on very robust knowledge graph construction and I wanted to know if any startups/companies/open-source have built either free or paid production ready solutions for the unstructured text to knowledge graph pipeline.&lt;/p&gt; &lt;p&gt;UPDATE:&lt;/p&gt; &lt;p&gt;Diffbot seems to have a pretty good API that is compatiable with Llama Index and Langchain&lt;/p&gt; &lt;p&gt;this tutorial for Llama Index was released the same day I posted this and looks promising: &lt;a href=&quot;https://www.llamaindex.ai/blog/customizing-property-graph-index-in-llamaindex&quot;&gt;https://www.llamaindex.ai/blog/customizing-property-graph-index-in-llamaindex&lt;/a&gt;​&lt;/p&gt; &lt;p&gt;And Here is one for Langchain &lt;a href=&quot;https://python.langchain.com/v0.1/docs/integrations/graphs/diffbot/&quot;&gt;Diffbot | 🦜️🔗 LangChain&lt;/a&gt;​&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Disneyskidney&quot;&gt; /u/Disneyskidney &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ddvywe/production_ready_unstructured_text_to_knowledge/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ddvywe/production_ready_unstructured_text_to_knowledge/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1ddvywe</id><link href="https://www.reddit.com/r/LangChain/comments/1ddvywe/production_ready_unstructured_text_to_knowledge/" /><updated>2024-06-12T02:29:12+00:00</updated><published>2024-06-12T02:29:12+00:00</published><title>Production Ready Unstructured Text to Knowledge Graph</title></entry><entry><author><name>/u/emersounds</name><uri>https://www.reddit.com/user/emersounds</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1de6u3j/deploy_langgraph_in_google_cloud/&quot;&gt; &lt;img src=&quot;https://b.thumbs.redditmedia.com/dl6NGxPnMwN1WkcIeQnqc9aC-oujGT8_wgAtEnVpwxA.jpg&quot; alt=&quot;Deploy Langgraph in Google Cloud?&quot; title=&quot;Deploy Langgraph in Google Cloud?&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I have been searching extensively but haven&amp;#39;t found any guide on deploying a Langgraph runnable with Google Cloud. &lt;/p&gt; &lt;p&gt;Currently, I am using an Reasoning Engine (Vertex AI) with the LangchainAgent template (from Google Cloud documentation) &lt;/p&gt; &lt;p&gt;Now, I tried to deploy my custom Reasoning Engine agent based on Langgraph and I can&amp;#39;t. &lt;/p&gt; &lt;p&gt;I would greatly appreciate any kind of help. &lt;/p&gt; &lt;p&gt;Regards. &lt;/p&gt; &lt;p&gt;PD: Langchain image to bait.&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/03fqi0fm856d1.jpg?width=1024&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=0bab82582504720db2d45a2d82cbec2aad0981a6&quot;&gt;https://preview.redd.it/03fqi0fm856d1.jpg?width=1024&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=0bab82582504720db2d45a2d82cbec2aad0981a6&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/emersounds&quot;&gt; /u/emersounds &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1de6u3j/deploy_langgraph_in_google_cloud/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1de6u3j/deploy_langgraph_in_google_cloud/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1de6u3j</id><media:thumbnail url="https://b.thumbs.redditmedia.com/dl6NGxPnMwN1WkcIeQnqc9aC-oujGT8_wgAtEnVpwxA.jpg" /><link href="https://www.reddit.com/r/LangChain/comments/1de6u3j/deploy_langgraph_in_google_cloud/" /><updated>2024-06-12T13:36:16+00:00</updated><published>2024-06-12T13:36:16+00:00</published><title>Deploy Langgraph in Google Cloud?</title></entry><entry><author><name>/u/These-Butterfly8819</name><uri>https://www.reddit.com/user/These-Butterfly8819</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I have been given a requirement from my company to look into and try to comeup with a chatbot that would be integrated into the web application. Specifically, we have a list of Companies and their details like name, what they do, their revenues, etc. and some uploaded pdf files that contain more information regarding the company. So the chatbot will be integrated into the details page of the companies. User could then ask any question regarding the company and the chatbot should provide a relevant answer for that company.&lt;/p&gt; &lt;p&gt;I am fairly new to this, but was able to find out that we can use RAG for achieving this, wherein we take all the data and embed it in a vector database. Then fetch relevant vectors per the question asked and provide it as context to the LLM for answer.&lt;/p&gt; &lt;p&gt;However the issue is that some of the data of the company can change with time.&lt;/p&gt; &lt;p&gt;Is there a way to do it so that the pdf data can use vector store, but the rest of the data can be obtained from API calls? That way, we will always have the most recent data of the company, but also have the additional data from the pdf docs?&lt;/p&gt; &lt;p&gt;How would all these things fit? How would the decision be made when to use data from vector database or when to fetch data from API?&lt;/p&gt; &lt;p&gt;Do you guys have any experience with something like this or any recommendations or resources where I can look into for this project?&lt;/p&gt; &lt;p&gt;That would be very helpful.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/These-Butterfly8819&quot;&gt; /u/These-Butterfly8819 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1de6133/help_regarding_application_specific_chatbot/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1de6133/help_regarding_application_specific_chatbot/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1de6133</id><link href="https://www.reddit.com/r/LangChain/comments/1de6133/help_regarding_application_specific_chatbot/" /><updated>2024-06-12T12:58:22+00:00</updated><published>2024-06-12T12:58:22+00:00</published><title>Help regarding application specific chatbot</title></entry><entry><author><name>/u/sarthakai</name><uri>https://www.reddit.com/user/sarthakai</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;They prepare a QA task to observe hallucinations, on both Known examples (training instances similar to the info that the model has seen during its initial training) and Unknown examples (that introduce new info that the model hasn&amp;#39;t been exposed to before).&lt;/p&gt; &lt;p&gt;They see that:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Unknown examples in the fine-tuning dataset bring down performance, the more you train, because of overfitting. They lead to hallucinations and reduce accuracy. Known examples positively impact performance.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Early stopping helps avoid this, which might mean that Unknown examples are neutral in shorter training.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;The slower fitting of Unknown examples also indicates that models struggle to acquire new knowledge through fine-tuning.&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Paper: &lt;a href=&quot;https://arxiv.org/pdf/2405.05904&quot;&gt;https://arxiv.org/pdf/2405.05904&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I share high quality AI updates and tutorials daily.&lt;/p&gt; &lt;p&gt;If you like this post and want to stay updated on latest AI research, you can check out: &lt;a href=&quot;https://linktr.ee/sarthakrastogi&quot;&gt;https://linktr.ee/sarthakrastogi&lt;/a&gt; or my Twitter: &lt;a href=&quot;https://x.com/sarthakai&quot;&gt;https://x.com/sarthakai&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/sarthakai&quot;&gt; /u/sarthakai &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1de5ury/google_study_says_finetuning_an_llm_linearly/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1de5ury/google_study_says_finetuning_an_llm_linearly/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1de5ury</id><link href="https://www.reddit.com/r/LangChain/comments/1de5ury/google_study_says_finetuning_an_llm_linearly/" /><updated>2024-06-12T12:49:29+00:00</updated><published>2024-06-12T12:49:29+00:00</published><title>Google study says fine-tuning an LLM linearly increases hallucinations? 😐</title></entry><entry><author><name>/u/AnxiousEmu3480</name><uri>https://www.reddit.com/user/AnxiousEmu3480</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;m building a chatbot using &lt;code&gt;LangChain&lt;/code&gt;, &lt;code&gt;Next.js&lt;/code&gt;,and &lt;code&gt;CosmosDB&lt;/code&gt; (vector store). My implementation is based on &lt;a href=&quot;https://github.com/langchain-ai/langchain-nextjs-template/blob/main/app/api/chat/retrieval/route.ts&quot;&gt;this&lt;/a&gt;. I&amp;#39;m trying to display the source documents used by the LLM in my UI, but I&amp;#39;m facing two issues:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Source documents not displaying: Despite using a &lt;strong&gt;StreamingTextResponse&lt;/strong&gt; to send the source information in the headers as JSON chunks (see code snippet below), they don&amp;#39;t show up in my UI. There are no console errors.&lt;/li&gt; &lt;li&gt;Incorrect sources: When some source documents do appear, they are not the ones actually used by the LLM or contain unrelated information.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Here&amp;#39;s the part supposed to return the sources:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;import { NextRequest, NextResponse } from &amp;quot;next/server&amp;quot;; import { Message as VercelChatMessage, StreamingTextResponse } from &amp;quot;ai&amp;quot;;import { AzureCosmosDBVectorStore } from &amp;quot;@langchain/community/vectorstores/azure_cosmosdb&amp;quot;; import { AzureOpenAIEmbeddings, AzureChatOpenAI, } from &amp;quot;@langchain/azure-openai&amp;quot;; import { PromptTemplate } from &amp;quot;@langchain/core/prompts&amp;quot;; import { Document } from &amp;quot;@langchain/core/documents&amp;quot;; import { RunnableSequence } from &amp;quot;@langchain/core/runnables&amp;quot;; import { BytesOutputParser, StringOutputParser, } from &amp;quot;@langchain/core/output_parsers&amp;quot;; const combineDocumentsFn = (docs: Document[]) =&amp;gt; { const serializedDocs = docs.map((doc) =&amp;gt; doc.pageContent); return serializedDocs.join(&amp;quot;\n\n&amp;quot;); }; const formatVercelMessages = (chatHistory: VercelChatMessage[]) =&amp;gt; { const formattedDialogueTurns = chatHistory.map((message) =&amp;gt; { if (message.role === &amp;quot;user&amp;quot;) { return `Human: ${message.content}`; } else if (message.role === &amp;quot;assistant&amp;quot;) { return `Assistant: ${message.content}`; } else { return `${message.role}: ${message.content}`; } }); return formattedDialogueTurns.join(&amp;quot;\n&amp;quot;); }; const CONDENSE_QUESTION_TEMPLATE = `Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.&amp;lt;chat_history&amp;gt; {chat_history} &amp;lt;/chat_history&amp;gt; Follow Up Input: {question} Standalone question:`; const condenseQuestionPrompt = PromptTemplate.fromTemplate( CONDENSE_QUESTION_TEMPLATE ); const ANSWER_TEMPLATE = `Answer the question based only on the following context and chat history: &amp;lt;context&amp;gt; {context} &amp;lt;/context&amp;gt; &amp;lt;chat_history&amp;gt; {chat_history} &amp;lt;/chat_history&amp;gt; Question: {question} `; const answerPrompt = PromptTemplate.fromTemplate(ANSWER_TEMPLATE); export async function POST(req: NextRequest) { try { const body = await req.json(); const messages = body.messages ?? []; const previousMessages = messages.slice(0, -1); const currentMessageContent = messages[messages.length - 1].content; const vectorstore = new AzureCosmosDBVectorStore( new AzureOpenAIEmbeddings(), { databaseName: process.env.DB_NAME, collectionName: process.env.DB_COLLECTION_NAME, } ); const model = new AzureChatOpenAI({ azureOpenAIEndpoint: process.env.AZURE_OPENAI_API_ENDPOINT, azureOpenAIApiKey: process.env.AZURE_OPENAI_API_KEY, azureOpenAIApiDeploymentName: process.env.AZURE_OPENAI_API_DEPLOYMENT_NAME, modelName: process.env.AZURE_OPENAI_MODEL_NAME, }); const standaloneQuestionChain = RunnableSequence.from([ condenseQuestionPrompt, model, new StringOutputParser(), ]); let resolveWithDocuments: (value: Document[]) =&amp;gt; void; const documentPromise = new Promise&amp;lt;Document[]&amp;gt;((resolve) =&amp;gt; { resolveWithDocuments = resolve; }); const retriever = vectorstore.asRetriever({ callbacks: [ { handleRetrieverEnd(documents) { resolveWithDocuments(documents); }, }, ], }); const retrievalChain = retriever.pipe(combineDocumentsFn); const answerChain = RunnableSequence.from([ { context: RunnableSequence.from([ (input) =&amp;gt; input.question, retrievalChain, ]), chat_history: (input) =&amp;gt; input.chat_history, question: (input) =&amp;gt; input.question, }, answerPrompt, model, ]); const conversationalRetrievalQAChain = RunnableSequence.from([ { question: standaloneQuestionChain, chat_history: (input) =&amp;gt; input.chat_history, }, answerChain, new BytesOutputParser(), ]); const stream = await conversationalRetrievalQAChain.stream({ question: currentMessageContent, chat_history: formatVercelMessages(previousMessages), }); const documents = await documentPromise; console.log(&amp;quot;documents &amp;quot;, documents.length); const serializedSources = Buffer.from( JSON.stringify( documents.map((doc) =&amp;gt; { return { pageContent: doc.pageContent.slice(0, 50) + &amp;quot;...&amp;quot;, metadata: doc.metadata, }; }) ) ).toString(&amp;quot;base64&amp;quot;); const sourceMetadata = documents.map((doc) =&amp;gt; ({ title: doc.metadata.title, // Or whatever metadata you want url: doc.metadata.url, })); return new StreamingTextResponse(stream, { headers: { &amp;quot;x-message-index&amp;quot;: (previousMessages.length + 1).toString(), &amp;quot;x-message-sources&amp;quot;: serializedSources, }, }); } catch (e: any) { return NextResponse.json({ error: e.message }, { status: e.status ?? 500 }); } } &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;So my questions:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Is my approach to streaming source documents via &lt;strong&gt;StreamingTextResponse&lt;/strong&gt; wrong?&lt;/li&gt; &lt;li&gt;How can I ensure I&amp;#39;m associating the correct source documents with each LLM response?&lt;/li&gt; &lt;li&gt;What debugging techniques can I use to pinpoint where the source information is getting lost or mismatched?&lt;br/&gt;&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/AnxiousEmu3480&quot;&gt; /u/AnxiousEmu3480 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1de50ah/langchainnextjs_chatbot_displaying_incorrect/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1de50ah/langchainnextjs_chatbot_displaying_incorrect/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1de50ah</id><link href="https://www.reddit.com/r/LangChain/comments/1de50ah/langchainnextjs_chatbot_displaying_incorrect/" /><updated>2024-06-12T12:05:33+00:00</updated><published>2024-06-12T12:05:33+00:00</published><title>LangChain/Next.js chatbot displaying incorrect sources</title></entry><entry><author><name>/u/curious-airesearcher</name><uri>https://www.reddit.com/user/curious-airesearcher</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;m currently working on a local codebase and was using Cody to ask questions from my local code base context. But was wondering if there is an open-source project that&amp;#39;s similar. Ideally, the tool would:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Index all project files&lt;/li&gt; &lt;li&gt;Retrieve specific code snippets from all files or tell about specific local variables&lt;/li&gt; &lt;li&gt;Use as Chatbot?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Document loaders and file embeddings could work but I&amp;#39;m not sure on how to handle function interdependencies. Do I need to also additionally pass AST for it to organize it better? Not really sure on what direction to take?&lt;/p&gt; &lt;p&gt;Anyone has tried something similar? What approach did you take? and how was the result?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/curious-airesearcher&quot;&gt; /u/curious-airesearcher &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1de4yz8/local_source_code_indexing_querying_with_rag/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1de4yz8/local_source_code_indexing_querying_with_rag/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1de4yz8</id><link href="https://www.reddit.com/r/LangChain/comments/1de4yz8/local_source_code_indexing_querying_with_rag/" /><updated>2024-06-12T12:03:35+00:00</updated><published>2024-06-12T12:03:35+00:00</published><title>Local Source Code Indexing &amp; Querying with RAG</title></entry><entry><author><name>/u/Highlight-Content</name><uri>https://www.reddit.com/user/Highlight-Content</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;In this &lt;a href=&quot;https://medium.com/ama-tech-blog/combining-langchain-and-llamaindex-to-build-your-first-agentic-rag-system-6e8e2e7825e7&quot;&gt;Medium article&lt;/a&gt;, the agent has three tools:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;&amp;quot;lyft_10k&amp;quot;: &amp;quot;Provides information about Lyft financials for year 2021. &amp;quot;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&amp;quot;uber_10k&amp;quot;: &amp;quot;Provides information about Uber financials for year 2021. &amp;quot;&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;and&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&amp;#39;DuckDuckGoSearch&amp;#39;: &amp;#39;Use for when you need to perform an internet search to find information that another tool can not provide.&amp;#39;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;In one of the test cases, the author queries the agent&lt;/p&gt; &lt;pre&gt;&lt;code&gt;&amp;quot;List me the names of Uber&amp;#39;s board of directors.&amp;quot; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Intuitively, one would assume the agent will invoke the &amp;quot;uber_10k&amp;quot; tool. However, the agent invokes &amp;quot;DuckDuckGoSearch&amp;quot;.&lt;/p&gt; &lt;p&gt;The author explains that:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Since this information is out-of-scope for any of the retriever tools, the agent correctly decided to invoke the external search tool.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;How does the agent know that question is out-of-scope for the &amp;quot;uber_10k&amp;quot; retriever?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Highlight-Content&quot;&gt; /u/Highlight-Content &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ddr9hj/how_does_this_langchain_agent_correctly_identify/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ddr9hj/how_does_this_langchain_agent_correctly_identify/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1ddr9hj</id><link href="https://www.reddit.com/r/LangChain/comments/1ddr9hj/how_does_this_langchain_agent_correctly_identify/" /><updated>2024-06-11T22:36:45+00:00</updated><published>2024-06-11T22:36:45+00:00</published><title>How does this LangChain agent correctly identify the tool to use?</title></entry><entry><author><name>/u/thewanitz</name><uri>https://www.reddit.com/user/thewanitz</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey guys!&lt;/p&gt; &lt;p&gt;So I’ve been looking at a lot of tutorials to build a basic RAG search which does the following:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Takes the user query and put it into the state “user_query”&lt;/li&gt; &lt;li&gt;Searches the internet for results. These results are then populated as text in the state “internet_search_results” field with the url and title of the text&lt;/li&gt; &lt;li&gt; Does the same but searches the local database and populates the state “local_search_results” field with the post ID and title of the search results. &lt;/li&gt; &lt;li&gt;Then passes the state with the information above into a summariser function which uses GPT 3.5 to return structured output with the following fields: (i) the text response, (ii) an array of the sources which include the title, the type (web search or local post), and either the url or the post ID. &lt;/li&gt; &lt;/ol&gt; &lt;p&gt;I’m at a loss on this as can’t find any good tutorials for this. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/thewanitz&quot;&gt; /u/thewanitz &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1de095x/good_tutorials_for_rag_with_structured_state_and/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1de095x/good_tutorials_for_rag_with_structured_state_and/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1de095x</id><link href="https://www.reddit.com/r/LangChain/comments/1de095x/good_tutorials_for_rag_with_structured_state_and/" /><updated>2024-06-12T06:48:15+00:00</updated><published>2024-06-12T06:48:15+00:00</published><title>Good Tutorials For RAG with Structured State and Output?</title></entry><entry><author><name>/u/Outrageous-Tap-6665</name><uri>https://www.reddit.com/user/Outrageous-Tap-6665</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Last week we came across a serious security issue with GPT4o. In both ChatGPT and OpenAI APIs. Until OpenAI fixes it, we manage to fix from our side. We would like to share it with the community. We implemented it in LlamaIndex but should be easy to implement using Langchain as well.&lt;br/&gt; This is the medium article about the fix - &lt;a href=&quot;https://medium.com/@deltaaruna/fixing-a-serious-security-issue-in-gpt4o-api-via-llamaindex-4aa1368b5d2f&quot;&gt;https://medium.com/@deltaaruna/fixing-a-serious-security-issue-in-gpt4o-api-via-llamaindex-4aa1368b5d2f&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This is the medium article about the issue - &lt;a href=&quot;https://medium.com/@deltaaruna/how-anyone-can-hack-chatgpt-aa7959684ef0&quot;&gt;https://medium.com/@deltaaruna/how-anyone-can-hack-chatgpt-aa7959684ef0&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Outrageous-Tap-6665&quot;&gt; /u/Outrageous-Tap-6665 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1de7ar1/a_serious_security_issue_with_gpt4o/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1de7ar1/a_serious_security_issue_with_gpt4o/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1de7ar1</id><link href="https://www.reddit.com/r/LangChain/comments/1de7ar1/a_serious_security_issue_with_gpt4o/" /><updated>2024-06-12T13:57:22+00:00</updated><published>2024-06-12T13:57:22+00:00</published><title>A serious security issue with GPT4o</title></entry><entry><author><name>/u/Medium_Eggplant795</name><uri>https://www.reddit.com/user/Medium_Eggplant795</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I am not a software engineer but an enthusiast of RAG and LLM agents. I wanted to know where is the real bottleneck in building an agent who would build documents based on chat that I am currently having with an LLM based chat interface and embed the chat text using embedding models and store it in vector db for the user to search in later? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Medium_Eggplant795&quot;&gt; /u/Medium_Eggplant795 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ddyhkq/question_regarding_limitation_of_agent_use/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ddyhkq/question_regarding_limitation_of_agent_use/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1ddyhkq</id><link href="https://www.reddit.com/r/LangChain/comments/1ddyhkq/question_regarding_limitation_of_agent_use/" /><updated>2024-06-12T04:51:46+00:00</updated><published>2024-06-12T04:51:46+00:00</published><title>Question regarding limitation of agent use</title></entry><entry><author><name>/u/diptanuc</name><uri>https://www.reddit.com/user/diptanuc</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi folks, we built a TypeScript library to improve search results in RAG Applications. If you are building a RAG application on top of vector indexes, re-ranking search results will always improve LLM&amp;#39;s response synthesis. We implemented two commonly used re-ranking techniques - Reciprocal Rank Fusion(RRF) and LLM Based Re-Ranking(using Llama3 from Groq and GPT-4). Hope this is useful to folks building LLM Applications in React/NextJS.&lt;/p&gt; &lt;p&gt;Code - &lt;a href=&quot;https://github.com/tensorlakeai/rerank-ts&quot;&gt;https://github.com/tensorlakeai/rerank-ts&lt;/a&gt; &lt;/p&gt; &lt;p&gt;We were building a consumer application with our open source data framework &lt;a href=&quot;https://github.com/tensorlakeai/indexify&quot;&gt;https://github.com/tensorlakeai/indexify&lt;/a&gt; and were not able to find a good re-ranking library in TypeScript. So we decided to build one, and it works really well to re-rank ~100 results. We get latency of around 1 second with Llama3/Groq. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/diptanuc&quot;&gt; /u/diptanuc &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ddhd9t/rerankts_typescript_library_for_improving_search/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ddhd9t/rerankts_typescript_library_for_improving_search/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1ddhd9t</id><link href="https://www.reddit.com/r/LangChain/comments/1ddhd9t/rerankts_typescript_library_for_improving_search/" /><updated>2024-06-11T15:46:38+00:00</updated><published>2024-06-11T15:46:38+00:00</published><title>rerank-ts: TypeScript Library for Improving Search Results in RAG Applications</title></entry><entry><author><name>/u/Fit_Influence_1576</name><uri>https://www.reddit.com/user/Fit_Influence_1576</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I’m looking for a chatbot frontend with citations, that utilizes a fast api/langserve backend. Anyone have good suggestions? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Fit_Influence_1576&quot;&gt; /u/Fit_Influence_1576 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ddm4a1/full_stack_starter/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ddm4a1/full_stack_starter/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1ddm4a1</id><link href="https://www.reddit.com/r/LangChain/comments/1ddm4a1/full_stack_starter/" /><updated>2024-06-11T19:01:34+00:00</updated><published>2024-06-11T19:01:34+00:00</published><title>Full stack starter</title></entry><entry><author><name>/u/youniss_k</name><uri>https://www.reddit.com/user/youniss_k</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Using langchain sometimes feels like gambling with costs to me. I never really know how much my requests would actually cost when I send it. I know there are detailed charts which we should read, but who really does? Instead I wanted to ask if anybody knows of an automated way to calculate costs before sending the requests? For my use case, specifically for OpenAI, but maybe there is another way.&lt;/p&gt; &lt;p&gt;And if there isnt anything like that, maybe this would be an interessting project... Like a package which calculates your LLM costs before the requests, depending on the specific platform you use&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/youniss_k&quot;&gt; /u/youniss_k &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ddft47/calculating_llm_costs_before_sending_requests/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ddft47/calculating_llm_costs_before_sending_requests/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1ddft47</id><link href="https://www.reddit.com/r/LangChain/comments/1ddft47/calculating_llm_costs_before_sending_requests/" /><updated>2024-06-11T14:41:50+00:00</updated><published>2024-06-11T14:41:50+00:00</published><title>Calculating LLM costs before sending requests?</title></entry><entry><author><name>/u/Gvascons</name><uri>https://www.reddit.com/user/Gvascons</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;So, I&amp;#39;m building this simple rag pipeline with langchain and ollama that takes in a PDF document and returns it&amp;#39;s summary as bulletpoints.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;file_path = &amp;quot;paper.pdf&amp;quot; loader = PyPDFLoader(file_path) docs = loader.load() embeddings = (OllamaEmbeddings(model=&amp;#39;llama3&amp;#39;)) text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200) splits = text_splitter.split_documents(docs) vectorstore = Chroma.from_documents(documents=splits, embedding=embeddings, persist_directory=&amp;quot;emb&amp;quot;) retriever = as_retriever( embeddings=embeddings, chroma=vectorstore ) prompt_template = &amp;quot;&amp;quot;&amp;quot;Based on the following information and being really specific about it&amp;#39;s data: &amp;#39;{text}&amp;#39;.\n\n Here are the goals, methodology, and conclusions/achievements of the paper, written as bullet points:&amp;quot;&amp;quot;&amp;quot; prompt = PromptTemplate.from_template(prompt_template) llm = Ollama(model=&amp;quot;llama3&amp;quot;) chain = ( retriever | prompt | llm ) result = chain.invoke({}) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The calling of the chain just seems too much like a workaround, since I didn&amp;#39;t have a specific question about the reference document, therefore I just had to use the prompt_template as the instruction to treat the pdf. It just seems like there are a lot of way to get to this same result. Whether to call the llm by it&amp;#39;s default completion object or through it&amp;#39;s chat variation. Whether to use a LLMChain(), a RetrievalQA.from_chain_type() or a simple chain() specifying it&amp;#39;s common parameters etc. Ins&amp;#39;t there a way to standardize this workflow according to your needs?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Gvascons&quot;&gt; /u/Gvascons &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dddjew/multiple_ways_to_get_to_the_same_result_w_rag/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dddjew/multiple_ways_to_get_to_the_same_result_w_rag/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dddjew</id><link href="https://www.reddit.com/r/LangChain/comments/1dddjew/multiple_ways_to_get_to_the_same_result_w_rag/" /><updated>2024-06-11T13:00:58+00:00</updated><published>2024-06-11T13:00:58+00:00</published><title>Multiple ways to get to the same result w/ RAG</title></entry><entry><author><name>/u/ss1seekining</name><uri>https://www.reddit.com/user/ss1seekining</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi guys I was struggling for quite sometime on how to host Langserve in AWS ECS. So prepared this two repos&lt;/p&gt; &lt;p&gt;1st one creates a VPC &lt;a href=&quot;https://github.com/mathlover777/shared-vpc&quot;&gt;https://github.com/mathlover777/shared-vpc&lt;/a&gt;&lt;/p&gt; &lt;p&gt;2nd one deploys in the same VPC &lt;a href=&quot;https://github.com/mathlover777/langserve-cdk-ecs&quot;&gt;https://github.com/mathlover777/langserve-cdk-ecs&lt;/a&gt;&lt;/p&gt; &lt;p&gt;You can deployment multiple stages in the same VPC also as AWS has a soft limit on number of VPCs.&lt;/p&gt; &lt;p&gt;This does not have autoscale added, as I dont know how to do it myself in ecs, will update when I get time.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/ss1seekining&quot;&gt; /u/ss1seekining &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ddb6qz/deploying_langserve_in_ecs_with_cdk/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ddb6qz/deploying_langserve_in_ecs_with_cdk/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1ddb6qz</id><link href="https://www.reddit.com/r/LangChain/comments/1ddb6qz/deploying_langserve_in_ecs_with_cdk/" /><updated>2024-06-11T10:50:50+00:00</updated><published>2024-06-11T10:50:50+00:00</published><title>Deploying Langserve in ECS with CDK</title></entry><entry><author><name>/u/Not-That-rpg</name><uri>https://www.reddit.com/user/Not-That-rpg</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Recently when I try to do &lt;code&gt;import langchain_anthropic&lt;/code&gt; I have been getting errors like this: ```&lt;/p&gt; &lt;blockquote&gt; &lt;blockquote&gt; &lt;blockquote&gt; &lt;p&gt;import langchain&lt;em&gt;anthropic Traceback (most recent call last): File &amp;quot;&amp;lt;stdin&amp;gt;&amp;quot;, line 1, in &amp;lt;module&amp;gt; File &amp;quot;/usr/local/lib/python3.12/site-packages/langchain_anthropic/&lt;/em&gt;&lt;em&gt;init&lt;/em&gt;&lt;em&gt;.py&amp;quot;, line 1, in &amp;lt;module&amp;gt; from langchain_anthropic.chat_models import ChatAnthropic, ChatAnthropicMessages File &amp;quot;/usr/local/lib/python3.12/site-packages/langchain_anthropic/chat_models.py&amp;quot;, line 26, in &amp;lt;module&amp;gt; from langchain_core.callbacks import ( File &amp;quot;/usr/local/lib/python3.12/site-packages/langchain_core/callbacks/&lt;/em&gt;&lt;em&gt;init&lt;/em&gt;&lt;em&gt;.py&amp;quot;, line 22, in &amp;lt;module&amp;gt; from langchain_core.callbacks.manager import ( File &amp;quot;/usr/local/lib/python3.12/site-packages/langchain_core/callbacks/manager.py&amp;quot;, line 29, in &amp;lt;module&amp;gt; from langsmith.run_helpers import get_run_tree_context File &amp;quot;/usr/local/lib/python3.12/site-packages/langsmith/run_helpers.py&amp;quot;, line 40, in &amp;lt;module&amp;gt; from langsmith import client as ls_client File &amp;quot;/usr/local/lib/python3.12/site-packages/langsmith/client.py&amp;quot;, line 52, in &amp;lt;module&amp;gt; from langsmith import env as ls_env File &amp;quot;/usr/local/lib/python3.12/site-packages/langsmith/env/&lt;/em&gt;&lt;em&gt;init&lt;/em&gt;&lt;em&gt;.py&amp;quot;, line 3, in &amp;lt;module&amp;gt; from langsmith.env._runtime_env import ( File &amp;quot;/usr/local/lib/python3.12/site-packages/langsmith/env/_runtime_env.py&amp;quot;, line 10, in &amp;lt;module&amp;gt; from langsmith.utils import get_docker_compose_command File &amp;quot;/usr/local/lib/python3.12/site-packages/langsmith/utils.py&amp;quot;, line 31, in &amp;lt;module&amp;gt; from langsmith import schemas as ls_schemas File &amp;quot;/usr/local/lib/python3.12/site-packages/langsmith/schemas.py&amp;quot;, line 69, in &amp;lt;module&amp;gt; class Example(ExampleBase): File &amp;quot;/usr/local/lib/python3.12/site-packages/pydantic/v1/main.py&amp;quot;, line 286, in __new&lt;/em&gt;_ cls.&lt;strong&gt;try_update_forward_refs&lt;/strong&gt;() File &amp;quot;/usr/local/lib/python3.12/site-packages/pydantic/v1/main.py&amp;quot;, line 807, in &lt;strong&gt;try_update_forward_refs&lt;/strong&gt; update&lt;em&gt;model_forward_refs(cls, cls.&lt;/em&gt;&lt;em&gt;fields&lt;/em&gt;&lt;em&gt;.values(), cls.&lt;/em&gt;&lt;em&gt;config&lt;/em&gt;&lt;em&gt;.json_encoders, localns, (NameError,)) File &amp;quot;/usr/local/lib/python3.12/site-packages/pydantic/v1/typing.py&amp;quot;, line 554, in update_model_forward_refs update_field_forward_refs(f, globalns=globalns, localns=localns) File &amp;quot;/usr/local/lib/python3.12/site-packages/pydantic/v1/typing.py&amp;quot;, line 520, in update_field_forward_refs field.type&lt;/em&gt; = evaluate&lt;em&gt;forwardref(field.type&lt;/em&gt;, globalns, localns or None) File &amp;quot;/usr/local/lib/python3.12/site-packages/pydantic/v1/typing.py&amp;quot;, line 66, in evaluate&lt;em&gt;forwardref return cast(Any, type&lt;/em&gt;)._evaluate(globalns, localns, set()) TypeError: ForwardRef._evaluate() missing 1 required keyword-only argument: &amp;#39;recursive_guard&amp;#39;&lt;/p&gt; &lt;p&gt;``&lt;code&gt; That smells like maybe there&amp;#39;s an un-captured dependency from the&lt;/code&gt;langchain_anthropic` code onto pydantic or something, but I don&amp;#39;t have any great ideas about how to debug this. Anyone else seen this? Any fix or work-around?&lt;/p&gt; &lt;/blockquote&gt; &lt;/blockquote&gt; &lt;/blockquote&gt; &lt;p&gt;Checking, it looks like I have &lt;code&gt;langchain-anthropic==0.1.13&lt;/code&gt; and &lt;code&gt;langchain==0.1.20&lt;/code&gt; if that helps.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Not-That-rpg&quot;&gt; /u/Not-That-rpg &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ddort5/errors_loading_langchain_anthropic/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ddort5/errors_loading_langchain_anthropic/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1ddort5</id><link href="https://www.reddit.com/r/LangChain/comments/1ddort5/errors_loading_langchain_anthropic/" /><updated>2024-06-11T20:50:13+00:00</updated><published>2024-06-11T20:50:13+00:00</published><title>Errors loading `langchain_anthropic`</title></entry><entry><author><name>/u/Sweaty-Wolf2228</name><uri>https://www.reddit.com/user/Sweaty-Wolf2228</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I use chainlit as UI in my rag system, the first qst passes ok, but when i inser the second one it translated automaticly to english. So How to strictly change translation for all users to a specific language? File xx-XX.json is placed in the translations folder. Deletion of en-US.json doesn&amp;#39;t work (it is generated each time again) &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Sweaty-Wolf2228&quot;&gt; /u/Sweaty-Wolf2228 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ddfqps/chainlit_translation_in_a_rag_system/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ddfqps/chainlit_translation_in_a_rag_system/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1ddfqps</id><link href="https://www.reddit.com/r/LangChain/comments/1ddfqps/chainlit_translation_in_a_rag_system/" /><updated>2024-06-11T14:39:07+00:00</updated><published>2024-06-11T14:39:07+00:00</published><title>Chainlit translation in a rag system</title></entry><entry><author><name>/u/pantulis</name><uri>https://www.reddit.com/user/pantulis</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hello, as a learning side project I am trying to have a simple Agent that queries an authenticated external API. Authentication is with a standard Bearer token.&lt;/p&gt; &lt;p&gt;I have two tools, one is called fetch_token that knows how to request a valid access token. And then there is another tool which does the real work and fetches certain value from an external https endpoint using the previously retrieved access token. These are non public APIs and in my tool functions I am using &amp;#39;requests&amp;#39; to programatically access and parse the JSON to extract the relevant values back to the Agent.&lt;/p&gt; &lt;p&gt;So given a user&amp;#39;s query, the Agent must invoke the first tool, fetch the access token and then invoke the second one passing the token as a parameter.&lt;/p&gt; &lt;p&gt;The thing is working, (yay!!), even when the input of the user makes the agent call the second tool repeatedly with different input values (but the same access token).&lt;/p&gt; &lt;p&gt;But my issue is that the agent is terribly slow. I suspect this happens because the bearer token (a quite long and random string, it is 2330 hexadecimal chars) is being passed each time to the LLM (OpenAI, &amp;#39;gpt4-turbo-preview&amp;#39;) and that takes a lot of context and processing for the LLM, which perhaps only be concerned with the fact that the access token is already present, not its value.&lt;/p&gt; &lt;p&gt;So I was thinking of storing the token in the Agent state, but I am not aware of a way that the output of a tool can be stored in the Agent state, and I also suspect that the whole Agent state is what is already being sent to the LLM so this would not defeat the purpose of this hoop.&lt;/p&gt; &lt;p&gt;So I am at a loss, my Agent is roughly working but is very slow! Are there any suggestions, resources or examples for this patterns?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/pantulis&quot;&gt; /u/pantulis &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ddl2k0/newbie_question_langgraph_and_authenticated_tools/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ddl2k0/newbie_question_langgraph_and_authenticated_tools/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1ddl2k0</id><link href="https://www.reddit.com/r/LangChain/comments/1ddl2k0/newbie_question_langgraph_and_authenticated_tools/" /><updated>2024-06-11T18:19:15+00:00</updated><published>2024-06-11T18:19:15+00:00</published><title>Newbie question: Langgraph and authenticated tools</title></entry><entry><author><name>/u/HotDogDelusions</name><uri>https://www.reddit.com/user/HotDogDelusions</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;So this is my first time ever hearing about vector databases - I know very little about them and I&amp;#39;m running into some trouble with a simple rag script I threw together.&lt;/p&gt; &lt;p&gt;I followed one of langchain&amp;#39;s documentation tutorials and was able to get a basic RAG setup going with some text files. Well now I&amp;#39;m trying to expand on it to be something useful - so I currently have ~800 documents totaling to just under 50 MB of data that I want to store in the vector DB. For some reason, the `Chroma.from_documents()` method will hang for a very long period of time (been running for over 10 minutes now) - and I can&amp;#39;t seem to figure out why it&amp;#39;s so slow.&lt;/p&gt; &lt;p&gt;Firstly, this `Chroma.from_documents` method - is what its doing called &amp;quot;indexing&amp;quot;? I keep seeing this term thrown around, not entirely sure what it means.&lt;/p&gt; &lt;p&gt;Second, Is it normal for creating the vector store to take this long with the amount of data I have? I figured 50MB of data and only 800 documents would be pretty trivial, as I&amp;#39;ve seen other posts about people having millions of documents.&lt;/p&gt; &lt;p&gt;Any help would be appreciated.&lt;/p&gt; &lt;p&gt;For reference, here is the relevant code:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;loader = DirectoryLoader(&amp;#39;docs2&amp;#39;, glob=&amp;#39;**/*.htm&amp;#39;, loader_cls=BSHTMLLoader, loader_kwargs={&amp;#39;open_encoding&amp;#39;: &amp;#39;utf8&amp;#39;}, show_progress=True, use_multithreading=True) docs = loader.load() text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000) splits = text_splitter.split_documents(docs) model = &amp;#39;Alibaba-NLP/gte-large-en-v1.5&amp;#39; model_kwargs = model_kwargs = {&amp;#39;device&amp;#39;:&amp;#39;cpu&amp;#39;, &amp;#39;trust_remote_code&amp;#39;: True} encode_kwargs = {&amp;#39;normalize_embeddings&amp;#39;: True} embeddings = HuggingFaceEmbeddings( model_name=model, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs ) vectorstore = Chroma.from_documents(documents=splits, embedding=embeddings) &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/HotDogDelusions&quot;&gt; /u/HotDogDelusions &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ddk9b1/chroma_db_taking_extremely_long_time_to_create/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ddk9b1/chroma_db_taking_extremely_long_time_to_create/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1ddk9b1</id><link href="https://www.reddit.com/r/LangChain/comments/1ddk9b1/chroma_db_taking_extremely_long_time_to_create/" /><updated>2024-06-11T17:46:56+00:00</updated><published>2024-06-11T17:46:56+00:00</published><title>Chroma DB taking extremely long time to create.</title></entry><entry><author><name>/u/Informal-Victory8655</name><uri>https://www.reddit.com/user/Informal-Victory8655</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi guys, I&amp;#39;m working on this project where initially we have 700 csvs to be ingested and build a poc with some ui for querying the csvs database with text and and metadata parameters selected via ui.&lt;/p&gt; &lt;p&gt;Need opinions fro experts on how to approach this project, considering the production use and ingesting more cvs into ai system. &lt;/p&gt; &lt;p&gt;For initial POC, I&amp;#39;m planning to use chromadb and streamlit for UI. Better options?&lt;/p&gt; &lt;p&gt;I can build above RAG, but I&amp;#39;m asking for expert opinions keeping in mind the production use case and scaling to more pdfs &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Informal-Victory8655&quot;&gt; /u/Informal-Victory8655 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dd8ssq/best_way_forward_and_vector_db_for_an_ai_rag/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dd8ssq/best_way_forward_and_vector_db_for_an_ai_rag/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dd8ssq</id><link href="https://www.reddit.com/r/LangChain/comments/1dd8ssq/best_way_forward_and_vector_db_for_an_ai_rag/" /><updated>2024-06-11T08:02:51+00:00</updated><published>2024-06-11T08:02:51+00:00</published><title>Best way forward and vector db for an AI RAG system for CVs ranking using query and some metadata</title></entry></feed>