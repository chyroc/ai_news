<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/"><category term="LangChain" label="r/LangChain"/><updated>2024-04-08T17:18:58+00:00</updated><icon>https://www.redditstatic.com/icon.png/</icon><id>/r/LangChain.rss</id><link rel="self" href="https://www.reddit.com/r/LangChain.rss" type="application/atom+xml" /><link rel="alternate" href="https://www.reddit.com/r/LangChain" type="text/html" /><subtitle>LangChain is an open-source framework and developer toolkit that helps developers get LLM applications from prototype to production. It is available for Python and Javascript at https://www.langchain.com/.</subtitle><title>LangChain</title><entry><author><name>/u/zchaarm</name><uri>https://www.reddit.com/user/zchaarm</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;A place for members of &lt;a href=&quot;/r/LangChain&quot;&gt;r/LangChain&lt;/a&gt; to chat with each other&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/zchaarm&quot;&gt; /u/zchaarm &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_10ljho9</id><link href="https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/" /><updated>2023-01-26T04:33:05+00:00</updated><published>2023-01-26T04:33:05+00:00</published><title>r/LangChain Lounge</title></entry><entry><author><name>/u/mehul_gupta1997</name><uri>https://www.reddit.com/user/mehul_gupta1997</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bz06c5/should_i_partner_with_packt_for_my_book_on/&quot;&gt; &lt;img src=&quot;https://preview.redd.it/lhxs7e5ht9tc1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f68902b93adb073db5b44516f1872c67a5bd793e&quot; alt=&quot;Should I partner with Packt for my book on LangChain?&quot; title=&quot;Should I partner with Packt for my book on LangChain?&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Recently I launched my debut book &amp;quot;LangChain in your Pocket: Beginners guide to building Generative AI applications using LLMs&amp;quot; which is going a bestseller since release. Recently, Packt, one of the biggest tech book publishers contacted me for partnering with them for the distribution of the book. As expected, it would reach a wider audience but the price may go up exponentially. What should I do?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/mehul_gupta1997&quot;&gt; /u/mehul_gupta1997 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://i.redd.it/lhxs7e5ht9tc1.png&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bz06c5/should_i_partner_with_packt_for_my_book_on/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1bz06c5</id><media:thumbnail url="https://preview.redd.it/lhxs7e5ht9tc1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=f68902b93adb073db5b44516f1872c67a5bd793e" /><link href="https://www.reddit.com/r/LangChain/comments/1bz06c5/should_i_partner_with_packt_for_my_book_on/" /><updated>2024-04-08T15:05:00+00:00</updated><published>2024-04-08T15:05:00+00:00</published><title>Should I partner with Packt for my book on LangChain?</title></entry><entry><author><name>/u/IlEstLaPapi</name><uri>https://www.reddit.com/user/IlEstLaPapi</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;tldr: Some insights and learnings from a LLM enthusiast working on a complex Chatbot using multiple agents built with LangGraph, LCEL and Chainlit.&lt;/p&gt; &lt;p&gt;Hi everyone! I have seen a lot of interest in multi-agent systems recently, and, as I&amp;#39;m currently working on a complex one, I thought I might as well share some feedback on my project. Maybe some of you might find it interesting, give some useful feedback, or make some suggestions.&lt;/p&gt; &lt;h2&gt;Introduction: Why am I doing this project?&lt;/h2&gt; &lt;p&gt;I&amp;#39;m a business owner and a tech guy with a background in math, coding, and ML. Since early 2023, I&amp;#39;ve fallen in love with the LLM world. So, I decided to start a new business with 2 friends: a consulting firm on generative AI. As expected, we don&amp;#39;t have many references. Thus, we decided to create a tool to demonstrate our skillset to potential clients.&lt;/p&gt; &lt;p&gt;After a brainstorm, we quickly identified that a) RAG is the main selling point, so we need something that uses a RAG; b) We believe in agents to automate tasks; c) ChatGPT has shown that asking questions to a chatbot is a much more human-friendly interface than a website; d) Our main weakness is that we are all tech guys, so we might as well compensate for that by building a seller.&lt;/p&gt; &lt;p&gt;From here, the idea was clear: instead, or more exactly, alongside our website, build a chatbot that would answer questions about our company, &amp;quot;sell&amp;quot; our offer, and potentially schedule meetings with our consultants. Then make some posts on LinkedIn and pray...&lt;/p&gt; &lt;p&gt;Spoiler alert: This project isn&amp;#39;t finished yet. The idea is to share some insights and learnings with the community and get some feedback.&lt;/p&gt; &lt;h2&gt;Functional specifications&lt;/h2&gt; &lt;p&gt;The first step was to list some specifications: * We want a RAG that can answer any question the user might have about our company. For that, we will use the content of the company website. Of course, we also need to prevent hallucination, especially on two topics: the website has no information about pricing, and we don&amp;#39;t offer SLAs. * We want it to answer as quickly as possible and limit the budget. For that, we will use smaller models like GPT-3.5 and Claude Haiku as often as possible. But that limits the reasoning capabilities of our agents, so we need to find a sweet spot. * We want consistency in the responses, which is a big problem for RAGs. Questions with similar meanings should generate the same answers, for example, &amp;quot;What&amp;#39;s your offer?&amp;quot;, &amp;quot;What services do you provide?&amp;quot;, and &amp;quot;What do you do?&amp;quot;. * Obviously, we don&amp;#39;t want visitors to be able to ask off-topic questions (e.g., &amp;quot;How is the weather in North Carolina?&amp;quot;), so we need a way to filter out off-topic, prompt injection, and toxic questions. * We want to demonstrate that GenAI can be used to deliver more than just chatbots, so we want the agents to be able to schedule meetings, send emails to visitors, etc. * Ideally, we also want the agents to be able to qualify the visitor: who they are, what their job is, what their organization is, whether they are a tech person or a manager, and if they are looking for something specific with a defined need or are just curious about us. * Ideally, we also want the agents to &amp;quot;sell&amp;quot; our company: if the visitor indicates their need, match it with our offer and &amp;quot;push&amp;quot; that offer. If they show some interest, let&amp;#39;s &amp;quot;push&amp;quot; for a meeting with our consultants!&lt;/p&gt; &lt;h2&gt;Architecture&lt;/h2&gt; &lt;h3&gt;Stack&lt;/h3&gt; &lt;p&gt;We aren&amp;#39;t a startup, we haven&amp;#39;t raised funds, and we don&amp;#39;t have months to do this. We can&amp;#39;t afford to spend more than 20 days to get an MVP. Besides, our main selling point is that GenAI projects don&amp;#39;t require as much time or budget as ML ones.&lt;/p&gt; &lt;p&gt;So, in order to move fast, we needed to use some open-source frameworks: * For the chatbot, the data is public, so let&amp;#39;s use GPT and Claude as they are the best right now and the API cost is low. * For the chatbot, Chainlit provides everything we need, except background processing. Let&amp;#39;s use that. * Langchain and LCEL are both flexible and unify the interfaces with the LLMs. * We&amp;#39;ll need a rather complicated agent workflow, in fact, multiple ones. LangGraph is more flexible than crew.ai or autogen. Let&amp;#39;s use that!&lt;/p&gt; &lt;h3&gt;Design and early versions&lt;/h3&gt; &lt;h4&gt;First version&lt;/h4&gt; &lt;p&gt;From the start, we knew it was impossible to do it using a &amp;quot;one prompt, one agent&amp;quot; solution. So we started with a 3-agent solution: one to &amp;quot;find&amp;quot; the required elements on our website (a RAG), one to sell and set up meetings, and one to generate the final answer.&lt;/p&gt; &lt;p&gt;The meeting logic was very easy to implement. However, as expected, the chatbot was hallucinating a lot: &amp;quot;Here is a full project for 1kâ‚¬, with an SLA 7/7 2 hours 99.999%&amp;quot;. And it was a bad seller, with conversations such as &amp;quot;Hi, who are you?&amp;quot; &amp;quot;I&amp;#39;m Sellbotix, how can I help you? Do you want a meeting with one of our consultants?&amp;quot;&lt;/p&gt; &lt;p&gt;At this stage, after 10 hours of work, we knew that it was probably doable but would require much more than 3 agents.&lt;/p&gt; &lt;h4&gt;Second version&lt;/h4&gt; &lt;p&gt;The second version used a more complex architecture: a guard to filter the questions, a strategist to make a plan, a seller to find some selling points, a seeker and a documentalist for the RAG, a secretary for the schedule meeting function, and a manager to coordinate everything.&lt;/p&gt; &lt;p&gt;It was slow, so we included logic to distribute the work between the agents in parallel. Sadly, this can&amp;#39;t be implemented using LangGraph, as all agent calls are made using coroutines but are awaited, and you can&amp;#39;t have parallel branches. So we implemented our own logic.&lt;/p&gt; &lt;p&gt;The result was much better, but far from perfect. And it was a nightmare to improve because changing one agent&amp;#39;s system prompt would generate side effects on most of the other agents. We also had a hard time defining what each agent would need to see and what to hide. Sending every piece of information to every agent is a waste of time and tokens.&lt;/p&gt; &lt;p&gt;And last but not least, the codebase was a mess as we did it in a rush. So we decided to restart from scratch.&lt;/p&gt; &lt;h2&gt;Third version, WIP&lt;/h2&gt; &lt;p&gt;So currently, we are working on the third version. This project is, by far, much more ambitious than what most of our clients ask us to do (another RAG?). And so far, we have learned a ton. I honestly don&amp;#39;t know if we will finish it, or even if it&amp;#39;s realistic, but it was worth it. &amp;quot;It isn&amp;#39;t the destination that matters, it&amp;#39;s the journey&amp;quot; has rarely been so true.&lt;/p&gt; &lt;p&gt;Currently, we are working on the architecture, and we have nearly finished it. Here are a few insights that we are using, and I wanted to share with you.&lt;/p&gt; &lt;h3&gt;Separation of concern&lt;/h3&gt; &lt;p&gt;The two main difficulties when working with a network of agents are a) they don&amp;#39;t know when to stop, and b) any change to any agent&amp;#39;s system prompt impacts the whole system. It&amp;#39;s hard to fix. When building a complex system, separation of concern is key: agents must be split into groups, each one with clear responsibilities and interfaces.&lt;/p&gt; &lt;p&gt;The cool thing is that a LangGraph graph is also a Runnable, so you can build graphs that use graphs. So we ended up with this: a main graph for the guard and final answer logic. It calls a &amp;quot;think&amp;quot; graph that decides which subgraphs should be called. Those are a &amp;quot;sell&amp;quot; graph, a &amp;quot;handle&amp;quot; graph, and a &amp;quot;find&amp;quot; graph (so far).&lt;/p&gt; &lt;h3&gt;Async, parallelism, and conditional calls&lt;/h3&gt; &lt;p&gt;If you want a system to be fast, you need to NOT call all the agents every time. For that, you need two things: a planner that decides which subgraph should be called (in our think graph), and you need to use &lt;code&gt;asyncio.gather&lt;/code&gt; instead of letting LangGraph call every graph and await them one by one.&lt;/p&gt; &lt;p&gt;So in the think graph, we have planner and manager agents. We use a standard doer/critic pattern here. When they agree on what needs to be done, they generate a list of instructions and activation orders for each subgraph that are passed to a &amp;quot;do&amp;quot; node. This node then creates a list of coroutines and awaits an &lt;code&gt;asyncio.gather&lt;/code&gt;.&lt;/p&gt; &lt;h3&gt;Limit what each graph must see&lt;/h3&gt; &lt;p&gt;We want the system to be fast and cost-efficient. Every node of every subgraph doesn&amp;#39;t need to be aware of what every other agent does. So we need to decide exactly what each agent gets as input. That&amp;#39;s honestly quite hard, but doable. It means fewer tokens, so it reduces the cost and speeds up the response.&lt;/p&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;This post is already quite long, so I won&amp;#39;t go into the details of every subgraph here. However, if you&amp;#39;re interested, feel free to let me know. I might decide to write some additional posts about those and the specific challenges we encountered and how we solved them (or not). In any case, if you&amp;#39;ve read this far, thank you!&lt;/p&gt; &lt;p&gt;If you have any feedback, don&amp;#39;t hesitate to share. I&amp;#39;d be very happy to read your thoughts and suggestions!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/IlEstLaPapi&quot;&gt; /u/IlEstLaPapi &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1byz3lr/insights_and_learnings_from_building_a_complex/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1byz3lr/insights_and_learnings_from_building_a_complex/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1byz3lr</id><link href="https://www.reddit.com/r/LangChain/comments/1byz3lr/insights_and_learnings_from_building_a_complex/" /><updated>2024-04-08T14:20:55+00:00</updated><published>2024-04-08T14:20:55+00:00</published><title>Insights and Learnings from Building a Complex Multi-Agent System</title></entry><entry><author><name>/u/KarbohJorneKraft</name><uri>https://www.reddit.com/user/KarbohJorneKraft</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I wrote this as the intro to a problem I am working in. anyone else thinking about this?&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Strategic Risk Reduction in AI Operations: Enhancing Systemic Controls in Agentic Workflows&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;It is possible for a workflow comprised of a chain of agentic assistants to drift away from the desired operational baseline without hallucinating or scoring poorly on standardized quality tests such as those for relevance, faithfulness, and alignment. Agentic assistants may be observed, measured, and managed on an individualized basis, but nodal evaluations may accurately analyze a point in time step in a workflow while failing to capture systemic distortion only observable at the workflow level while the agentic workflows are in motion.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Agentic Workflow Distortion in the Absence of Systemic Self-Reflection&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The concepts of Agentic Workflow Distortion and Systemic Self-Reflection pertain to the dynamics and evaluation within automated systems, particularly those that are structured around the autonomous operation of individual agents, referred to as &amp;quot;agentic workflows.&amp;quot; .....tbc&lt;/p&gt; &lt;p&gt;** I have the rest of this writeup if is anyone is interested &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/KarbohJorneKraft&quot;&gt; /u/KarbohJorneKraft &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1byyx9f/agentic_workflow_distortion_in_the_absence_of/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1byyx9f/agentic_workflow_distortion_in_the_absence_of/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1byyx9f</id><link href="https://www.reddit.com/r/LangChain/comments/1byyx9f/agentic_workflow_distortion_in_the_absence_of/" /><updated>2024-04-08T14:13:31+00:00</updated><published>2024-04-08T14:13:31+00:00</published><title>Agentic Workflow Distortion in the Absence of Systemic Self-Reflection</title></entry><entry><author><name>/u/cryptokaykay</name><uri>https://www.reddit.com/user/cryptokaykay</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1byrqps/langtrace_preview_of_the_new_evaluation_dashboard/&quot;&gt; &lt;img src=&quot;https://external-preview.redd.it/Z86Ol2Jj2ayoGOd-Pfgkem_533RuJprm3gD5eJdyH8c.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=040d97e52a5d67184964ab3339a8b243b2414601&quot; alt=&quot;Langtrace: Preview of the new Evaluation dashboard&quot; title=&quot;Langtrace: Preview of the new Evaluation dashboard&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey,&lt;/p&gt; &lt;p&gt;I am building an open source project called Langtrace which lets you monitor, debug and evaluate the LLM requests made by your application.&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/Scale3-Labs/langtrace&quot;&gt;https://github.com/Scale3-Labs/langtrace&lt;/a&gt; . The integration is only 2 lines of code.&lt;/p&gt; &lt;p&gt;Currently building an Evaluations dashboard which is launching this week. It lets you do the following:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Create tests - like factual accuracy, bias detection etc. &lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Automatically capture the LLM calls to specific tests by passing a testId to the langtrace SDK installed in your code.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Evaluate and measure the overall success % and how success % trends over time.&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;The goal here is to get confidence with the model or RAG before deploying it to production.&lt;/p&gt; &lt;p&gt;Please check out the repository. Would love to hear your thoughts! Thanks!&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/5bracw5ki7tc1.png?width=2932&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1fe6fac6661d9a5c0c7f701c44d50435f45c7d7f&quot;&gt;https://preview.redd.it/5bracw5ki7tc1.png?width=2932&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1fe6fac6661d9a5c0c7f701c44d50435f45c7d7f&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/cryptokaykay&quot;&gt; /u/cryptokaykay &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1byrqps/langtrace_preview_of_the_new_evaluation_dashboard/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1byrqps/langtrace_preview_of_the_new_evaluation_dashboard/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1byrqps</id><media:thumbnail url="https://external-preview.redd.it/Z86Ol2Jj2ayoGOd-Pfgkem_533RuJprm3gD5eJdyH8c.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=040d97e52a5d67184964ab3339a8b243b2414601" /><link href="https://www.reddit.com/r/LangChain/comments/1byrqps/langtrace_preview_of_the_new_evaluation_dashboard/" /><updated>2024-04-08T07:21:23+00:00</updated><published>2024-04-08T07:21:23+00:00</published><title>Langtrace: Preview of the new Evaluation dashboard</title></entry><entry><author><name>/u/ArcuisAlezanzo</name><uri>https://www.reddit.com/user/ArcuisAlezanzo</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;m building RAG chat bot with azure open ai and azure ai search&lt;/p&gt; &lt;p&gt;Right now I&amp;#39;m only developing POC &lt;/p&gt; &lt;p&gt;using streamlit as frontend and maintain chat history using session state (not focusing on persistent state rgt now)&lt;/p&gt; &lt;p&gt;Data ingestion pipeline is big headache for me even with static data(no updation once used )&lt;/p&gt; &lt;p&gt;Working with PDF , PPT , docx , excel(mostly technical documentation and excel is logs of ticket)&lt;/p&gt; &lt;p&gt;Each pdf have different structure Ppt is in different structure Docx is in diff structure &lt;/p&gt; &lt;p&gt;Right now , i converted ppt and docx to pdf and used pymupdf to extract text and chucked with recursive character split(size =1024, overlap=120) Used direct pymupdf lib instead of langchain abstract why you ask?&lt;/p&gt; &lt;p&gt;LET us consider PDF A &lt;/p&gt; &lt;p&gt;PDF A consist most of images with little text so chunking page wise results in character length of 150&lt;/p&gt; &lt;p&gt;So I extracted whole pdf text applied chunking on that. &lt;strong&gt;Is this optimal way ? I lose meta data by this method.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Coming to logs of excel . TBH I don&amp;#39;t know how work with this. I combined important columns as one column.&lt;/p&gt; &lt;p&gt;For Eg: (Title: .... Desc: ....) I combined Title and desc column as one with column heading in each row so context not missed out. Here also Each row converted as document object with size comes around 250 characters.&lt;/p&gt; &lt;p&gt;I feel wasting resource too much &lt;/p&gt; &lt;p&gt;What best ways you guys would suggest?&lt;/p&gt; &lt;p&gt;Thanks in advance&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/ArcuisAlezanzo&quot;&gt; /u/ArcuisAlezanzo &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bz2wpr/need_help_finding_better_methods_implementing_rag/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bz2wpr/need_help_finding_better_methods_implementing_rag/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bz2wpr</id><link href="https://www.reddit.com/r/LangChain/comments/1bz2wpr/need_help_finding_better_methods_implementing_rag/" /><updated>2024-04-08T16:51:45+00:00</updated><published>2024-04-08T16:51:45+00:00</published><title>Need help finding better methods implementing RAG</title></entry><entry><author><name>/u/jzone3</name><uri>https://www.reddit.com/user/jzone3</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Open source language models are no serious competitors. I have been migrating a lot of my prompts to open source models, and I wrote up this tutorial about how I do it.&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://blog.promptlayer.com/migrating-prompts-to-open-source-models-c21e1d482d6f&quot;&gt;https://blog.promptlayer.com/migrating-prompts-to-open-source-models-c21e1d482d6f&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/jzone3&quot;&gt; /u/jzone3 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bz1cuq/migrating_my_prompts_to_open_source_language/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bz1cuq/migrating_my_prompts_to_open_source_language/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bz1cuq</id><link href="https://www.reddit.com/r/LangChain/comments/1bz1cuq/migrating_my_prompts_to_open_source_language/" /><updated>2024-04-08T15:52:00+00:00</updated><published>2024-04-08T15:52:00+00:00</published><title>Migrating my prompts to open source language models</title></entry><entry><author><name>/u/_depressedmillenial</name><uri>https://www.reddit.com/user/_depressedmillenial</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;So I want to automate the conversion of a legal document (5-20 pages) into a different type of document with plain/lay English + adhere to a specific style and format guidelines (20-100 pages) that are in 3 separate reference pdf documents.&lt;/p&gt; &lt;p&gt;I tried the simplest approach I could think of at first, which was extracting and then providing the expected output format (headers/sub-headers) in my prompt using a &amp;quot;custom GPT&amp;quot; on the openai front-end, as well as a one-shot example pair of legal doc/converted doc in the prompt window, plus I also uploaded the reference docs to the customgpt (which I think is used as RAG by the GPT?). &lt;/p&gt; &lt;p&gt;The result is okay - it gets the format right for the most part, but it ignores many of the style guidelines, and summarizes a lot needlessly which leads to information loss. &lt;/p&gt; &lt;p&gt;I want to now try either more advanced RAG (I am a python user and with the exception of recent LCEL releases, am familiar with Langchain as well as LlamaIndex), but was also considering finetuning llama-2 with 4-bit quantization. &lt;/p&gt; &lt;p&gt;Is finetuning without a label even possible in this case? What RAG retrievers or embeddings would you suggest? Any other suggestions? Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/_depressedmillenial&quot;&gt; /u/_depressedmillenial &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bz17bg/texttotext_generation_finetuning_vs_rag_vs_fewshot/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bz17bg/texttotext_generation_finetuning_vs_rag_vs_fewshot/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bz17bg</id><link href="https://www.reddit.com/r/LangChain/comments/1bz17bg/texttotext_generation_finetuning_vs_rag_vs_fewshot/" /><updated>2024-04-08T15:45:39+00:00</updated><published>2024-04-08T15:45:39+00:00</published><title>Text-to-text generation: finetuning vs RAG vs few-shot?</title></entry><entry><author><name>/u/Mediocre-Card8046</name><uri>https://www.reddit.com/user/Mediocre-Card8046</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;I was wondering if anyone tried out to use a retriever from Llamaindex as retriever in a Langchain chain?&lt;/p&gt; &lt;p&gt;For me this is interesting because for now it is difficult to persistently save a ParentDocumentRetriever in Langchain but I think this is possible with Llamaindex. So I thought I am just using the Llamaindex retriever and pass the results to my chain.&lt;/p&gt; &lt;p&gt;Is there anything I should consider or are there any expericences with this approach?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Mediocre-Card8046&quot;&gt; /u/Mediocre-Card8046 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1byvt0t/use_llamaindex_retriever_in_langchain_chain/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1byvt0t/use_llamaindex_retriever_in_langchain_chain/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1byvt0t</id><link href="https://www.reddit.com/r/LangChain/comments/1byvt0t/use_llamaindex_retriever_in_langchain_chain/" /><updated>2024-04-08T11:46:19+00:00</updated><published>2024-04-08T11:46:19+00:00</published><title>Use Llamaindex Retriever in Langchain chain</title></entry><entry><author><name>/u/Zealousideal_Wolf624</name><uri>https://www.reddit.com/user/Zealousideal_Wolf624</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I just read their &lt;a href=&quot;https://blog.langchain.dev/langchain-documentation-refresh/&quot;&gt;new blog post&lt;/a&gt;, about the new documentation website. It&amp;#39;s very curious and funny.&lt;/p&gt; &lt;p&gt;It goes through the Diataxis taxonomy for documentation, which I find useful and aligns with how my brain works.&lt;/p&gt; &lt;p&gt;Just to throw everything out of the window and say: we mixed and matched every section of Diataxis and you can find tutorials spread all over the place, mixed with reference and explanations!&lt;/p&gt; &lt;p&gt;Take a look at this section of the post:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;This section should contain mostly conceptual Tutorials, References, and Explanations of the components they cover.&lt;/p&gt; &lt;p&gt;Note: As a general rule of thumb, everything covered in the Expression Language and Components sections (with the exception of the Composition section of components) should cover only components that exist in langchain_core.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Impressive! They need to explain what&amp;#39;s where, and even introduce a rule about langchain_core that is broken from the get go. And when you go to the socs the components section isn&amp;#39;t even in the menu to be selected!&lt;/p&gt; &lt;p&gt;I mean, just make it simple:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Tutorials (quick start, use cases with in depth explanations, etc)&lt;/li&gt; &lt;li&gt;How to guides (terse, context free guides such as how to create a chain, new runnable from scratch, new agent from scratch, how to visualize a chain, how to pass a system prompt to a model, how to make models spit structured output, etc)&lt;/li&gt; &lt;li&gt;Explanation (langchain purpose, package organization, what is LCEL, what is a chain/agent/runnable/etc, model vs chat model, what is a tool/toolkit, what is a function call etc). Accept a small amount of repetition from what we have in tutorials.&lt;/li&gt; &lt;li&gt;Reference (API docs)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Wouldn&amp;#39;t that be simpler? I&amp;#39;m so frustrated with this...&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Zealousideal_Wolf624&quot;&gt; /u/Zealousideal_Wolf624 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1by72bo/new_documentation_is_still_bad/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1by72bo/new_documentation_is_still_bad/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1by72bo</id><link href="https://www.reddit.com/r/LangChain/comments/1by72bo/new_documentation_is_still_bad/" /><updated>2024-04-07T15:24:32+00:00</updated><published>2024-04-07T15:24:32+00:00</published><title>New documentation is still bad</title></entry><entry><author><name>/u/OtherAd3010</name><uri>https://www.reddit.com/user/OtherAd3010</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Tiger: Neuralink for AI Agents (MIT) (Python)&lt;/p&gt; &lt;p&gt;Hello, we are developing a superstructure that provides an AI-Computer interface for AI agents created through the LangChain library, we have published it completely openly under the MIT license.&lt;/p&gt; &lt;p&gt;What it does: Just like human developers, it has some abilities such as running the codes it writes, making mouse and keyboard movements, writing and running Python functions for functions it does not have. AI literally thinks and the interface we provide transforms with real computer actions.&lt;/p&gt; &lt;p&gt;Those who want to contribute can provide support under the MIT license and code conduct. &lt;a href=&quot;https://github.com/Upsonic/Tiger&quot;&gt;https://github.com/Upsonic/Tiger&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/OtherAd3010&quot;&gt; /u/OtherAd3010 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bykpua/github_upsonictiger_neuralink_for_your_langchain/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bykpua/github_upsonictiger_neuralink_for_your_langchain/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bykpua</id><link href="https://www.reddit.com/r/LangChain/comments/1bykpua/github_upsonictiger_neuralink_for_your_langchain/" /><updated>2024-04-08T00:56:17+00:00</updated><published>2024-04-08T00:56:17+00:00</published><title>GitHub - Upsonic/Tiger: Neuralink for your LangChain Agents</title></entry><entry><author><name>/u/Vri7</name><uri>https://www.reddit.com/user/Vri7</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;While going through the Langchain documentation, I came across the fact that LC is providing separate html and markdown splitter and you also have an option for the same two in code splitter as well.&lt;br/&gt; What is the difference between the two?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Vri7&quot;&gt; /u/Vri7 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1byrbxw/htmlmarkdown_splitter_vs_code_splitter_with/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1byrbxw/htmlmarkdown_splitter_vs_code_splitter_with/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1byrbxw</id><link href="https://www.reddit.com/r/LangChain/comments/1byrbxw/htmlmarkdown_splitter_vs_code_splitter_with/" /><updated>2024-04-08T06:54:02+00:00</updated><published>2024-04-08T06:54:02+00:00</published><title>HTML/MARKDOWN splitter vs Code splitter with html/markdown as language.</title></entry><entry><author><name>/u/Calm_Pea_2428</name><uri>https://www.reddit.com/user/Calm_Pea_2428</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Do you guys really think that using DSPy is a good idea over Langchain? For me I think, DSPy is not mature enough and LangChain provides so many things.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Calm_Pea_2428&quot;&gt; /u/Calm_Pea_2428 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1byn9o7/langchain_vs_dspy/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1byn9o7/langchain_vs_dspy/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1byn9o7</id><link href="https://www.reddit.com/r/LangChain/comments/1byn9o7/langchain_vs_dspy/" /><updated>2024-04-08T02:59:53+00:00</updated><published>2024-04-08T02:59:53+00:00</published><title>LangChain vs DSPy</title></entry><entry><author><name>/u/Calm_Pea_2428</name><uri>https://www.reddit.com/user/Calm_Pea_2428</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;m thinking about writing a detailed blog on the Challenges you face while scaling your RAG apps. Please comment some suggestions you would like me to discuss in the blog. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Calm_Pea_2428&quot;&gt; /u/Calm_Pea_2428 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1by7s2m/challenges_of_scaling_rag_applications/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1by7s2m/challenges_of_scaling_rag_applications/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1by7s2m</id><link href="https://www.reddit.com/r/LangChain/comments/1by7s2m/challenges_of_scaling_rag_applications/" /><updated>2024-04-07T15:55:01+00:00</updated><published>2024-04-07T15:55:01+00:00</published><title>Challenges of Scaling RAG applications</title></entry><entry><author><name>/u/Proud_Plant_826</name><uri>https://www.reddit.com/user/Proud_Plant_826</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I want to make a website where I want to display the responses I am generating using langchain agent. It is actually analysis of educational data, and generated responses are helpful to be used as teaching material. Hence, I want to document it. What would be the easiest and best way to direclty document it onto the website? It would be better to document the intermediate steps generated by the agent as well. Thank you&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Proud_Plant_826&quot;&gt; /u/Proud_Plant_826 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1byqonr/creation_of_website_to_visualize_responses/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1byqonr/creation_of_website_to_visualize_responses/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1byqonr</id><link href="https://www.reddit.com/r/LangChain/comments/1byqonr/creation_of_website_to_visualize_responses/" /><updated>2024-04-08T06:11:13+00:00</updated><published>2024-04-08T06:11:13+00:00</published><title>Creation of website to visualize responses generated by LangChain pandas dataframe agent</title></entry><entry><author><name>/u/Turbulent_Month_8771</name><uri>https://www.reddit.com/user/Turbulent_Month_8771</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey LangChain devs! ðŸ‘‹ Check out SolidEdge â€“ the open source code comprehension tool you&amp;#39;ve been waiting for. Powered by GPT and built by devs like us, it&amp;#39;s your personal coding sidekick. With Notion integration, it keeps you aligned with business goals. No more feeling lost in code â€“ embrace true developer-centric coding! Star us on [GitHub](&lt;a href=&quot;https://github.com/AI-Citizen/SolidGPT&quot;&gt;https://github.com/AI-Citizen/SolidGPT&lt;/a&gt;) and dive in on the [VSCode Marketplace](&lt;a href=&quot;https://marketplace.visualstudio.com/items?itemName=AICT.solidgpt&quot;&gt;https://marketplace.visualstudio.com/items?itemName=AICT.solidgpt&lt;/a&gt;). Let&amp;#39;s revolutionize coding together! ðŸš€&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Turbulent_Month_8771&quot;&gt; /u/Turbulent_Month_8771 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1byk8nk/solidedge_the_open_source_developerfirst_code/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1byk8nk/solidedge_the_open_source_developerfirst_code/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1byk8nk</id><link href="https://www.reddit.com/r/LangChain/comments/1byk8nk/solidedge_the_open_source_developerfirst_code/" /><updated>2024-04-08T00:33:56+00:00</updated><published>2024-04-08T00:33:56+00:00</published><title>ðŸš€ SolidEdge - The Open Source, Developer-First Code Comprehension Tool with Notion Integration</title></entry><entry><author><name>/u/wordlessilence</name><uri>https://www.reddit.com/user/wordlessilence</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I just joined here not long ago. I have learned knowledges from the official documents,but i think it&amp;#39;s not suitable for me. I have some programming basics in python,i hope to get some friendly and excellent tutorials that use ollama to create rather than openai based in langchain.In additional,i want to know how to get high-quality information sources.English is not my native language,i&amp;#39;m sorry to any grammar errors.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/wordlessilence&quot;&gt; /u/wordlessilence &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1byn1u2/help_for_tutorial/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1byn1u2/help_for_tutorial/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1byn1u2</id><link href="https://www.reddit.com/r/LangChain/comments/1byn1u2/help_for_tutorial/" /><updated>2024-04-08T02:48:51+00:00</updated><published>2024-04-08T02:48:51+00:00</published><title>Help for tutorial</title></entry><entry><author><name>/u/alexndr2022</name><uri>https://www.reddit.com/user/alexndr2022</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;m currently working with AWS Bedrock and Langchain while it retrieves good answers when I want to ask it stuff like comparing documents or listing the documents on its data sources Its unable to do it. It seems like its not conscious of its environment. Does anyone has some experience working with this? Like I want it to be a typical RAG application based on some documents but I want it to be conscious of the data it have like comparing versions of the same documents...&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/alexndr2022&quot;&gt; /u/alexndr2022 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1by8u01/how_to_make_a_rag_conscious_of_the_documents_it/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1by8u01/how_to_make_a_rag_conscious_of_the_documents_it/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1by8u01</id><link href="https://www.reddit.com/r/LangChain/comments/1by8u01/how_to_make_a_rag_conscious_of_the_documents_it/" /><updated>2024-04-07T16:39:39+00:00</updated><published>2024-04-07T16:39:39+00:00</published><title>How to make a RAG conscious of the documents it have? Amazon Bedrock</title></entry><entry><author><name>/u/UpvoteBeast</name><uri>https://www.reddit.com/user/UpvoteBeast</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;We are trying to get our feet wet with RAG with a small engineering team. I want to build a RAG system querying an extensive internal documents system. With the available choice of LLMs, embedding models, vector databases, hyperparameters it&amp;#39;s easy to get overwhelmed. So what I want is to create a test dataset manually with like ten-twenty questions and answers we would like to receive (or multiple answer options for each question??) and automate deployment of several combinations of different LLMs, hyperparameters, embedding models, etc and compare the actuals against the gold standard answers (using ROUGE score maybe??). Does that make sense? Are there any tools/frameworks I need to be aware of to do something like that for me? Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/UpvoteBeast&quot;&gt; /u/UpvoteBeast &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1by54rq/evaluating_rag_on_custom_qas/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1by54rq/evaluating_rag_on_custom_qas/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1by54rq</id><link href="https://www.reddit.com/r/LangChain/comments/1by54rq/evaluating_rag_on_custom_qas/" /><updated>2024-04-07T13:58:11+00:00</updated><published>2024-04-07T13:58:11+00:00</published><title>Evaluating RAG on custom Q&amp;As</title></entry><entry><author><name>/u/No_Garbage9512</name><uri>https://www.reddit.com/user/No_Garbage9512</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I have been working on the large data. The data consist of talks from the different anchor persons, also there are comments in numerical representation like for example &amp;quot;&lt;em&gt;how many people agreed and how many are disagreed&amp;quot;.&lt;/em&gt; and on which session they are disscusing the point of agenda and on which bill number they place a talk. &lt;/p&gt; &lt;p&gt;So my question is: I want to generate the document a large document which contains multiple sections almost 20 sections. Each section has diverse and different instructions so how do I manage my vectordb calls. ? because each section of the document is different so how to make a calls to retreival automatically based on the sections conditions. ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/No_Garbage9512&quot;&gt; /u/No_Garbage9512 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1by9iqn/working_with_diverse_data_to_create_30_to_35/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1by9iqn/working_with_diverse_data_to_create_30_to_35/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1by9iqn</id><link href="https://www.reddit.com/r/LangChain/comments/1by9iqn/working_with_diverse_data_to_create_30_to_35/" /><updated>2024-04-07T17:08:55+00:00</updated><published>2024-04-07T17:08:55+00:00</published><title>Working with diverse data to create 30 to 35 pages document and Managing the retrieval.</title></entry><entry><author><name>/u/Me7a1hed</name><uri>https://www.reddit.com/user/Me7a1hed</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;ve been playing around with pulling data from APIs, feeding that into the language model&amp;#39;s chat and then conversing over that context. One recent use case I&amp;#39;ve been considering is allowing the agent to do multiple tasks, so the API calls are determined by the AI solution rather than being hard coded.&lt;/p&gt; &lt;p&gt;The obvious solution to me is to use a conversational chat agent that can react and use tools that make the API calls. The problem I&amp;#39;m experiencing though is that the returned content from the API call function doesn&amp;#39;t appear to get stored in my ConversationBufferMemory. It only stores the human inputs, and the AI&amp;#39;s outputs, but not the &amp;quot;Observation&amp;quot; that contains the returned full context.&lt;/p&gt; &lt;p&gt;The reason I&amp;#39;d like this in the history is to prevent the need for additional API calls when the user begins to ask follow-up questions about the data which has already been retrieved by a tool. It seems very inefficient to pull the same data every time they have a follow-up question, which is what it&amp;#39;s currently doing. Below is an example of what I&amp;#39;m doing.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;template = &amp;quot;First, evaluate the context provided by preceding messages to inform your response. Only if you can&amp;#39;t answer the questions from that should you use available tools.&amp;quot; memory = ConversationBufferMemory( memory_key=&amp;quot;chat_history&amp;quot;, return_messages=True ) tools = [get_ticket_info] agent = initialize_agent( tools=tools, llm=llm, agent=&amp;#39;chat-conversational-react-description&amp;#39;, memory=memory, verbose=True, agent_kwargs={&amp;quot;prefix&amp;quot;: template} ) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;I&amp;#39;ve also tried using the create_react_agent() function to create the agent which I know is the new way to do this, but it doesn&amp;#39;t appear to be conversational as it will not remember any message history. This is why I&amp;#39;m using the initialize_agent() function instead. At this point I&amp;#39;m tempted to go away with LangChain and use OpenAI&amp;#39;s API directly where I would have more control over what goes into the message history.&lt;/p&gt; &lt;p&gt;Any tricks you&amp;#39;re aware of, or am I approaching this in a naive way? Appreciate any wisdom you can bestow.&lt;/p&gt; &lt;p&gt;Edit: I got this to work easily enough using OpenAI directly. See my response below to one of my replies if interested.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Me7a1hed&quot;&gt; /u/Me7a1hed &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1byd0e8/storing_tool_retrieved_data_in_conversation/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1byd0e8/storing_tool_retrieved_data_in_conversation/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1byd0e8</id><link href="https://www.reddit.com/r/LangChain/comments/1byd0e8/storing_tool_retrieved_data_in_conversation/" /><updated>2024-04-07T19:31:38+00:00</updated><published>2024-04-07T19:31:38+00:00</published><title>Storing tool retrieved data in conversation history</title></entry><entry><author><name>/u/redd-dev</name><uri>https://www.reddit.com/user/redd-dev</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey guys, so I am building a chatbot which uses a RAG-tuned LLM in AWS Bedrock (and deployed using AWS Lambda endpoints).&lt;/p&gt; &lt;p&gt;How do I avoid my LLM from being having to be RAG-tuned every single time a user asks his/her first question? I am thinking of storing the RAG-tuned LLM in an AWS S3 bucket. If I do this, I believe I will have to store the LLM model parameters and the vector store index in the S3 bucket. Doing this would mean every single time a user asks his/her first question (and subsequent questions), I will just be loading the the RAG-tuned LLM from the S3 bucket (rather than having to run RAG-tuning every single time when a user asks his/her first question, which will save me RAG-tuning costs and latency).&lt;/p&gt; &lt;p&gt;Would this design work? I have a sample of my script below:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;import os import json import boto3 from langchain.document_loaders import PyPDFLoader from langchain.text_splitter import RecursiveCharacterTextSplitter from langchain.embeddings import BedrockEmbeddings from langchain.vectorstores import FAISS from langchain.indexes import VectorstoreIndexCreator from langchain.llms.bedrock import Bedrock def save_to_s3(model_params, vector_store_index, bucket_name, model_key, index_key): s3 = boto3.client(&amp;#39;s3&amp;#39;) # Save model parameters to S3 s3.put_object(Body=model_params, Bucket=bucket_name, Key=model_key) # Save vector store index to S3 s3.put_object(Body=vector_store_index, Bucket=bucket_name, Key=index_key) def load_from_s3(bucket_name, model_key, index_key): s3 = boto3.client(&amp;#39;s3&amp;#39;) # Load model parameters from S3 model_params = s3.get_object(Bucket=bucket_name, Key=model_key)[&amp;#39;Body&amp;#39;].read() # Load vector store index from S3 vector_store_index = s3.get_object(Bucket=bucket_name, Key=index_key)[&amp;#39;Body&amp;#39;].read() return model_params, vector_store_index def initialize_hr_system(bucket_name, model_key, index_key): s3 = boto3.client(&amp;#39;s3&amp;#39;) try: # Check if model parameters and vector store index exist in S3 s3.head_object(Bucket=bucket_name, Key=model_key) s3.head_object(Bucket=bucket_name, Key=index_key) # Load model parameters and vector store index from S3 model_params, vector_store_index = load_from_s3(bucket_name, model_key, index_key) # Deserialize and reconstruct the RAG-tuned LLM and vector store index llm = Bedrock.deserialize(json.loads(model_params)) index = VectorstoreIndexCreator.deserialize(json.loads(vector_store_index)) except s3.exceptions.ClientError: # Model parameters and vector store index don&amp;#39;t exist in S3 # Create them and save to S3 data_load = PyPDFLoader(&amp;#39;Glossary_of_Terms.pdf&amp;#39;) data_split = RecursiveCharacterTextSplitter(separators=[&amp;quot;\n\n&amp;quot;, &amp;quot;\n&amp;quot;, &amp;quot; &amp;quot;, &amp;quot;&amp;quot;], chunk_size=100, chunk_overlap=10) data_embeddings = BedrockEmbeddings(credentials_profile_name=&amp;#39;default&amp;#39;, model_id=&amp;#39;amazon.titan-embed-text-v1&amp;#39;) data_index = VectorstoreIndexCreator(text_splitter=data_split, embedding=data_embeddings, vectorstore_cls=FAISS) index = data_index.from_loaders([data_load]) llm = Bedrock( credentials_profile_name=&amp;#39;default&amp;#39;, model_id=&amp;#39;mistral.mixtral-8x7b-instruct-v0:1&amp;#39;, model_kwargs={ &amp;quot;max_tokens_to_sample&amp;quot;: 3000, &amp;quot;temperature&amp;quot;: 0.1, &amp;quot;top_p&amp;quot;: 0.9 } ) # Serialize model parameters and vector store index serialized_model_params = json.dumps(llm.serialize()) serialized_vector_store_index = json.dumps(index.serialize()) # Save model parameters and vector store index to S3 save_to_s3(serialized_model_params, serialized_vector_store_index, bucket_name, model_key, index_key) return index, llm def hr_rag_response(index, llm, question): hr_rag_query = index.query(question=question, llm=llm) return hr_rag_query # S3 bucket configuration bucket_name = &amp;#39;your-bucket-name&amp;#39; model_key = &amp;#39;models/chatbot_model.json&amp;#39; index_key = &amp;#39;indexes/chatbot_index.json&amp;#39; # Initialize the system index, llm = initialize_hr_system(bucket_name, model_key, index_key) # Serve user requests while True: user_question = input(&amp;quot;User: &amp;quot;) response = hr_rag_response(index, llm, user_question) print(&amp;quot;Chatbot:&amp;quot;, response) &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/redd-dev&quot;&gt; /u/redd-dev &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1by17qh/how_to_deploy_a_ragtuned_ai_chatbotllm_using_aws/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1by17qh/how_to_deploy_a_ragtuned_ai_chatbotllm_using_aws/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1by17qh</id><link href="https://www.reddit.com/r/LangChain/comments/1by17qh/how_to_deploy_a_ragtuned_ai_chatbotllm_using_aws/" /><updated>2024-04-07T10:23:23+00:00</updated><published>2024-04-07T10:23:23+00:00</published><title>How to deploy a RAG-tuned AI chatbot/LLM using AWS Bedrock (with Langchain functions)</title></entry><entry><author><name>/u/Crazy_Cut_7250</name><uri>https://www.reddit.com/user/Crazy_Cut_7250</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1by66py/quota_exceeded_error/&quot;&gt; &lt;img src=&quot;https://preview.redd.it/5gxw1qi4l2tc1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=de243bd8dbaaf01ba810158680c37480a924fc4a&quot; alt=&quot;Quota exceeded error&quot; title=&quot;Quota exceeded error&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Crazy_Cut_7250&quot;&gt; /u/Crazy_Cut_7250 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://i.redd.it/5gxw1qi4l2tc1.png&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1by66py/quota_exceeded_error/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1by66py</id><media:thumbnail url="https://preview.redd.it/5gxw1qi4l2tc1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=de243bd8dbaaf01ba810158680c37480a924fc4a" /><link href="https://www.reddit.com/r/LangChain/comments/1by66py/quota_exceeded_error/" /><updated>2024-04-07T14:45:55+00:00</updated><published>2024-04-07T14:45:55+00:00</published><title>Quota exceeded error</title></entry><entry><author><name>/u/chaitu9701</name><uri>https://www.reddit.com/user/chaitu9701</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I have one banking related document with several overlapping topics. Say one topic is related to credit card request, another related to cheque book request, another relating to account deactivation request. Mind that each of topic in itself are lengthy.&lt;/p&gt; &lt;p&gt;When in the retrieval chain, I ask a question &amp;quot;how to raise requests&amp;quot;, the result is a mixture from all of the above topics. First few lines describe credit card procedure and then bridge to checkbook. Which is wrong as each process has a different steps.&lt;/p&gt; &lt;p&gt;I&amp;#39;m using chunking strategy of 1000, default sentence transformers embedding, qdrant for as retriever, and gpt3.5 turbo 16k for llm.&lt;/p&gt; &lt;p&gt;Also the llm gives a disclaimer/note at the end saying that steps vary per organisation. Tried several prompts to remove disclaimer but nothing seems to work.&lt;/p&gt; &lt;p&gt;Any help / prompt would be greatly appreciated.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/chaitu9701&quot;&gt; /u/chaitu9701 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bxwxtu/rag_returns_concocted_results/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bxwxtu/rag_returns_concocted_results/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bxwxtu</id><link href="https://www.reddit.com/r/LangChain/comments/1bxwxtu/rag_returns_concocted_results/" /><updated>2024-04-07T05:48:32+00:00</updated><published>2024-04-07T05:48:32+00:00</published><title>RAG returns concocted results</title></entry><entry><author><name>/u/ramirez_tn</name><uri>https://www.reddit.com/user/ramirez_tn</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;ol&gt; &lt;li&gt;I loaded and split a PDF document using PDFMiner (I also tried a couple of other loaders)&lt;/li&gt; &lt;li&gt;I embedded the result and stored it in VectorDB&lt;/li&gt; &lt;li&gt;I retrieved the Data with RetrievalQA and a question like &amp;quot;What did this document say about Eye safety ?&amp;quot; which is mentioned a couple of times in the 80 pages document&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;The LLM always answers with : &amp;quot;it looks like there nothing mentioned about Eye safety &amp;quot;&lt;/p&gt; &lt;p&gt;FYI: When I check how the PDF is loaded it shows the content related to eye safety in the pages but it has a lot of \n and it include headers. I don&amp;#39;t know if this is contributing to the bad behavior&lt;/p&gt; &lt;p&gt;I am new to Langchain and it is driving me crazy, please help !&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/ramirez_tn&quot;&gt; /u/ramirez_tn &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bxs6g9/retriever_is_not_returning_proper_answers_to/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bxs6g9/retriever_is_not_returning_proper_answers_to/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bxs6g9</id><link href="https://www.reddit.com/r/LangChain/comments/1bxs6g9/retriever_is_not_returning_proper_answers_to/" /><updated>2024-04-07T01:33:13+00:00</updated><published>2024-04-07T01:33:13+00:00</published><title>retriever is not returning proper answers to obvious questions</title></entry><entry><author><name>/u/ZuckyFox</name><uri>https://www.reddit.com/user/ZuckyFox</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bxs70y/gemini/&quot;&gt; &lt;img src=&quot;https://external-preview.redd.it/MTd0eWtwZnZueXNjMY4zffG6ljuKgIwhcpPfTvcb1KD0RlSYVhImNFkRFBXF.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a49cb05502dd40e63d0e9349ef7de5ad9cbcbbdd&quot; alt=&quot;Gemini ðŸ‘ðŸŒš&quot; title=&quot;Gemini ðŸ‘ðŸŒš&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Never knew 14 yrs ago, Rick astley taught about LangChain through his songs. ðŸ¤¯ðŸ˜‚ðŸ˜‚&lt;/p&gt; &lt;h1&gt;aiml #aiforfun #rofl #gemini #google&lt;/h1&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/ZuckyFox&quot;&gt; /u/ZuckyFox &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://v.redd.it/um4exjmvnysc1&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bxs70y/gemini/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1bxs70y</id><media:thumbnail url="https://external-preview.redd.it/MTd0eWtwZnZueXNjMY4zffG6ljuKgIwhcpPfTvcb1KD0RlSYVhImNFkRFBXF.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=a49cb05502dd40e63d0e9349ef7de5ad9cbcbbdd" /><link href="https://www.reddit.com/r/LangChain/comments/1bxs70y/gemini/" /><updated>2024-04-07T01:34:04+00:00</updated><published>2024-04-07T01:34:04+00:00</published><title>Gemini ðŸ‘ðŸŒš</title></entry></feed>