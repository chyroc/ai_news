<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/"><category term="LangChain" label="r/LangChain"/><updated>2024-05-26T04:05:30+00:00</updated><icon>https://www.redditstatic.com/icon.png/</icon><id>/r/LangChain.rss</id><link rel="self" href="https://www.reddit.com/r/LangChain.rss" type="application/atom+xml" /><link rel="alternate" href="https://www.reddit.com/r/LangChain" type="text/html" /><subtitle>LangChain is an open-source framework and developer toolkit that helps developers get LLM applications from prototype to production. It is available for Python and Javascript at https://www.langchain.com/.</subtitle><title>LangChain</title><entry><author><name>/u/zchaarm</name><uri>https://www.reddit.com/user/zchaarm</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;A place for members of &lt;a href=&quot;/r/LangChain&quot;&gt;r/LangChain&lt;/a&gt; to chat with each other&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/zchaarm&quot;&gt; /u/zchaarm &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_10ljho9</id><link href="https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/" /><updated>2023-01-26T04:33:05+00:00</updated><published>2023-01-26T04:33:05+00:00</published><title>r/LangChain Lounge</title></entry><entry><author><name>/u/learning-machine1964</name><uri>https://www.reddit.com/user/learning-machine1964</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey guys, I am using langchain to generate structured output, but for some reason, the output is not properly formatted json so when I run json.loads(), it gives me an error. I tried to make a clean_response function but there are too many edge cases to consider. Am I using the wrong function to get the LLM output? Thanks in advance!&lt;/p&gt; &lt;p&gt;Here is my code:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;from abc import ABC import os import pydantic from langchain_groq import ChatGroq from langchain.pydantic_v1 import validator from langchain.output_parsers import PydanticOutputParser from langchain.prompts import PromptTemplate from langchain.pydantic_v1 import BaseModel import json from dotenv import load_dotenv load_dotenv() tags_req = [&amp;quot;HS Seniors Only&amp;quot;, &amp;quot;Need Based&amp;quot;, &amp;quot;Merit Based&amp;quot;, &amp;quot;Essay Required&amp;quot;, &amp;quot;US Citizen&amp;quot;, &amp;quot;Arts&amp;quot;, &amp;quot;STEM&amp;quot;, &amp;quot;Community Service&amp;quot;, &amp;quot;Leadership&amp;quot;] class Tags(BaseModel): tags: list[str] @validator(&amp;#39;tags&amp;#39;) def must_use_only_these_values(cls, v): for tag in v: if tag not in tags_req: raise ValueError(f&amp;quot;tag {tag} is not in {tags_req}&amp;quot;) return v class Description(BaseModel): description: str class Generator(ABC): def __init__(self ) -&amp;gt; None: pass def generate(self, *args): pass class GroqGenerator(Generator): def __init__( self, ): super().__init__() self.llm = ChatGroq( temperature=0.1, groq_api_key=os.getenv(&amp;quot;GROQ_API_KEY&amp;quot;), model_name=&amp;quot;mixtral-8x7b-32768&amp;quot;, ) def generate( self, name, available, opens, closes, details, description, need_based, merit_based, ): msg1 = ( f&amp;quot;Name: {name}\nAvailable: {available}\nOpens: {opens}\nCloses: {closes}\nDetails: {details}\nDescription: {description}\nNeed Based: {need_based}\nMerit Based: {merit_based}\n\n Generate Below: &amp;quot; ) msg2 = ( f&amp;quot;\nName: {name}\nAvailable: {available}\nOpens: {opens}\nCloses: {closes}\nDetails: {details}\nDescription: {description}\nNeed Based: {need_based}\nMerit Based: {merit_based}\n\n Generate Below: &amp;quot;, ) tags = self.inference(msg1, Tags)[&amp;quot;tags&amp;quot;] desc = self.inference(msg2, Description)[&amp;quot;description&amp;quot;] return tags, desc def inference(self, msg, pydantic_object): parser = PydanticOutputParser(pydantic_object=pydantic_object) if pydantic_object.__class__.__name__ == &amp;quot;Tags&amp;quot;: prompt = PromptTemplate( template=&amp;quot;Please follow the instructions of the following user query.\n{format_instructions}\n{query}\n&amp;quot;, input_variables=[&amp;quot;query&amp;quot;], partial_variables={&amp;quot;format_instructions&amp;quot;: parser.get_format_instructions() + f&amp;quot; Make sure only generate the array from this bank of tags and no other tags: {tags_req}&amp;quot;}, ) else: prompt = PromptTemplate( template=&amp;quot;Please follow the instructions of the following user query.\n{format_instructions}\n{query}\n&amp;quot;, input_variables=[&amp;quot;query&amp;quot;], partial_variables={&amp;quot;format_instructions&amp;quot;: parser.get_format_instructions()}, ) _input = prompt.format_prompt(query=msg) response = self.llm.invoke(_input.to_string()).content cleaned_response = self.clean_response(response) print(cleaned_response) return json.loads(cleaned_response) def clean_response(self, response): # Replace curly quotes and other problematic characters response = response.replace(&amp;#39;“&amp;#39;, &amp;#39;&amp;quot;&amp;#39;).replace(&amp;#39;”&amp;#39;, &amp;#39;&amp;quot;&amp;#39;) response = response.replace(&amp;quot;‘&amp;quot;, &amp;quot;&amp;#39;&amp;quot;).replace(&amp;quot;’&amp;quot;, &amp;quot;&amp;#39;&amp;quot;) return response def test(self, msg): return self.inference(msg) if __name__ == &amp;quot;__main__&amp;quot;: groq = GroqGenerator([&amp;quot;scholarship&amp;quot;]) print(groq.test(&amp;quot;Q: What is a scholarship? A:&amp;quot;)) &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/learning-machine1964&quot;&gt; /u/learning-machine1964 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d0q4g6/am_i_doing_something_wrong_in_my_code/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d0q4g6/am_i_doing_something_wrong_in_my_code/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1d0q4g6</id><link href="https://www.reddit.com/r/LangChain/comments/1d0q4g6/am_i_doing_something_wrong_in_my_code/" /><updated>2024-05-26T01:02:40+00:00</updated><published>2024-05-26T01:02:40+00:00</published><title>Am I doing something wrong in my code?</title></entry><entry><author><name>/u/NoAssumption999</name><uri>https://www.reddit.com/user/NoAssumption999</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hello,&lt;/p&gt; &lt;p&gt;I want to extract paragraphs, title etc from a PDF while.maintaining the separation boundaries. What library to use?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/NoAssumption999&quot;&gt; /u/NoAssumption999 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d0ryqx/extract_text_from_pdf_maintaining_partitions/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d0ryqx/extract_text_from_pdf_maintaining_partitions/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1d0ryqx</id><link href="https://www.reddit.com/r/LangChain/comments/1d0ryqx/extract_text_from_pdf_maintaining_partitions/" /><updated>2024-05-26T02:51:43+00:00</updated><published>2024-05-26T02:51:43+00:00</published><title>Extract text from PDF maintaining partitions</title></entry><entry><author><name>/u/throwaway0134hdj</name><uri>https://www.reddit.com/user/throwaway0134hdj</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;When you are making RAG chatbots using Graph and Vectors how are you storing the internal data? What’s the general approach?&lt;/p&gt; &lt;p&gt;For example, say you are asked to ingest all your companies files, like word docs PDFs and everything in between. If you use RAG with Graph and Vector embeddedings where are you storing the data from the documents? I’m curious what the general approach is to chunking, tokenizing, and embedding are?&lt;/p&gt; &lt;p&gt;If you had to ingest your companies documents using a RAG, Graph, and vector approach how would you set this up? What would the schema be of the Graph, where would the vectors be stored?&lt;/p&gt; &lt;p&gt;Thanks&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/throwaway0134hdj&quot;&gt; /u/throwaway0134hdj &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d0j0hc/how_do_you_go_about_creating_a_rag_chatbot_using/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d0j0hc/how_do_you_go_about_creating_a_rag_chatbot_using/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1d0j0hc</id><link href="https://www.reddit.com/r/LangChain/comments/1d0j0hc/how_do_you_go_about_creating_a_rag_chatbot_using/" /><updated>2024-05-25T19:03:06+00:00</updated><published>2024-05-25T19:03:06+00:00</published><title>How do you go about creating a RAG chatbot using Graph and Vector on internal documents?</title></entry><entry><author><name>/u/Puzzleheaded_Move35</name><uri>https://www.reddit.com/user/Puzzleheaded_Move35</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hello, I am trying to create a chatbot using Langchain where I am using RetrievalQA and OpenAI API. I need to create chains where if the user asks a question which is unrelated to the context, basically retrieve from a document provided, the chatbot should bypass the retrieval steps and just answer the query directly. And if it asks related questions it should apply RAG and retrieve the relevant info to answer the questions. I am totally stuck here and don’t know how to move forward. Any help will be appreciated. &lt;/p&gt; &lt;p&gt;Code:&lt;/p&gt; &lt;p&gt;llm = ChatOpenAI( api_key= api_key,&lt;br/&gt; # openai_api_key= os.environ[&amp;quot;OPENAI_API_KEY&amp;quot;],&lt;br/&gt; model_name=&amp;#39;gpt-4o&amp;#39; &lt;/p&gt; &lt;p&gt;) template = &amp;quot;&amp;quot;&amp;quot; Use the following context provided (delimited by &amp;lt;ctx&amp;gt;&amp;lt;/ctx&amp;gt;), answer the questions properly and the chat history (delimited by &amp;lt;hs&amp;gt;&amp;lt;/hs&amp;gt;) to answer the questions from the user. &lt;/p&gt; &lt;h2&gt;If they are asking questions not related to the context, skip performing RAG and just straight up answer their query&amp;quot;:&lt;/h2&gt; &lt;p&gt;&amp;lt;ctx&amp;gt; {context}&lt;/p&gt; &lt;h2&gt;&amp;lt;/ctx&amp;gt;&lt;/h2&gt; &lt;p&gt;&amp;lt;hs&amp;gt; {history}&lt;/p&gt; &lt;h2&gt;&amp;lt;/hs&amp;gt;&lt;/h2&gt; &lt;p&gt;{question} Answer: &amp;quot;&amp;quot;&amp;quot; prompt = PromptTemplate( input_variables=[&amp;quot;history&amp;quot;, &amp;quot;context&amp;quot;, &amp;quot;question&amp;quot;], template=template, )&lt;/p&gt; &lt;p&gt;memory = ConversationBufferMemory( memory_key=&amp;quot;history&amp;quot;, input_key=&amp;quot;question&amp;quot; )&lt;/p&gt; &lt;p&gt;qa = RetrievalQA.from_chain_type( llm=llm, chain_type=&amp;#39;stuff&amp;#39;, retriever=vectorstore.as_retriever(search_type=&amp;quot;similarity&amp;quot;, search_kwargs={&amp;#39;k&amp;#39;: 20}), verbose=True, chain_type_kwargs={ &amp;quot;verbose&amp;quot;: True, &amp;quot;prompt&amp;quot;: prompt, &amp;quot;memory&amp;quot;: memory, } )&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Puzzleheaded_Move35&quot;&gt; /u/Puzzleheaded_Move35 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d0e7ov/how_to_ignore_retrieval_step_rag_when_it_is_not/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d0e7ov/how_to_ignore_retrieval_step_rag_when_it_is_not/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1d0e7ov</id><link href="https://www.reddit.com/r/LangChain/comments/1d0e7ov/how_to_ignore_retrieval_step_rag_when_it_is_not/" /><updated>2024-05-25T15:15:49+00:00</updated><published>2024-05-25T15:15:49+00:00</published><title>How to ignore retrieval step (RAG) when it is not necessary</title></entry><entry><author><name>/u/AnEdgeLordWeeb</name><uri>https://www.reddit.com/user/AnEdgeLordWeeb</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey guys. I&amp;#39;m trying to get comfortable with LangGraph in an attempt to then develop a chatbot based on this framework.&lt;br/&gt; When trying to test the idea of a node in my chatbot, I found myself faced with this error.&lt;br/&gt; Could somebody please help me understand what&amp;#39;s wrong with my code and how can I solve this problem?&lt;br/&gt; I would be truly thankful!&lt;/p&gt; &lt;p&gt;The code:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;from typing import Annotated, List from langchain_openai import ChatOpenAI from langchain_community.tools.tavily_search import TavilySearchResults from langchain_core.prompts import ChatPromptTemplate, PromptTemplate from langchain_core.messages import BaseMessage from typing_extensions import TypedDict from langgraph.graph import StateGraph, END from langgraph.graph.message import add_messages from langgraph.prebuilt import ToolNode, tools_condition # Define AgentState class with the proper typing class AgentState(TypedDict): messages: Annotated[List[BaseMessage], add_messages] query: str games: List[str] # Initialize the tool and LLM tool = TavilySearchResults(max_results=3) tools = [tool] llm = ChatOpenAI(model=&amp;quot;gpt-3.5-turbo&amp;quot;, temperature=0) llm_with_tools = llm.bind_tools(tools) # Define the function for the game title search def game_title_search(state: AgentState): game_search_prompt = PromptTemplate( template=&amp;quot;&amp;quot;&amp;quot;You are part of a chatbot that provides personalized video game recommendations based on user preferences. \n Your task is to search for video games that match the user query, using the Tavily API. \n Only return the titles of the games. \n The number of games to return is limited to 5. \n\n The results provided will look as follows (Python list): \n [&amp;#39;game_title_1&amp;#39;, &amp;#39;game_title_2&amp;#39;, &amp;#39;game_title_3&amp;#39;, ...] User Query: {query}&amp;quot;&amp;quot;&amp;quot;, input_variables=[&amp;quot;query&amp;quot;], ) game_search = game_search_prompt | llm_with_tools game_search_result = game_search.invoke({&amp;quot;query&amp;quot;: state[&amp;quot;query&amp;quot;]}) return {&amp;quot;messages&amp;quot;: [game_search_result]} # Also, I need to extract the game titles from the tool&amp;#39;s results and update the state attribute &amp;quot;games&amp;quot; - how can I do this? # Build the graph graph_builder = StateGraph(AgentState) graph_builder.add_node(&amp;quot;game_search&amp;quot;, game_title_search) tool_node = ToolNode(tools=[tool]) graph_builder.add_node(&amp;quot;tools&amp;quot;, tool_node) graph_builder.add_conditional_edges( &amp;quot;game_search&amp;quot;, tools_condition, ) graph_builder.add_edge(&amp;quot;tools&amp;quot;, &amp;quot;game_search&amp;quot;) graph_builder.set_entry_point(&amp;quot;game_search&amp;quot;) graph = graph_builder.compile() # Define the initial state input_state = { &amp;quot;messages&amp;quot;: [], &amp;quot;query&amp;quot;: &amp;quot;&amp;quot;, &amp;quot;games&amp;quot;: [] } user_input = &amp;quot;What games are similar to The Witcher 3?&amp;quot; input_state[&amp;quot;query&amp;quot;] = user_input input_state[&amp;quot;messages&amp;quot;] = [(&amp;quot;user&amp;quot;, user_input)] output = graph.invoke(input_state, config={&amp;quot;recursion_limit&amp;quot;: 50}) print(output) &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/AnEdgeLordWeeb&quot;&gt; /u/AnEdgeLordWeeb &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d0om4f/help_recursion_limit_when_trying_to_use_chain/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d0om4f/help_recursion_limit_when_trying_to_use_chain/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1d0om4f</id><link href="https://www.reddit.com/r/LangChain/comments/1d0om4f/help_recursion_limit_when_trying_to_use_chain/" /><updated>2024-05-25T23:38:32+00:00</updated><published>2024-05-25T23:38:32+00:00</published><title>Help! &quot;Recursion limit&quot; when trying to use chain with &quot;llm.bind_tools&quot; - LangGraph</title></entry><entry><author><name>/u/mehul_gupta1997</name><uri>https://www.reddit.com/user/mehul_gupta1997</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d00vla/my_langchain_book_now_available_on_packt_and/&quot;&gt; &lt;img src=&quot;https://b.thumbs.redditmedia.com/6qJv0lGS_OUWt49TIW0VLit2wRHq59aNal2DwSpSfgU.jpg&quot; alt=&quot;My LangChain book now available on Packt and O'Reilly&quot; title=&quot;My LangChain book now available on Packt and O'Reilly&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;m glad to share that my debut book, &amp;quot;&lt;strong&gt;LangChain in your Pocket: Beginner&amp;#39;s Guide to Building Generative AI Applications using LLMs,&lt;/strong&gt;&amp;quot; has been republished by Packt and is now available on their official website and partner publications like O&amp;#39;Reilly, Barnes &amp;amp; Noble, etc. A big thanks for the support! The first version is still available on Amazon&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/5b0trmcl7h2d1.png?width=1080&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6f12126f846d5fc174768628ebc42c9921017687&quot;&gt;https://preview.redd.it/5b0trmcl7h2d1.png?width=1080&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6f12126f846d5fc174768628ebc42c9921017687&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/4xdgzk9l7h2d1.png?width=1080&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bfe4aac06ce89bff475a415b8c0091f830ba10e3&quot;&gt;https://preview.redd.it/4xdgzk9l7h2d1.png?width=1080&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bfe4aac06ce89bff475a415b8c0091f830ba10e3&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/mehul_gupta1997&quot;&gt; /u/mehul_gupta1997 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d00vla/my_langchain_book_now_available_on_packt_and/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d00vla/my_langchain_book_now_available_on_packt_and/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1d00vla</id><media:thumbnail url="https://b.thumbs.redditmedia.com/6qJv0lGS_OUWt49TIW0VLit2wRHq59aNal2DwSpSfgU.jpg" /><link href="https://www.reddit.com/r/LangChain/comments/1d00vla/my_langchain_book_now_available_on_packt_and/" /><updated>2024-05-25T01:32:55+00:00</updated><published>2024-05-25T01:32:55+00:00</published><title>My LangChain book now available on Packt and O'Reilly</title></entry><entry><author><name>/u/AustinChanKL</name><uri>https://www.reddit.com/user/AustinChanKL</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hello everyone, need some pointer here. Is there a way to exclude intermediate steps when streaming the events with AgentExecutor? I only want the agent to stream the final output without streaming the intermediate steps (Observations).&lt;/p&gt; &lt;p&gt;So I&amp;#39;m using create_tool_calling_agent() to create an agent and passing multiple tools. And one of the tool itself actually is using langchain csv agent. When the agent using this particular tool, it start to stream the intermediate observation steps in my chat application, which is weird for my user point of view. I want to exclude all the intermediate steps from this tool.&lt;/p&gt; &lt;p&gt;When I was using langchain==0.1.16, it behave exactly what I wanted, which it won&amp;#39;t stream any output or intermediate steps from the tool that using langchain csv agent. After I upgrade it to 0.2.1, it started to stream intermediate steps.&lt;/p&gt; &lt;p&gt;Here&amp;#39;s some example code I have:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;async def chat_stream(...) -&amp;gt; str: prompt = ChatPromptTemplate.from_messages(...) tools = [query_data_from_csv] llm = ChatOpenAI(...) agent = create_tool_calling_agent(llm, tools, prompt) agent_executor = AgentExecutor(agent=agent, tools=tools, handle_parsing_errors=True) return agent_executor.astream_events(..., version=&amp;quot;v2&amp;quot;) @ tool def query_data_from_csv(question: str, csv_url: str) -&amp;gt; str: &amp;quot;&amp;quot;&amp;quot;Tool to query and interact with data in CSV format.&amp;quot;&amp;quot;&amp;quot; ai_agent = create_csv_agent( ChatOpenAI(temperature=0, model=&amp;quot;gpt-4-turbo&amp;quot;), csv_url, agent_type=AgentType.ZERO_SHOT_REACT_DESCRIPTION, return_intermediate_steps=False ) return ai_agent.invoke(question) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Your help is greatly appreciated. Thank you.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/AustinChanKL&quot;&gt; /u/AustinChanKL &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d0b3zs/is_there_anyway_to_prevent_agentexecutorastream/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d0b3zs/is_there_anyway_to_prevent_agentexecutorastream/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1d0b3zs</id><link href="https://www.reddit.com/r/LangChain/comments/1d0b3zs/is_there_anyway_to_prevent_agentexecutorastream/" /><updated>2024-05-25T12:37:48+00:00</updated><published>2024-05-25T12:37:48+00:00</published><title>Is there anyway to prevent AgentExecutor.astream_events() streaming intermediate steps?</title></entry><entry><author><name>/u/Pokedrive123</name><uri>https://www.reddit.com/user/Pokedrive123</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I wanted to know if the chain method and ChatOpenAI from langchain_openai supports gpt4o image inputs and if there are any guides out there showing us how to use it? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Pokedrive123&quot;&gt; /u/Pokedrive123 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d03y7b/has_langchain_been_updated_with_gpt4o/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d03y7b/has_langchain_been_updated_with_gpt4o/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1d03y7b</id><link href="https://www.reddit.com/r/LangChain/comments/1d03y7b/has_langchain_been_updated_with_gpt4o/" /><updated>2024-05-25T04:30:41+00:00</updated><published>2024-05-25T04:30:41+00:00</published><title>Has langchain been updated with Gpt4o?</title></entry><entry><author><name>/u/UpvoteBeast</name><uri>https://www.reddit.com/user/UpvoteBeast</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1czfu8v/understanding_the_magic_deconstructing_langchains/&quot;&gt; &lt;img src=&quot;https://external-preview.redd.it/PuvfwiESasEaQDZvjllJMtQHd1LLNsCK92LgBTOvras.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=dc7f8a79051cfb7f73b1c8cdc014ce2e73509532&quot; alt=&quot;Understanding the Magic: Deconstructing Langchain’s SQL Agent &quot; title=&quot;Understanding the Magic: Deconstructing Langchain’s SQL Agent &quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/UpvoteBeast&quot;&gt; /u/UpvoteBeast &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://dly.to/K6CJFoxPldx&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1czfu8v/understanding_the_magic_deconstructing_langchains/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1czfu8v</id><media:thumbnail url="https://external-preview.redd.it/PuvfwiESasEaQDZvjllJMtQHd1LLNsCK92LgBTOvras.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=dc7f8a79051cfb7f73b1c8cdc014ce2e73509532" /><link href="https://www.reddit.com/r/LangChain/comments/1czfu8v/understanding_the_magic_deconstructing_langchains/" /><updated>2024-05-24T08:20:52+00:00</updated><published>2024-05-24T08:20:52+00:00</published><title>Understanding the Magic: Deconstructing Langchain’s SQL Agent</title></entry><entry><author><name>/u/nik0-bellic</name><uri>https://www.reddit.com/user/nik0-bellic</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Im planning to do an endpoint that given a user question it makes the underlying work to get the query and i would like to only receive the final answer as im going to show it on a Streamlit chat app. Any idea on how to extract only that?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/nik0-bellic&quot;&gt; /u/nik0-bellic &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1czv6k8/how_could_i_just_return_the_final_answer_from_sql/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1czv6k8/how_could_i_just_return_the_final_answer_from_sql/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1czv6k8</id><link href="https://www.reddit.com/r/LangChain/comments/1czv6k8/how_could_i_just_return_the_final_answer_from_sql/" /><updated>2024-05-24T20:59:31+00:00</updated><published>2024-05-24T20:59:31+00:00</published><title>How could I just return the final answer from SQL Agent?</title></entry><entry><author><name>/u/newpeak</name><uri>https://www.reddit.com/user/newpeak</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1czk90g/implementing_a_longcontext_rag_based_on_raptor/&quot;&gt; &lt;img src=&quot;https://external-preview.redd.it/0cS0XK03rFRxZqaK6QPMgfSagZE4Vn9YYciJnzYH9mk.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=77af57d9f34ce52533ece926c923557a6f272dbb&quot; alt=&quot; Implementing a long-context RAG based on RAPTOR &quot; title=&quot; Implementing a long-context RAG based on RAPTOR &quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/newpeak&quot;&gt; /u/newpeak &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://medium.com/@infiniflowai/implementing-a-long-context-rag-based-on-raptor-0538a354ada3&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1czk90g/implementing_a_longcontext_rag_based_on_raptor/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1czk90g</id><media:thumbnail url="https://external-preview.redd.it/0cS0XK03rFRxZqaK6QPMgfSagZE4Vn9YYciJnzYH9mk.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=77af57d9f34ce52533ece926c923557a6f272dbb" /><link href="https://www.reddit.com/r/LangChain/comments/1czk90g/implementing_a_longcontext_rag_based_on_raptor/" /><updated>2024-05-24T13:04:48+00:00</updated><published>2024-05-24T13:04:48+00:00</published><title>Implementing a long-context RAG based on RAPTOR</title></entry><entry><author><name>/u/Ok_Comfort_4103</name><uri>https://www.reddit.com/user/Ok_Comfort_4103</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;m new to LLMs, but I&amp;#39;m planning to build an application that answers technical questions about my API using a RAG system based on my tech docs (e.g., &amp;quot;how can I configure the API request to wait for the response and to retry if the request times out?&amp;quot;). To summarize ...Goal: answer technical questions (found in my /docs/ subdirectory) so the user doesn&amp;#39;t have to search and dig for it. My Plan: &lt;/p&gt; &lt;ol&gt; &lt;li&gt;Setup LangChain (are there any tips or tricks I should keep in mind?)&lt;/li&gt; &lt;li&gt;Connect to Weaviate (if there&amp;#39;s a better VectorDB to use, I&amp;#39;d love recommendations &amp;amp; rationale)&lt;/li&gt; &lt;li&gt;Connect to ChatGPT 3.5 (pls let me know if there&amp;#39;s a better model to use)&lt;/li&gt; &lt;li&gt;Vectorize my /docs/ directory (I&amp;#39;ve never done this before, but it looks like I need a chunking strategy &amp;amp; embedding model)&lt;/li&gt; &lt;li&gt;Create &amp;amp; style a simple modal UI for the chatbot in Airtable (again, if there&amp;#39;s a better/quicker way to do this, I&amp;#39;m all ears)&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;I also want to make it not simply a Q&amp;amp;A bot so that if the answer returned is not valid to the user&amp;#39;s use case, there is a feedback query used to improve the process, giving the tool either more context or showing where the documentation should be expanded/refined.&lt;/p&gt; &lt;p&gt;Thanks in advance for any guidance and/or pitfalls to avoid :-)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Ok_Comfort_4103&quot;&gt; /u/Ok_Comfort_4103 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1czronv/attempt_to_be_forwardlooking_on_a_new_project/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1czronv/attempt_to_be_forwardlooking_on_a_new_project/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1czronv</id><link href="https://www.reddit.com/r/LangChain/comments/1czronv/attempt_to_be_forwardlooking_on_a_new_project/" /><updated>2024-05-24T18:28:41+00:00</updated><published>2024-05-24T18:28:41+00:00</published><title>Attempt to be Forward-looking on a New Project</title></entry><entry><author><name>/u/cryptokaykay</name><uri>https://www.reddit.com/user/cryptokaykay</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Looking to understand what evaluation tools/methods people like and use the most?&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://www.reddit.com/poll/1czpsmq&quot;&gt;View Poll&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/cryptokaykay&quot;&gt; /u/cryptokaykay &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1czpsmq/what_evaluation_toolsmethods_do_you_use/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1czpsmq/what_evaluation_toolsmethods_do_you_use/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1czpsmq</id><link href="https://www.reddit.com/r/LangChain/comments/1czpsmq/what_evaluation_toolsmethods_do_you_use/" /><updated>2024-05-24T17:08:15+00:00</updated><published>2024-05-24T17:08:15+00:00</published><title>What evaluation tools/methods do you use?</title></entry><entry><author><name>/u/Lablade04</name><uri>https://www.reddit.com/user/Lablade04</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey folks! My team recently worked on building this bot to help orgs monitor and even forecast costs on the Snowflake Data Warehouse. We used LangChain, Streamlit, Snowflake Arctic + Cortex and GPT 4 Turbo for this. &lt;/p&gt; &lt;p&gt;We just open sourced this Agent and even wrote a guide on how to create one yourself, check it out here: &lt;a href=&quot;https://medium.com/snowflake/crystalcosts-building-an-ai-agent-for-cost-monitoring-on-snowflake-c9d49645f5c4&quot;&gt;https://medium.com/snowflake/crystalcosts-building-an-ai-agent-for-cost-monitoring-on-snowflake-c9d49645f5c4&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Would love to get inputs on this! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Lablade04&quot;&gt; /u/Lablade04 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1czfe4s/ai_agent_for_monitoring_snowflake_costs/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1czfe4s/ai_agent_for_monitoring_snowflake_costs/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1czfe4s</id><link href="https://www.reddit.com/r/LangChain/comments/1czfe4s/ai_agent_for_monitoring_snowflake_costs/" /><updated>2024-05-24T07:47:01+00:00</updated><published>2024-05-24T07:47:01+00:00</published><title>AI Agent for monitoring Snowflake Costs!</title></entry><entry><author><name>/u/business24_ai</name><uri>https://www.reddit.com/user/business24_ai</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;&lt;a href=&quot;https://youtu.be/gflsu_6R_8g&quot;&gt;https://youtu.be/gflsu_6R_8g&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/business24_ai&quot;&gt; /u/business24_ai &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1czdcpz/langgraph_essentials_create_your_first_graph_with/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1czdcpz/langgraph_essentials_create_your_first_graph_with/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1czdcpz</id><link href="https://www.reddit.com/r/LangChain/comments/1czdcpz/langgraph_essentials_create_your_first_graph_with/" /><updated>2024-05-24T05:25:50+00:00</updated><published>2024-05-24T05:25:50+00:00</published><title>LangGraph Essentials: Create Your First Graph with Ease!</title></entry><entry><author><name>/u/Ok_Comfort_4103</name><uri>https://www.reddit.com/user/Ok_Comfort_4103</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey all, looking to learn :)&lt;/p&gt; &lt;p&gt;What are some good robust platforms that I can use to vectorize various types of data and implement RAG to generate results from LLMs? I have a few projects I am working on each with their own types of data and models to use. Wondering if there are any all-in-one platforms you know of that would limit my time spent learning new technologies that will have to be updated as the methods progress. Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Ok_Comfort_4103&quot;&gt; /u/Ok_Comfort_4103 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cz9zga/vector_embedding_and_rag_platforms/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cz9zga/vector_embedding_and_rag_platforms/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cz9zga</id><link href="https://www.reddit.com/r/LangChain/comments/1cz9zga/vector_embedding_and_rag_platforms/" /><updated>2024-05-24T02:09:28+00:00</updated><published>2024-05-24T02:09:28+00:00</published><title>Vector Embedding and RAG Platforms</title></entry><entry><author><name>/u/ZuckyFox</name><uri>https://www.reddit.com/user/ZuckyFox</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Which is your favourite caching technique in LLM Applications. Is it in memory or something else. Which caching integration you like the most and why for a scalable and reliable application.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/ZuckyFox&quot;&gt; /u/ZuckyFox &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cz3ls8/caching_in_llm_apps/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cz3ls8/caching_in_llm_apps/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cz3ls8</id><link href="https://www.reddit.com/r/LangChain/comments/1cz3ls8/caching_in_llm_apps/" /><updated>2024-05-23T21:01:41+00:00</updated><published>2024-05-23T21:01:41+00:00</published><title>Caching in LLM Apps</title></entry><entry><author><name>/u/Confident-Addendum-2</name><uri>https://www.reddit.com/user/Confident-Addendum-2</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Been struggling with parsing pdf with complex layout, table, imagines.&lt;/p&gt; &lt;p&gt;The option that I am testing is multi modal vector, based on unstructured library for pdf extraction. &lt;/p&gt; &lt;p&gt;I recently discovered llamaparse proprietary solution. Excluding the facts that isn&amp;#39;t open source and limited for commercial use. Would it perform better then the unstructured approach for parsing?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Confident-Addendum-2&quot;&gt; /u/Confident-Addendum-2 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cyplp8/parsing_solutions_for_pdf/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cyplp8/parsing_solutions_for_pdf/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cyplp8</id><link href="https://www.reddit.com/r/LangChain/comments/1cyplp8/parsing_solutions_for_pdf/" /><updated>2024-05-23T10:31:22+00:00</updated><published>2024-05-23T10:31:22+00:00</published><title>Parsing solutions for PDF</title></entry><entry><author><name>/u/link2ani</name><uri>https://www.reddit.com/user/link2ani</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;We’re building a RAG based application which works on internal documents. We’re experimenting with OpenAI for embedding models, Milvus (Zilliz cloud) for embedding storage and similarity search, Postgres for all other data and AWS for hosting.&lt;/p&gt; &lt;p&gt;Our main priorities are: - being fast to market - above average performance - costs that don’t scale exponentially with scale - being scalable so we don’t have to refactor all of the code, if we achieve any scale&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/link2ani&quot;&gt; /u/link2ani &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cyjfap/best_stack_for_rag/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cyjfap/best_stack_for_rag/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cyjfap</id><link href="https://www.reddit.com/r/LangChain/comments/1cyjfap/best_stack_for_rag/" /><updated>2024-05-23T03:39:37+00:00</updated><published>2024-05-23T03:39:37+00:00</published><title>Best stack for RAG?</title></entry><entry><author><name>/u/AndrewShf</name><uri>https://www.reddit.com/user/AndrewShf</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;langchain_core.messages.human.HumanMessage&lt;/p&gt; &lt;p&gt;langchain.schema.messages.HumanMessage&lt;/p&gt; &lt;p&gt;I got unsupported HumanMessage error when using langchain and found out two kinds of messages. Why?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/AndrewShf&quot;&gt; /u/AndrewShf &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cyz7kw/why_two_different_kinds_of_messages/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cyz7kw/why_two_different_kinds_of_messages/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cyz7kw</id><link href="https://www.reddit.com/r/LangChain/comments/1cyz7kw/why_two_different_kinds_of_messages/" /><updated>2024-05-23T17:59:49+00:00</updated><published>2024-05-23T17:59:49+00:00</published><title>why two different kinds of messages?</title></entry><entry><author><name>/u/Mean-Night6324</name><uri>https://www.reddit.com/user/Mean-Night6324</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi!&lt;/p&gt; &lt;p&gt;I&amp;#39;m currently facing this issue of trying to get an XML out of a model and I use that XML structure to extract and format a document that I generate but, no matter how I prompt the model, or even, using different calls generate the answer and to structure it into the required format, sometimes going through different stages of structuring (like first just bullet points, then try to only put stuff into a basic XML format before going into nested.), it still sometimes generate an answer that&amp;#39;s not structured.&lt;/p&gt; &lt;p&gt;I included retries on those calls hoping that the model in its second generation would structure the output correctly but often this doesn&amp;#39;t work.&lt;/p&gt; &lt;p&gt;I was wondering how the community handles this issue or if there are creative ways you stumbled upon that deal well with it.&lt;/p&gt; &lt;p&gt;I have seen in the past some libraries that force the generation in some kind of way like the grammars from llama-cpp, or outlines. Maybe there was guidance as well. But I don&amp;#39;t think they work with LLMs from providers. I&amp;#39;m facing this problem with mistral-large.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Mean-Night6324&quot;&gt; /u/Mean-Night6324 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cyp7ij/what_are_some_ways_to_enforce_structured_outputs/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cyp7ij/what_are_some_ways_to_enforce_structured_outputs/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cyp7ij</id><link href="https://www.reddit.com/r/LangChain/comments/1cyp7ij/what_are_some_ways_to_enforce_structured_outputs/" /><updated>2024-05-23T10:04:32+00:00</updated><published>2024-05-23T10:04:32+00:00</published><title>What are some ways to enforce structured outputs from LLMs not in your control beyond basic prompting?</title></entry><entry><author><name>/u/AnEdgeLordWeeb</name><uri>https://www.reddit.com/user/AnEdgeLordWeeb</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey guys! I&amp;#39;m trying to develop a chatbot that offers video games recommendations based on user input.&lt;br/&gt; Problem is, I&amp;#39;m stuck at the chain which objective is to use Tavily API tool to search for video games&amp;#39; titles that fit the user&amp;#39;s criteria.&lt;/p&gt; &lt;p&gt;Here&amp;#39;s what I&amp;#39;ve tried:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;# Game Title Search prompt = PromptTemplate( template=&amp;quot;&amp;quot;&amp;quot;You are part of a chatbot that provides personalized video game recommendations based on user preferences. \n Your task is to search for the top 5 video games that match the user query. \n Only return the titles of the games. \n\n User Query: {query}&amp;quot;&amp;quot;&amp;quot;, input_variables=[&amp;quot;query&amp;quot;], ) game_title_search = prompt | llm.bind_tools(tools) QUERY = &amp;quot;&amp;quot;&amp;quot;What games are similar to Skyrim?&amp;quot;&amp;quot;&amp;quot; result = game_title_search.invoke({&amp;quot;query&amp;quot;: QUERY}) print(result) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Problem is, when I print result it gives me this instead of the response that I&amp;#39;m expecting (which are the video games&amp;#39; titles:&lt;/p&gt; &lt;p&gt;&lt;code&gt;content=&amp;#39;&amp;#39; additional_kwargs={&amp;#39;tool_calls&amp;#39;: [{&amp;#39;id&amp;#39;: &amp;#39;call_xJGybVhCtBAYGHyNkEE04U1c&amp;#39;, &amp;#39;function&amp;#39;: {&amp;#39;arguments&amp;#39;: &amp;#39;{&amp;quot;query&amp;quot;:&amp;quot;games similar to Skyrim&amp;quot;}&amp;#39;, &amp;#39;name&amp;#39;: &amp;#39;tavily_search_results_json&amp;#39;}, &amp;#39;type&amp;#39;: &amp;#39;function&amp;#39;}]} response_metadata={&amp;#39;token_usage&amp;#39;: {&amp;#39;completion_tokens&amp;#39;: 21, &amp;#39;prompt_tokens&amp;#39;: 141, &amp;#39;total_tokens&amp;#39;: 162}, &amp;#39;model_name&amp;#39;: &amp;#39;gpt-3.5-turbo-1106&amp;#39;, &amp;#39;system_fingerprint&amp;#39;: None, &amp;#39;finish_reason&amp;#39;: &amp;#39;tool_calls&amp;#39;, &amp;#39;logprobs&amp;#39;: None} id=&amp;#39;run-c7c33094-2173-43d8-9e9a-319c80265f57-0&amp;#39; tool_calls=[{&amp;#39;name&amp;#39;: &amp;#39;tavily_search_results_json&amp;#39;, &amp;#39;args&amp;#39;: {&amp;#39;query&amp;#39;: &amp;#39;games similar to Skyrim&amp;#39;}, &amp;#39;id&amp;#39;: &amp;#39;call_xJGybVhCtBAYGHyNkEE04U1c&amp;#39;}]&lt;/code&gt;&lt;/p&gt; &lt;p&gt;How can I solve this and use the tools alongside the ChatModel and the PromptTemplate to achieve what I want?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/AnEdgeLordWeeb&quot;&gt; /u/AnEdgeLordWeeb &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cyt7uf/how_can_i_properly_use_tools_within_a_chain_in/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cyt7uf/how_can_i_properly_use_tools_within_a_chain_in/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cyt7uf</id><link href="https://www.reddit.com/r/LangChain/comments/1cyt7uf/how_can_i_properly_use_tools_within_a_chain_in/" /><updated>2024-05-23T13:48:44+00:00</updated><published>2024-05-23T13:48:44+00:00</published><title>How can I properly use tools within a chain in LangGraph?</title></entry><entry><author><name>/u/Mission_Tip4316</name><uri>https://www.reddit.com/user/Mission_Tip4316</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;What worked for me was to create small modular functions out of one big function with different parameters. I broke down my API for the bot to use into smaller, modular endpoints with maximum of two parameters each. &lt;/p&gt; &lt;p&gt;I have been able to use gpt-3.5 to get satisfactory outputs without fails. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Mission_Tip4316&quot;&gt; /u/Mission_Tip4316 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cyn34y/for_those_struggling_with_api_function_calls/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cyn34y/for_those_struggling_with_api_function_calls/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cyn34y</id><link href="https://www.reddit.com/r/LangChain/comments/1cyn34y/for_those_struggling_with_api_function_calls/" /><updated>2024-05-23T07:31:44+00:00</updated><published>2024-05-23T07:31:44+00:00</published><title>For those struggling with API function calls</title></entry><entry><author><name>/u/mehulgupta7991</name><uri>https://www.reddit.com/user/mehulgupta7991</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/mehulgupta7991&quot;&gt; /u/mehulgupta7991 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;/r/ArtificialInteligence/comments/1cytvn5/generative_ai_for_time_series/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cyu0sy/timegpt_generative_ai_for_time_series/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cyu0sy</id><link href="https://www.reddit.com/r/LangChain/comments/1cyu0sy/timegpt_generative_ai_for_time_series/" /><updated>2024-05-23T14:23:42+00:00</updated><published>2024-05-23T14:23:42+00:00</published><title>TimeGPT: Generative AI for Time Series</title></entry><entry><author><name>/u/TripleCheeeze</name><uri>https://www.reddit.com/user/TripleCheeeze</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;pre&gt;&lt;code&gt;from langchain.embeddings import OpenAIEmbeddings from langchain.retrievers import ParentDocumentRetriever from langchain.schema import Document from langchain.storage import InMemoryStore from langchain.text_splitter import RecursiveCharacterTextSplitter from langchain.vectorstores.chroma import Chroma vectorstore = Chroma( collection_name=&amp;quot;full_documents&amp;quot;, embedding_function=OpenAIEmbeddings() ) store = InMemoryStore() docs = [Document(page_content=txt, metadata={&amp;quot;id&amp;quot;: id}) for txt, id in [(&amp;quot;aaaaaa&amp;quot;, 1), (&amp;quot;bbbbbb&amp;quot;, 2)]] ParentDocumentRetriever( vectorstore=vectorstore, docstore=store, id_key=&amp;quot;id&amp;quot;, parent_splitter=RecursiveCharacterTextSplitter( chunk_size = 2, chunk_overlap = 0, length_function = len, add_start_index = True, ), child_splitter=RecursiveCharacterTextSplitter( chunk_size = 1, chunk_overlap = 0, length_function = len, add_start_index = True, ), ).add_documents(docs,ids=[doc.metadata[&amp;quot;id&amp;quot;] for doc in docs])from langchain.embeddings import OpenAIEmbeddings from langchain.retrievers import ParentDocumentRetriever from langchain.schema import Document from langchain.storage import InMemoryStore from langchain.text_splitter import RecursiveCharacterTextSplitter from langchain.vectorstores.chroma import Chroma vectorstore = Chroma( collection_name=&amp;quot;full_documents&amp;quot;, embedding_function=OpenAIEmbeddings() ) store = InMemoryStore() docs = [Document(page_content=txt, metadata={&amp;quot;id&amp;quot;: id}) for txt, id in [(&amp;quot;aaaaaa&amp;quot;, 1), (&amp;quot;bbbbbb&amp;quot;, 2)]] ParentDocumentRetriever( vectorstore=vectorstore, docstore=store, id_key=&amp;quot;id&amp;quot;, parent_splitter=RecursiveCharacterTextSplitter( chunk_size = 2, chunk_overlap = 0, length_function = len, add_start_index = True, ), child_splitter=RecursiveCharacterTextSplitter( chunk_size = 1, chunk_overlap = 0, length_function = len, add_start_index = True, ), ).add_documents(docs,ids=[doc.metadata[&amp;quot;id&amp;quot;] for doc in docs]) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The error :&lt;/p&gt; &lt;pre&gt;&lt;code&gt;ValueError: Got uneven list of documents and ids. If `ids` is provided, should be same length as `documents`. &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The size of documents list and ids list are nevertheless equal, i don&amp;#39;t understand this error &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/TripleCheeeze&quot;&gt; /u/TripleCheeeze &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cytwsx/parentdocumentretrieveradd_document_function_with/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cytwsx/parentdocumentretrieveradd_document_function_with/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cytwsx</id><link href="https://www.reddit.com/r/LangChain/comments/1cytwsx/parentdocumentretrieveradd_document_function_with/" /><updated>2024-05-23T14:18:57+00:00</updated><published>2024-05-23T14:18:57+00:00</published><title>ParentDocumentRetriever.add_document function with &quot;ids&quot; parameter - can't fix an error</title></entry></feed>