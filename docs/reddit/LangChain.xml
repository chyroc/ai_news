<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/"><category term="LangChain" label="r/LangChain"/><updated>2024-04-09T19:39:47+00:00</updated><icon>https://www.redditstatic.com/icon.png/</icon><id>/r/LangChain.rss</id><link rel="self" href="https://www.reddit.com/r/LangChain.rss" type="application/atom+xml" /><link rel="alternate" href="https://www.reddit.com/r/LangChain" type="text/html" /><subtitle>LangChain is an open-source framework and developer toolkit that helps developers get LLM applications from prototype to production. It is available for Python and Javascript at https://www.langchain.com/.</subtitle><title>LangChain</title><entry><author><name>/u/zchaarm</name><uri>https://www.reddit.com/user/zchaarm</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;A place for members of &lt;a href=&quot;/r/LangChain&quot;&gt;r/LangChain&lt;/a&gt; to chat with each other&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/zchaarm&quot;&gt; /u/zchaarm &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_10ljho9</id><link href="https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/" /><updated>2023-01-26T04:33:05+00:00</updated><published>2023-01-26T04:33:05+00:00</published><title>r/LangChain Lounge</title></entry><entry><author><name>/u/isthatashark</name><uri>https://www.reddit.com/user/isthatashark</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/isthatashark&quot;&gt; /u/isthatashark &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://vectorize.io/what-is-a-vector-database/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c000jh/the_ultimate_guide_to_vector_database_success_in/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1c000jh</id><link href="https://www.reddit.com/r/LangChain/comments/1c000jh/the_ultimate_guide_to_vector_database_success_in/" /><updated>2024-04-09T18:54:40+00:00</updated><published>2024-04-09T18:54:40+00:00</published><title>The Ultimate Guide To Vector Database Success In AI</title></entry><entry><author><name>/u/mehul_gupta1997</name><uri>https://www.reddit.com/user/mehul_gupta1997</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/mehul_gupta1997&quot;&gt; /u/mehul_gupta1997 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;/r/ArtificialInteligence/comments/1bzuuov/tested_code_gemma_by_google/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bzuxvz/tested_code_gemma_by_google/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bzuxvz</id><link href="https://www.reddit.com/r/LangChain/comments/1bzuxvz/tested_code_gemma_by_google/" /><updated>2024-04-09T15:26:22+00:00</updated><published>2024-04-09T15:26:22+00:00</published><title>Tested Code Gemma by Google</title></entry><entry><author><name>/u/VegetableAddendum888</name><uri>https://www.reddit.com/user/VegetableAddendum888</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;So guys there‚Äôs vectara‚Äôs upcoming hackathon,anybody interested to participate and needs a team.DM me‚Ä¶&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/VegetableAddendum888&quot;&gt; /u/VegetableAddendum888 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c00e4g/need_teammates_for_a_rag_hackathon/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c00e4g/need_teammates_for_a_rag_hackathon/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1c00e4g</id><link href="https://www.reddit.com/r/LangChain/comments/1c00e4g/need_teammates_for_a_rag_hackathon/" /><updated>2024-04-09T19:09:43+00:00</updated><published>2024-04-09T19:09:43+00:00</published><title>Need teammates for a RAG hackathon</title></entry><entry><author><name>/u/bwenneker</name><uri>https://www.reddit.com/user/bwenneker</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;m building several chat based apps with LangChain for clients. I&amp;#39;m asking for feedback with each answer, users can leave a üëç or üëé.&lt;/p&gt; &lt;p&gt;Often I get the question: &amp;quot;does this &amp;#39;self-improve&amp;#39;?&amp;quot;&lt;/p&gt; &lt;p&gt;This got me thinking, why not use the positive feedback to improve future answers? Has anyone tried something like this:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Store (positive) user feedback in a VectorDB with questions-answer pairs.&lt;/li&gt; &lt;li&gt;When a new question is asked, run the usual pipeline (RAG for example).&lt;/li&gt; &lt;li&gt;Then also query the feedback VectorDB and add the top-k feedback question-answer pairs with high relevance to the question and add it as extra context.&lt;/li&gt; &lt;li&gt;Let the LLM answer the question using the context and top-k feedback items.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Looking forward to your experience, otherwise I might build this, it doesn&amp;#39;t seem to hard to make.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/bwenneker&quot;&gt; /u/bwenneker &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bzntdm/using_user_feedback_to_optimize_rag/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bzntdm/using_user_feedback_to_optimize_rag/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bzntdm</id><link href="https://www.reddit.com/r/LangChain/comments/1bzntdm/using_user_feedback_to_optimize_rag/" /><updated>2024-04-09T09:25:46+00:00</updated><published>2024-04-09T09:25:46+00:00</published><title>Using user feedback to optimize RAG</title></entry><entry><author><name>/u/anderl1980</name><uri>https://www.reddit.com/user/anderl1980</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I‚Äôd be interested in whether anyone here is using LangChain‚Äôs SQL Agent (or similar self-built agents with LangChain or autogen). I‚Äôd love to conenct to learn from your experiences as I have not seen it be used in productive systems yet!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/anderl1980&quot;&gt; /u/anderl1980 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bzn1yw/sql_agent_in_production/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bzn1yw/sql_agent_in_production/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bzn1yw</id><link href="https://www.reddit.com/r/LangChain/comments/1bzn1yw/sql_agent_in_production/" /><updated>2024-04-09T08:29:57+00:00</updated><published>2024-04-09T08:29:57+00:00</published><title>SQL Agent in production?</title></entry><entry><author><name>/u/mehul_gupta1997</name><uri>https://www.reddit.com/user/mehul_gupta1997</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Checkout how you can leverage Multi-Agent Orchestration for developing an auto Interview system where the Interviewer asks questions to interviewee, evaluates it and eventually shares whether the candidate should be selected or not. Right now, both interviewer and interviewee are played by AI agents. &lt;a href=&quot;https://youtu.be/VrjqR4dIawo?si=1sMYs7lI-c8WZrwP&quot;&gt;https://youtu.be/VrjqR4dIawo?si=1sMYs7lI-c8WZrwP&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/mehul_gupta1997&quot;&gt; /u/mehul_gupta1997 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bzkzkt/multiagent_interview_using_langgraph/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bzkzkt/multiagent_interview_using_langgraph/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bzkzkt</id><link href="https://www.reddit.com/r/LangChain/comments/1bzkzkt/multiagent_interview_using_langgraph/" /><updated>2024-04-09T06:07:23+00:00</updated><published>2024-04-09T06:07:23+00:00</published><title>Multi-Agent Interview using LangGraph</title></entry><entry><author><name>/u/Chrex_007</name><uri>https://www.reddit.com/user/Chrex_007</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;So I have a requirement of being able to chat with csv files and when the chatbot can&amp;#39;t find any relevant information from the csv files it should use the Bing API to search on the web and gather information and answer. I tried to make a custom langchain agent with Bing API as a tool but it&amp;#39;s not able to perform the observation, action loop, the model I&amp;#39;m using is Mistral-7B-Instruct-v0.1 which I can&amp;#39;t change I think model is not powerful enough for this task. But still does anybody have idea how can I make this possible? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Chrex_007&quot;&gt; /u/Chrex_007 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bzmovg/how_to_create_a_chatbot_to_chat_with_csv_files/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bzmovg/how_to_create_a_chatbot_to_chat_with_csv_files/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bzmovg</id><link href="https://www.reddit.com/r/LangChain/comments/1bzmovg/how_to_create_a_chatbot_to_chat_with_csv_files/" /><updated>2024-04-09T08:02:54+00:00</updated><published>2024-04-09T08:02:54+00:00</published><title>How to create a chatbot to chat with csv files and internet (Bing API)?</title></entry><entry><author><name>/u/ramirez_tn</name><uri>https://www.reddit.com/user/ramirez_tn</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I would like interact with LLMs in sequential way like:&lt;/p&gt; &lt;p&gt;Step 1 : load documents Step 2 : ask about the products described in the documents Step 3: based on the response lookup where can I buy these products Step 4: check if the store has other options ‚Ä¶&lt;/p&gt; &lt;p&gt;I am currently doing it in a very basic way. Loading the documents, retrieving using a question, using the output as input for another retriever , ‚Ä¶ Is there a more sophisticated way of doing it ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/ramirez_tn&quot;&gt; /u/ramirez_tn &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bzsgxa/interacting_with_llms_in_steps/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bzsgxa/interacting_with_llms_in_steps/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bzsgxa</id><link href="https://www.reddit.com/r/LangChain/comments/1bzsgxa/interacting_with_llms_in_steps/" /><updated>2024-04-09T13:41:04+00:00</updated><published>2024-04-09T13:41:04+00:00</published><title>Interacting with LLMs in steps</title></entry><entry><author><name>/u/IlEstLaPapi</name><uri>https://www.reddit.com/user/IlEstLaPapi</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;tldr: Some insights and learnings from a LLM enthusiast working on a complex Chatbot using multiple agents built with LangGraph, LCEL and Chainlit.&lt;/p&gt; &lt;p&gt;Hi everyone! I have seen a lot of interest in multi-agent systems recently, and, as I&amp;#39;m currently working on a complex one, I thought I might as well share some feedback on my project. Maybe some of you might find it interesting, give some useful feedback, or make some suggestions.&lt;/p&gt; &lt;h2&gt;Introduction: Why am I doing this project?&lt;/h2&gt; &lt;p&gt;I&amp;#39;m a business owner and a tech guy with a background in math, coding, and ML. Since early 2023, I&amp;#39;ve fallen in love with the LLM world. So, I decided to start a new business with 2 friends: a consulting firm on generative AI. As expected, we don&amp;#39;t have many references. Thus, we decided to create a tool to demonstrate our skillset to potential clients.&lt;/p&gt; &lt;p&gt;After a brainstorm, we quickly identified that a) RAG is the main selling point, so we need something that uses a RAG; b) We believe in agents to automate tasks; c) ChatGPT has shown that asking questions to a chatbot is a much more human-friendly interface than a website; d) Our main weakness is that we are all tech guys, so we might as well compensate for that by building a seller.&lt;/p&gt; &lt;p&gt;From here, the idea was clear: instead, or more exactly, alongside our website, build a chatbot that would answer questions about our company, &amp;quot;sell&amp;quot; our offer, and potentially schedule meetings with our consultants. Then make some posts on LinkedIn and pray...&lt;/p&gt; &lt;p&gt;Spoiler alert: This project isn&amp;#39;t finished yet. The idea is to share some insights and learnings with the community and get some feedback.&lt;/p&gt; &lt;h2&gt;Functional specifications&lt;/h2&gt; &lt;p&gt;The first step was to list some specifications: * We want a RAG that can answer any question the user might have about our company. For that, we will use the content of the company website. Of course, we also need to prevent hallucination, especially on two topics: the website has no information about pricing, and we don&amp;#39;t offer SLAs. * We want it to answer as quickly as possible and limit the budget. For that, we will use smaller models like GPT-3.5 and Claude Haiku as often as possible. But that limits the reasoning capabilities of our agents, so we need to find a sweet spot. * We want consistency in the responses, which is a big problem for RAGs. Questions with similar meanings should generate the same answers, for example, &amp;quot;What&amp;#39;s your offer?&amp;quot;, &amp;quot;What services do you provide?&amp;quot;, and &amp;quot;What do you do?&amp;quot;. * Obviously, we don&amp;#39;t want visitors to be able to ask off-topic questions (e.g., &amp;quot;How is the weather in North Carolina?&amp;quot;), so we need a way to filter out off-topic, prompt injection, and toxic questions. * We want to demonstrate that GenAI can be used to deliver more than just chatbots, so we want the agents to be able to schedule meetings, send emails to visitors, etc. * Ideally, we also want the agents to be able to qualify the visitor: who they are, what their job is, what their organization is, whether they are a tech person or a manager, and if they are looking for something specific with a defined need or are just curious about us. * Ideally, we also want the agents to &amp;quot;sell&amp;quot; our company: if the visitor indicates their need, match it with our offer and &amp;quot;push&amp;quot; that offer. If they show some interest, let&amp;#39;s &amp;quot;push&amp;quot; for a meeting with our consultants!&lt;/p&gt; &lt;h2&gt;Architecture&lt;/h2&gt; &lt;h3&gt;Stack&lt;/h3&gt; &lt;p&gt;We aren&amp;#39;t a startup, we haven&amp;#39;t raised funds, and we don&amp;#39;t have months to do this. We can&amp;#39;t afford to spend more than 20 days to get an MVP. Besides, our main selling point is that GenAI projects don&amp;#39;t require as much time or budget as ML ones.&lt;/p&gt; &lt;p&gt;So, in order to move fast, we needed to use some open-source frameworks: * For the chatbot, the data is public, so let&amp;#39;s use GPT and Claude as they are the best right now and the API cost is low. * For the chatbot, Chainlit provides everything we need, except background processing. Let&amp;#39;s use that. * Langchain and LCEL are both flexible and unify the interfaces with the LLMs. * We&amp;#39;ll need a rather complicated agent workflow, in fact, multiple ones. LangGraph is more flexible than crew.ai or autogen. Let&amp;#39;s use that!&lt;/p&gt; &lt;h3&gt;Design and early versions&lt;/h3&gt; &lt;h4&gt;First version&lt;/h4&gt; &lt;p&gt;From the start, we knew it was impossible to do it using a &amp;quot;one prompt, one agent&amp;quot; solution. So we started with a 3-agent solution: one to &amp;quot;find&amp;quot; the required elements on our website (a RAG), one to sell and set up meetings, and one to generate the final answer.&lt;/p&gt; &lt;p&gt;The meeting logic was very easy to implement. However, as expected, the chatbot was hallucinating a lot: &amp;quot;Here is a full project for 1k‚Ç¨, with an SLA 7/7 2 hours 99.999%&amp;quot;. And it was a bad seller, with conversations such as &amp;quot;Hi, who are you?&amp;quot; &amp;quot;I&amp;#39;m Sellbotix, how can I help you? Do you want a meeting with one of our consultants?&amp;quot;&lt;/p&gt; &lt;p&gt;At this stage, after 10 hours of work, we knew that it was probably doable but would require much more than 3 agents.&lt;/p&gt; &lt;h4&gt;Second version&lt;/h4&gt; &lt;p&gt;The second version used a more complex architecture: a guard to filter the questions, a strategist to make a plan, a seller to find some selling points, a seeker and a documentalist for the RAG, a secretary for the schedule meeting function, and a manager to coordinate everything.&lt;/p&gt; &lt;p&gt;It was slow, so we included logic to distribute the work between the agents in parallel. Sadly, this can&amp;#39;t be implemented using LangGraph, as all agent calls are made using coroutines but are awaited, and you can&amp;#39;t have parallel branches. So we implemented our own logic.&lt;/p&gt; &lt;p&gt;The result was much better, but far from perfect. And it was a nightmare to improve because changing one agent&amp;#39;s system prompt would generate side effects on most of the other agents. We also had a hard time defining what each agent would need to see and what to hide. Sending every piece of information to every agent is a waste of time and tokens.&lt;/p&gt; &lt;p&gt;And last but not least, the codebase was a mess as we did it in a rush. So we decided to restart from scratch.&lt;/p&gt; &lt;h2&gt;Third version, WIP&lt;/h2&gt; &lt;p&gt;So currently, we are working on the third version. This project is, by far, much more ambitious than what most of our clients ask us to do (another RAG?). And so far, we have learned a ton. I honestly don&amp;#39;t know if we will finish it, or even if it&amp;#39;s realistic, but it was worth it. &amp;quot;It isn&amp;#39;t the destination that matters, it&amp;#39;s the journey&amp;quot; has rarely been so true.&lt;/p&gt; &lt;p&gt;Currently, we are working on the architecture, and we have nearly finished it. Here are a few insights that we are using, and I wanted to share with you.&lt;/p&gt; &lt;h3&gt;Separation of concern&lt;/h3&gt; &lt;p&gt;The two main difficulties when working with a network of agents are a) they don&amp;#39;t know when to stop, and b) any change to any agent&amp;#39;s system prompt impacts the whole system. It&amp;#39;s hard to fix. When building a complex system, separation of concern is key: agents must be split into groups, each one with clear responsibilities and interfaces.&lt;/p&gt; &lt;p&gt;The cool thing is that a LangGraph graph is also a Runnable, so you can build graphs that use graphs. So we ended up with this: a main graph for the guard and final answer logic. It calls a &amp;quot;think&amp;quot; graph that decides which subgraphs should be called. Those are a &amp;quot;sell&amp;quot; graph, a &amp;quot;handle&amp;quot; graph, and a &amp;quot;find&amp;quot; graph (so far).&lt;/p&gt; &lt;h3&gt;Async, parallelism, and conditional calls&lt;/h3&gt; &lt;p&gt;If you want a system to be fast, you need to NOT call all the agents every time. For that, you need two things: a planner that decides which subgraph should be called (in our think graph), and you need to use &lt;code&gt;asyncio.gather&lt;/code&gt; instead of letting LangGraph call every graph and await them one by one.&lt;/p&gt; &lt;p&gt;So in the think graph, we have planner and manager agents. We use a standard doer/critic pattern here. When they agree on what needs to be done, they generate a list of instructions and activation orders for each subgraph that are passed to a &amp;quot;do&amp;quot; node. This node then creates a list of coroutines and awaits an &lt;code&gt;asyncio.gather&lt;/code&gt;.&lt;/p&gt; &lt;h3&gt;Limit what each graph must see&lt;/h3&gt; &lt;p&gt;We want the system to be fast and cost-efficient. Every node of every subgraph doesn&amp;#39;t need to be aware of what every other agent does. So we need to decide exactly what each agent gets as input. That&amp;#39;s honestly quite hard, but doable. It means fewer tokens, so it reduces the cost and speeds up the response.&lt;/p&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;This post is already quite long, so I won&amp;#39;t go into the details of every subgraph here. However, if you&amp;#39;re interested, feel free to let me know. I might decide to write some additional posts about those and the specific challenges we encountered and how we solved them (or not). In any case, if you&amp;#39;ve read this far, thank you!&lt;/p&gt; &lt;p&gt;If you have any feedback, don&amp;#39;t hesitate to share. I&amp;#39;d be very happy to read your thoughts and suggestions!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/IlEstLaPapi&quot;&gt; /u/IlEstLaPapi &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1byz3lr/insights_and_learnings_from_building_a_complex/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1byz3lr/insights_and_learnings_from_building_a_complex/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1byz3lr</id><link href="https://www.reddit.com/r/LangChain/comments/1byz3lr/insights_and_learnings_from_building_a_complex/" /><updated>2024-04-08T14:20:55+00:00</updated><published>2024-04-08T14:20:55+00:00</published><title>Insights and Learnings from Building a Complex Multi-Agent System</title></entry><entry><author><name>/u/sarthak_uchiha</name><uri>https://www.reddit.com/user/sarthak_uchiha</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;How can we prevent data poision in llms , for example if our database it self is corrupt and we need llm not to send that data , how can we achieve that &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/sarthak_uchiha&quot;&gt; /u/sarthak_uchiha &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bzpcc6/data_poision_in_llms/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bzpcc6/data_poision_in_llms/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bzpcc6</id><link href="https://www.reddit.com/r/LangChain/comments/1bzpcc6/data_poision_in_llms/" /><updated>2024-04-09T11:04:04+00:00</updated><published>2024-04-09T11:04:04+00:00</published><title>Data poision in llms</title></entry><entry><author><name>/u/DanShmuelSasha</name><uri>https://www.reddit.com/user/DanShmuelSasha</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;ve been using Langchain and Langsmith to create a benchmarking workflow for my LLM prompts. Everything&amp;#39;s been working well, until I had to delete and re-create my OpenAI API key.&lt;/p&gt; &lt;p&gt;I&amp;#39;ve updated the API key in the env.py as the main environmental variable, and this works- I&amp;#39;ve tested this with the OpenAI Chat Completion.&lt;/p&gt; &lt;p&gt;I&amp;#39;ve also updated the API key in LangSmith by going into the requested prompt&amp;#39;s playground &amp;gt; Secrets &amp;amp; API keys &amp;gt; updated it manually. I even checked under my organization&amp;#39;s Settings &amp;gt; Secrets, and see the API key is updated.&lt;/p&gt; &lt;p&gt;However, when I try to use the &amp;quot;arun_on_dataset&amp;quot; function in the aforementioned Python environment, it gives me an error-&lt;/p&gt; &lt;pre&gt;&lt;code&gt;&amp;quot;AuthenticationError(&amp;quot;Error code: 401 - {&amp;#39;error&amp;#39;: {&amp;#39;message&amp;#39;: &amp;#39;Incorrect API key provided: sk-O6ZSB***************************************TNxS. You can find your API key at https://platform.openai.com/account/api-keys.&amp;quot; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The prefix for the API key the error throws is indeed the previous, non-functional API key, but I don&amp;#39;t know where else to adjust it so it&amp;#39;ll read the current API key.&lt;/p&gt; &lt;p&gt;Nothing was changed in the code, which ran well previously, and the only external adjustment was the API key.&lt;/p&gt; &lt;p&gt;Should I change the API key elsewhere?&lt;/p&gt; &lt;p&gt;Any thoughts and ideas are welcome!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/DanShmuelSasha&quot;&gt; /u/DanShmuelSasha &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bzoc45/openai_api_key_integration_mismatch/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bzoc45/openai_api_key_integration_mismatch/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bzoc45</id><link href="https://www.reddit.com/r/LangChain/comments/1bzoc45/openai_api_key_integration_mismatch/" /><updated>2024-04-09T10:01:49+00:00</updated><published>2024-04-09T10:01:49+00:00</published><title>OpenAI API key integration mismatch</title></entry><entry><author><name>/u/sarthak_uchiha</name><uri>https://www.reddit.com/user/sarthak_uchiha</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I am using redis as vector database . I am getting no permission error when adding text to redis using langchain , manually I am able to add it , but getting error when using langchain , can someone suggest an alternative or how can we achieve it with langchain &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/sarthak_uchiha&quot;&gt; /u/sarthak_uchiha &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bznz3t/redis_as_vector_database/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bznz3t/redis_as_vector_database/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bznz3t</id><link href="https://www.reddit.com/r/LangChain/comments/1bznz3t/redis_as_vector_database/" /><updated>2024-04-09T09:37:09+00:00</updated><published>2024-04-09T09:37:09+00:00</published><title>Redis as vector database</title></entry><entry><author><name>/u/onsies</name><uri>https://www.reddit.com/user/onsies</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hello,&lt;/p&gt; &lt;p&gt;I‚Äôm very new to LangChain and LLM altogether. Very excited!&lt;/p&gt; &lt;p&gt;I started following a LangChain example: &lt;a href=&quot;https://python.langchain.com/docs/use_cases/question_answering/chat_history/&quot;&gt;https://python.langchain.com/docs/use_cases/question_answering/chat_history/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I modified it to use a DirectoryLoader. Next, I invoked a prompt. The response is narrowed down to a particular chunk when the custom data is split and misses information from other document chunks. For example, the custom data contains information about best practices spread across several pages. My prompt is ‚Äúlist 5 of the best practices‚Äù. The response would only show one best practice, while ignoring the other document chunks that contain other practices.&lt;/p&gt; &lt;p&gt;Other example of a prompt is ‚ÄúSummary the most important best practice‚Äù. The response seems to randomly pick a document chunk and consider that the most important.&lt;/p&gt; &lt;p&gt;How should I go about ensure that all chunks of every document is used as part of the response?&lt;/p&gt; &lt;p&gt;Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/onsies&quot;&gt; /u/onsies &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bzkkdt/langchain_embeddings/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bzkkdt/langchain_embeddings/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bzkkdt</id><link href="https://www.reddit.com/r/LangChain/comments/1bzkkdt/langchain_embeddings/" /><updated>2024-04-09T05:40:54+00:00</updated><published>2024-04-09T05:40:54+00:00</published><title>LangChain Embeddings</title></entry><entry><author><name>/u/SustainedSuspense</name><uri>https://www.reddit.com/user/SustainedSuspense</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Does an LLM like this exist? Also, will this kill my machine? Im not needing a large model trained on a lot of tokens because I just want it to work with the data I provide it in the context during inference but I the context I have is about 21k. &lt;/p&gt; &lt;p&gt;Can I use Amazon to host a private LLM instead of running locally? Is that what Bedrock offers? &lt;/p&gt; &lt;p&gt;Any insights are appreciated. Thanks.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/SustainedSuspense&quot;&gt; /u/SustainedSuspense &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bzjzk0/im_worried_about_privacy_and_was_wondering_if/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bzjzk0/im_worried_about_privacy_and_was_wondering_if/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bzjzk0</id><link href="https://www.reddit.com/r/LangChain/comments/1bzjzk0/im_worried_about_privacy_and_was_wondering_if/" /><updated>2024-04-09T05:04:54+00:00</updated><published>2024-04-09T05:04:54+00:00</published><title>Im worried about privacy and was wondering if there is an LLM I can run locally on my i7 Mac that has at least a 25k context window?</title></entry><entry><author><name>/u/jdogbro12</name><uri>https://www.reddit.com/user/jdogbro12</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bz3vl7/anthropics_haiku_beats_gpt4_turbo_in_tool_use/&quot;&gt; &lt;img src=&quot;https://external-preview.redd.it/4YoBVzPLpUKvo5J-N_JeyaBnqnH19q7OHy5i_W-uThY.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f12d9ca66ee9dbf848967e620d263e7150530256&quot; alt=&quot;Anthropic's Haiku Beats GPT-4 Turbo in Tool Use&quot; title=&quot;Anthropic's Haiku Beats GPT-4 Turbo in Tool Use&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/jdogbro12&quot;&gt; /u/jdogbro12 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://docs.parea.ai/blog/benchmarking-anthropic-beta-tool-use&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bz3vl7/anthropics_haiku_beats_gpt4_turbo_in_tool_use/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1bz3vl7</id><media:thumbnail url="https://external-preview.redd.it/4YoBVzPLpUKvo5J-N_JeyaBnqnH19q7OHy5i_W-uThY.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=f12d9ca66ee9dbf848967e620d263e7150530256" /><link href="https://www.reddit.com/r/LangChain/comments/1bz3vl7/anthropics_haiku_beats_gpt4_turbo_in_tool_use/" /><updated>2024-04-08T17:29:23+00:00</updated><published>2024-04-08T17:29:23+00:00</published><title>Anthropic's Haiku Beats GPT-4 Turbo in Tool Use</title></entry><entry><author><name>/u/Gon_Buruwa</name><uri>https://www.reddit.com/user/Gon_Buruwa</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;m building a knowledge management system. I have implemented each individual functionality separately. Since accessing everything separately is cumbersome, I have decided to use a chat interface with function calling. I&amp;#39;m currently using the Mistral 8x7b 4-bit quantized version as my LLM. However, due to hardware limitations, directly performing function calling is slow and does not give the expected results.&lt;/p&gt; &lt;p&gt;Then, I decided to use a semantic-router with OpenAI&amp;#39;s Ada model to classify each input to determine which task it belongs to and then send it to the LLM to extract relevant information. This approach works well, but since I intend to use a local solution, OpenAI&amp;#39;s model is not an option.&lt;/p&gt; &lt;p&gt;I tried using local models, but the faster ones produce suboptimal results, while the better ones are slower. Is there any simple and fast solution that can classify chat messages into the appropriate task? My current tasks include image search, Retrieval Augmented Generation (RAG), document summarization, and document search.&lt;/p&gt; &lt;p&gt;I thought of building a simple classifier using traditional machine learning methods like TF-IDF, Gradient Boosting, Logistic Regression, etc. Would that work? Has anyone done this kind of work before? Thanks in advance.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Gon_Buruwa&quot;&gt; /u/Gon_Buruwa &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bzihid/seeking_advice_simple_and_fast_solution_for/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bzihid/seeking_advice_simple_and_fast_solution_for/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bzihid</id><link href="https://www.reddit.com/r/LangChain/comments/1bzihid/seeking_advice_simple_and_fast_solution_for/" /><updated>2024-04-09T03:43:22+00:00</updated><published>2024-04-09T03:43:22+00:00</published><title>Seeking Advice: Simple and Fast Solution for Classifying Chat Messages into Tasks</title></entry><entry><author><name>/u/tsensei_dev</name><uri>https://www.reddit.com/user/tsensei_dev</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Langchain python had this for some time now, but the typescript implementation lacked this chunking mechanism. So, followed &lt;a href=&quot;https://github.com/FullStackRetrieval-com/RetrievalTutorials/blob/main/tutorials/LevelsOfTextSplitting/5_Levels_Of_Text_Splitting.ipynb&quot;&gt;this&lt;/a&gt; notebook by Greg Kamadt and implemented it myself. &lt;/p&gt; &lt;p&gt;You&amp;#39;re free to use the code :) Everything is documented in jsDoc &lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/tsensei/Semantic-Chunking-Typescript&quot;&gt;https://github.com/tsensei/Semantic-Chunking-Typescript&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/tsensei_dev&quot;&gt; /u/tsensei_dev &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bz3ngb/wrote_a_semantic_chunker_for_rag_pipelines_in/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bz3ngb/wrote_a_semantic_chunker_for_rag_pipelines_in/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bz3ngb</id><link href="https://www.reddit.com/r/LangChain/comments/1bz3ngb/wrote_a_semantic_chunker_for_rag_pipelines_in/" /><updated>2024-04-08T17:20:44+00:00</updated><published>2024-04-08T17:20:44+00:00</published><title>Wrote a semantic chunker for RAG pipelines in Typescript</title></entry><entry><author><name>/u/mehul_gupta1997</name><uri>https://www.reddit.com/user/mehul_gupta1997</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bz06c5/should_i_partner_with_packt_for_my_book_on/&quot;&gt; &lt;img src=&quot;https://preview.redd.it/lhxs7e5ht9tc1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f68902b93adb073db5b44516f1872c67a5bd793e&quot; alt=&quot;Should I partner with Packt for my book on LangChain?&quot; title=&quot;Should I partner with Packt for my book on LangChain?&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Recently I launched my debut book &amp;quot;LangChain in your Pocket: Beginners guide to building Generative AI applications using LLMs&amp;quot; which is going a bestseller since release. Recently, Packt, one of the biggest tech book publishers contacted me for partnering with them for the distribution of the book. As expected, it would reach a wider audience but the price may go up exponentially. What should I do?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/mehul_gupta1997&quot;&gt; /u/mehul_gupta1997 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://i.redd.it/lhxs7e5ht9tc1.png&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bz06c5/should_i_partner_with_packt_for_my_book_on/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1bz06c5</id><media:thumbnail url="https://preview.redd.it/lhxs7e5ht9tc1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=f68902b93adb073db5b44516f1872c67a5bd793e" /><link href="https://www.reddit.com/r/LangChain/comments/1bz06c5/should_i_partner_with_packt_for_my_book_on/" /><updated>2024-04-08T15:05:00+00:00</updated><published>2024-04-08T15:05:00+00:00</published><title>Should I partner with Packt for my book on LangChain?</title></entry><entry><author><name>/u/ArcuisAlezanzo</name><uri>https://www.reddit.com/user/ArcuisAlezanzo</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;m building RAG chat bot with azure open ai and azure ai search&lt;/p&gt; &lt;p&gt;Right now I&amp;#39;m only developing POC &lt;/p&gt; &lt;p&gt;using streamlit as frontend and maintain chat history using session state (not focusing on persistent state rgt now)&lt;/p&gt; &lt;p&gt;Data ingestion pipeline is big headache for me even with static data(no updation once used )&lt;/p&gt; &lt;p&gt;Working with PDF , PPT , docx , excel(mostly technical documentation and excel is logs of ticket)&lt;/p&gt; &lt;p&gt;Each pdf have different structure Ppt is in different structure Docx is in diff structure &lt;/p&gt; &lt;p&gt;Right now , i converted ppt and docx to pdf and used pymupdf to extract text and chucked with recursive character split(size =1024, overlap=120) Used direct pymupdf lib instead of langchain abstract why you ask?&lt;/p&gt; &lt;p&gt;LET us consider PDF A &lt;/p&gt; &lt;p&gt;PDF A consist most of images with little text so chunking page wise results in character length of 150&lt;/p&gt; &lt;p&gt;So I extracted whole pdf text applied chunking on that. &lt;strong&gt;Is this optimal way ? I lose meta data by this method.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Coming to logs of excel . TBH I don&amp;#39;t know how work with this. I combined important columns as one column.&lt;/p&gt; &lt;p&gt;For Eg: (Title: .... Desc: ....) I combined Title and desc column as one with column heading in each row so context not missed out. Here also Each row converted as document object with size comes around 250 characters.&lt;/p&gt; &lt;p&gt;I feel wasting resource too much &lt;/p&gt; &lt;p&gt;What best ways you guys would suggest?&lt;/p&gt; &lt;p&gt;Thanks in advance&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/ArcuisAlezanzo&quot;&gt; /u/ArcuisAlezanzo &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bz2wpr/need_help_finding_better_methods_implementing_rag/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bz2wpr/need_help_finding_better_methods_implementing_rag/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bz2wpr</id><link href="https://www.reddit.com/r/LangChain/comments/1bz2wpr/need_help_finding_better_methods_implementing_rag/" /><updated>2024-04-08T16:51:45+00:00</updated><published>2024-04-08T16:51:45+00:00</published><title>Need help finding better methods implementing RAG</title></entry><entry><author><name>/u/SwissTricky</name><uri>https://www.reddit.com/user/SwissTricky</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hello, after working on a classic RAG chatbot to answer questions about IT support, we want to improve it so that, instead of showing a list of steps retrieved from the KB, it can interact with the user asking questions. For example:&lt;br/&gt; U: X does not work.&lt;br/&gt; A: please try A, did it work?&lt;br/&gt; U: No&lt;br/&gt; A: ok, if A did not work, please try B.&lt;br/&gt; U: it worked.&lt;br/&gt; A: happy to be of help.&lt;br/&gt; etc. etc.&lt;br/&gt; Of course the conversation can become longer with multiple steps, etc.&lt;br/&gt; I can imagine mixing a classic chatbot with a scripted flow with an LLM to get better understanding of utterances, a bit like the experimental mode of Rasa does, but I&amp;#39;m asking myself if there is a better and more modern way to handle such data collection/scripted conversations. At the end it&amp;#39;s a kind of data gathering mixed with a decision tree and wondering what&amp;#39;s the more modern approach to such a problem.&lt;br/&gt; What do you guys think?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/SwissTricky&quot;&gt; /u/SwissTricky &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bz80zh/creating_agents_with_scripted_conversations/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bz80zh/creating_agents_with_scripted_conversations/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bz80zh</id><link href="https://www.reddit.com/r/LangChain/comments/1bz80zh/creating_agents_with_scripted_conversations/" /><updated>2024-04-08T20:12:49+00:00</updated><published>2024-04-08T20:12:49+00:00</published><title>Creating agents with &quot;Scripted conversations&quot;</title></entry><entry><author><name>/u/KarbohJorneKraft</name><uri>https://www.reddit.com/user/KarbohJorneKraft</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I wrote this as the intro to a problem I am working in. anyone else thinking about this?&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Strategic Risk Reduction in AI Operations: Enhancing Systemic Controls in Agentic Workflows&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;It is possible for a workflow comprised of a chain of agentic assistants to drift away from the desired operational baseline without hallucinating or scoring poorly on standardized quality tests such as those for relevance, faithfulness, and alignment. Agentic assistants may be observed, measured, and managed on an individualized basis, but nodal evaluations may accurately analyze a point in time step in a workflow while failing to capture systemic distortion only observable at the workflow level while the agentic workflows are in motion.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Agentic Workflow Distortion in the Absence of Systemic Self-Reflection&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The concepts of Agentic Workflow Distortion and Systemic Self-Reflection pertain to the dynamics and evaluation within automated systems, particularly those that are structured around the autonomous operation of individual agents, referred to as &amp;quot;agentic workflows.&amp;quot; .....tbc&lt;/p&gt; &lt;p&gt;** I have the rest of this writeup if is anyone is interested &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/KarbohJorneKraft&quot;&gt; /u/KarbohJorneKraft &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1byyx9f/agentic_workflow_distortion_in_the_absence_of/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1byyx9f/agentic_workflow_distortion_in_the_absence_of/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1byyx9f</id><link href="https://www.reddit.com/r/LangChain/comments/1byyx9f/agentic_workflow_distortion_in_the_absence_of/" /><updated>2024-04-08T14:13:31+00:00</updated><published>2024-04-08T14:13:31+00:00</published><title>Agentic Workflow Distortion in the Absence of Systemic Self-Reflection</title></entry><entry><author><name>/u/cryptokaykay</name><uri>https://www.reddit.com/user/cryptokaykay</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1byrqps/langtrace_preview_of_the_new_evaluation_dashboard/&quot;&gt; &lt;img src=&quot;https://external-preview.redd.it/Z86Ol2Jj2ayoGOd-Pfgkem_533RuJprm3gD5eJdyH8c.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=040d97e52a5d67184964ab3339a8b243b2414601&quot; alt=&quot;Langtrace: Preview of the new Evaluation dashboard&quot; title=&quot;Langtrace: Preview of the new Evaluation dashboard&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey,&lt;/p&gt; &lt;p&gt;I am building an open source project called Langtrace which lets you monitor, debug and evaluate the LLM requests made by your application.&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/Scale3-Labs/langtrace&quot;&gt;https://github.com/Scale3-Labs/langtrace&lt;/a&gt; . The integration is only 2 lines of code.&lt;/p&gt; &lt;p&gt;Currently building an Evaluations dashboard which is launching this week. It lets you do the following:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Create tests - like factual accuracy, bias detection etc. &lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Automatically capture the LLM calls to specific tests by passing a testId to the langtrace SDK installed in your code.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Evaluate and measure the overall success % and how success % trends over time.&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;The goal here is to get confidence with the model or RAG before deploying it to production.&lt;/p&gt; &lt;p&gt;Please check out the repository. Would love to hear your thoughts! Thanks!&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/5bracw5ki7tc1.png?width=2932&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1fe6fac6661d9a5c0c7f701c44d50435f45c7d7f&quot;&gt;https://preview.redd.it/5bracw5ki7tc1.png?width=2932&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1fe6fac6661d9a5c0c7f701c44d50435f45c7d7f&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/cryptokaykay&quot;&gt; /u/cryptokaykay &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1byrqps/langtrace_preview_of_the_new_evaluation_dashboard/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1byrqps/langtrace_preview_of_the_new_evaluation_dashboard/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1byrqps</id><media:thumbnail url="https://external-preview.redd.it/Z86Ol2Jj2ayoGOd-Pfgkem_533RuJprm3gD5eJdyH8c.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=040d97e52a5d67184964ab3339a8b243b2414601" /><link href="https://www.reddit.com/r/LangChain/comments/1byrqps/langtrace_preview_of_the_new_evaluation_dashboard/" /><updated>2024-04-08T07:21:23+00:00</updated><published>2024-04-08T07:21:23+00:00</published><title>Langtrace: Preview of the new Evaluation dashboard</title></entry><entry><author><name>/u/jzone3</name><uri>https://www.reddit.com/user/jzone3</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Open source language models are no serious competitors. I have been migrating a lot of my prompts to open source models, and I wrote up this tutorial about how I do it.&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://blog.promptlayer.com/migrating-prompts-to-open-source-models-c21e1d482d6f&quot;&gt;https://blog.promptlayer.com/migrating-prompts-to-open-source-models-c21e1d482d6f&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/jzone3&quot;&gt; /u/jzone3 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bz1cuq/migrating_my_prompts_to_open_source_language/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bz1cuq/migrating_my_prompts_to_open_source_language/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bz1cuq</id><link href="https://www.reddit.com/r/LangChain/comments/1bz1cuq/migrating_my_prompts_to_open_source_language/" /><updated>2024-04-08T15:52:00+00:00</updated><published>2024-04-08T15:52:00+00:00</published><title>Migrating my prompts to open source language models</title></entry><entry><author><name>/u/_depressedmillenial</name><uri>https://www.reddit.com/user/_depressedmillenial</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;So I want to automate the conversion of a legal document (5-20 pages) into a different type of document with plain/lay English + adhere to a specific style and format guidelines (20-100 pages) that are in 3 separate reference pdf documents.&lt;/p&gt; &lt;p&gt;I tried the simplest approach I could think of at first, which was extracting and then providing the expected output format (headers/sub-headers) in my prompt using a &amp;quot;custom GPT&amp;quot; on the openai front-end, as well as a one-shot example pair of legal doc/converted doc in the prompt window, plus I also uploaded the reference docs to the customgpt (which I think is used as RAG by the GPT?). &lt;/p&gt; &lt;p&gt;The result is okay - it gets the format right for the most part, but it ignores many of the style guidelines, and summarizes a lot needlessly which leads to information loss. &lt;/p&gt; &lt;p&gt;I want to now try either more advanced RAG (I am a python user and with the exception of recent LCEL releases, am familiar with Langchain as well as LlamaIndex), but was also considering finetuning llama-2 with 4-bit quantization. &lt;/p&gt; &lt;p&gt;Is finetuning without a label even possible in this case? What RAG retrievers or embeddings would you suggest? Any other suggestions? Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/_depressedmillenial&quot;&gt; /u/_depressedmillenial &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bz17bg/texttotext_generation_finetuning_vs_rag_vs_fewshot/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bz17bg/texttotext_generation_finetuning_vs_rag_vs_fewshot/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bz17bg</id><link href="https://www.reddit.com/r/LangChain/comments/1bz17bg/texttotext_generation_finetuning_vs_rag_vs_fewshot/" /><updated>2024-04-08T15:45:39+00:00</updated><published>2024-04-08T15:45:39+00:00</published><title>Text-to-text generation: finetuning vs RAG vs few-shot?</title></entry><entry><author><name>/u/Mediocre-Card8046</name><uri>https://www.reddit.com/user/Mediocre-Card8046</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;I was wondering if anyone tried out to use a retriever from Llamaindex as retriever in a Langchain chain?&lt;/p&gt; &lt;p&gt;For me this is interesting because for now it is difficult to persistently save a ParentDocumentRetriever in Langchain but I think this is possible with Llamaindex. So I thought I am just using the Llamaindex retriever and pass the results to my chain.&lt;/p&gt; &lt;p&gt;Is there anything I should consider or are there any expericences with this approach?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Mediocre-Card8046&quot;&gt; /u/Mediocre-Card8046 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1byvt0t/use_llamaindex_retriever_in_langchain_chain/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1byvt0t/use_llamaindex_retriever_in_langchain_chain/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1byvt0t</id><link href="https://www.reddit.com/r/LangChain/comments/1byvt0t/use_llamaindex_retriever_in_langchain_chain/" /><updated>2024-04-08T11:46:19+00:00</updated><published>2024-04-08T11:46:19+00:00</published><title>Use Llamaindex Retriever in Langchain chain</title></entry></feed>