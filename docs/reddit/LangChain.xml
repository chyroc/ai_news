<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/"><category term="LangChain" label="r/LangChain"/><updated>2024-03-23T00:44:40+00:00</updated><icon>https://www.redditstatic.com/icon.png/</icon><id>/r/LangChain.rss</id><link rel="self" href="https://www.reddit.com/r/LangChain.rss" type="application/atom+xml" /><link rel="alternate" href="https://www.reddit.com/r/LangChain" type="text/html" /><subtitle>LangChain is an open-source framework and developer toolkit that helps developers get LLM applications from prototype to production. It is available for Python and Javascript at https://www.langchain.com/.</subtitle><title>LangChain</title><entry><author><name>/u/zchaarm</name><uri>https://www.reddit.com/user/zchaarm</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;A place for members of &lt;a href=&quot;/r/LangChain&quot;&gt;r/LangChain&lt;/a&gt; to chat with each other&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/zchaarm&quot;&gt; /u/zchaarm &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_10ljho9</id><link href="https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/" /><updated>2023-01-26T04:33:05+00:00</updated><published>2023-01-26T04:33:05+00:00</published><title>r/LangChain Lounge</title></entry><entry><author><name>/u/ridiculoys</name><uri>https://www.reddit.com/user/ridiculoys</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi! I&amp;#39;m new to Langchain and tinkering with LLMs in general, I&amp;#39;m just doing a small project on Langchain&amp;#39;s capabilities on document loading, chunking, and of course using a similarity search on a vectorstore and then using the information I retrieve in a chain to get an answer.&lt;/p&gt; &lt;p&gt;I&amp;#39;m only testing on a small dataset, so it&amp;#39;s easy for me to see the specific files and pages to cross check whether it is the best result among the different files. But it got me thinking: if I try to work with a larger dataset, how exactly do I verify if the answer is the best result in the ranking and if it is indeed correct?&lt;/p&gt; &lt;p&gt;Is it possible to get datasets where it contains a PDF, some test input prompts, and an expected certain correct output? This way, I would be able to use my project to ingest that data and see if I get similar results? Or is this too good to be true?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/ridiculoys&quot;&gt; /u/ridiculoys &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bl1p6d/how_do_you_verify_aside_from_manually_checking/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bl1p6d/how_do_you_verify_aside_from_manually_checking/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bl1p6d</id><link href="https://www.reddit.com/r/LangChain/comments/1bl1p6d/how_do_you_verify_aside_from_manually_checking/" /><updated>2024-03-22T15:16:39+00:00</updated><published>2024-03-22T15:16:39+00:00</published><title>How do you verify, aside from manually checking the PDFs, that your answers are correct from a simple RAG implementation using Langchain?</title></entry><entry><author><name>/u/gamalibr</name><uri>https://www.reddit.com/user/gamalibr</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey folks!&lt;/p&gt; &lt;p&gt;I&amp;#39;m building a function calling prompt, but the results are terrible.&lt;/p&gt; &lt;h1&gt;Prompt&lt;/h1&gt; &lt;blockquote&gt; &lt;p&gt;Please select the best function to answer user questions and context. Follow this instructions: &lt;/p&gt; &lt;p&gt;Only use functions that you have access to; &lt;/p&gt; &lt;p&gt;Consider the user questions and history to understand which tool should be used; &lt;/p&gt; &lt;p&gt;You can call more than one function, understand all the context, and consider the best tools to answer user questions completely; &lt;/p&gt; &lt;p&gt;History is only necessary for context; the question is most important.&lt;br/&gt; You have access to these tools:&lt;br/&gt; GetColumnsConfigTool: This tool retrieves the ID and Name of columns(To Do, WIP, and Done) from a board with their respective order.&lt;br/&gt; GetWorkItemsDeliveryStatusTool: Get the delivery status of work items (whether late or on track).&lt;br/&gt; GetWorkItensTool: Get Work Items by Type (inWIP, byWeek, last24hours, byType)&lt;/p&gt; &lt;/blockquote&gt; &lt;h1&gt;Functions structure&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;GetDeliveryStatusForWorkItems&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Get the delivery status (whether late or on track) for work items. Response structure: {{&amp;quot;workItemsWithDeliveryStatus&amp;quot;: [{&amp;quot;id&amp;quot;: &amp;quot;string&amp;quot;, &amp;quot;key&amp;quot;: &amp;quot;string&amp;quot;, &amp;quot;title&amp;quot;: &amp;quot;string&amp;quot;, &amp;quot;actualStatus&amp;quot;: &amp;quot;string&amp;quot;, &amp;quot;assignedTo&amp;quot;: &amp;quot;string&amp;quot;, &amp;quot;leadTimeToEnd&amp;quot;: &amp;quot;number&amp;quot;, &amp;quot;leadTimeUsed&amp;quot;: &amp;quot;number&amp;quot;, &amp;quot;percentageLeadTimeAlreadyUsed&amp;quot;: &amp;quot;number&amp;quot;, &amp;quot;leadTimeToEndWithLeadTimeAlreadyUsed&amp;quot;: &amp;quot;number&amp;quot;, &amp;quot;percentageLeadTimeExceeded&amp;quot;: &amp;quot;number&amp;quot;, &amp;quot;isLate&amp;quot;: &amp;quot;boolean&amp;quot;, &amp;quot;onTrackFlag&amp;quot;: &amp;quot;string&amp;quot;}]}}&lt;/p&gt; &lt;p&gt;&lt;strong&gt;GetWorkItensTool&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Get work itens by keys or ids passed as parameters. Response structure: { &amp;quot;WorkItems&amp;quot;: [{&amp;quot;key&amp;quot;:&amp;quot;string&amp;quot;,&amp;quot;id&amp;quot;:&amp;quot;string&amp;quot;,&amp;quot;name&amp;quot;:&amp;quot;string&amp;quot;,&amp;quot;description&amp;quot;:&amp;quot;string&amp;quot;,&amp;quot;created&amp;quot;:&amp;quot;string&amp;quot;,&amp;quot;updated&amp;quot;:&amp;quot;string&amp;quot;,&amp;quot;changelog&amp;quot;:[{&amp;quot;id&amp;quot;:&amp;quot;string&amp;quot;,&amp;quot;createdAt&amp;quot;:&amp;quot;string&amp;quot;,&amp;quot;movements&amp;quot;:[{&amp;quot;field&amp;quot;:&amp;quot;string&amp;quot;,&amp;quot;fromColumnId&amp;quot;:&amp;quot;string&amp;quot;,&amp;quot;fromColumnName&amp;quot;:&amp;quot;string&amp;quot;,&amp;quot;toColumnId&amp;quot;:&amp;quot;string&amp;quot;,&amp;quot;toColumnName&amp;quot;:&amp;quot;string&amp;quot;}]}],&amp;quot;workItemCreatedAt&amp;quot;:&amp;quot;string&amp;quot;,&amp;quot;columnName&amp;quot;:&amp;quot;string&amp;quot;,&amp;quot;priority&amp;quot;:&amp;quot;string&amp;quot;,&amp;quot;flagged&amp;quot;:&amp;quot;boolean&amp;quot;,&amp;quot;assignee&amp;quot;:{&amp;quot;accountId&amp;quot;:&amp;quot;string&amp;quot;,&amp;quot;userName&amp;quot;:&amp;quot;string&amp;quot;},&amp;quot;workItemType&amp;quot;:{&amp;quot;name&amp;quot;:&amp;quot;string&amp;quot;,&amp;quot;id&amp;quot;:&amp;quot;string&amp;quot;,&amp;quot;description&amp;quot;:&amp;quot;string&amp;quot;,&amp;quot;subtask&amp;quot;:&amp;quot;boolean&amp;quot;},&amp;quot;status&amp;quot;:{&amp;quot;name&amp;quot;:&amp;quot;string&amp;quot;,&amp;quot;id&amp;quot;:&amp;quot;string&amp;quot;,&amp;quot;statusCategory&amp;quot;:{&amp;quot;name&amp;quot;:&amp;quot;string&amp;quot;,&amp;quot;id&amp;quot;:&amp;quot;number&amp;quot;}}}]}&lt;/p&gt; &lt;h1&gt;User question&lt;/h1&gt; &lt;p&gt;What are the delivery dates for tasks that are in progress?&lt;/p&gt; &lt;h1&gt;Expected result&lt;/h1&gt; &lt;p&gt;functions GetWorkItensTool and GetDeliveryStatusForWorkItems.&lt;/p&gt; &lt;h1&gt;Actual result&lt;/h1&gt; &lt;p&gt;function GetWorkItensTool&lt;/p&gt; &lt;p&gt;----------&lt;/p&gt; &lt;p&gt;Would you happen to have any tips about how to improve it? Is there any better model than openAI GPT-4-turbo for it?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/gamalibr&quot;&gt; /u/gamalibr &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bley1d/improve_function_calling/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bley1d/improve_function_calling/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bley1d</id><link href="https://www.reddit.com/r/LangChain/comments/1bley1d/improve_function_calling/" /><updated>2024-03-23T00:31:07+00:00</updated><published>2024-03-23T00:31:07+00:00</published><title>Improve function calling</title></entry><entry><author><name>/u/drunkmute</name><uri>https://www.reddit.com/user/drunkmute</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi, currently I am working on an RAG tool for my company. I have to load multiple PDFs and Powerpoints, for which I use the UnstructuredPDFLoader and UnstructuredPowerPointLoader, because a lot of these documents contain images with text on them and these loaders allow you to extract said text through OCR. However, when I run this and the program goes through the retrieval steps, I get an error coming from a deprecated function, along with the output of each document split that will be used to answer my query. The answers to my questions are not of a high quality, and I believe that it may be attributed to something being wrong with my loaders because the output I am seeing for some document splits is &amp;quot;NO_OUTPUT&amp;quot; or &amp;quot;NO_OUTPUT_OUTPUT&amp;quot;.&lt;/p&gt; &lt;p&gt;I am wondering if any of you have run into this problem. In addition, as a bonus question, how do you all maintain metadata like source information in your document splits? I always lose mine.&lt;/p&gt; &lt;p&gt;Below is the retrieval code:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;# Vector Database u/st.cache_resource def vector_db_init(_folder_id, _model): &amp;quot;&amp;quot;&amp;quot;Vector db initializer, plus contextual compression addition&amp;quot;&amp;quot;&amp;quot; persist_directory = &amp;quot;./db/&amp;quot; # Persist directory path # Embeddings to be applied embeddings = VertexAIEmbeddings( model_name=&amp;quot;textembedding-gecko-multilingual&amp;quot;, credentials=CREDENTIALS, project_id=PROJECT_ID, ) # Document splitting, embedding and vector database loading # DOES NOT have to be done in every run, just once and after you can simply refer to the db if not os.path.exists(persist_directory): # Data Pre-processing # TODO- loader that can correct typos and what-not pdf_loader = DirectoryLoader(&amp;quot;/Users/marconardoneguerra/Desktop/e3_Consulting/Other/AI/Proposal RAG/docs&amp;quot;, glob=&amp;quot;**/*.pdf&amp;quot;, recursive=True, show_progress=True, loader_cls=UnstructuredPDFLoader, loader_kwargs={ #&amp;quot;extract_images&amp;quot;:True, &amp;quot;post_processors&amp;quot;:[clean_extra_whitespace, clean_non_ascii_chars, clean], # data cleaning &amp;quot;mode&amp;quot;:&amp;quot;single&amp;quot;, &amp;quot;strategy&amp;quot;:&amp;quot;hi_res&amp;quot;, &amp;quot;high_res_model_name&amp;quot;:&amp;quot;detectron2_onnx&amp;quot;, #&amp;quot;encoding&amp;quot;:&amp;quot;unicode&amp;quot; }) ppt_loader = DirectoryLoader(&amp;quot;/Users/marconardoneguerra/Desktop/e3_Consulting/Other/AI/Proposal RAG/docs&amp;quot;, glob=&amp;quot;**/*.pptx&amp;quot;, recursive=True, show_progress=True, loader_cls=UnstructuredPowerPointLoader, loader_kwargs={ #&amp;quot;extract_images&amp;quot;:True, &amp;quot;post_processors&amp;quot;:[clean_extra_whitespace, clean_non_ascii_chars, clean], # data cleaning &amp;quot;mode&amp;quot;:&amp;quot;single&amp;quot;, &amp;quot;strategy&amp;quot;:&amp;quot;hi_res&amp;quot;, &amp;quot;high_res_model_name&amp;quot;:&amp;quot;detectron2_onnx&amp;quot;, #&amp;quot;encoding&amp;quot;:&amp;quot;unicode&amp;quot; }) loaded_pdfs = pdf_loader.load() loaded_ppts = ppt_loader.load() print(&amp;quot;# of PDFs:&amp;quot; + str(len(loaded_pdfs))) print(&amp;quot;# of PPTs:&amp;quot; + str(len(loaded_ppts))) loaded_docs = loaded_pdfs + loaded_ppts # gdrive_documents = doc_processor(gdrive_documents, image_captioning) context = &amp;quot;\n\n&amp;quot;.join(str(p.page_content) for p in loaded_docs) # had to remove below because this splitter is new and does not have a max token size, hence has given chunks too large to handle # splitter = SemanticChunker(embeddings) splitter = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=25) data = splitter.split_text(context) print(&amp;quot;Data Processing Complete&amp;quot;) vectordb = Chroma.from_texts( data, embeddings, persist_directory=persist_directory ) vectordb.persist() print(&amp;quot;Vector DB Creating Complete\n&amp;quot;) elif os.path.exists(persist_directory): vectordb = Chroma( persist_directory=persist_directory, embedding_function=embeddings ) print(&amp;quot;Vector DB Loaded\n&amp;quot;) # Compresses what is contextually needed for query answer (?) compressor = LLMChainExtractor.from_llm(_model, ) compression_retriever = ContextualCompressionRetriever(base_compressor=compressor, base_retriever=vectordb.as_retriever(search_type=&amp;quot;similarity&amp;quot;, search_kwargs={&amp;quot;k&amp;quot;: 24})) return compression_retriever @st.cache_resource def chain_init(_model, _retriever): &amp;quot;&amp;quot;&amp;quot;Initializes chain for retrieval&amp;quot;&amp;quot;&amp;quot; # DONE- conversational template template = &amp;quot;&amp;quot;&amp;quot; Who you are: You are an expert on everything about e3 Consulting, AKA e3, a consulting firm based in San Juan, Puerto Rico. Your firm specializes in IT consulting. \ ------ Instructions: You will be receiving questions about e3 and their previous work. \ You will gather knowledge to deliver a good response to the user (separated with &amp;lt;ctx&amp;gt;&amp;lt;/ctx&amp;gt;). \ If you don&amp;#39;t know the answer, answer with &amp;quot;Unfortunately, I don&amp;#39;t have the information.&amp;quot; \ If you don&amp;#39;t find enough information below, also answer with &amp;quot;Unfortunately, I don&amp;#39;t have the information.&amp;quot; \ The context will most likely have typos in it, please correct them when you formulate your answer. \ ------ &amp;lt;ctx&amp;gt; {context} &amp;lt;/ctx&amp;gt; ------ {question} Answer: &amp;quot;&amp;quot;&amp;quot; # template above question_prompt_template = PromptTemplate(template=template, input_variables=[&amp;quot;question&amp;quot;, &amp;quot;context&amp;quot;]) # We create a qa chain with our llm, retriever, and memory # Use chain_type refine so we cna build off of different information, # in addition to being wary of our context window # TODO make this conversational (could be complex) qa_chain = RetrievalQA.from_chain_type( llm=_model, chain_type=&amp;quot;stuff&amp;quot;, return_source_documents=True, retriever=_retriever, verbose=True, ) # qa_chain = RetrievalQA | StrOutputParser() return qa_chain &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Error examples:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;NO_OUTPUT_OUTPUT/Users/marconardoneguerra/anaconda3/envs/proposal-rag/lib/python3.12/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain. warnings.warn( NO_OUTPUT_OUTPUT/Users/marconardoneguerra/anaconda3/envs/proposal-rag/lib/python3.12/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain. warnings.warn( NO_OUTPUT_OUTPUT/Users/marconardoneguerra/anaconda3/envs/proposal-rag/lib/python3.12/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain. warnings.warn( NO_OUTPUT_OUTPUT/Users/marconardoneguerra/anaconda3/envs/proposal-rag/lib/python3.12/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain. &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/drunkmute&quot;&gt; /u/drunkmute &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1blc6q2/document_loaders_outputting_no_output/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1blc6q2/document_loaders_outputting_no_output/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1blc6q2</id><link href="https://www.reddit.com/r/LangChain/comments/1blc6q2/document_loaders_outputting_no_output/" /><updated>2024-03-22T22:31:29+00:00</updated><published>2024-03-22T22:31:29+00:00</published><title>Document Loaders Outputting &quot;NO_OUTPUT&quot;</title></entry><entry><author><name>/u/e3e6</name><uri>https://www.reddit.com/user/e3e6</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi community, looking for some guidance to explain how this is called when I specify the Tool&amp;#39;s parameters as a description rather than Fields. &lt;/p&gt; &lt;p&gt;For some reason Fields does not work in that project, giving me exception like&lt;/p&gt; &lt;pre&gt;&lt;code&gt;ERROR:root:An error occurred ZeroShotAgent does not support multi-input tool &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;Here is some code&lt;/p&gt; &lt;pre&gt;&lt;code&gt;class SimpleInputs(BaseModel): input: str class GetEvents(BaseTool): name = &amp;quot;get_calendar_event&amp;quot; description = &amp;#39;Use this tool with arguments like &amp;quot;{{&amp;quot;start_date&amp;quot;: &amp;quot;yyyy-mm-dd&amp;quot;, &amp;quot;max_results&amp;quot;: int}}&amp;quot; when you need to retrieve events from Calendar.&amp;#39; args_schema: Type[BaseModel] = SimpleInputs return_direct: bool = False --- agent = initialize_agent( agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, tools=tools, llm=self.llm, verbose=True, &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/e3e6&quot;&gt; /u/e3e6 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1blaz9d/langchain_single_input_agent_params/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1blaz9d/langchain_single_input_agent_params/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1blaz9d</id><link href="https://www.reddit.com/r/LangChain/comments/1blaz9d/langchain_single_input_agent_params/" /><updated>2024-03-22T21:41:07+00:00</updated><published>2024-03-22T21:41:07+00:00</published><title>LangChain single input Agent params</title></entry><entry><author><name>/u/digital-bolkonsky</name><uri>https://www.reddit.com/user/digital-bolkonsky</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Does anyone have a codebase or tutorial for using LLMs with LangChain to summarize each row in a database and generate output for each? I have a database that is updated weekly, which you can think of as a record of transactions. I&amp;#39;m looking for a way to read each row of this database weekly, summarize the contents of those records, and have it tweeted out. I&amp;#39;m curious if there&amp;#39;s a tutorial or codebase somewhere that does this. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/digital-bolkonsky&quot;&gt; /u/digital-bolkonsky &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bl7k4g/does_anyone_have_a_codebase_or_tutorial_for_using/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bl7k4g/does_anyone_have_a_codebase_or_tutorial_for_using/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bl7k4g</id><link href="https://www.reddit.com/r/LangChain/comments/1bl7k4g/does_anyone_have_a_codebase_or_tutorial_for_using/" /><updated>2024-03-22T19:18:59+00:00</updated><published>2024-03-22T19:18:59+00:00</published><title>Does anyone have a codebase or tutorial for using LLMs with LangChain to summarize each row in a database and generate output for each?</title></entry><entry><author><name>/u/Fit-Set6851</name><uri>https://www.reddit.com/user/Fit-Set6851</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;If I have 100 documents in my vector db. In the metadata t are total of 5 sources and each source have 20 documents in the vector db. &lt;/p&gt; &lt;p&gt;So now as query is given by the user I want to process relevant documents of each source separately and then combine the answers. &lt;/p&gt; &lt;p&gt;Can somebody help me on how to do this in an optimized way? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Fit-Set6851&quot;&gt; /u/Fit-Set6851 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bkysyf/how_to_process_each_source_in_vector_db/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bkysyf/how_to_process_each_source_in_vector_db/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bkysyf</id><link href="https://www.reddit.com/r/LangChain/comments/1bkysyf/how_to_process_each_source_in_vector_db/" /><updated>2024-03-22T13:07:15+00:00</updated><published>2024-03-22T13:07:15+00:00</published><title>How to process each source in Vector db individually ?</title></entry><entry><author><name>/u/smtabatabaie</name><uri>https://www.reddit.com/user/smtabatabaie</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey everyone, just wanted to ask if you know any Langchain GPTs that actually works and is updated with the latest Langchain documents?&lt;br/&gt; Thanks&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/smtabatabaie&quot;&gt; /u/smtabatabaie &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bl3lpg/langchain_gpt/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bl3lpg/langchain_gpt/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bl3lpg</id><link href="https://www.reddit.com/r/LangChain/comments/1bl3lpg/langchain_gpt/" /><updated>2024-03-22T16:36:17+00:00</updated><published>2024-03-22T16:36:17+00:00</published><title>Langchain GPT</title></entry><entry><author><name>/u/cryptokaykay</name><uri>https://www.reddit.com/user/cryptokaykay</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;What are some challenges you face after deploying your LLM based application in production? &lt;/p&gt; &lt;p&gt;My only goal is to improve the accuracy of my chatbot. It seems like everything boils down to this unless there are any other special usecases you are using the LLMs for. Basically. I try to monitor for all the responses of my chatbot and measure them objectively so I can tweak and improve the accuracy. This seems pretty basic. But, what are some of the other levers that I can pull to improve the accuracy of my RAG based chat application?&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;I am also building a tooling for tracing and monitoring the responses with higher cardinality compared to the ones that are in the market. Plan to open source it pretty soon.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/cryptokaykay&quot;&gt; /u/cryptokaykay &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bklgf7/daily_struggles_with_my_llm_based_chatbot_in/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bklgf7/daily_struggles_with_my_llm_based_chatbot_in/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bklgf7</id><link href="https://www.reddit.com/r/LangChain/comments/1bklgf7/daily_struggles_with_my_llm_based_chatbot_in/" /><updated>2024-03-21T23:53:39+00:00</updated><published>2024-03-21T23:53:39+00:00</published><title>Daily struggles with my LLM based chatbot in production</title></entry><entry><author><name>/u/heisenberg-principle</name><uri>https://www.reddit.com/user/heisenberg-principle</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Couldn&amp;#39;t find any other post on this topic but I&amp;#39;m having an issue with langchain_mistralai library. We&amp;#39;re using weaviate_client 4.5.4 which requires httpx version 0.27.0. However langchain_mistralai is not compatible with httpx versions &amp;gt; 0.26.0. Will this be fixed at some point or should I give up and find a workaround? (downgrading weaviate_client is not an option, since it has needed functionalities which can&amp;#39;t be sacrificed XD)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/heisenberg-principle&quot;&gt; /u/heisenberg-principle &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bkxdyu/dependency_issues_when_adding_langchain_mistralai/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bkxdyu/dependency_issues_when_adding_langchain_mistralai/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bkxdyu</id><link href="https://www.reddit.com/r/LangChain/comments/1bkxdyu/dependency_issues_when_adding_langchain_mistralai/" /><updated>2024-03-22T11:51:14+00:00</updated><published>2024-03-22T11:51:14+00:00</published><title>Dependency issues when adding langchain_mistralai to the project dependencies</title></entry><entry><author><name>/u/Putrid_Spinach3961</name><uri>https://www.reddit.com/user/Putrid_Spinach3961</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey i am getting this error:&lt;/p&gt; &lt;p&gt;openai.NotFoundError: Error code: 404 - {&amp;#39;error&amp;#39;:{&amp;#39;code&amp;#39;:&amp;#39;404&amp;#39;, &amp;#39;message&amp;#39;: &amp;#39;Resource not found&amp;#39;}}&lt;/p&gt; &lt;p&gt;I used: From langchain_community.llms import OpenAI&lt;/p&gt; &lt;p&gt;From langchain.chains import LLMChain&lt;/p&gt; &lt;p&gt;Code: llm= OpenAI(model_name=&amp;quot;modelname&amp;quot;)&lt;/p&gt; &lt;p&gt;Output=LLMChain(prompt=prompt, llm=llm).run(&amp;#39;query&amp;#39;)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Putrid_Spinach3961&quot;&gt; /u/Putrid_Spinach3961 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bkxck3/why_this_error/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bkxck3/why_this_error/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bkxck3</id><link href="https://www.reddit.com/r/LangChain/comments/1bkxck3/why_this_error/" /><updated>2024-03-22T11:49:06+00:00</updated><published>2024-03-22T11:49:06+00:00</published><title>Why this error?</title></entry><entry><author><name>/u/s8ntinel69</name><uri>https://www.reddit.com/user/s8ntinel69</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bkwdjn/why_is_my_chaininvoke_command_giving_the_full/&quot;&gt; &lt;img src=&quot;https://b.thumbs.redditmedia.com/tkmq7LaPtW_OrBODlSdRs4PFlx5mNx5A-oAsIBwbBhQ.jpg&quot; alt=&quot;Why is my chain.invoke({}) command giving the full model response instead of just AIMessage(content=' ')&quot; title=&quot;Why is my chain.invoke({}) command giving the full model response instead of just AIMessage(content=' ')&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I am using ChatVertexAI and the ChatPromptTemplate to provide the model with a system message, and a user message, both of which are stored in separate variables which return a string. &lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/6raj796a7vpc1.png?width=831&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=61b9af92cc549d82da840310606772c3d3d0c51e&quot;&gt;Prompt template&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The chain uses LCEL to define the chain for the invoke command &lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/1ud0215i7vpc1.png?width=847&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=604c30b0fdf719ff723c86abda5757ccd1146613&quot;&gt;The chain and invoke command&lt;/a&gt;&lt;/p&gt; &lt;p&gt;However, the output that I get includes details that I should get if the command was chain.generate() and not chain.invoke(). It should not include all of the response metadata that is being printed here. &lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/vzocmpoy7vpc1.png?width=1103&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3890c438dfef3a70cb5801c820380236a7a62435&quot;&gt;https://preview.redd.it/vzocmpoy7vpc1.png?width=1103&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3890c438dfef3a70cb5801c820380236a7a62435&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Should&amp;#39;nt the output contain only AIMessage(content=&amp;#39;.........&amp;#39;) and not anything else? I Know I can use definition.content in this case, but in reality I cannot use that as this output is going to be used by langgraph for creating a reflection agent, in which I need to use the output like it is. &lt;/p&gt; &lt;p&gt;I checked all documentation and my prompt template as well the call to the LLM is exactly as it should be, but in the examples they show, the chain.invoke command should not print response metadata.&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/s8ntinel69&quot;&gt; /u/s8ntinel69 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bkwdjn/why_is_my_chaininvoke_command_giving_the_full/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bkwdjn/why_is_my_chaininvoke_command_giving_the_full/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1bkwdjn</id><media:thumbnail url="https://b.thumbs.redditmedia.com/tkmq7LaPtW_OrBODlSdRs4PFlx5mNx5A-oAsIBwbBhQ.jpg" /><link href="https://www.reddit.com/r/LangChain/comments/1bkwdjn/why_is_my_chaininvoke_command_giving_the_full/" /><updated>2024-03-22T10:50:34+00:00</updated><published>2024-03-22T10:50:34+00:00</published><title>Why is my chain.invoke({}) command giving the full model response instead of just AIMessage(content=' ')</title></entry><entry><author><name>/u/BichonFrise_</name><uri>https://www.reddit.com/user/BichonFrise_</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;[New in my AI agent journey]&lt;/p&gt; &lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;As I mentionned in the title of this post, I&amp;#39;m wondering if Langchain is the best framework to build AI agents that are able to retrieve information online. &lt;/p&gt; &lt;p&gt;I&amp;#39;ll give you one example : &lt;/p&gt; &lt;ul&gt; &lt;li&gt;Step 1 :I would like to give my agent a list of website and ask them if this company is a B2C company or a B2B company.&lt;/li&gt; &lt;li&gt;Step 2 : Chain this agent to another : if it&amp;#39;s a B2B company find the pricing&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Is is possible to do so with Langchain ? &lt;/p&gt; &lt;p&gt;If yes, do you know where I could find a tutorial to get me started ?&lt;/p&gt; &lt;p&gt;If not what is the best framework out there ? I saw &lt;a href=&quot;https://github.com/joaomdmoura/crewai/&quot;&gt;https://github.com/joaomdmoura/crewai/&lt;/a&gt; &amp;amp; &lt;a href=&quot;https://superagent.sh&quot;&gt;superagent.sh&lt;/a&gt; but I&amp;#39;m not sure if these are exactly what I&amp;#39;m looking for. &lt;/p&gt; &lt;p&gt;Thanks for your help ! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/BichonFrise_&quot;&gt; /u/BichonFrise_ &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bktz0j/how_to_build_ai_agents_for_information_retrieval/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bktz0j/how_to_build_ai_agents_for_information_retrieval/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bktz0j</id><link href="https://www.reddit.com/r/LangChain/comments/1bktz0j/how_to_build_ai_agents_for_information_retrieval/" /><updated>2024-03-22T08:00:15+00:00</updated><published>2024-03-22T08:00:15+00:00</published><title>How to build AI agents for information retrieval online</title></entry><entry><author><name>/u/o3omoomin</name><uri>https://www.reddit.com/user/o3omoomin</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I wrote this after looking at the Ensemble Retriever docs.&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://docs.llamaindex.ai/en/stable/examples/retrievers/ensemble_retrieval/&quot;&gt;https://docs.llamaindex.ai/en/stable/examples/retrievers/ensemble_retrieval/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;And can you explain the code for the evaluation part in detail with comments? A post is happening, but I don&amp;#39;t know what it is. Below is the evaluation code&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;from llama_index.core.evaluation import ( CorrectnessEvaluator, SemanticSimilarityEvaluator, RelevancyEvaluator, FaithfulnessEvaluator, PairwiseComparisonEvaluator, ) from llama_index.core.evaluation.eval_utils import ( get_responses; get_results_df; ) from llama_index.core.evaluation import BatchEvalRunner import numpy as np evaluator_c = CorrectnessEvaluator(llm=eval_llm) evaluator_s = SemanticSimilarityEvaluator() evaluator_r = RelevancyEvaluator(llm=eval_llm) evaluator_f = FaithfulnessEvaluator(llm=eval_llm) pairwise_evaluator = PairwiseComparisonEvaluator(llm=eval_llm) max_samples = 5 eval_qs = eval_dataset.questions qr_pairs = eval_dataset.qr_pairs ref_response_strs = [r for (_, r) in qr_pairs] base_query_engine = vector_indices[-1].as_query_engine(similarity_top_k=2) query_engine = RetrieverQueryEngine(retriever, node_postprocessors=[reranker]) base_pred_responses = get_responses( eval_qs[:max_samples], base_query_engine, show_progress=True ) pred_responses = get_responses( eval_qs[:max_samples], query_engine, show_progress=True ) sponse_strs = [str(p) for p in pred_responses] base_pred_response_strs = [str(p) for p in base_pred_responses] evaluator_dict = { &amp;quot;correctness&amp;quot;: evaluator_c, &amp;quot;faithfulness&amp;quot;: evaluator_f, &amp;quot;semantic_similarity&amp;quot;: evaluator_s, } batch_runner = BatchEvalRunner(evaluator_dict, workers=1, show_progress=True) eval_results = await batch_runner. evaluate_responses( queries=eval_qs[:max_samples], responses=pred_responses[:max_samples], reference=ref_response_strs[:max_samples], ) base_eval_results = await batch_runner.aevaluate_responses( queries=eval_qs[:max_samples], responses=base_pred_responses[:max_samples], reference=ref_response_strs[:max_samples], ) results_df = get_results_df( [eval_results, base_eval_results], [&amp;quot;Ensemble Retriever&amp;quot;, &amp;quot;Base Retriever&amp;quot;], [&amp;quot;correctness&amp;quot;, &amp;quot;faithfulness&amp;quot;, &amp;quot;semantic_similarity&amp;quot;], ) display(results_df) &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/o3omoomin&quot;&gt; /u/o3omoomin &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bktatx/please_explain_the_logic_behind_the_evaluation_of/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bktatx/please_explain_the_logic_behind_the_evaluation_of/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bktatx</id><link href="https://www.reddit.com/r/LangChain/comments/1bktatx/please_explain_the_logic_behind_the_evaluation_of/" /><updated>2024-03-22T07:09:51+00:00</updated><published>2024-03-22T07:09:51+00:00</published><title>Please explain the logic behind the evaluation of the llama index.</title></entry><entry><author><name>/u/The-Tank-849</name><uri>https://www.reddit.com/user/The-Tank-849</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Any of you are happy and have almost perfect result either their LLM chatbots with business data? Happy to discuss&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/The-Tank-849&quot;&gt; /u/The-Tank-849 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bkmo3b/chatbot_in_production/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bkmo3b/chatbot_in_production/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bkmo3b</id><link href="https://www.reddit.com/r/LangChain/comments/1bkmo3b/chatbot_in_production/" /><updated>2024-03-22T00:49:45+00:00</updated><published>2024-03-22T00:49:45+00:00</published><title>Chatbot in production</title></entry><entry><author><name>/u/rpatel09</name><uri>https://www.reddit.com/user/rpatel09</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;m looking to build a RAG app on our github repositories but wanted to ask if anyone has done something like this and what chunking strategies worked and didn&amp;#39;t work. I&amp;#39;ve been looking at semantic chunking but unsure how this would work with code? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/rpatel09&quot;&gt; /u/rpatel09 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bkkqel/chunking_strategies_for_code/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bkkqel/chunking_strategies_for_code/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bkkqel</id><link href="https://www.reddit.com/r/LangChain/comments/1bkkqel/chunking_strategies_for_code/" /><updated>2024-03-21T23:21:06+00:00</updated><published>2024-03-21T23:21:06+00:00</published><title>chunking strategies for code?</title></entry><entry><author><name>/u/internetcookiez</name><uri>https://www.reddit.com/user/internetcookiez</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I have a ChromaDB database which I can query information about a specific data, however, this data also has numerical data that I would like to transform into a SQL database, in .db form.&lt;/p&gt; &lt;p&gt;However, I want to be able to infer whether the LLM should call the vector db, and go through a ChromaDB chain for the answer, or go through an SQL chain. &lt;/p&gt; &lt;p&gt;How can I make this happen, automatically as the user asks questions? Basically, how can I create an agent that can determine which one to run?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/internetcookiez&quot;&gt; /u/internetcookiez &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bkqww1/how_can_i_combine_chromadb_and_sql_in_langchain/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bkqww1/how_can_i_combine_chromadb_and_sql_in_langchain/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bkqww1</id><link href="https://www.reddit.com/r/LangChain/comments/1bkqww1/how_can_i_combine_chromadb_and_sql_in_langchain/" /><updated>2024-03-22T04:29:18+00:00</updated><published>2024-03-22T04:29:18+00:00</published><title>How can I combine ChromaDB and SQL in langchain?</title></entry><entry><author><name>/u/FigureClassic6675</name><uri>https://www.reddit.com/user/FigureClassic6675</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/FigureClassic6675&quot;&gt; /u/FigureClassic6675 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bkp8fv/how_to_create_lead_capture_chatbot_with_function/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bkp8fv/how_to_create_lead_capture_chatbot_with_function/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bkp8fv</id><link href="https://www.reddit.com/r/LangChain/comments/1bkp8fv/how_to_create_lead_capture_chatbot_with_function/" /><updated>2024-03-22T02:57:18+00:00</updated><published>2024-03-22T02:57:18+00:00</published><title>How to create lead capture chatbot with function calling feature</title></entry><entry><author><name>/u/zkid18</name><uri>https://www.reddit.com/user/zkid18</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey folks,&lt;br/&gt; I am in the process of building my first custom GPT and have some questions regarding how to work properly with multimodal data. Let me explain what I am trying to achieve.&lt;br/&gt; I am creating a helper tool that will assist me in analyzing various pricing strategies of different SaaS tools. I have a dataset of 100k SaaS companies that have been labeled in some way, so I can cluster them based on their industry, category, etc.&lt;/p&gt; &lt;p&gt;Here is what I have as an input for my GPT so far:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;I have collected screenshots of their pricing pages, which are stored in S3.&lt;/li&gt; &lt;li&gt;I have collected the HTML for the pricing pages, which is stored in MongoDB.&lt;/li&gt; &lt;li&gt;I have a table of the companies with enriched data.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;I would like to build RAG on top of these documents, but I am a bit concerned about the next steps. My plan is to start with a simple one and use LlamaIndex. Here are the steps I have in mind:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Connect the data to the LlamaHub and pick the proper database. I want to keep the connection between the three mediums. and thus, I am not sure which database is best for my case. Should I use a vector database, graph database, or key-value database here?&lt;/li&gt; &lt;li&gt;Query the data and come up with evaluation metrics based on expert knowledge.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;I have some questions along the way:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Should I parse the data from the screenshots and HTML structure beforehand, or can I put it into storage as it is? Will it help with the quality of RAG?&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/zkid18&quot;&gt; /u/zkid18 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bkhdqe/need_advice_for_structuring_multimodal_data_for/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bkhdqe/need_advice_for_structuring_multimodal_data_for/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bkhdqe</id><link href="https://www.reddit.com/r/LangChain/comments/1bkhdqe/need_advice_for_structuring_multimodal_data_for/" /><updated>2024-03-21T21:02:22+00:00</updated><published>2024-03-21T21:02:22+00:00</published><title>Need advice for structuring multimodal data for RAG</title></entry><entry><author><name>/u/Obvious-Ad2752</name><uri>https://www.reddit.com/user/Obvious-Ad2752</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Does LangChain for Node.js offer the same level of functionality as its Python counterpart when it comes to functions and features? &lt;/p&gt; &lt;p&gt;If not, is there an alternative framework ?&lt;/p&gt; &lt;p&gt;Context : I am familiar with Node.js. I am looking to interact with an LLM for text extraction, NER, and coherence. I aim to take the response to create nodes, relationships, and labels in a Neo4j graph database.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Obvious-Ad2752&quot;&gt; /u/Obvious-Ad2752 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bkg3bg/langchain_functionality_in_nodejs_and_python_for/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bkg3bg/langchain_functionality_in_nodejs_and_python_for/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bkg3bg</id><link href="https://www.reddit.com/r/LangChain/comments/1bkg3bg/langchain_functionality_in_nodejs_and_python_for/" /><updated>2024-03-21T20:09:50+00:00</updated><published>2024-03-21T20:09:50+00:00</published><title>LangChain Functionality in Node.js and Python for Text Processing</title></entry><entry><author><name>/u/pratikkoti04</name><uri>https://www.reddit.com/user/pratikkoti04</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I want to build a LLM Chatbot that can follow a particular flow the one we build in intent based chatbot frameworks. I want the llm to collect some information from user based on it fetch some data handle fallback queries and it should not deviate from the flow handling multi turn conversations.Any idea or open source framework that does this? Basically I want to use RASA stories and feed it to LLM so that it can follow a particular conversational flow. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/pratikkoti04&quot;&gt; /u/pratikkoti04 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bkcllq/rule_based_llm_chatbot/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bkcllq/rule_based_llm_chatbot/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bkcllq</id><link href="https://www.reddit.com/r/LangChain/comments/1bkcllq/rule_based_llm_chatbot/" /><updated>2024-03-21T17:47:25+00:00</updated><published>2024-03-21T17:47:25+00:00</published><title>Rule Based LLM Chatbot</title></entry><entry><author><name>/u/BossHoggHazzard</name><uri>https://www.reddit.com/user/BossHoggHazzard</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;We know 3-large has a 8191 token context window. I have text articles that are anywhere from 2500-4500 tokens each. Is there any advantage to chunking these? Or will I lose some of the context splitting articles into pieces?&lt;/p&gt; &lt;p&gt;Is it better to just get embeddings on whole articles or is it still a good idea to split them up into paragraphs? Or both? Feed it whole articles and paragraphs?&lt;/p&gt; &lt;p&gt;Thanks in advance for your insight.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/BossHoggHazzard&quot;&gt; /u/BossHoggHazzard &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bkh8wq/textembedding3large_chunking_question_for_rag/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bkh8wq/textembedding3large_chunking_question_for_rag/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bkh8wq</id><link href="https://www.reddit.com/r/LangChain/comments/1bkh8wq/textembedding3large_chunking_question_for_rag/" /><updated>2024-03-21T20:57:09+00:00</updated><published>2024-03-21T20:57:09+00:00</published><title>text-embedding-3-large chunking question for RAG</title></entry><entry><author><name>/u/halixness</name><uri>https://www.reddit.com/user/halixness</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi. Im testing a variety of LLaMa2 7b and 13b (Hermes2Pro, MistralInstruct0.2, Chat, Solar10) as base for the React agent, but I cant get outputs consistently as Im encountering these issues:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;After providing the final answer, the LLM keeps running other actions and it gets off track with eg. Non relevant questions or even random programming code.&lt;/li&gt; &lt;li&gt;Sometimes it calls the tools incorrectly, especially if I switch to the structured chat agent (non existing arguments or swapped args between functions).&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;What Im guessing is that I need larger models. I would appreciate someone elses experience and takes on this. Thank you very much!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/halixness&quot;&gt; /u/halixness &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bkb1kc/suggestions_on_working_agents_and_base_llms/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bkb1kc/suggestions_on_working_agents_and_base_llms/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bkb1kc</id><link href="https://www.reddit.com/r/LangChain/comments/1bkb1kc/suggestions_on_working_agents_and_base_llms/" /><updated>2024-03-21T16:44:08+00:00</updated><published>2024-03-21T16:44:08+00:00</published><title>Suggestions on working agents and base LLMs?</title></entry><entry><author><name>/u/No_Donut_5349</name><uri>https://www.reddit.com/user/No_Donut_5349</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;m building a software application. I want to use a LLM as the basis. I will finetune the model with about 20,000 pages of legal text. Specifically laws all around the country. I will then use the trained model to answer help companies create compliant products and services. At the moment I am unsure as to the best way to go about it. My initial thought was to use gpt 3.5 and then further train it with the 20,000 pages of text. The text will be broken down by state and federal agencies. This text will be added to as new laws and regulations are passed.&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;The goal is for the model to only answer based on the data set it&amp;#39;s finetuned with. The data will be broken down by state. For example if they want information about loan requirements, they will ask the system and it will return with an outline of the requirements for loans by state. It will respond in a way that&amp;#39;s easy to understand.&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;My thinking is once I have all the data collected, using Langchain to fine tune the LLM. Am I on the right path here?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/No_Donut_5349&quot;&gt; /u/No_Donut_5349 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bk8t5f/need_input_on_software_project/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bk8t5f/need_input_on_software_project/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bk8t5f</id><link href="https://www.reddit.com/r/LangChain/comments/1bk8t5f/need_input_on_software_project/" /><updated>2024-03-21T15:10:34+00:00</updated><published>2024-03-21T15:10:34+00:00</published><title>Need input on Software Project</title></entry><entry><author><name>/u/guidsen15</name><uri>https://www.reddit.com/user/guidsen15</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi folks!&lt;/p&gt; &lt;p&gt;I have been interested in AI for some time, but now the time has come for I want to create a self-browsing agent that answers a certain question.&lt;/p&gt; &lt;p&gt;Couple of examples:&lt;/p&gt; &lt;p&gt;1 Prompt: what is the company &lt;a href=&quot;https://www.segment.com&quot;&gt;www.segment.com&lt;/a&gt; about?&lt;/p&gt; &lt;p&gt;- Should execute a Google search&lt;br/&gt; - Navigate to the about-us page&lt;br/&gt; - Read the page and return the result&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;2 Prompt: what is the difference between &lt;a href=&quot;https://www.segment.com&quot;&gt;www.segment.com&lt;/a&gt; and &lt;a href=&quot;https://www.intercom.com&quot;&gt;www.intercom.com&lt;/a&gt;?&lt;/p&gt; &lt;p&gt;- Should split this into multiple tasks, doing something similar to the workflow above.&lt;br/&gt; - Returns a detailed comparison based on the scanned pages&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;What are the best ways of implementing this? Are there any open-source frameworks that I might get inspired by?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/guidsen15&quot;&gt; /u/guidsen15 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bk4v99/best_way_to_create_an_ai_browsing_agent/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bk4v99/best_way_to_create_an_ai_browsing_agent/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bk4v99</id><link href="https://www.reddit.com/r/LangChain/comments/1bk4v99/best_way_to_create_an_ai_browsing_agent/" /><updated>2024-03-21T12:00:36+00:00</updated><published>2024-03-21T12:00:36+00:00</published><title>Best way to create an AI browsing agent</title></entry><entry><author><name>/u/ninja24x7</name><uri>https://www.reddit.com/user/ninja24x7</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I am implementing RAG and trying to understand what&amp;#39;s the advantage of Overlapping.&lt;br/&gt; Consider this text:&lt;br/&gt; &lt;code&gt;&amp;quot;One of the most important things I didn&amp;#39;t understand about the world when I was a child is the degree to which the returns for performance are superlinear.&amp;quot;&lt;/code&gt;&lt;br/&gt; which is chunked and overlapped as using Naive or any strategy :&lt;br/&gt; &lt;code&gt;chunk 1 : One of the most important things&lt;/code&gt;&lt;br/&gt; &lt;code&gt;Chunk 2 : things I didn&amp;#39;t understand about&lt;/code&gt;&lt;br/&gt; &lt;code&gt;chunk 3: about the world when I was a child&lt;/code&gt;&lt;br/&gt; and so on..&lt;br/&gt; As you can see there is a word overlap with the chunks.&lt;br/&gt; What advantage does LLM get when you feed overlapping &lt;code&gt;chunk2&lt;/code&gt; and &lt;code&gt;chunk3&lt;/code&gt; to execute RAG prompt against a user query. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/ninja24x7&quot;&gt; /u/ninja24x7 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bjxvov/what_is_the_advantage_of_overlapping_in_chunking/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bjxvov/what_is_the_advantage_of_overlapping_in_chunking/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bjxvov</id><link href="https://www.reddit.com/r/LangChain/comments/1bjxvov/what_is_the_advantage_of_overlapping_in_chunking/" /><updated>2024-03-21T04:12:54+00:00</updated><published>2024-03-21T04:12:54+00:00</published><title>what is the advantage of overlapping in chunking strategy</title></entry></feed>