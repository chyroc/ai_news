<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/"><category term="LangChain" label="r/LangChain"/><updated>2024-04-08T08:07:58+00:00</updated><icon>https://www.redditstatic.com/icon.png/</icon><id>/r/LangChain.rss</id><link rel="self" href="https://www.reddit.com/r/LangChain.rss" type="application/atom+xml" /><link rel="alternate" href="https://www.reddit.com/r/LangChain" type="text/html" /><subtitle>LangChain is an open-source framework and developer toolkit that helps developers get LLM applications from prototype to production. It is available for Python and Javascript at https://www.langchain.com/.</subtitle><title>LangChain</title><entry><author><name>/u/zchaarm</name><uri>https://www.reddit.com/user/zchaarm</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;A place for members of &lt;a href=&quot;/r/LangChain&quot;&gt;r/LangChain&lt;/a&gt; to chat with each other&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/zchaarm&quot;&gt; /u/zchaarm &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_10ljho9</id><link href="https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/" /><updated>2023-01-26T04:33:05+00:00</updated><published>2023-01-26T04:33:05+00:00</published><title>r/LangChain Lounge</title></entry><entry><author><name>/u/cryptokaykay</name><uri>https://www.reddit.com/user/cryptokaykay</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1byrqps/langtrace_preview_of_the_new_evaluation_dashboard/&quot;&gt; &lt;img src=&quot;https://external-preview.redd.it/Z86Ol2Jj2ayoGOd-Pfgkem_533RuJprm3gD5eJdyH8c.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=040d97e52a5d67184964ab3339a8b243b2414601&quot; alt=&quot;Langtrace: Preview of the new Evaluation dashboard&quot; title=&quot;Langtrace: Preview of the new Evaluation dashboard&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey,&lt;/p&gt; &lt;p&gt;I am building an open source project called Langtrace which lets you monitor, debug and evaluate the LLM requests made by your application.&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/Scale3-Labs/langtrace&quot;&gt;https://github.com/Scale3-Labs/langtrace&lt;/a&gt; . The integration is only 2 lines of code.&lt;/p&gt; &lt;p&gt;Currently building an Evaluations dashboard which is launching this week. It lets you do the following:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Create tests - like factual accuracy, bias detection etc. &lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Automatically capture the LLM calls to specific tests by passing a testId to the langtrace SDK installed in your code.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Evaluate and measure the overall success % and how success % trends over time.&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;The goal here is to get confidence with the model or RAG before deploying it to production.&lt;/p&gt; &lt;p&gt;Please check out the repository. Would love to hear your thoughts! Thanks!&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/5bracw5ki7tc1.png?width=2932&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1fe6fac6661d9a5c0c7f701c44d50435f45c7d7f&quot;&gt;https://preview.redd.it/5bracw5ki7tc1.png?width=2932&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1fe6fac6661d9a5c0c7f701c44d50435f45c7d7f&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/cryptokaykay&quot;&gt; /u/cryptokaykay &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1byrqps/langtrace_preview_of_the_new_evaluation_dashboard/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1byrqps/langtrace_preview_of_the_new_evaluation_dashboard/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1byrqps</id><media:thumbnail url="https://external-preview.redd.it/Z86Ol2Jj2ayoGOd-Pfgkem_533RuJprm3gD5eJdyH8c.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=040d97e52a5d67184964ab3339a8b243b2414601" /><link href="https://www.reddit.com/r/LangChain/comments/1byrqps/langtrace_preview_of_the_new_evaluation_dashboard/" /><updated>2024-04-08T07:21:23+00:00</updated><published>2024-04-08T07:21:23+00:00</published><title>Langtrace: Preview of the new Evaluation dashboard</title></entry><entry><author><name>/u/Zealousideal_Wolf624</name><uri>https://www.reddit.com/user/Zealousideal_Wolf624</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I just read their &lt;a href=&quot;https://blog.langchain.dev/langchain-documentation-refresh/&quot;&gt;new blog post&lt;/a&gt;, about the new documentation website. It&amp;#39;s very curious and funny.&lt;/p&gt; &lt;p&gt;It goes through the Diataxis taxonomy for documentation, which I find useful and aligns with how my brain works.&lt;/p&gt; &lt;p&gt;Just to throw everything out of the window and say: we mixed and matched every section of Diataxis and you can find tutorials spread all over the place, mixed with reference and explanations!&lt;/p&gt; &lt;p&gt;Take a look at this section of the post:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;This section should contain mostly conceptual Tutorials, References, and Explanations of the components they cover.&lt;/p&gt; &lt;p&gt;Note: As a general rule of thumb, everything covered in the Expression Language and Components sections (with the exception of the Composition section of components) should cover only components that exist in langchain_core.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Impressive! They need to explain what&amp;#39;s where, and even introduce a rule about langchain_core that is broken from the get go. And when you go to the socs the components section isn&amp;#39;t even in the menu to be selected!&lt;/p&gt; &lt;p&gt;I mean, just make it simple:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Tutorials (quick start, use cases with in depth explanations, etc)&lt;/li&gt; &lt;li&gt;How to guides (terse, context free guides such as how to create a chain, new runnable from scratch, new agent from scratch, how to visualize a chain, how to pass a system prompt to a model, how to make models spit structured output, etc)&lt;/li&gt; &lt;li&gt;Explanation (langchain purpose, package organization, what is LCEL, what is a chain/agent/runnable/etc, model vs chat model, what is a tool/toolkit, what is a function call etc). Accept a small amount of repetition from what we have in tutorials.&lt;/li&gt; &lt;li&gt;Reference (API docs)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Wouldn&amp;#39;t that be simpler? I&amp;#39;m so frustrated with this...&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Zealousideal_Wolf624&quot;&gt; /u/Zealousideal_Wolf624 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1by72bo/new_documentation_is_still_bad/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1by72bo/new_documentation_is_still_bad/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1by72bo</id><link href="https://www.reddit.com/r/LangChain/comments/1by72bo/new_documentation_is_still_bad/" /><updated>2024-04-07T15:24:32+00:00</updated><published>2024-04-07T15:24:32+00:00</published><title>New documentation is still bad</title></entry><entry><author><name>/u/Vri7</name><uri>https://www.reddit.com/user/Vri7</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;While going through the Langchain documentation, I came across the fact that LC is providing separate html and markdown splitter and you also have an option for the same two in code splitter as well.&lt;br/&gt; What is the difference between the two?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Vri7&quot;&gt; /u/Vri7 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1byrbxw/htmlmarkdown_splitter_vs_code_splitter_with/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1byrbxw/htmlmarkdown_splitter_vs_code_splitter_with/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1byrbxw</id><link href="https://www.reddit.com/r/LangChain/comments/1byrbxw/htmlmarkdown_splitter_vs_code_splitter_with/" /><updated>2024-04-08T06:54:02+00:00</updated><published>2024-04-08T06:54:02+00:00</published><title>HTML/MARKDOWN splitter vs Code splitter with html/markdown as language.</title></entry><entry><author><name>/u/Proud_Plant_826</name><uri>https://www.reddit.com/user/Proud_Plant_826</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I want to make a website where I want to display the responses I am generating using langchain agent. It is actually analysis of educational data, and generated responses are helpful to be used as teaching material. Hence, I want to document it. What would be the easiest and best way to direclty document it onto the website? It would be better to document the intermediate steps generated by the agent as well. Thank you&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Proud_Plant_826&quot;&gt; /u/Proud_Plant_826 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1byqonr/creation_of_website_to_visualize_responses/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1byqonr/creation_of_website_to_visualize_responses/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1byqonr</id><link href="https://www.reddit.com/r/LangChain/comments/1byqonr/creation_of_website_to_visualize_responses/" /><updated>2024-04-08T06:11:13+00:00</updated><published>2024-04-08T06:11:13+00:00</published><title>Creation of website to visualize responses generated by LangChain pandas dataframe agent</title></entry><entry><author><name>/u/OtherAd3010</name><uri>https://www.reddit.com/user/OtherAd3010</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Tiger: Neuralink for AI Agents (MIT) (Python)&lt;/p&gt; &lt;p&gt;Hello, we are developing a superstructure that provides an AI-Computer interface for AI agents created through the LangChain library, we have published it completely openly under the MIT license.&lt;/p&gt; &lt;p&gt;What it does: Just like human developers, it has some abilities such as running the codes it writes, making mouse and keyboard movements, writing and running Python functions for functions it does not have. AI literally thinks and the interface we provide transforms with real computer actions.&lt;/p&gt; &lt;p&gt;Those who want to contribute can provide support under the MIT license and code conduct. &lt;a href=&quot;https://github.com/Upsonic/Tiger&quot;&gt;https://github.com/Upsonic/Tiger&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/OtherAd3010&quot;&gt; /u/OtherAd3010 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bykpua/github_upsonictiger_neuralink_for_your_langchain/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bykpua/github_upsonictiger_neuralink_for_your_langchain/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bykpua</id><link href="https://www.reddit.com/r/LangChain/comments/1bykpua/github_upsonictiger_neuralink_for_your_langchain/" /><updated>2024-04-08T00:56:17+00:00</updated><published>2024-04-08T00:56:17+00:00</published><title>GitHub - Upsonic/Tiger: Neuralink for your LangChain Agents</title></entry><entry><author><name>/u/Calm_Pea_2428</name><uri>https://www.reddit.com/user/Calm_Pea_2428</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;m thinking about writing a detailed blog on the Challenges you face while scaling your RAG apps. Please comment some suggestions you would like me to discuss in the blog. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Calm_Pea_2428&quot;&gt; /u/Calm_Pea_2428 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1by7s2m/challenges_of_scaling_rag_applications/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1by7s2m/challenges_of_scaling_rag_applications/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1by7s2m</id><link href="https://www.reddit.com/r/LangChain/comments/1by7s2m/challenges_of_scaling_rag_applications/" /><updated>2024-04-07T15:55:01+00:00</updated><published>2024-04-07T15:55:01+00:00</published><title>Challenges of Scaling RAG applications</title></entry><entry><author><name>/u/Turbulent_Month_8771</name><uri>https://www.reddit.com/user/Turbulent_Month_8771</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey LangChain devs! üëã Check out SolidEdge ‚Äì the open source code comprehension tool you&amp;#39;ve been waiting for. Powered by GPT and built by devs like us, it&amp;#39;s your personal coding sidekick. With Notion integration, it keeps you aligned with business goals. No more feeling lost in code ‚Äì embrace true developer-centric coding! Star us on [GitHub](&lt;a href=&quot;https://github.com/AI-Citizen/SolidGPT&quot;&gt;https://github.com/AI-Citizen/SolidGPT&lt;/a&gt;) and dive in on the [VSCode Marketplace](&lt;a href=&quot;https://marketplace.visualstudio.com/items?itemName=AICT.solidgpt&quot;&gt;https://marketplace.visualstudio.com/items?itemName=AICT.solidgpt&lt;/a&gt;). Let&amp;#39;s revolutionize coding together! üöÄ&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Turbulent_Month_8771&quot;&gt; /u/Turbulent_Month_8771 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1byk8nk/solidedge_the_open_source_developerfirst_code/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1byk8nk/solidedge_the_open_source_developerfirst_code/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1byk8nk</id><link href="https://www.reddit.com/r/LangChain/comments/1byk8nk/solidedge_the_open_source_developerfirst_code/" /><updated>2024-04-08T00:33:56+00:00</updated><published>2024-04-08T00:33:56+00:00</published><title>üöÄ SolidEdge - The Open Source, Developer-First Code Comprehension Tool with Notion Integration</title></entry><entry><author><name>/u/Calm_Pea_2428</name><uri>https://www.reddit.com/user/Calm_Pea_2428</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Do you guys really think that using DSPy is a good idea over Langchain? For me I think, DSPy is not mature enough and LangChain provides so many things.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Calm_Pea_2428&quot;&gt; /u/Calm_Pea_2428 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1byn9o7/langchain_vs_dspy/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1byn9o7/langchain_vs_dspy/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1byn9o7</id><link href="https://www.reddit.com/r/LangChain/comments/1byn9o7/langchain_vs_dspy/" /><updated>2024-04-08T02:59:53+00:00</updated><published>2024-04-08T02:59:53+00:00</published><title>LangChain vs DSPy</title></entry><entry><author><name>/u/wordlessilence</name><uri>https://www.reddit.com/user/wordlessilence</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I just joined here not long ago. I have learned knowledges from the official documents,but i think it&amp;#39;s not suitable for me. I have some programming basics in python,i hope to get some friendly and excellent tutorials that use ollama to create rather than openai based in langchain.In additional,i want to know how to get high-quality information sources.English is not my native language,i&amp;#39;m sorry to any grammar errors.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/wordlessilence&quot;&gt; /u/wordlessilence &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1byn1u2/help_for_tutorial/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1byn1u2/help_for_tutorial/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1byn1u2</id><link href="https://www.reddit.com/r/LangChain/comments/1byn1u2/help_for_tutorial/" /><updated>2024-04-08T02:48:51+00:00</updated><published>2024-04-08T02:48:51+00:00</published><title>Help for tutorial</title></entry><entry><author><name>/u/alexndr2022</name><uri>https://www.reddit.com/user/alexndr2022</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;m currently working with AWS Bedrock and Langchain while it retrieves good answers when I want to ask it stuff like comparing documents or listing the documents on its data sources Its unable to do it. It seems like its not conscious of its environment. Does anyone has some experience working with this? Like I want it to be a typical RAG application based on some documents but I want it to be conscious of the data it have like comparing versions of the same documents...&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/alexndr2022&quot;&gt; /u/alexndr2022 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1by8u01/how_to_make_a_rag_conscious_of_the_documents_it/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1by8u01/how_to_make_a_rag_conscious_of_the_documents_it/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1by8u01</id><link href="https://www.reddit.com/r/LangChain/comments/1by8u01/how_to_make_a_rag_conscious_of_the_documents_it/" /><updated>2024-04-07T16:39:39+00:00</updated><published>2024-04-07T16:39:39+00:00</published><title>How to make a RAG conscious of the documents it have? Amazon Bedrock</title></entry><entry><author><name>/u/UpvoteBeast</name><uri>https://www.reddit.com/user/UpvoteBeast</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;We are trying to get our feet wet with RAG with a small engineering team. I want to build a RAG system querying an extensive internal documents system. With the available choice of LLMs, embedding models, vector databases, hyperparameters it&amp;#39;s easy to get overwhelmed. So what I want is to create a test dataset manually with like ten-twenty questions and answers we would like to receive (or multiple answer options for each question??) and automate deployment of several combinations of different LLMs, hyperparameters, embedding models, etc and compare the actuals against the gold standard answers (using ROUGE score maybe??). Does that make sense? Are there any tools/frameworks I need to be aware of to do something like that for me? Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/UpvoteBeast&quot;&gt; /u/UpvoteBeast &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1by54rq/evaluating_rag_on_custom_qas/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1by54rq/evaluating_rag_on_custom_qas/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1by54rq</id><link href="https://www.reddit.com/r/LangChain/comments/1by54rq/evaluating_rag_on_custom_qas/" /><updated>2024-04-07T13:58:11+00:00</updated><published>2024-04-07T13:58:11+00:00</published><title>Evaluating RAG on custom Q&amp;As</title></entry><entry><author><name>/u/No_Garbage9512</name><uri>https://www.reddit.com/user/No_Garbage9512</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I have been working on the large data. The data consist of talks from the different anchor persons, also there are comments in numerical representation like for example &amp;quot;&lt;em&gt;how many people agreed and how many are disagreed&amp;quot;.&lt;/em&gt; and on which session they are disscusing the point of agenda and on which bill number they place a talk. &lt;/p&gt; &lt;p&gt;So my question is: I want to generate the document a large document which contains multiple sections almost 20 sections. Each section has diverse and different instructions so how do I manage my vectordb calls. ? because each section of the document is different so how to make a calls to retreival automatically based on the sections conditions. ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/No_Garbage9512&quot;&gt; /u/No_Garbage9512 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1by9iqn/working_with_diverse_data_to_create_30_to_35/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1by9iqn/working_with_diverse_data_to_create_30_to_35/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1by9iqn</id><link href="https://www.reddit.com/r/LangChain/comments/1by9iqn/working_with_diverse_data_to_create_30_to_35/" /><updated>2024-04-07T17:08:55+00:00</updated><published>2024-04-07T17:08:55+00:00</published><title>Working with diverse data to create 30 to 35 pages document and Managing the retrieval.</title></entry><entry><author><name>/u/Me7a1hed</name><uri>https://www.reddit.com/user/Me7a1hed</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;ve been playing around with pulling data from APIs, feeding that into the language model&amp;#39;s chat and then conversing over that context. One recent use case I&amp;#39;ve been considering is allowing the agent to do multiple tasks, so the API calls are determined by the AI solution rather than being hard coded.&lt;/p&gt; &lt;p&gt;The obvious solution to me is to use a conversational chat agent that can react and use tools that make the API calls. The problem I&amp;#39;m experiencing though is that the returned content from the API call function doesn&amp;#39;t appear to get stored in my ConversationBufferMemory. It only stores the human inputs, and the AI&amp;#39;s outputs, but not the &amp;quot;Observation&amp;quot; that contains the returned full context.&lt;/p&gt; &lt;p&gt;The reason I&amp;#39;d like this in the history is to prevent the need for additional API calls when the user begins to ask follow-up questions about the data which has already been retrieved by a tool. It seems very inefficient to pull the same data every time they have a follow-up question, which is what it&amp;#39;s currently doing. Below is an example of what I&amp;#39;m doing.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;template = &amp;quot;First, evaluate the context provided by preceding messages to inform your response. Only if you can&amp;#39;t answer the questions from that should you use available tools.&amp;quot; memory = ConversationBufferMemory( memory_key=&amp;quot;chat_history&amp;quot;, return_messages=True ) tools = [get_ticket_info] agent = initialize_agent( tools=tools, llm=llm, agent=&amp;#39;chat-conversational-react-description&amp;#39;, memory=memory, verbose=True, agent_kwargs={&amp;quot;prefix&amp;quot;: template} ) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;I&amp;#39;ve also tried using the create_react_agent() function to create the agent which I know is the new way to do this, but it doesn&amp;#39;t appear to be conversational as it will not remember any message history. This is why I&amp;#39;m using the initialize_agent() function instead. At this point I&amp;#39;m tempted to go away with LangChain and use OpenAI&amp;#39;s API directly where I would have more control over what goes into the message history.&lt;/p&gt; &lt;p&gt;Any tricks you&amp;#39;re aware of, or am I approaching this in a naive way? Appreciate any wisdom you can bestow.&lt;/p&gt; &lt;p&gt;Edit: I got this to work easily enough using OpenAI directly. See my response below to one of my replies if interested.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Me7a1hed&quot;&gt; /u/Me7a1hed &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1byd0e8/storing_tool_retrieved_data_in_conversation/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1byd0e8/storing_tool_retrieved_data_in_conversation/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1byd0e8</id><link href="https://www.reddit.com/r/LangChain/comments/1byd0e8/storing_tool_retrieved_data_in_conversation/" /><updated>2024-04-07T19:31:38+00:00</updated><published>2024-04-07T19:31:38+00:00</published><title>Storing tool retrieved data in conversation history</title></entry><entry><author><name>/u/redd-dev</name><uri>https://www.reddit.com/user/redd-dev</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey guys, so I am building a chatbot which uses a RAG-tuned LLM in AWS Bedrock (and deployed using AWS Lambda endpoints).&lt;/p&gt; &lt;p&gt;How do I avoid my LLM from being having to be RAG-tuned every single time a user asks his/her first question? I am thinking of storing the RAG-tuned LLM in an AWS S3 bucket. If I do this, I believe I will have to store the LLM model parameters and the vector store index in the S3 bucket. Doing this would mean every single time a user asks his/her first question (and subsequent questions), I will just be loading the the RAG-tuned LLM from the S3 bucket (rather than having to run RAG-tuning every single time when a user asks his/her first question, which will save me RAG-tuning costs and latency).&lt;/p&gt; &lt;p&gt;Would this design work? I have a sample of my script below:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;import os import json import boto3 from langchain.document_loaders import PyPDFLoader from langchain.text_splitter import RecursiveCharacterTextSplitter from langchain.embeddings import BedrockEmbeddings from langchain.vectorstores import FAISS from langchain.indexes import VectorstoreIndexCreator from langchain.llms.bedrock import Bedrock def save_to_s3(model_params, vector_store_index, bucket_name, model_key, index_key): s3 = boto3.client(&amp;#39;s3&amp;#39;) # Save model parameters to S3 s3.put_object(Body=model_params, Bucket=bucket_name, Key=model_key) # Save vector store index to S3 s3.put_object(Body=vector_store_index, Bucket=bucket_name, Key=index_key) def load_from_s3(bucket_name, model_key, index_key): s3 = boto3.client(&amp;#39;s3&amp;#39;) # Load model parameters from S3 model_params = s3.get_object(Bucket=bucket_name, Key=model_key)[&amp;#39;Body&amp;#39;].read() # Load vector store index from S3 vector_store_index = s3.get_object(Bucket=bucket_name, Key=index_key)[&amp;#39;Body&amp;#39;].read() return model_params, vector_store_index def initialize_hr_system(bucket_name, model_key, index_key): s3 = boto3.client(&amp;#39;s3&amp;#39;) try: # Check if model parameters and vector store index exist in S3 s3.head_object(Bucket=bucket_name, Key=model_key) s3.head_object(Bucket=bucket_name, Key=index_key) # Load model parameters and vector store index from S3 model_params, vector_store_index = load_from_s3(bucket_name, model_key, index_key) # Deserialize and reconstruct the RAG-tuned LLM and vector store index llm = Bedrock.deserialize(json.loads(model_params)) index = VectorstoreIndexCreator.deserialize(json.loads(vector_store_index)) except s3.exceptions.ClientError: # Model parameters and vector store index don&amp;#39;t exist in S3 # Create them and save to S3 data_load = PyPDFLoader(&amp;#39;Glossary_of_Terms.pdf&amp;#39;) data_split = RecursiveCharacterTextSplitter(separators=[&amp;quot;\n\n&amp;quot;, &amp;quot;\n&amp;quot;, &amp;quot; &amp;quot;, &amp;quot;&amp;quot;], chunk_size=100, chunk_overlap=10) data_embeddings = BedrockEmbeddings(credentials_profile_name=&amp;#39;default&amp;#39;, model_id=&amp;#39;amazon.titan-embed-text-v1&amp;#39;) data_index = VectorstoreIndexCreator(text_splitter=data_split, embedding=data_embeddings, vectorstore_cls=FAISS) index = data_index.from_loaders([data_load]) llm = Bedrock( credentials_profile_name=&amp;#39;default&amp;#39;, model_id=&amp;#39;mistral.mixtral-8x7b-instruct-v0:1&amp;#39;, model_kwargs={ &amp;quot;max_tokens_to_sample&amp;quot;: 3000, &amp;quot;temperature&amp;quot;: 0.1, &amp;quot;top_p&amp;quot;: 0.9 } ) # Serialize model parameters and vector store index serialized_model_params = json.dumps(llm.serialize()) serialized_vector_store_index = json.dumps(index.serialize()) # Save model parameters and vector store index to S3 save_to_s3(serialized_model_params, serialized_vector_store_index, bucket_name, model_key, index_key) return index, llm def hr_rag_response(index, llm, question): hr_rag_query = index.query(question=question, llm=llm) return hr_rag_query # S3 bucket configuration bucket_name = &amp;#39;your-bucket-name&amp;#39; model_key = &amp;#39;models/chatbot_model.json&amp;#39; index_key = &amp;#39;indexes/chatbot_index.json&amp;#39; # Initialize the system index, llm = initialize_hr_system(bucket_name, model_key, index_key) # Serve user requests while True: user_question = input(&amp;quot;User: &amp;quot;) response = hr_rag_response(index, llm, user_question) print(&amp;quot;Chatbot:&amp;quot;, response) &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/redd-dev&quot;&gt; /u/redd-dev &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1by17qh/how_to_deploy_a_ragtuned_ai_chatbotllm_using_aws/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1by17qh/how_to_deploy_a_ragtuned_ai_chatbotllm_using_aws/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1by17qh</id><link href="https://www.reddit.com/r/LangChain/comments/1by17qh/how_to_deploy_a_ragtuned_ai_chatbotllm_using_aws/" /><updated>2024-04-07T10:23:23+00:00</updated><published>2024-04-07T10:23:23+00:00</published><title>How to deploy a RAG-tuned AI chatbot/LLM using AWS Bedrock (with Langchain functions)</title></entry><entry><author><name>/u/Crazy_Cut_7250</name><uri>https://www.reddit.com/user/Crazy_Cut_7250</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1by66py/quota_exceeded_error/&quot;&gt; &lt;img src=&quot;https://preview.redd.it/5gxw1qi4l2tc1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=de243bd8dbaaf01ba810158680c37480a924fc4a&quot; alt=&quot;Quota exceeded error&quot; title=&quot;Quota exceeded error&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Crazy_Cut_7250&quot;&gt; /u/Crazy_Cut_7250 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://i.redd.it/5gxw1qi4l2tc1.png&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1by66py/quota_exceeded_error/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1by66py</id><media:thumbnail url="https://preview.redd.it/5gxw1qi4l2tc1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=de243bd8dbaaf01ba810158680c37480a924fc4a" /><link href="https://www.reddit.com/r/LangChain/comments/1by66py/quota_exceeded_error/" /><updated>2024-04-07T14:45:55+00:00</updated><published>2024-04-07T14:45:55+00:00</published><title>Quota exceeded error</title></entry><entry><author><name>/u/chaitu9701</name><uri>https://www.reddit.com/user/chaitu9701</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I have one banking related document with several overlapping topics. Say one topic is related to credit card request, another related to cheque book request, another relating to account deactivation request. Mind that each of topic in itself are lengthy.&lt;/p&gt; &lt;p&gt;When in the retrieval chain, I ask a question &amp;quot;how to raise requests&amp;quot;, the result is a mixture from all of the above topics. First few lines describe credit card procedure and then bridge to checkbook. Which is wrong as each process has a different steps.&lt;/p&gt; &lt;p&gt;I&amp;#39;m using chunking strategy of 1000, default sentence transformers embedding, qdrant for as retriever, and gpt3.5 turbo 16k for llm.&lt;/p&gt; &lt;p&gt;Also the llm gives a disclaimer/note at the end saying that steps vary per organisation. Tried several prompts to remove disclaimer but nothing seems to work.&lt;/p&gt; &lt;p&gt;Any help / prompt would be greatly appreciated.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/chaitu9701&quot;&gt; /u/chaitu9701 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bxwxtu/rag_returns_concocted_results/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bxwxtu/rag_returns_concocted_results/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bxwxtu</id><link href="https://www.reddit.com/r/LangChain/comments/1bxwxtu/rag_returns_concocted_results/" /><updated>2024-04-07T05:48:32+00:00</updated><published>2024-04-07T05:48:32+00:00</published><title>RAG returns concocted results</title></entry><entry><author><name>/u/ramirez_tn</name><uri>https://www.reddit.com/user/ramirez_tn</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;ol&gt; &lt;li&gt;I loaded and split a PDF document using PDFMiner (I also tried a couple of other loaders)&lt;/li&gt; &lt;li&gt;I embedded the result and stored it in VectorDB&lt;/li&gt; &lt;li&gt;I retrieved the Data with RetrievalQA and a question like &amp;quot;What did this document say about Eye safety ?&amp;quot; which is mentioned a couple of times in the 80 pages document&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;The LLM always answers with : &amp;quot;it looks like there nothing mentioned about Eye safety &amp;quot;&lt;/p&gt; &lt;p&gt;FYI: When I check how the PDF is loaded it shows the content related to eye safety in the pages but it has a lot of \n and it include headers. I don&amp;#39;t know if this is contributing to the bad behavior&lt;/p&gt; &lt;p&gt;I am new to Langchain and it is driving me crazy, please help !&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/ramirez_tn&quot;&gt; /u/ramirez_tn &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bxs6g9/retriever_is_not_returning_proper_answers_to/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bxs6g9/retriever_is_not_returning_proper_answers_to/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bxs6g9</id><link href="https://www.reddit.com/r/LangChain/comments/1bxs6g9/retriever_is_not_returning_proper_answers_to/" /><updated>2024-04-07T01:33:13+00:00</updated><published>2024-04-07T01:33:13+00:00</published><title>retriever is not returning proper answers to obvious questions</title></entry><entry><author><name>/u/ZuckyFox</name><uri>https://www.reddit.com/user/ZuckyFox</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bxs70y/gemini/&quot;&gt; &lt;img src=&quot;https://external-preview.redd.it/MTd0eWtwZnZueXNjMY4zffG6ljuKgIwhcpPfTvcb1KD0RlSYVhImNFkRFBXF.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a49cb05502dd40e63d0e9349ef7de5ad9cbcbbdd&quot; alt=&quot;Gemini üëçüåö&quot; title=&quot;Gemini üëçüåö&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Never knew 14 yrs ago, Rick astley taught about LangChain through his songs. ü§ØüòÇüòÇ&lt;/p&gt; &lt;h1&gt;aiml #aiforfun #rofl #gemini #google&lt;/h1&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/ZuckyFox&quot;&gt; /u/ZuckyFox &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://v.redd.it/um4exjmvnysc1&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bxs70y/gemini/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1bxs70y</id><media:thumbnail url="https://external-preview.redd.it/MTd0eWtwZnZueXNjMY4zffG6ljuKgIwhcpPfTvcb1KD0RlSYVhImNFkRFBXF.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=a49cb05502dd40e63d0e9349ef7de5ad9cbcbbdd" /><link href="https://www.reddit.com/r/LangChain/comments/1bxs70y/gemini/" /><updated>2024-04-07T01:34:04+00:00</updated><published>2024-04-07T01:34:04+00:00</published><title>Gemini üëçüåö</title></entry><entry><author><name>/u/crookedhell</name><uri>https://www.reddit.com/user/crookedhell</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;This is a code snippet from my chatbot model&lt;/p&gt; &lt;pre&gt;&lt;code&gt;def create_embeddings(): embeddings = HuggingFaceEmbeddings(model_name=&amp;#39;sentence-transformers/all-MiniLM-L6-v2&amp;#39;, model_kwargs={&amp;#39;device&amp;#39;: &amp;#39;cuda&amp;#39;}) return embeddings &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Initially I ran it using &amp;#39;device&amp;#39; : &amp;#39;cpu&amp;#39; but the chatbot was extremely slow.&lt;br/&gt; So I installed the cuda toolkit along with nsight. The code gave me a &amp;quot;torch not compiled with cuda enabled&amp;quot; error.&lt;br/&gt; So I uninstalled and reinstalled torch with cuda and the code started working just fine.&lt;br/&gt; But the chatbot was giving outputs as slow as it was earlier, when I checked the task manager, python was still heavily utilizing my cpu and not utilizing the gpu at all.&lt;br/&gt; I have a gtx1650 and this is a code snippet from a chatbot in a virtual environment (all libraries installed there). Am I making a stupid error?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/crookedhell&quot;&gt; /u/crookedhell &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bxmka8/python_not_utilizing_gpu_even_with_cuda_enabled/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bxmka8/python_not_utilizing_gpu_even_with_cuda_enabled/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bxmka8</id><link href="https://www.reddit.com/r/LangChain/comments/1bxmka8/python_not_utilizing_gpu_even_with_cuda_enabled/" /><updated>2024-04-06T21:19:16+00:00</updated><published>2024-04-06T21:19:16+00:00</published><title>Python not utilizing GPU even with CUDA enabled</title></entry><entry><author><name>/u/BrilliantNose2000</name><uri>https://www.reddit.com/user/BrilliantNose2000</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I have a SQL database and want users to be able to query it using English sentences. Currently I have implemented it using a simple NET application, calling OpenAPI. In short:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;User enter a query&lt;/li&gt; &lt;li&gt;The query is converted to SQL using OpenAPI&lt;/li&gt; &lt;li&gt;The query is run towards a SQL database&lt;/li&gt; &lt;li&gt;In case of syntax errors, the application feeds them back to OpenAPI and asks for a fix &lt;/li&gt; &lt;li&gt;The data is shown to the user&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Works maybe 80-90% of the time. &lt;/p&gt; &lt;p&gt;I have been reading about Langchain and watching various tutorials on YouTube but I don&amp;#39;t really get what it is adding.&lt;/p&gt; &lt;p&gt;Cound someone help me understand how Langchain would help implementing the above? Would it add something which I&amp;#39;m just not seeing? The application is very simple so far, about 100 lines of code including presentation. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/BrilliantNose2000&quot;&gt; /u/BrilliantNose2000 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bxm44q/should_i_be_using_langchain/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bxm44q/should_i_be_using_langchain/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bxm44q</id><link href="https://www.reddit.com/r/LangChain/comments/1bxm44q/should_i_be_using_langchain/" /><updated>2024-04-06T21:00:10+00:00</updated><published>2024-04-06T21:00:10+00:00</published><title>Should I be using Langchain?</title></entry><entry><author><name>/u/KarbohJorneKraft</name><uri>https://www.reddit.com/user/KarbohJorneKraft</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;When integrating a Retriever-Augmented Generation (RAG) model with a Large Language Model (LLM) to process documents containing tabular data and embedded decision trees, the goal is to respond to user prompts that necessitate traversing the documents (retrieved by the RAG) and evaluating a decision tree. is anyone working on this? it is non-trivial &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/KarbohJorneKraft&quot;&gt; /u/KarbohJorneKraft &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bxf6bi/using_rag_nlp_llm_for_decision_tree_evaluation/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bxf6bi/using_rag_nlp_llm_for_decision_tree_evaluation/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bxf6bi</id><link href="https://www.reddit.com/r/LangChain/comments/1bxf6bi/using_rag_nlp_llm_for_decision_tree_evaluation/" /><updated>2024-04-06T16:07:23+00:00</updated><published>2024-04-06T16:07:23+00:00</published><title>using RAG, NLP, LLM for decision tree evaluation when embedded in tabular data within otherwise unstructured documents</title></entry><entry><author><name>/u/Silver_Equivalent_58</name><uri>https://www.reddit.com/user/Silver_Equivalent_58</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I have a simple RAG pipeline, but now say the user is not satisfied with the response(basically a thumbs up or down), how can i incorporate this feedback to improve my RAG in a continuous manner? Thanks&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Silver_Equivalent_58&quot;&gt; /u/Silver_Equivalent_58 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bx74lo/how_to_incorporate_user_feedback_in_rag/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bx74lo/how_to_incorporate_user_feedback_in_rag/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bx74lo</id><link href="https://www.reddit.com/r/LangChain/comments/1bx74lo/how_to_incorporate_user_feedback_in_rag/" /><updated>2024-04-06T09:05:31+00:00</updated><published>2024-04-06T09:05:31+00:00</published><title>How to incorporate user feedback in RAG?</title></entry><entry><author><name>/u/vishal2045ks</name><uri>https://www.reddit.com/user/vishal2045ks</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;ol&gt; &lt;li&gt;You have to solve a multi-label classification problem statement.&lt;/li&gt; &lt;li&gt;It contains two files: train.csv and test.csv.&lt;/li&gt; &lt;li&gt;The dataset contains the following columns: &lt;ul&gt; &lt;li&gt;LossDescription: Description of Event&lt;/li&gt; &lt;li&gt;ResultingInjuryDesc: Injury Description&lt;/li&gt; &lt;li&gt;PartInjuredDesc: Body Part Injured Description&lt;/li&gt; &lt;li&gt;Cause - Hierarchy 1: Cause Hierarchy 1&lt;/li&gt; &lt;li&gt;Body Part - Hierarchy 1: Body Part Hierarchy 1&lt;/li&gt; &lt;li&gt;Index: Identifier&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Tasks:&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;Perform exploratory data analysis (EDA) on the dataset.&lt;/li&gt; &lt;li&gt;Train multi-label classification models to predict &amp;quot;Cause - Hierarchy 1&amp;quot; and &amp;quot;Body Part - Hierarchy 1&amp;quot; when other columns are given. Two models will be required to predict each target variable. please tell the approach to solve this problem&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/vishal2045ks&quot;&gt; /u/vishal2045ks &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bxp7p6/need_help_regarding_llm_project/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bxp7p6/need_help_regarding_llm_project/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bxp7p6</id><link href="https://www.reddit.com/r/LangChain/comments/1bxp7p6/need_help_regarding_llm_project/" /><updated>2024-04-06T23:15:00+00:00</updated><published>2024-04-06T23:15:00+00:00</published><title>Need help regarding LLM project</title></entry><entry><author><name>/u/sarthak_uchiha</name><uri>https://www.reddit.com/user/sarthak_uchiha</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I am building a RAG of patients clinical trial and over which preventing prompt injection also , first I am finding some data on clinical trials can some suggest ,where can I get a sample data like that &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/sarthak_uchiha&quot;&gt; /u/sarthak_uchiha &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bx8l2f/need_clinical_trials_dataset/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bx8l2f/need_clinical_trials_dataset/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bx8l2f</id><link href="https://www.reddit.com/r/LangChain/comments/1bx8l2f/need_clinical_trials_dataset/" /><updated>2024-04-06T10:43:46+00:00</updated><published>2024-04-06T10:43:46+00:00</published><title>Need clinical trials dataset</title></entry><entry><author><name>/u/yashdeep1929</name><uri>https://www.reddit.com/user/yashdeep1929</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I have a dataset of 400 resumes in .txt format. I want to build a model that can generate responses to specific candidate queries like &amp;#39;Tell me the skillset of XYZ,&amp;#39; but also handle generic queries like &amp;#39;Tell me the names of people who went to Ivy League schools.&amp;#39; While RAG using OpenAI works well for candidate-specific queries, it struggles with generic ones.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/yashdeep1929&quot;&gt; /u/yashdeep1929 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bwz6op/need_help_regarding_a_llm_project/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bwz6op/need_help_regarding_a_llm_project/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bwz6op</id><link href="https://www.reddit.com/r/LangChain/comments/1bwz6op/need_help_regarding_a_llm_project/" /><updated>2024-04-06T01:28:09+00:00</updated><published>2024-04-06T01:28:09+00:00</published><title>Need help regarding a LLM project</title></entry></feed>