<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/"><category term="LangChain" label="r/LangChain"/><updated>2024-05-28T21:56:25+00:00</updated><icon>https://www.redditstatic.com/icon.png/</icon><id>/r/LangChain.rss</id><link rel="self" href="https://www.reddit.com/r/LangChain.rss" type="application/atom+xml" /><link rel="alternate" href="https://www.reddit.com/r/LangChain" type="text/html" /><subtitle>LangChain is an open-source framework and developer toolkit that helps developers get LLM applications from prototype to production. It is available for Python and Javascript at https://www.langchain.com/.</subtitle><title>LangChain</title><entry><author><name>/u/zchaarm</name><uri>https://www.reddit.com/user/zchaarm</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;A place for members of &lt;a href=&quot;/r/LangChain&quot;&gt;r/LangChain&lt;/a&gt; to chat with each other&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/zchaarm&quot;&gt; /u/zchaarm &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_10ljho9</id><link href="https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/" /><updated>2023-01-26T04:33:05+00:00</updated><published>2023-01-26T04:33:05+00:00</published><title>r/LangChain Lounge</title></entry><entry><author><name>/u/GeorgiaWitness1</name><uri>https://www.reddit.com/user/GeorgiaWitness1</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d2iubg/a_library_just_for_document_extraction_with_llms/&quot;&gt; &lt;img src=&quot;https://b.thumbs.redditmedia.com/Nfj1Xt2q-wi-SUE2TM66lJeoEkGv1In07CH5M7vbu_o.jpg&quot; alt=&quot;A library just for Document Extraction with LLMs, connector to Langchain | ExtractThinker&quot; title=&quot;A library just for Document Extraction with LLMs, connector to Langchain | ExtractThinker&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;A month back I did a &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c6afp1/creating_a_framework_like_langchain_but_just_for/&quot;&gt;post&lt;/a&gt; about creating a library just focused on Extraction for documents. Was well received here and in other places, including by some companies. So I gave it a try.&lt;/p&gt; &lt;p&gt;After a month and 2k+ lines of code, I created this repo, based on the previous one, that will contain a full-open source code. Contains already close to 200+ (as the writing of this post).&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/enoch3712/ExtractThinker&quot;&gt;https://github.com/enoch3712/ExtractThinker&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;The motivation&lt;/h1&gt; &lt;p&gt;Langchain works great integrating the several pieces &lt;strong&gt;but tends to be a pain to extract data from documents and other sources&lt;/strong&gt;. ExtractThinker falls into the class of tools like instructor (pydantic outputs) and litellm (agnostic call between LLM models), which solves a specific problem. A bit more high level yes, but the focus is the same. &lt;strong&gt;Extraction for documents like ORM with LLM.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;You can read in detail here:&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://medium.com/towards-artificial-intelligence/extractthinker-ai-document-intelligence-with-llms-72cbce1890ef&quot;&gt;https://medium.com/towards-artificial-intelligence/extractthinker-ai-document-intelligence-with-llms-72cbce1890ef&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Basic use case and idea&lt;/h1&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/6qqufnee063d1.png?width=904&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=55bbd1aad1606fe6b83f4f07f338efab4deb6023&quot;&gt;https://preview.redd.it/6qqufnee063d1.png?width=904&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=55bbd1aad1606fe6b83f4f07f338efab4deb6023&lt;/a&gt;&lt;/p&gt; &lt;p&gt;You can then use a middleware to inject the QR code content, and so on. I think you get the drill&lt;/p&gt; &lt;h1&gt;Why is this useful? Just do GPT-4o everything, use vision if needed&lt;/h1&gt; &lt;p&gt;The project will focus in two things:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Reducing the pain of leading with document extraction&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The project will handle tasks such as classifying and grouping documents. For example, it can be used to separate content within a collection of PDFs with random pages.&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/k10voqea063d1.png?width=1170&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a64eaf8a015668ef85e0126226dd6e942ca53e5a&quot;&gt;splitting in action&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This would give you a list already separated and extracted (e.g first 2 pages invoice, last page driver&amp;#39;s license). This classification will expand to multiple strategies and techniques.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Reducing costs for scalability&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Build your architecture that works as well as GPT-4 with scalability and low cost. More in this article:&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://medium.com/@enoch3712/how-companies-use-llms-to-process-4-000-cvs-for-1-extractthinker-3fa0815057c3?sk=7fe626701a203135370e95f68bcb59f1&quot;&gt;https://medium.com/@enoch3712/how-companies-use-llms-to-process-4-000-cvs-for-1-extractthinker-3fa0815057c3?sk=7fe626701a203135370e95f68bcb59f1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Just finished the final touches for the code, and it&amp;#39;s a real use case that worked out great using inexpensive quantized models from deepinfra.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The code still not production-ready and missing most of the features, but will make more sense with templates once i build the documentation.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I intend to eventually integrate this into langchain to be used as &lt;strong&gt;pypdf&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;If you can assist me with features as issues, use cases, or if anyone is interested in giving it a try, I would greatly appreciate it.&lt;/p&gt; &lt;p&gt;Thank you for your time.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/GeorgiaWitness1&quot;&gt; /u/GeorgiaWitness1 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d2iubg/a_library_just_for_document_extraction_with_llms/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d2iubg/a_library_just_for_document_extraction_with_llms/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1d2iubg</id><media:thumbnail url="https://b.thumbs.redditmedia.com/Nfj1Xt2q-wi-SUE2TM66lJeoEkGv1In07CH5M7vbu_o.jpg" /><link href="https://www.reddit.com/r/LangChain/comments/1d2iubg/a_library_just_for_document_extraction_with_llms/" /><updated>2024-05-28T12:58:12+00:00</updated><published>2024-05-28T12:58:12+00:00</published><title>A library just for Document Extraction with LLMs, connector to Langchain | ExtractThinker</title></entry><entry><author><name>/u/Gloomy-Traffic4964</name><uri>https://www.reddit.com/user/Gloomy-Traffic4964</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I find trying to do streaming makes everything else harder to do. Especially the checkpointer for message history.&lt;/p&gt; &lt;p&gt;This is my setup:&lt;/p&gt; &lt;pre&gt;&lt;code&gt; class GraphState(TypedDict): question: str messages: Annotated[list, add_messages] documents: List[str] workflow = StateGraph(GraphState) workflow.add_node(&amp;quot;retrieve&amp;quot;, retrieve) workflow.add_node(&amp;quot;generate&amp;quot;, generate) workflow.set_entry_point(&amp;quot;retrieve&amp;quot;) workflow.add_edge(&amp;quot;retrieve&amp;quot;, &amp;quot;generate&amp;quot;) workflow.add_edge(&amp;quot;generate&amp;quot;, END) pool = AsyncConnectionPool( conninfo=&amp;quot;postgresql://...&amp;quot;, max_size=20, ) # PostgresSaver.create_tables(pool) checkpoint = PostgresSaver( #package is langchain_postgres==0.0.3 serializer=PickleCheckpointSerializer(), async_connection=pool, ) app = workflow.compile(checkpointer=checkpoint) config = { &amp;quot;configurable&amp;quot;: { &amp;quot;thread_id&amp;quot;: &amp;quot;1223&amp;quot; } } async for event in app.astream_events({&amp;quot;messages&amp;quot;: [(&amp;quot;user&amp;quot;, user_content)], &amp;quot;question&amp;quot;: user_content}, config, version=&amp;quot;v1&amp;quot;): state = app.get_state(config) #this is the line that errors try: kind = event[&amp;quot;event&amp;quot;] except: kind = None if kind and kind == &amp;quot;on_chat_model_stream&amp;quot;: content = event[&amp;quot;data&amp;quot;][&amp;quot;chunk&amp;quot;].content if content: message = {&amp;quot;content&amp;quot;: content} await self.send(text_data=json.dumps(message)) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Specifically, I want to be able to use get_state() to get the document references from an answer (which is in the GraphState), as well as to do conditional interrupt like in &lt;a href=&quot;https://langchain-ai.github.io/langgraph/tutorials/customer-support/customer-support/#state-assistant&quot;&gt;this &lt;/a&gt;Customer Service LangGraph example (which requires get_state()).&lt;/p&gt; &lt;p&gt;In generral, is there a better way to handle the checkpointer while streaming the response?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Gloomy-Traffic4964&quot;&gt; /u/Gloomy-Traffic4964 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d2li6s/async_streaming_makes_everything_else_harder/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d2li6s/async_streaming_makes_everything_else_harder/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1d2li6s</id><link href="https://www.reddit.com/r/LangChain/comments/1d2li6s/async_streaming_makes_everything_else_harder/" /><updated>2024-05-28T14:58:09+00:00</updated><published>2024-05-28T14:58:09+00:00</published><title>Async streaming makes everything else harder</title></entry><entry><author><name>/u/rai_shi</name><uri>https://www.reddit.com/user/rai_shi</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d2eooc/how_to_make_rag_chain_faster_so_we_get_answer/&quot;&gt; &lt;img src=&quot;https://b.thumbs.redditmedia.com/mWWERioLyboV63rfWmqbccwffIRrWFxV91i_3-l0gFo.jpg&quot; alt=&quot;How to make RAG chain faster so we get answer faster&quot; title=&quot;How to make RAG chain faster so we get answer faster&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;We created a chatbot with Llama2-7b-chat, RAG architecture using LangChain, Qdrant VectorDB, and for web side Django. Everything works well. Even we hold the chat history. Now the only problem is the time we get the answer. Sometimes it takes 15 seconds to return the answer from the RAG chain and it&amp;#39;s too bad.&lt;/p&gt; &lt;p&gt;What should we do to make the system faster?&lt;/p&gt; &lt;p&gt;If it&amp;#39;s necessary I can share some of the codes.&lt;/p&gt; &lt;p&gt;Edit:&lt;/p&gt; &lt;p&gt;We are running all system in local. Because system have to be in local. Also we are running in GPU.&lt;/p&gt; &lt;p&gt;GPU: RTX A4000, 16GB&lt;/p&gt; &lt;p&gt;RAM: 32 GB&lt;/p&gt; &lt;p&gt;CPU: Intel® Xeon(R) W-2235 CPU @ 3.80GHz × 12&lt;/p&gt; &lt;p&gt;OS: Ubuntu 22.04.4 LTS&lt;/p&gt; &lt;p&gt;We realized that retrieving from the DB chain is taking the most time of the work. Actually, we configured it with some parameters like collection is now with Manhattan distance, 384 size (because of the embedded model), and binary quantization. We read that binary is x40 faster. But could we make more configuration?&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/30knup1kq63d1.png?width=1918&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8faf69a2a62d5e628fd46c4da9f909a3d5838c09&quot;&gt;https://preview.redd.it/30knup1kq63d1.png?width=1918&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8faf69a2a62d5e628fd46c4da9f909a3d5838c09&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/bedycqvkq63d1.png?width=1916&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8ab2aec755a0e58f155026d49d9c4442b408e90b&quot;&gt;https://preview.redd.it/bedycqvkq63d1.png?width=1916&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8ab2aec755a0e58f155026d49d9c4442b408e90b&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/rai_shi&quot;&gt; /u/rai_shi &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d2eooc/how_to_make_rag_chain_faster_so_we_get_answer/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d2eooc/how_to_make_rag_chain_faster_so_we_get_answer/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1d2eooc</id><media:thumbnail url="https://b.thumbs.redditmedia.com/mWWERioLyboV63rfWmqbccwffIRrWFxV91i_3-l0gFo.jpg" /><link href="https://www.reddit.com/r/LangChain/comments/1d2eooc/how_to_make_rag_chain_faster_so_we_get_answer/" /><updated>2024-05-28T08:39:52+00:00</updated><published>2024-05-28T08:39:52+00:00</published><title>How to make RAG chain faster so we get answer faster</title></entry><entry><author><name>/u/nicoloboschi</name><uri>https://www.reddit.com/user/nicoloboschi</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;This is a poetry plugin to generate Dockerfile and images automatically.&lt;/p&gt; &lt;p&gt;This is a perfect choice if you built the Langchain application and you’re looking for to distribute is as microservice in the cloud. &lt;/p&gt; &lt;p&gt;This project lets you generate a docker image or just a Dockerfile for your poetry application without manual setup&lt;/p&gt; &lt;p&gt;It is meant for production images.&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/nicoloboschi/poetry-dockerize-plugin&quot;&gt;https://github.com/nicoloboschi/poetry-dockerize-plugin&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://pypi.org/project/poetry-dockerize-plugin/&quot;&gt;https://pypi.org/project/poetry-dockerize-plugin/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Get started with&lt;/p&gt; &lt;pre&gt;&lt;code&gt;poetry self add poetry-dockerize-plugin@latest &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This command generates a production-ready, optimized python image:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;poetry dockerize &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;or to generate a Dockerfile&lt;/p&gt; &lt;pre&gt;&lt;code&gt;poetry dockerize --generate &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/nicoloboschi&quot;&gt; /u/nicoloboschi &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d2rq1k/dockerize_langchainlangserve_applications/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d2rq1k/dockerize_langchainlangserve_applications/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1d2rq1k</id><link href="https://www.reddit.com/r/LangChain/comments/1d2rq1k/dockerize_langchainlangserve_applications/" /><updated>2024-05-28T19:14:29+00:00</updated><published>2024-05-28T19:14:29+00:00</published><title>Dockerize langchain/langserve applications</title></entry><entry><author><name>/u/phicreative1997</name><uri>https://www.reddit.com/user/phicreative1997</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d2qqqy/building_an_agent_for_data_visualization_plotly/&quot;&gt; &lt;img src=&quot;https://external-preview.redd.it/vr9_wzciKe4iumRmBa3g5c3KTA0PkUeTfMxce8sgy8U.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e97032457cd9f6f0da1334fcf91b3a6e02ee9234&quot; alt=&quot;Building an Agent for Data Visualization (Plotly)&quot; title=&quot;Building an Agent for Data Visualization (Plotly)&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/phicreative1997&quot;&gt; /u/phicreative1997 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://medium.com/@arslanshahid-1997/building-an-agent-for-data-visualization-plotly-39310034c4e9&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d2qqqy/building_an_agent_for_data_visualization_plotly/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1d2qqqy</id><media:thumbnail url="https://external-preview.redd.it/vr9_wzciKe4iumRmBa3g5c3KTA0PkUeTfMxce8sgy8U.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=e97032457cd9f6f0da1334fcf91b3a6e02ee9234" /><link href="https://www.reddit.com/r/LangChain/comments/1d2qqqy/building_an_agent_for_data_visualization_plotly/" /><updated>2024-05-28T18:35:17+00:00</updated><published>2024-05-28T18:35:17+00:00</published><title>Building an Agent for Data Visualization (Plotly)</title></entry><entry><author><name>/u/cryptokaykay</name><uri>https://www.reddit.com/user/cryptokaykay</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d2nfuz/shopify_all_in_on_promptfoo/&quot;&gt; &lt;img src=&quot;https://preview.redd.it/qgg45w3d073d1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ab39be24a8d163f8797a5621ffba8c8ee470b5a6&quot; alt=&quot;Shopify all in on Promptfoo&quot; title=&quot;Shopify all in on Promptfoo&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I am a big fan of Promptfoo aswell. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/cryptokaykay&quot;&gt; /u/cryptokaykay &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://i.redd.it/qgg45w3d073d1.jpeg&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d2nfuz/shopify_all_in_on_promptfoo/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1d2nfuz</id><media:thumbnail url="https://preview.redd.it/qgg45w3d073d1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=ab39be24a8d163f8797a5621ffba8c8ee470b5a6" /><link href="https://www.reddit.com/r/LangChain/comments/1d2nfuz/shopify_all_in_on_promptfoo/" /><updated>2024-05-28T16:19:36+00:00</updated><published>2024-05-28T16:19:36+00:00</published><title>Shopify all in on Promptfoo</title></entry><entry><author><name>/u/rai_shi</name><uri>https://www.reddit.com/user/rai_shi</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;We created a chatbot with Llama2-7b-chat, RAG architecture using LangChain, Qdrant VectorDB, and for web side Django. Everything works well. Additionally, our system works locally. Now the only problem is the time we get the answer. Sometimes it takes 15 seconds to return the answer from the RAG chain and it&amp;#39;s too bad. After monitoring with LangSmith we saw that retrieve_documents takes most of the run time.&lt;/p&gt; &lt;p&gt;Our Qdrant VectorDB collections distance type Manhattan and binary quantization. The embedding model is sentence-transformers/all-MiniLM-L6-v2 so the size is 384. Also we clear the cache everytime. Here is the chains code and the invoke , (We write two system prompt for holding the chat history.)&lt;/p&gt; &lt;pre&gt;&lt;code&gt;class RAG(): def _init_(self): self.embed_model_id = &amp;#39;sentence-transformers/all-MiniLM-L6-v2&amp;#39; self.initializeEmbeddingModel(device) self.model_id = &amp;#39;meta-llama/Llama-2-13b-chat-hf&amp;#39; self.hf_auth = &amp;#39;token&amp;#39; self.initializeLLM() self.data_collection_name = &amp;quot;collection_name&amp;quot; self.db_url = &amp;quot;qdrant-docker-port&amp;quot; self.initializeDBclient() self.initializesearchDB() self.LLMpipeline() self.configSystemPrompt() self.RAGpipeline() def initializeEmbeddingModel(device): pass def initializeLLM(self): bnb_config = transformers.BitsAndBytesConfig( load_in_4bit=True, bnb_4bit_quant_type=&amp;#39;nf4&amp;#39;, bnb_4bit_use_double_quant=True, bnb_4bit_compute_dtype=bfloat16 ) model_config = transformers.AutoConfig.from_pretrained( self.model_id, token=self.hf_auth ) self.LLMmodel = transformers.AutoModelForCausalLM.from_pretrained( self.model_id, trust_remote_code=True, config=model_config, quantization_config=bnb_config, device_map=&amp;#39;auto&amp;#39;, token=self.hf_auth ) self.LLMmodel.eval() def initializeDBclient(self): pass def initializesearchDB(self): # self.searchDB creation pass def LLMpipeline(self): tokenizer = transformers.AutoTokenizer.from_pretrained( self.model_id, token=self.hf_auth ) pipeline = transformers.pipeline( model=self.LLMmodel, tokenizer=tokenizer, return_full_text=True, task=&amp;#39;text-generation&amp;#39;, temperature=0.0, max_new_tokens=512, repetition_penalty=1.1, do_sample=False ) self.llm = HuggingFacePipeline(pipeline=pipeline) def configSystemPrompt(self): # sohbet geçmişi için alt prompt ve genel system promptu tanımlıyoruz contextualize_q_system_prompt = &amp;quot;&amp;quot;&amp;quot;prompt&amp;quot;&amp;quot;&amp;quot; self.contextualize_q_prompt = ChatPromptTemplate.from_messages( [ (&amp;quot;system&amp;quot;, contextualize_q_system_prompt), MessagesPlaceholder(&amp;quot;chat_history&amp;quot;), (&amp;quot;human&amp;quot;, &amp;quot;{input}&amp;quot;), ] ) qa_system_prompt = &amp;quot;&amp;quot;&amp;quot;prompt/ {context}&amp;quot;&amp;quot;&amp;quot; self.qa_prompt = ChatPromptTemplate.from_messages( [ (&amp;quot;system&amp;quot;, qa_system_prompt), MessagesPlaceholder(&amp;quot;chat_history&amp;quot;), (&amp;quot;human&amp;quot;, &amp;quot;{input}&amp;quot;), ] ) def RAGpipeline(self): retriever = self.searchDB.as_retriever(search_kwargs={&amp;quot;k&amp;quot;:6 }) history_aware_retriever = create_history_aware_retriever( self.llm, retriever, self.contextualize_q_prompt ) question_answer_chain = create_stuff_documents_chain(self.llm, self.qa_prompt) self.rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain) def ragQA(self, question, history): ai_msg = self.rag_chain.invoke({&amp;quot;input&amp;quot;: question, &amp;quot;chat_history&amp;quot;: history}) return ai_msg def clear_cache(self): torch.cuda.empty_cache() gc.collect() &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;LangSmith monitor result is here,&lt;/p&gt; &lt;p&gt;retrieval_chain 15.71s&lt;/p&gt; &lt;ul&gt; &lt;li&gt;retrieve_documents 11.66s &lt;ul&gt; &lt;li&gt;HuggingFacePipeline gpt2 11.61s&lt;/li&gt; &lt;li&gt;Retriever 0.04s&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;stuff_documents_chain 3.96s &lt;ul&gt; &lt;li&gt;format_inputs 0.01s &lt;ul&gt; &lt;li&gt;format_docs 0.00s&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;HuggingFacePipeline 3.94s&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;So the problem is mostly retrieve_documents. What should we do to make the system faster?&lt;/p&gt; &lt;p&gt;Edit:&lt;/p&gt; &lt;p&gt;GPU: RTX A4000, 16GB&lt;/p&gt; &lt;p&gt;RAM: 32 GB&lt;/p&gt; &lt;p&gt;CPU: Intel® Xeon(R) W-2235 CPU @ 3.80GHz × 12&lt;/p&gt; &lt;p&gt;OS: Ubuntu 22.04.4 LTS&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/rai_shi&quot;&gt; /u/rai_shi &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d2ooua/how_to_make_rag_retrieve_documents_chain_make/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d2ooua/how_to_make_rag_retrieve_documents_chain_make/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1d2ooua</id><link href="https://www.reddit.com/r/LangChain/comments/1d2ooua/how_to_make_rag_retrieve_documents_chain_make/" /><updated>2024-05-28T17:12:13+00:00</updated><published>2024-05-28T17:12:13+00:00</published><title>How to make RAG retrieve_documents chain make faster?</title></entry><entry><author><name>/u/Ok_Session_7304</name><uri>https://www.reddit.com/user/Ok_Session_7304</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hello everyone,&lt;/p&gt; &lt;p&gt;I&amp;#39;m exploring multi-agent systems and am curious about the role of an orchestrator in managing tasks among specialized agents. For instance, imagine a scenario where there are four agents, each designed to perform one of the basic mathematical operations: addition, subtraction, multiplication, and division.&lt;/p&gt; &lt;p&gt;If the orchestrator receives a question like &amp;quot;How much is 4 + 4?&amp;quot;, how does it determine which agent to send the query to? What logic or algorithms might it use to parse the question and delegate the task appropriately?&lt;/p&gt; &lt;p&gt;Additionally, if anyone could provide insights or resources into how such systems are generally designed or any examples of such orchestrators in action, it would be greatly appreciated.&lt;/p&gt; &lt;p&gt;Thanks in advance for your help and sharing your knowledge!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Ok_Session_7304&quot;&gt; /u/Ok_Session_7304 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d2omoe/how_does_an_llm_orchestrator_decide_which_agent/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d2omoe/how_does_an_llm_orchestrator_decide_which_agent/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1d2omoe</id><link href="https://www.reddit.com/r/LangChain/comments/1d2omoe/how_does_an_llm_orchestrator_decide_which_agent/" /><updated>2024-05-28T17:09:49+00:00</updated><published>2024-05-28T17:09:49+00:00</published><title>How does an LLM orchestrator decide which agent to use in a multi-agent system?</title></entry><entry><author><name>/u/JKSenior</name><uri>https://www.reddit.com/user/JKSenior</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi, I am working with langChain right now and created a FAISS vector store. Since today, my kernel crashes when running a similarity search on my vector store. Has anyone an idea why this is happening?&lt;/p&gt; &lt;pre&gt;&lt;code&gt;from langchain_community.document_loaders import PyPDFLoader from langchain_community.vectorstores import FAISS f = open(&amp;#39;credentials.txt&amp;#39;) OPENAI_API_KEY = f.read() embeddings_model = OpenAIEmbeddings(api_key=OPENAI_API_KEY) document_loader = PyPDFLoader(&amp;#39;filename.pdf&amp;#39;) text_splitter=RecursiveCharacterTextSplitter() documents = document_loader.load_and_split(text_splitter) vectorstore = FAISS.from_documents(documents, embeddings_model) vectorstore.similarity_search(&amp;#39;query&amp;#39;) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;MacBook Pro intel, python 3.9, jupyter notebook, langchain 0.2.0&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/JKSenior&quot;&gt; /u/JKSenior &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d2gxqi/kernel_crashes_for_faiss_similarity_search/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d2gxqi/kernel_crashes_for_faiss_similarity_search/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1d2gxqi</id><link href="https://www.reddit.com/r/LangChain/comments/1d2gxqi/kernel_crashes_for_faiss_similarity_search/" /><updated>2024-05-28T11:13:40+00:00</updated><published>2024-05-28T11:13:40+00:00</published><title>Kernel crashes for FAISS similarity search</title></entry><entry><author><name>/u/Minute_Scientist8107</name><uri>https://www.reddit.com/user/Minute_Scientist8107</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d2i9fr/llm_to_get_the_relevant_table_from_db/&quot;&gt; &lt;img src=&quot;https://b.thumbs.redditmedia.com/mM2WSN5QqrKBeDXL-Bo76v--LrrG6-cOIltF3tsZb5w.jpg&quot; alt=&quot;LLM to get the relevant table from db &quot; title=&quot;LLM to get the relevant table from db &quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey guys ! I have Azure OpenAI GPT 4 LLM connected to Azure SQL DB. When I use this command db.get_usable_table_names() must return all the tables available, how every I&amp;#39;m getting the names of the ones that are dbo.table_name&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/fw1477bhu53d1.png?width=334&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=55b2f27dd632bf0c984eb3f71b22d7f8d052071c&quot;&gt;https://preview.redd.it/fw1477bhu53d1.png?width=334&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=55b2f27dd632bf0c984eb3f71b22d7f8d052071c&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/rwqeu6nku53d1.png?width=377&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ca4eaeffdebd441b75ad22122f3e4798ecc67ae3&quot;&gt;https://preview.redd.it/rwqeu6nku53d1.png?width=377&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ca4eaeffdebd441b75ad22122f3e4798ecc67ae3&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The tables in my db are as follows:&lt;/p&gt; &lt;p&gt;Question 1:How can I have the llm return all the available tables ?&lt;br/&gt; Question 2: Let&amp;#39;s say I prompt the llm like &amp;quot;Aggregate what age group customers prefer what products.&amp;quot; This prompt has two tables: Customer and Product. So I want the llm to join these two tables, and generate a response. So we want the llm to go through multiple tables and give a response.&lt;br/&gt; How can I achieve this?&lt;br/&gt; Thanks for your help !&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Minute_Scientist8107&quot;&gt; /u/Minute_Scientist8107 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d2i9fr/llm_to_get_the_relevant_table_from_db/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d2i9fr/llm_to_get_the_relevant_table_from_db/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1d2i9fr</id><media:thumbnail url="https://b.thumbs.redditmedia.com/mM2WSN5QqrKBeDXL-Bo76v--LrrG6-cOIltF3tsZb5w.jpg" /><link href="https://www.reddit.com/r/LangChain/comments/1d2i9fr/llm_to_get_the_relevant_table_from_db/" /><updated>2024-05-28T12:28:13+00:00</updated><published>2024-05-28T12:28:13+00:00</published><title>LLM to get the relevant table from db</title></entry><entry><author><name>/u/jy2k</name><uri>https://www.reddit.com/user/jy2k</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;What do you use to handle agents permissin to retrieve access certain data? Let&amp;#39;s say your building an agent in a large org. How do you make sure it can access finance question if you asked it a question about finance. It needs to validate if you are permitted to access that kind of data.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/jy2k&quot;&gt; /u/jy2k &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d2cezb/agent_permission/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d2cezb/agent_permission/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1d2cezb</id><link href="https://www.reddit.com/r/LangChain/comments/1d2cezb/agent_permission/" /><updated>2024-05-28T05:56:42+00:00</updated><published>2024-05-28T05:56:42+00:00</published><title>Agent permission</title></entry><entry><author><name>/u/glip-glop-evil</name><uri>https://www.reddit.com/user/glip-glop-evil</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;m working on a project that needs to create long term memory with a knowledge graph. Does anyone have any experience with any specific libraries to achieve this?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/glip-glop-evil&quot;&gt; /u/glip-glop-evil &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d2dwpo/knowledge_graphs_and_longterm_memory/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d2dwpo/knowledge_graphs_and_longterm_memory/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1d2dwpo</id><link href="https://www.reddit.com/r/LangChain/comments/1d2dwpo/knowledge_graphs_and_longterm_memory/" /><updated>2024-05-28T07:41:15+00:00</updated><published>2024-05-28T07:41:15+00:00</published><title>Knowledge Graphs and long-term memory</title></entry><entry><author><name>/u/OtherAd3010</name><uri>https://www.reddit.com/user/OtherAd3010</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d2ebjg/i_just_made_a_openai_chatgpt_macos_clone_for/&quot;&gt; &lt;img src=&quot;https://external-preview.redd.it/yiLZNizOqeTGU311KWeYM6UbyTqjaCXj0ToJTGscvOo.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=86bea2b068f187fe638d3e41fddc1b81e06b399b&quot; alt=&quot;I just made a OpenAI ChatGPT MacOS Clone for Windows and Ubuntu and MIT licenced&quot; title=&quot;I just made a OpenAI ChatGPT MacOS Clone for Windows and Ubuntu and MIT licenced&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;h1&gt;Explanation and Reason&lt;/h1&gt; &lt;p&gt;Hi i am python developer and after OpenAI GPT-4o launch, i just i little bit angry because the app that they talked about just work in MacOS and a joke, it will come to windows november lol. Come on bro, if there is API, developers like us can make this app and release in just few days with langchain agent and tool infrastructure.&lt;/p&gt; &lt;p&gt;So i just release our GPT-4o clone and its totaly usable. You can use to take meeting notes, writing code and copying to your clipboard, or read and remember your calendar. There is unlimited possibilities.&lt;/p&gt; &lt;h1&gt;Current Features&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;**Screen Read**&lt;/li&gt; &lt;li&gt;Microphone&lt;/li&gt; &lt;li&gt;**System Audio**&lt;/li&gt; &lt;li&gt;Memory&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;**Clipboard**&lt;/li&gt; &lt;li&gt;Search Engines&lt;/li&gt; &lt;li&gt;**Python and SH Interpreters**&lt;/li&gt; &lt;li&gt;Writing and Running Scripts&lt;/li&gt; &lt;li&gt;Using your Telegram Account&lt;/li&gt; &lt;li&gt;Knowledge Management&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Open Call to devs&lt;/h1&gt; &lt;p&gt;If there is some people to interest and develop this and adding some features just like auto take screen shot each 5 second and say somethings if something wrong. We can open a &lt;strong&gt;GitHub Organization&lt;/strong&gt; and develop together. Just for open-source, just for competition.&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/vcbo2sw4l43d1.png?width=656&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=29c11615550af7eebdcaa7cdaf31d2c986536a44&quot;&gt;https://preview.redd.it/vcbo2sw4l43d1.png?width=656&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=29c11615550af7eebdcaa7cdaf31d2c986536a44&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/onuratakan/gpt-computer-assistant&quot;&gt;onuratakan/gpt-computer-assistant: gpt-4o Desktop Personel Asisstant (github.com)&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/OtherAd3010&quot;&gt; /u/OtherAd3010 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d2ebjg/i_just_made_a_openai_chatgpt_macos_clone_for/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d2ebjg/i_just_made_a_openai_chatgpt_macos_clone_for/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1d2ebjg</id><media:thumbnail url="https://external-preview.redd.it/yiLZNizOqeTGU311KWeYM6UbyTqjaCXj0ToJTGscvOo.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=86bea2b068f187fe638d3e41fddc1b81e06b399b" /><link href="https://www.reddit.com/r/LangChain/comments/1d2ebjg/i_just_made_a_openai_chatgpt_macos_clone_for/" /><updated>2024-05-28T08:11:18+00:00</updated><published>2024-05-28T08:11:18+00:00</published><title>I just made a OpenAI ChatGPT MacOS Clone for Windows and Ubuntu and MIT licenced</title></entry><entry><author><name>/u/SpaceXDragon17</name><uri>https://www.reddit.com/user/SpaceXDragon17</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hello everyone! I&amp;#39;m in the process of developing a custom LLM for my startup. I&amp;#39;m looking for help and would appreciate it if anyone on this forum has experience with this and would be willing to meet via Zoom to discuss it further.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/SpaceXDragon17&quot;&gt; /u/SpaceXDragon17 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d2g20b/need_to_build_custom_llm_expert_needed/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d2g20b/need_to_build_custom_llm_expert_needed/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1d2g20b</id><link href="https://www.reddit.com/r/LangChain/comments/1d2g20b/need_to_build_custom_llm_expert_needed/" /><updated>2024-05-28T10:16:58+00:00</updated><published>2024-05-28T10:16:58+00:00</published><title>Need to Build Custom LLM - Expert Needed</title></entry><entry><author><name>/u/Puzzleheaded_Bee5489</name><uri>https://www.reddit.com/user/Puzzleheaded_Bee5489</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;m using OpenAI GPT 3.5 turbo for summarising data from sensitive documents, which contains some of my personal information. Currently, I&amp;#39;m manually removing some of the sensitive data from the inputs. I want to know if LangChain or any other tool/library handles this automatically without me getting involved?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Puzzleheaded_Bee5489&quot;&gt; /u/Puzzleheaded_Bee5489 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d1v710/hashingmasking_sensitive_data_before_sending_out/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d1v710/hashingmasking_sensitive_data_before_sending_out/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1d1v710</id><link href="https://www.reddit.com/r/LangChain/comments/1d1v710/hashingmasking_sensitive_data_before_sending_out/" /><updated>2024-05-27T16:13:41+00:00</updated><published>2024-05-27T16:13:41+00:00</published><title>Hashing/Masking sensitive data before sending out to OpenAI</title></entry><entry><author><name>/u/AnEdgeLordWeeb</name><uri>https://www.reddit.com/user/AnEdgeLordWeeb</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey guys! I&amp;#39;m trying to build a chatbot that offer video games recommendations using LangGraph.&lt;/p&gt; &lt;p&gt;Right now, I was working on the agent called &amp;quot;Game Search Agent&amp;quot; which objective is to search the web for the best results to the user query.&lt;/p&gt; &lt;p&gt;Problem is, the execution never moves from this node to the __end__ one. It will always be stuck in a loop, where the tool function is called without providing a concrete final answer in the end.&lt;/p&gt; &lt;p&gt;Can somebody please take a look at this code snippet and tell me please what&amp;#39;s wrong? Thank you!&lt;/p&gt; &lt;pre&gt;&lt;code&gt;from typing import Annotated, List from langchain_openai import ChatOpenAI from langchain_community.tools.tavily_search import TavilySearchResults from langchain_core.prompts import PromptTemplate from langchain_core.messages import BaseMessage from typing_extensions import TypedDict from langgraph.graph import StateGraph from langgraph.graph.message import add_messages from langgraph.prebuilt import ToolNode, tools_condition os.environ[&amp;quot;TAVILY_API_KEY&amp;quot;] = &amp;quot;your_api_key&amp;quot; os.environ[&amp;quot;OPEN_AI_KEY&amp;quot;] = &amp;quot;your_api_key&amp;quot; class AgentState(TypedDict): messages: Annotated[List[BaseMessage], add_messages] query: str games: List[str] tool = TavilySearchResults(max_results=2) tools = [tool] llm = ChatOpenAI(model=&amp;quot;gpt-4o&amp;quot;, temperature=0) llm_with_tools = llm.bind_tools(tools) def game_title_search(state: AgentState): game_search_prompt = PromptTemplate( template=&amp;quot;&amp;quot;&amp;quot;You are part of a chatbot that provides personalized video game recommendations based on user preferences. \n Your task is to search for video games that match the user query, using the Tavily API. \n Only return the titles of the games. \n The number of games to return is limited to 5. \n\n The results provided will STRICTLY look as follows (Python list): \n [&amp;quot;game_title_1&amp;quot;, &amp;quot;game_title_2&amp;quot;, &amp;quot;game_title_3&amp;quot;, ...] \n\n User Query: {query}&amp;quot;&amp;quot;&amp;quot;, input_variables=[&amp;quot;query&amp;quot;], ) game_search = game_search_prompt | llm_with_tools return {&amp;quot;messages&amp;quot;: [game_search.invoke({&amp;quot;query&amp;quot;: state[&amp;quot;query&amp;quot;]})]} graph_builder = StateGraph(AgentState) graph_builder.add_node(&amp;quot;game_search&amp;quot;, game_title_search) tool_node = ToolNode(tools=[tool]) graph_builder.add_node(&amp;quot;tools&amp;quot;, tool_node) graph_builder.add_conditional_edges( &amp;quot;game_search&amp;quot;, tools_condition, ) graph_builder.add_edge(&amp;quot;tools&amp;quot;, &amp;quot;game_search&amp;quot;) graph_builder.set_entry_point(&amp;quot;game_search&amp;quot;) graph = graph_builder.compile() while True: user_input = input(&amp;quot;User: &amp;quot;) if user_input.lower() in [&amp;quot;quit&amp;quot;, &amp;quot;exit&amp;quot;, &amp;quot;q&amp;quot;]: print(&amp;quot;Goodbye!&amp;quot;) break for event in graph.stream({&amp;quot;messages&amp;quot;: [user_input], &amp;quot;query&amp;quot;: user_input}, {&amp;quot;recursion_limit&amp;quot;: 150}): for value in event.values(): if isinstance(value[&amp;quot;messages&amp;quot;][-1], BaseMessage): print(&amp;quot;Assistant:&amp;quot;, value[&amp;quot;messages&amp;quot;][-1].content) &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/AnEdgeLordWeeb&quot;&gt; /u/AnEdgeLordWeeb &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d24j6j/agent_enters_a_loop_of_continuous_tool_calling/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d24j6j/agent_enters_a_loop_of_continuous_tool_calling/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1d24j6j</id><link href="https://www.reddit.com/r/LangChain/comments/1d24j6j/agent_enters_a_loop_of_continuous_tool_calling/" /><updated>2024-05-27T22:45:47+00:00</updated><published>2024-05-27T22:45:47+00:00</published><title>Agent enters a loop of continuous tool calling without exiting and providing a final answer</title></entry><entry><author><name>/u/Minute_Scientist8107</name><uri>https://www.reddit.com/user/Minute_Scientist8107</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey all, I have a dataset in Azure and I have a SQL LLM. Now I want to generate visuals when a user gives a prompt. How can I implement this ??? Any help would be great &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Minute_Scientist8107&quot;&gt; /u/Minute_Scientist8107 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d1qll4/llm_to_generate_dashboard/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d1qll4/llm_to_generate_dashboard/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1d1qll4</id><link href="https://www.reddit.com/r/LangChain/comments/1d1qll4/llm_to_generate_dashboard/" /><updated>2024-05-27T12:40:21+00:00</updated><published>2024-05-27T12:40:21+00:00</published><title>LLM to generate dashboard</title></entry><entry><author><name>/u/cryptokaykay</name><uri>https://www.reddit.com/user/cryptokaykay</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d1afqa/awesome_prompting_techniques/&quot;&gt; &lt;img src=&quot;https://preview.redd.it/qpe806ybzt2d1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=573b65d4e7c2d59df66440f3a8ad24cea007627c&quot; alt=&quot;Awesome prompting techniques&quot; title=&quot;Awesome prompting techniques&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2312.16171v2&quot;&gt;https://arxiv.org/pdf/2312.16171v2&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/cryptokaykay&quot;&gt; /u/cryptokaykay &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://i.redd.it/qpe806ybzt2d1.jpeg&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d1afqa/awesome_prompting_techniques/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1d1afqa</id><media:thumbnail url="https://preview.redd.it/qpe806ybzt2d1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=573b65d4e7c2d59df66440f3a8ad24cea007627c" /><link href="https://www.reddit.com/r/LangChain/comments/1d1afqa/awesome_prompting_techniques/" /><updated>2024-05-26T20:30:43+00:00</updated><published>2024-05-26T20:30:43+00:00</published><title>Awesome prompting techniques</title></entry><entry><author><name>/u/Fireche</name><uri>https://www.reddit.com/user/Fireche</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Looking at the following tool: &lt;a href=&quot;https://python.langchain.com/v0.1/docs/integrations/tools/google_drive/&quot;&gt;https://python.langchain.com/v0.1/docs/integrations/tools/google_drive/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;What happens under the hood when this tool is called? For queries to find relevant results, does langchain simply make use of the public API which is based on a fullText search?&lt;/p&gt; &lt;p&gt;Any way to retrieve documents with a semantic search with langchain? I think this would actually be a quite neat feature, so we could pass an embedding model, db and it would create embeddings for all the documents.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Fireche&quot;&gt; /u/Fireche &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d1spe6/how_do_the_langchain_integrations_retrieve/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d1spe6/how_do_the_langchain_integrations_retrieve/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1d1spe6</id><link href="https://www.reddit.com/r/LangChain/comments/1d1spe6/how_do_the_langchain_integrations_retrieve/" /><updated>2024-05-27T14:24:28+00:00</updated><published>2024-05-27T14:24:28+00:00</published><title>How do the langchain integrations retrieve relevant documents?</title></entry><entry><author><name>/u/Relative_Winner_4588</name><uri>https://www.reddit.com/user/Relative_Winner_4588</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Newbie question here, How do I make a conversational LLM assistant in Streamlit that remembers all the chats but does not have to give a system prompt for each inference?&lt;br/&gt; I know I can use the conversational buffer memory of langchain for chat memory, but I do not want to waste my tokens on system prompts for each inference.&lt;/p&gt; &lt;p&gt;Generally, for each inference, my app takes system prompt + chat context as input for each output. I was wondering if there is a way to reduce the token consumption by sending the system prompt once and making the model remember it for the entire session.&lt;/p&gt; &lt;p&gt;Thank you.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Relative_Winner_4588&quot;&gt; /u/Relative_Winner_4588 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d1nduq/help_creating_a_conversational_assistant/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d1nduq/help_creating_a_conversational_assistant/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1d1nduq</id><link href="https://www.reddit.com/r/LangChain/comments/1d1nduq/help_creating_a_conversational_assistant/" /><updated>2024-05-27T09:09:11+00:00</updated><published>2024-05-27T09:09:11+00:00</published><title>Help creating a conversational assistant</title></entry><entry><author><name>/u/cr33dcode</name><uri>https://www.reddit.com/user/cr33dcode</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I need to build a tool for my fin research where if i ask in NLP to a rag i need the output of those 100PDFs i have uploaded to the RAG. it needs to be able to build charts, graphs and make sense of 100 other things. any OS tool like that&amp;gt; or any suggestions on the stack i should use? please advice. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/cr33dcode&quot;&gt; /u/cr33dcode &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d1mexn/need_some_advice_on_rag/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d1mexn/need_some_advice_on_rag/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1d1mexn</id><link href="https://www.reddit.com/r/LangChain/comments/1d1mexn/need_some_advice_on_rag/" /><updated>2024-05-27T07:56:05+00:00</updated><published>2024-05-27T07:56:05+00:00</published><title>need some advice on rag</title></entry><entry><author><name>/u/Rock-star-007</name><uri>https://www.reddit.com/user/Rock-star-007</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi Community,&lt;br/&gt; I am building a chatbot app for a specific domain. I am leveraging aws bedrock for storing documents and creating embeddings in pinecone vector db.&lt;/p&gt; &lt;p&gt;But I fee like I am conceptually stuck in how to maintain the conversation context and retrieved documents when trying to create a response for a new query. How to decide when to use the context and when to make a fresh retrieval? Appreciate any help here.&lt;/p&gt; &lt;p&gt;Please help answer in the setup of Django for rest, bedrock for s3 and embedding model, pinecone for vector db.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Rock-star-007&quot;&gt; /u/Rock-star-007 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d1attd/how_to_integrate_conversation_context_and/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d1attd/how_to_integrate_conversation_context_and/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1d1attd</id><link href="https://www.reddit.com/r/LangChain/comments/1d1attd/how_to_integrate_conversation_context_and/" /><updated>2024-05-26T20:48:46+00:00</updated><published>2024-05-26T20:48:46+00:00</published><title>How to integrate conversation context and retrieved documents for a new query for a RAG LLM chatbot app?</title></entry><entry><author><name>/u/sebpeterson</name><uri>https://www.reddit.com/user/sebpeterson</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi ,&lt;/p&gt; &lt;p&gt;I am using Langchain/Gemini1.5/Google Documents AI to OCR, to parse and ask questions to a set of documents. Working pretty sweet. Actually just published my side project here: &lt;a href=&quot;https://zdocs.ai/&quot;&gt;https://zdocs.ai/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I would like to be able to show where the answers to a prompt that is restricted to an uploaded set of documents are coming from?&lt;/p&gt; &lt;p&gt;Google Documents has the whole document structure availabe in JSON. However, I am not sure if the LLM (gemini in my case) can actually provide insights opn where the answer came from ?&lt;/p&gt; &lt;p&gt;Any tips would be welcome !&lt;/p&gt; &lt;p&gt;Cheers, &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/sebpeterson&quot;&gt; /u/sebpeterson &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d18agq/how_can_i_keep_track_of_document_source_being/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d18agq/how_can_i_keep_track_of_document_source_being/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1d18agq</id><link href="https://www.reddit.com/r/LangChain/comments/1d18agq/how_can_i_keep_track_of_document_source_being/" /><updated>2024-05-26T18:52:50+00:00</updated><published>2024-05-26T18:52:50+00:00</published><title>How can I keep track of document source being used in a prompt ?</title></entry><entry><author><name>/u/gordicaleksa</name><uri>https://www.reddit.com/user/gordicaleksa</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d0zm1w/building_llm_apps_in_production_with_hamel_husain/&quot;&gt; &lt;img src=&quot;https://external-preview.redd.it/1OyXglDey73O5mWYiw-lqoPAiX5PEI3CTsy0tnXvnvk.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=774a2d9f486266a3ca2c39236da3b979ca61f3be&quot; alt=&quot;Building LLM Apps in Production with Hamel Husain &quot; title=&quot;Building LLM Apps in Production with Hamel Husain &quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/gordicaleksa&quot;&gt; /u/gordicaleksa &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=MFSd-_pMExI&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d0zm1w/building_llm_apps_in_production_with_hamel_husain/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1d0zm1w</id><media:thumbnail url="https://external-preview.redd.it/1OyXglDey73O5mWYiw-lqoPAiX5PEI3CTsy0tnXvnvk.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=774a2d9f486266a3ca2c39236da3b979ca61f3be" /><link href="https://www.reddit.com/r/LangChain/comments/1d0zm1w/building_llm_apps_in_production_with_hamel_husain/" /><updated>2024-05-26T11:47:55+00:00</updated><published>2024-05-26T11:47:55+00:00</published><title>Building LLM Apps in Production with Hamel Husain</title></entry><entry><author><name>/u/mehul_gupta1997</name><uri>https://www.reddit.com/user/mehul_gupta1997</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/mehul_gupta1997&quot;&gt; /u/mehul_gupta1997 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;/r/learnmachinelearning/comments/1d0ul53/pandasai_generative_ai_for_pandas_dataframe/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d0uoni/pandasai_generative_ai_for_pandas_dataframe/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1d0uoni</id><link href="https://www.reddit.com/r/LangChain/comments/1d0uoni/pandasai_generative_ai_for_pandas_dataframe/" /><updated>2024-05-26T05:46:44+00:00</updated><published>2024-05-26T05:46:44+00:00</published><title>PandasAI: Generative AI for pandas dataframe</title></entry></feed>