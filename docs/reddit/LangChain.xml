<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/"><category term="LangChain" label="r/LangChain"/><updated>2024-06-23T20:21:09+00:00</updated><icon>https://www.redditstatic.com/icon.png/</icon><id>/r/LangChain.rss</id><link rel="self" href="https://www.reddit.com/r/LangChain.rss" type="application/atom+xml" /><link rel="alternate" href="https://www.reddit.com/r/LangChain" type="text/html" /><subtitle>LangChain is an open-source framework and developer toolkit that helps developers get LLM applications from prototype to production. It is available for Python and Javascript at https://www.langchain.com/.</subtitle><title>LangChain</title><entry><author><name>/u/zchaarm</name><uri>https://www.reddit.com/user/zchaarm</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;A place for members of &lt;a href=&quot;/r/LangChain&quot;&gt;r/LangChain&lt;/a&gt; to chat with each other&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/zchaarm&quot;&gt; /u/zchaarm &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_10ljho9</id><link href="https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/" /><updated>2023-01-26T04:33:05+00:00</updated><published>2023-01-26T04:33:05+00:00</published><title>r/LangChain Lounge</title></entry><entry><author><name>/u/sarthakai</name><uri>https://www.reddit.com/user/sarthakai</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Knowledge graphs can improve your RAG accuracy if your documents contain interconnected concepts.&lt;/p&gt; &lt;p&gt;And you can create+search on KGs for your existing documents automatically by using the latest version of the knowledge-graph-rag library.&lt;/p&gt; &lt;p&gt;All in just 3 lines of code.&lt;/p&gt; &lt;p&gt;In this example, I use medical documents. Here&amp;#39;s how the library works:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Extract entities from the corpus (such as organs, diseases, therapies, etc)&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Extract the relationships between them (such as mitigation effect of therapies, accumulation of plaques, etc.)&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Create a knowledge graph from these representations using GPT 3.5 / Haiku&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;When a user sends a query, break it down into entities to be searched.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Search the KG and use the results in the context of the LLM call.&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Here’s the repo: &lt;a href=&quot;https://github.com/sarthakrastogi/graph-rag&quot;&gt;https://github.com/sarthakrastogi/graph-rag&lt;/a&gt;&lt;/p&gt; &lt;p&gt;If you&amp;#39;d like to contribute or have suggestions for features, please raise them on Github.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/sarthakai&quot;&gt; /u/sarthakai &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dmm13w/building_a_python_library_to_quickly_createsearch/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dmm13w/building_a_python_library_to_quickly_createsearch/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dmm13w</id><link href="https://www.reddit.com/r/LangChain/comments/1dmm13w/building_a_python_library_to_quickly_createsearch/" /><updated>2024-06-23T13:21:53+00:00</updated><published>2024-06-23T13:21:53+00:00</published><title>Building a Python library to quickly create+search knowledge graphs for RAG -- want to contribute?</title></entry><entry><author><name>/u/link2ani</name><uri>https://www.reddit.com/user/link2ani</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Looking for a cost effective approach that doesn’t suck&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/link2ani&quot;&gt; /u/link2ani &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dmuxu9/recommendation_for_reranker_model_on_retrieved/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dmuxu9/recommendation_for_reranker_model_on_retrieved/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dmuxu9</id><link href="https://www.reddit.com/r/LangChain/comments/1dmuxu9/recommendation_for_reranker_model_on_retrieved/" /><updated>2024-06-23T20:06:10+00:00</updated><published>2024-06-23T20:06:10+00:00</published><title>Recommendation for re-ranker model on retrieved results?</title></entry><entry><author><name>/u/yaeha83</name><uri>https://www.reddit.com/user/yaeha83</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Although the plenty of integrations make our life easier if committed to langchain, it is disproportionally more difficult to optimize and configure their paramaters and especially the **kwargs. I understand that these usually refer to the underlying package that the langchain class wraps around.&lt;/p&gt; &lt;p&gt;All docs have vanilla params and when some further configuration is needed or even explored then things become a bit of a pain.&lt;/p&gt; &lt;p&gt;The only way so far to find what **kwargs are available for the integrations I use is to go deep in the langchain code to see whether these might be finally passed, read the wrapped package documentation and also do extensive google search as it is not just the kwargs but also the syntax to pass them on (eg dict).&lt;/p&gt; &lt;p&gt;I guess this is not a big deal for a seasoned developer, but is there an easier way to do this, especially in the LLM era?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/yaeha83&quot;&gt; /u/yaeha83 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dmtf4a/optimize_and_configure_integration_classes_wit/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dmtf4a/optimize_and_configure_integration_classes_wit/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dmtf4a</id><link href="https://www.reddit.com/r/LangChain/comments/1dmtf4a/optimize_and_configure_integration_classes_wit/" /><updated>2024-06-23T18:59:08+00:00</updated><published>2024-06-23T18:59:08+00:00</published><title>Optimize and configure integration classes wit **kwargs</title></entry><entry><author><name>/u/cwooters</name><uri>https://www.reddit.com/user/cwooters</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;m looking for some high-level advice around &lt;strong&gt;LangGraph&lt;/strong&gt; and was hoping that this community might have some creative ideas.&lt;/p&gt; &lt;p&gt;I&amp;#39;ve been playing with LangGraph and I love how it lets you control the flow of a conversation. But &lt;strong&gt;I&amp;#39;m struggling with how to design a graph in light of parallel tool calling&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;Background: So, let&amp;#39;s say I have a state that represents the first &amp;quot;stage&amp;quot; of a dialogue with an Agent. Once that stage is complete, I want the dialogue to move to the second stage. (Each stage is represented as a state.) I have a tool called &amp;quot;CompleteOrEscalate&amp;quot; (based on the LangGraph tutorials) that the LLM can use when it thinks that the task for stage 1 is complete. I also have a second tool called &amp;quot;ToFAQ&amp;quot; which can be used if the user asks a question that is not directly related to stage 1&amp;#39;s task. So, stage 1 can conditionally transition to two other stages/states, depending on what the user says. This works great, &lt;em&gt;most of the time&lt;/em&gt;.&lt;/p&gt; &lt;p&gt;But an issue arises when a user says something that causes the LLM to invoke more than one tool (i.e. the LLM is suggesting that we make more than one transition out of a state). For example, if the purpose of stage 1 is to confirm the user&amp;#39;s name, and the user says, &amp;quot;Yes, I&amp;#39;m John Smith. And I have a question about ...&amp;quot; That input both completes the task (confirming the user&amp;#39;s name) AND contains a question (requiring an FAQ response). So, with parallel tool calling enabled the LLM returns both tool calls (CompleteOrEscalate and ToFAQ). This is actually pretty cool, but I&amp;#39;m not sure how to handle this situation in the conditional transition?&lt;/p&gt; &lt;p&gt;I&amp;#39;ve considered turning off parallel tool calling. This would force the LLM to call only one tool at a time. But it seems like a waste of tokens/time not to allow the LLM to return &lt;em&gt;both/all&lt;/em&gt; tool calls.&lt;/p&gt; &lt;p&gt;Am I thinking about this all wrong? Is there a better way to handle this situation? TIA for any suggestions or thoughts you may have.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/cwooters&quot;&gt; /u/cwooters &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dmtcyn/looking_for_ideas_how_to_handle_parallel_tool/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dmtcyn/looking_for_ideas_how_to_handle_parallel_tool/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dmtcyn</id><link href="https://www.reddit.com/r/LangChain/comments/1dmtcyn/looking_for_ideas_how_to_handle_parallel_tool/" /><updated>2024-06-23T18:56:24+00:00</updated><published>2024-06-23T18:56:24+00:00</published><title>Looking for ideas: How to handle parallel tool calls in LangGraph?</title></entry><entry><author><name>/u/derelict5432</name><uri>https://www.reddit.com/user/derelict5432</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Just started using RAG with LangChain the last couple of weeks for a project at work.&lt;/p&gt; &lt;p&gt;First pass, I used this tutorial: &lt;a href=&quot;https://python.langchain.com/v0.2/docs/tutorials/rag/&quot;&gt;https://python.langchain.com/v0.2/docs/tutorials/rag/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Instead of a webloader, I used a textloader to load a small text file, a help file for a custom software framework.&lt;/p&gt; &lt;p&gt;I ran it, queried the model, and it worked great. I was excited.&lt;/p&gt; &lt;p&gt;The full amount of data I want to reference is about 18K small text documents, about 179MB. I decided to work up to that, and just used about 10MB in about 1000 text documents. Query results were much worse.&lt;/p&gt; &lt;p&gt;In one specific case, I asked about a scenario description that was stored in a file called ea.txt. For troubleshooting, I increased the number of docs to be retrieved to 5 and added logging to show which docs were being retrieved.&lt;/p&gt; &lt;p&gt;The answer was wrong, and ed.txt was referenced three times, along with two other irrelevant docs. In the directory to be loaded, ed.txt directly follows ea.txt. How is RAG determining which docs to retrieve? The scenario I was asking about started with &amp;#39;ea&amp;#39; (e.g. &amp;#39;scenario ea4003&amp;#39;). Why would it pass over the file with the correct information, which contains strings that are much more similar to what I&amp;#39;m asking about? &lt;/p&gt; &lt;p&gt;And does anyone have any advice on how to improve performance? Thanks.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/derelict5432&quot;&gt; /u/derelict5432 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dmo3am/how_to_improve_rag_performance/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dmo3am/how_to_improve_rag_performance/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dmo3am</id><link href="https://www.reddit.com/r/LangChain/comments/1dmo3am/how_to_improve_rag_performance/" /><updated>2024-06-23T15:01:38+00:00</updated><published>2024-06-23T15:01:38+00:00</published><title>How to Improve RAG Performance</title></entry><entry><author><name>/u/fra_bia91</name><uri>https://www.reddit.com/user/fra_bia91</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;m looking to implement a way for the users of my platform to upload CSV files and pass them to various LMs to analyze. I get how the process works with other files types, and I&amp;#39;ve already set up a RAG pipeline for pdf files. &lt;/p&gt; &lt;p&gt;However, with PDF files I can &amp;quot;simply&amp;quot; split it into chunks and generate embeddings with those (and later retrieve the most relevant ones), with CSV, since it&amp;#39;s mostly data that could relate to each other, I&amp;#39;m not sure how to proceed.&lt;/p&gt; &lt;p&gt;For example, which criteria should I use to split the document into chunks? And what about the retrieval? Are embeddings relevant for CSV files?&lt;/p&gt; &lt;p&gt;The main use case to RAG in this case -as compared to simply including the whole CSV as text in the prompt- is to save tokens, but is it possible to get decent results with RAG?&lt;/p&gt; &lt;p&gt;Thanks in advance&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/fra_bia91&quot;&gt; /u/fra_bia91 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dmj7p7/im_not_sure_i_understand_how_to_perform_rag_on/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dmj7p7/im_not_sure_i_understand_how_to_perform_rag_on/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dmj7p7</id><link href="https://www.reddit.com/r/LangChain/comments/1dmj7p7/im_not_sure_i_understand_how_to_perform_rag_on/" /><updated>2024-06-23T10:29:13+00:00</updated><published>2024-06-23T10:29:13+00:00</published><title>I'm not sure I understand how to perform RAG on CSV files...</title></entry><entry><author><name>/u/Active-Fuel-49</name><uri>https://www.reddit.com/user/Active-Fuel-49</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dmksbw/bridging_the_last_mile_in_langchain_application/&quot;&gt; &lt;img src=&quot;https://external-preview.redd.it/881AdqFUz0mznhLi2O_40GQW0p3T5Dv647jn0wc7VIU.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a4cc543a878eda2859d2cd4b88a41afb36d515b3&quot; alt=&quot;Bridging the Last Mile in LangChain Application Development&quot; title=&quot;Bridging the Last Mile in LangChain Application Development&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Active-Fuel-49&quot;&gt; /u/Active-Fuel-49 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://community.aws/content/2gYKTV25GGIqAzgRyAdYbYTtCTf/bridging-the-last-mile-in-langchain-application-development&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dmksbw/bridging_the_last_mile_in_langchain_application/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1dmksbw</id><media:thumbnail url="https://external-preview.redd.it/881AdqFUz0mznhLi2O_40GQW0p3T5Dv647jn0wc7VIU.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=a4cc543a878eda2859d2cd4b88a41afb36d515b3" /><link href="https://www.reddit.com/r/LangChain/comments/1dmksbw/bridging_the_last_mile_in_langchain_application/" /><updated>2024-06-23T12:13:19+00:00</updated><published>2024-06-23T12:13:19+00:00</published><title>Bridging the Last Mile in LangChain Application Development</title></entry><entry><author><name>/u/PurpleWho</name><uri>https://www.reddit.com/user/PurpleWho</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;m new to LangChain and slowly working my way through the docs. My intention is to build a chat interface that has a conversation with a user and then slowly fills out a form behind the scenes as answers come in. &lt;/p&gt; &lt;p&gt;Filling out the form directly is a lot of information upfront for the user whereas a chat interface lets me break the questions down into smaller chunks.&lt;/p&gt; &lt;p&gt;I&amp;#39;m trying to understand how I would use LangChain to do this. There are a lot of different moving parts to the framework and I was wondering if someone could point me in the right direction. That is, which modules I need to cover first or a relevant example of something similar.&lt;/p&gt; &lt;p&gt;Specific questions I have include:&lt;br/&gt; - How to keep costs down when polling the transcript to fill out the form. If i do this on every message submission it might get unnecessarily expensive. But if I do it too infrequently then the bot might end up asking questions it already has the answer to. Was hoping the framework had some best practices I could rely on in this regard.&lt;br/&gt; - How to redirect the redirect the bot&amp;#39;s focus based on which questions still need answering.&lt;br/&gt; - How to implement a system to determine if an answer is good enough or if I need to ask more follow up questions to get a more substantial answer.&lt;br/&gt; - How to detect when all the questions have been filled out so I can end the chat.&lt;/p&gt; &lt;p&gt;I&amp;#39;ll slowly figuring it out but any pointers would be much appreciated. &lt;/p&gt; &lt;p&gt;Also, are there other LangChain specific forums where I can ask these types of questions that anyone recommends?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/PurpleWho&quot;&gt; /u/PurpleWho &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dmbnet/using_a_chat_interface_to_help_people_fill_out_a/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dmbnet/using_a_chat_interface_to_help_people_fill_out_a/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dmbnet</id><link href="https://www.reddit.com/r/LangChain/comments/1dmbnet/using_a_chat_interface_to_help_people_fill_out_a/" /><updated>2024-06-23T02:10:59+00:00</updated><published>2024-06-23T02:10:59+00:00</published><title>Using a chat interface to help people fill out a form</title></entry><entry><author><name>/u/sarthakai</name><uri>https://www.reddit.com/user/sarthakai</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;So first, here&amp;#39;s what I understand of how they did it:&lt;/p&gt; &lt;p&gt;They made the KG by parsing customer support tickets into structured tree representations, preserving their internal relationships.&lt;/p&gt; &lt;p&gt;Tickets are linked based on contextual similarities, dependencies, and references — all of these make up a comprehensive graph.&lt;/p&gt; &lt;p&gt;Each node in the KG is embedded so they can do semantic search and retrieval.&lt;/p&gt; &lt;p&gt;The RAG QA system identifies relevant sub-graphs by doing traversal and searching by semantic similarity.&lt;/p&gt; &lt;p&gt;Then, it generates contextually aware answers from the KG, evaluating by MRR, which saw a significant improvement.&lt;/p&gt; &lt;p&gt;Paper: &lt;a href=&quot;https://arxiv.org/pdf/2404.17723&quot;&gt;https://arxiv.org/pdf/2404.17723&lt;/a&gt;&lt;/p&gt; &lt;p&gt;If you’d like to implement Graph RAG too, I’m creating a Python library which automatically creates this graph for the documents in your vectordb. It also makes it easy for you to retrieve relevant documents connected to the best matches.&lt;/p&gt; &lt;p&gt;If you&amp;#39;re interested in contributing or have suggestions please raise them on Github.&lt;/p&gt; &lt;p&gt;Here’s the repo for the library: &lt;a href=&quot;https://github.com/sarthakrastogi/graph-rag/tree/main&quot;&gt;https://github.com/sarthakrastogi/graph-rag/tree/main&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/sarthakai&quot;&gt; /u/sarthakai &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dlwc39/linkedin_used_graph_rag_to_cut_down_their_ticket/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dlwc39/linkedin_used_graph_rag_to_cut_down_their_ticket/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dlwc39</id><link href="https://www.reddit.com/r/LangChain/comments/1dlwc39/linkedin_used_graph_rag_to_cut_down_their_ticket/" /><updated>2024-06-22T14:00:46+00:00</updated><published>2024-06-22T14:00:46+00:00</published><title>LinkedIn used Graph RAG to cut down their ticket resolution time from 40 hrs to 15 hrs. Let's make a library to make it accessible to everyone?</title></entry><entry><author><name>/u/tuantruong84</name><uri>https://www.reddit.com/user/tuantruong84</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;As much as i like LangChain, there is some actual good points from this article &lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://www.octomind.dev/blog/why-we-no-longer-use-langchain-for-building-our-ai-agents&quot;&gt;https://www.octomind.dev/blog/why-we-no-longer-use-langchain-for-building-our-ai-agents&lt;/a&gt;&lt;/p&gt; &lt;p&gt;What you guys think ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/tuantruong84&quot;&gt; /u/tuantruong84 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dlu5t9/an_article_on_why_moving_away_from_langchain/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dlu5t9/an_article_on_why_moving_away_from_langchain/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dlu5t9</id><link href="https://www.reddit.com/r/LangChain/comments/1dlu5t9/an_article_on_why_moving_away_from_langchain/" /><updated>2024-06-22T12:03:37+00:00</updated><published>2024-06-22T12:03:37+00:00</published><title>An article on why moving away from langchain</title></entry><entry><author><name>/u/Minimum-You-9018</name><uri>https://www.reddit.com/user/Minimum-You-9018</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I just find that chat history in final request look like that - &lt;code&gt;&amp;lt;CHAT HISTORY&amp;gt; [HumanMessage(content=&amp;#39;Message one&amp;#39;), AIMessage(content=&amp;#39;Hey&amp;#39;), HumanMessage(content=&amp;#39;Message two&amp;#39;)] &amp;lt;/CHAT HISTORY&amp;gt;&lt;/code&gt;&lt;/p&gt; &lt;p&gt;i a bit confused, it how it should looks, is it correct? As i remember i was something like this before: &lt;code&gt;AI:message \n USER:Text Message&lt;/code&gt; &lt;/p&gt; &lt;p&gt;Can anyone clarify this for me?&lt;/p&gt; &lt;p&gt;P.S.: Does anyone have information on how models like OpenAI, Anthropic, or Gemini are trained to understand conversation history?&lt;/p&gt; &lt;p&gt;My research gives me this ideas:&lt;/p&gt; &lt;p&gt;&lt;code&gt; { &amp;quot;chat_history&amp;quot;: [ {&amp;quot;role&amp;quot;: &amp;quot;user&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;Message one&amp;quot;}, {&amp;quot;role&amp;quot;: &amp;quot;ai&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;Hey&amp;quot;}, {&amp;quot;role&amp;quot;: &amp;quot;user&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;Message two&amp;quot;} ] } &lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt; USER: Message one AI: Hey USER: Message two &lt;/code&gt;&lt;/p&gt; &lt;p&gt;``` &amp;lt;conversation&amp;gt; &amp;lt;user&amp;gt;Message one&amp;lt;/user&amp;gt; &amp;lt;ai&amp;gt;Hey&amp;lt;/ai&amp;gt; &amp;lt;user&amp;gt;Message two&amp;lt;/user&amp;gt; &amp;lt;/conversation&amp;gt;&lt;/p&gt; &lt;p&gt;```&lt;/p&gt; &lt;p&gt;Langchain use its own history structure for a reason?&lt;/p&gt; &lt;p&gt;This is how im execute it: &lt;code&gt; response = await self.llm_chain.ainvoke( {&amp;quot;input&amp;quot;: self.llm_input.content}, config={ &amp;quot;configurable&amp;quot;: { &amp;quot;user_id&amp;quot;: self.user_id, &amp;quot;session_type&amp;quot;: self.session_type, }, &amp;quot;callbacks&amp;quot;: [self.langfuse_handler], }, ) &lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;langchain = &amp;quot;&lt;sup&gt;0.2.5&amp;quot;&lt;/sup&gt; langchain-core = &amp;quot;&lt;sup&gt;0.2.9&amp;quot;&lt;/sup&gt; langchain-community = &amp;quot;&lt;sup&gt;0.2.5&amp;quot;&lt;/sup&gt;&lt;/em&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Minimum-You-9018&quot;&gt; /u/Minimum-You-9018 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dmeyfj/langchain_chat_history_data_structure_in_final/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dmeyfj/langchain_chat_history_data_structure_in_final/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dmeyfj</id><link href="https://www.reddit.com/r/LangChain/comments/1dmeyfj/langchain_chat_history_data_structure_in_final/" /><updated>2024-06-23T05:27:39+00:00</updated><published>2024-06-23T05:27:39+00:00</published><title>Langchain chat history data structure in final prompt</title></entry><entry><author><name>/u/CodingButStillAlive</name><uri>https://www.reddit.com/user/CodingButStillAlive</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I know that streamlit was popular, but neither optimized for chatbot interactivity, nor ready to set up for production.&lt;/p&gt; &lt;p&gt;I assume some TypeScript + REACT is state of the art, but I am a Data Scientist and no frontend developer.&lt;/p&gt; &lt;p&gt;Are there any new libraries that nicely integrate with LangGraph and also FastAPI?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/CodingButStillAlive&quot;&gt; /u/CodingButStillAlive &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dlrouj/what_is_the_best_python_library_for_chatbot_uis/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dlrouj/what_is_the_best_python_library_for_chatbot_uis/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dlrouj</id><link href="https://www.reddit.com/r/LangChain/comments/1dlrouj/what_is_the_best_python_library_for_chatbot_uis/" /><updated>2024-06-22T09:18:19+00:00</updated><published>2024-06-22T09:18:19+00:00</published><title>What is the best python library for chatbot UIs?</title></entry><entry><author><name>/u/AnEdgeLordWeeb</name><uri>https://www.reddit.com/user/AnEdgeLordWeeb</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey guys!&lt;br/&gt; Has anyone tried and managed to find a successful solution, as to how I can messages in LangGraph through the usage of FastAPI and React?&lt;br/&gt; I have multiple nodes in my LangGraph app, and each one is appending a message to the &amp;quot;messages&amp;quot; list attribute. I want these messages&amp;#39; content to be streamed as a single message in my React app, until I reach the END node.&lt;br/&gt; Does anyone have any idea as to how to do that?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/AnEdgeLordWeeb&quot;&gt; /u/AnEdgeLordWeeb &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dltljb/how_to_stream_messages_with_fastapi_and_react/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dltljb/how_to_stream_messages_with_fastapi_and_react/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dltljb</id><link href="https://www.reddit.com/r/LangChain/comments/1dltljb/how_to_stream_messages_with_fastapi_and_react/" /><updated>2024-06-22T11:29:52+00:00</updated><published>2024-06-22T11:29:52+00:00</published><title>How to stream messages with FastAPI and React? - LangGraph</title></entry><entry><author><name>/u/diptanuc</name><uri>https://www.reddit.com/user/diptanuc</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi folks, I often see questions about which open source pdf model or APIs are best for extraction from PDF. We attempt to help people make data-driven decisions by comparing the various models on their private documents.&lt;/p&gt; &lt;p&gt;We benchmarked several PDF models - Marker, EasyOCR, Unstructured and OCRMyPDF.&lt;/p&gt; &lt;p&gt;Marker is better than the others in terms of accuracy. EasyOCR comes second, and OCRMyPDF is pretty close.&lt;/p&gt; &lt;p&gt;You can run these benchmarks on your documents using our code - &lt;a href=&quot;https://github.com/tensorlakeai/indexify-extractors/tree/main/pdf/benchmark&quot;&gt;https://github.com/tensorlakeai/indexify-extractors/tree/main/pdf/benchmark&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The benchmark tool is using Indexify behind the scenes - &lt;a href=&quot;https://github.com/tensorlakeai/indexify&quot;&gt;https://github.com/tensorlakeai/indexify&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Indexify is a scalable unstructured data extraction engine for building multi-stage inference pipelines. The pipelines can handle extraction from 1000s of documents in parallel when deployed in a real cluster on the cloud.&lt;/p&gt; &lt;p&gt;I would love your feedback on what models and document layouts to benchmark next. &lt;/p&gt; &lt;p&gt;For some reason Reddit is marking this post as spam when I add pictures, so here is a link to the docs with some charts - &lt;a href=&quot;https://docs.getindexify.ai/usecases/pdf_extraction/#extractor-performance-analysis&quot;&gt;https://docs.getindexify.ai/usecases/pdf_extraction/#extractor-performance-analysis&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/diptanuc&quot;&gt; /u/diptanuc &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dlfth6/benchmarking_pdf_models_for_parsing_accuracy/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dlfth6/benchmarking_pdf_models_for_parsing_accuracy/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dlfth6</id><link href="https://www.reddit.com/r/LangChain/comments/1dlfth6/benchmarking_pdf_models_for_parsing_accuracy/" /><updated>2024-06-21T21:57:01+00:00</updated><published>2024-06-21T21:57:01+00:00</published><title>Benchmarking PDF models for parsing accuracy</title></entry><entry><author><name>/u/Informal-Victory8655</name><uri>https://www.reddit.com/user/Informal-Victory8655</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Informal-Victory8655&quot;&gt; /u/Informal-Victory8655 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dlsxte/how_to_use_rabbitmq_or_any_other_broker_with/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dlsxte/how_to_use_rabbitmq_or_any_other_broker_with/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dlsxte</id><link href="https://www.reddit.com/r/LangChain/comments/1dlsxte/how_to_use_rabbitmq_or_any_other_broker_with/" /><updated>2024-06-22T10:47:26+00:00</updated><published>2024-06-22T10:47:26+00:00</published><title>How to Use RabbitMQ or any other Broker with LangChain FastApi chatbot</title></entry><entry><author><name>/u/sarthakai</name><uri>https://www.reddit.com/user/sarthakai</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Suppose in your LLM you have the original weight matrix W of dimensions d x k.&lt;/p&gt; &lt;p&gt;Your traditional training process would update W directly -- that’s a huge number of parameters if d x k is large, needing a lot of compute.&lt;/p&gt; &lt;p&gt;So, we use Low-Rank Decomposition to break it down before weight update. Here’s how —We represent the weight update (Delta W) as a product of two lower-rank matrices A and B, such that Delta W = BA.&lt;/p&gt; &lt;p&gt;Here, A is a matrix of dimensions r x k and B is a matrix of dimensions d x r. And here, r (rank) is much smaller than both d and k.&lt;/p&gt; &lt;p&gt;Now, Matrix A is initialised with some random Gaussian values and matrix B is initialised with zeros.&lt;/p&gt; &lt;p&gt;Why? So that initially Delta W = BA can be 0.&lt;/p&gt; &lt;p&gt;Now comes the training process:&lt;/p&gt; &lt;p&gt;During weight update, only the smaller matrices A and B are updated — this reduces the number of parameters to be tuned by a huge margin.&lt;/p&gt; &lt;p&gt;The effective update to the original weight matrix W is Delta W = BA, which approximates the changes in W using fewer parameters.&lt;/p&gt; &lt;p&gt;Let’s compare the params to be updated before and after LoRA:&lt;/p&gt; &lt;p&gt;Earlier, the params to be updated were d x k (remember the dimensions of W).&lt;/p&gt; &lt;p&gt;But now, the no. of params is reduced to (d x r) + (r x k). This is much smaller because the rank r was taken to be much smaller than both d and k.&lt;/p&gt; &lt;p&gt;This is how low-rank approximation gives you efficient fine-tuning with this compact representation.&lt;/p&gt; &lt;p&gt;Training is faster and needs less compute and memory, while still capturing essential information from your fine-tuning dataset.&lt;/p&gt; &lt;p&gt;I also made a quick animation using Artifacts to explain (took like 10 secs):&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://www.linkedin.com/posts/sarthakrastogi_simply-explaining-how-lora-actually-works-activity-7209893533011333120-RSsz&quot;&gt;https://www.linkedin.com/posts/sarthakrastogi_simply-explaining-how-lora-actually-works-activity-7209893533011333120-RSsz&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/sarthakai&quot;&gt; /u/sarthakai &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dl53nn/simply_explaining_how_lora_actually_works_eli5/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dl53nn/simply_explaining_how_lora_actually_works_eli5/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dl53nn</id><link href="https://www.reddit.com/r/LangChain/comments/1dl53nn/simply_explaining_how_lora_actually_works_eli5/" /><updated>2024-06-21T14:17:39+00:00</updated><published>2024-06-21T14:17:39+00:00</published><title>Simply explaining how LoRA actually works (ELI5)</title></entry><entry><author><name>/u/goddamnit_1</name><uri>https://www.reddit.com/user/goddamnit_1</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dlaqn7/i_built_an_sql_agent_with_langchain_heres_my/&quot;&gt; &lt;img src=&quot;https://b.thumbs.redditmedia.com/yj28iUliBBMoZ3SIz1i2HUNtYl_ZhzBiXOQsjg5cL0c.jpg&quot; alt=&quot;I built an SQL Agent with Langchain - Here's my experience&quot; title=&quot;I built an SQL Agent with Langchain - Here's my experience&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;My agent writes queries to retrieve data from Sqlite Databases. This was my first time writing an agent with a good and serious usecase. The first framework i used for this was Langchain. &lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Very easy to implement: Its pretty convenient to import LLMs Gpt, Claude, Gemini. The documentation for it also clear.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Tools: This is my favourite part about the framework, writing tools and importing them is very easy and it helps in building for a lot of usecases.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Documentation can be improved since there are multiple versions and each time i click to the stable version, it goes back to the homepage.&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;a href=&quot;https://i.redd.it/1b8v8xv4vy7d1.gif&quot;&gt;https://i.redd.it/1b8v8xv4vy7d1.gif&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Here&amp;#39;s the &lt;a href=&quot;https://github.com/ComposioHQ/composio/tree/master/python/examples/sql_agent&quot;&gt;GITHUB LINK&lt;/a&gt; if you want to try it out.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/goddamnit_1&quot;&gt; /u/goddamnit_1 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dlaqn7/i_built_an_sql_agent_with_langchain_heres_my/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dlaqn7/i_built_an_sql_agent_with_langchain_heres_my/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1dlaqn7</id><media:thumbnail url="https://b.thumbs.redditmedia.com/yj28iUliBBMoZ3SIz1i2HUNtYl_ZhzBiXOQsjg5cL0c.jpg" /><link href="https://www.reddit.com/r/LangChain/comments/1dlaqn7/i_built_an_sql_agent_with_langchain_heres_my/" /><updated>2024-06-21T18:17:38+00:00</updated><published>2024-06-21T18:17:38+00:00</published><title>I built an SQL Agent with Langchain - Here's my experience</title></entry><entry><author><name>/u/mmkostov</name><uri>https://www.reddit.com/user/mmkostov</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I need to do RAG and web browsing. What other libraries can I use (except LangChain) that can achieve this functionality?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/mmkostov&quot;&gt; /u/mmkostov &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dlolna/langchain_alternatives_for_a_nextjs_project/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dlolna/langchain_alternatives_for_a_nextjs_project/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dlolna</id><link href="https://www.reddit.com/r/LangChain/comments/1dlolna/langchain_alternatives_for_a_nextjs_project/" /><updated>2024-06-22T05:42:33+00:00</updated><published>2024-06-22T05:42:33+00:00</published><title>LangChain alternatives for a Next.js project?</title></entry><entry><author><name>/u/Either-Ambassador738</name><uri>https://www.reddit.com/user/Either-Ambassador738</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I just wanted to ask all your opinion on Langgraph in production? I want to build a chatbot with multiple agents: one agent that connects to a database, one agent that performs RAG and one agent for conversational purposes, and Langgraph is the tool I see would fit the most but I&amp;#39;m not so sure if it&amp;#39;s ready to take to production.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Either-Ambassador738&quot;&gt; /u/Either-Ambassador738 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dl47vz/langgraph_in_production/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dl47vz/langgraph_in_production/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dl47vz</id><link href="https://www.reddit.com/r/LangChain/comments/1dl47vz/langgraph_in_production/" /><updated>2024-06-21T13:37:45+00:00</updated><published>2024-06-21T13:37:45+00:00</published><title>LangGraph in production?</title></entry><entry><author><name>/u/HomunMage</name><uri>https://www.reddit.com/user/HomunMage</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I make a repo &lt;a href=&quot;https://github.com/LangGraph-GUI/LangGraph-learn&quot;&gt;LangGraph-learn&lt;/a&gt;&lt;br/&gt; there are step by step to understand langgraph features and run on ollama &lt;/p&gt; &lt;p&gt;because many people feel langgraph too hard to learn. such &lt;a href=&quot;https://www.reddit.com/r/ArtificialInteligence/comments/1d4lxrv/am_i_the_only_one_langgraph_docs_suck/&quot;&gt;Am I the only one langgraph docs suck?&lt;/a&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d4lwt0/am_i_the_only_one_who_feels_langgraph/&quot;&gt;Am I the only one who feels LangGraph documentation and tutorials by lanfchain absolutely suck?&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/HomunMage&quot;&gt; /u/HomunMage &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dl6o2t/langgraph_with_ollama_learning_resource/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dl6o2t/langgraph_with_ollama_learning_resource/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dl6o2t</id><link href="https://www.reddit.com/r/LangChain/comments/1dl6o2t/langgraph_with_ollama_learning_resource/" /><updated>2024-06-21T15:25:36+00:00</updated><published>2024-06-21T15:25:36+00:00</published><title>LangGraph with Ollama learning resource</title></entry><entry><author><name>/u/thumbsdrivesmecrazy</name><uri>https://www.reddit.com/user/thumbsdrivesmecrazy</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;The talk among Itamar Friedman (CEO of CodiumAI) and Harrison Chase (CEO of LangChain) explores best practices, insights, examples, and hot takes on flow engineering: &lt;a href=&quot;https://www.youtube.com/watch?v=eBjxz7qrNBs&quot;&gt;Flow Engineering with LangChain/LangGraph and CodiumAI&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Flow Engineering can be used for many problems involving reasoning, and can outperform naive prompt engineering. Instead of using a single prompt to solve problems, Flow Engineering uses an interative process that repeatedly runs and refines the generated result. Better results can be obtained moving from a prompt:answer paradigm to a &amp;quot;flow&amp;quot; paradigm, where the answer is constructed iteratively.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/thumbsdrivesmecrazy&quot;&gt; /u/thumbsdrivesmecrazy &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dl6hl0/flow_engineering_with_langchainlanggraph_and/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dl6hl0/flow_engineering_with_langchainlanggraph_and/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dl6hl0</id><link href="https://www.reddit.com/r/LangChain/comments/1dl6hl0/flow_engineering_with_langchainlanggraph_and/" /><updated>2024-06-21T15:17:37+00:00</updated><published>2024-06-21T15:17:37+00:00</published><title>Flow Engineering with LangChain/LangGraph and CodiumAI - Harrison Chase interviews Itamar Friedman, CEO of CodiumAI</title></entry><entry><author><name>/u/DataaWolff</name><uri>https://www.reddit.com/user/DataaWolff</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;How can we leverage an NLP model or Generative AI pre-trained model like ChatGPT or Llama2 to compare two documents, like legal contracts or technical manuals, and find the deviation in the documents.&lt;/p&gt; &lt;p&gt;Please give me ideas or ways to achieve this or if you have any Youtube/Github links for the reference.&lt;/p&gt; &lt;p&gt;Thanks&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/DataaWolff&quot;&gt; /u/DataaWolff &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dldrbr/leveraging_nlppretrained_models_for_document/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dldrbr/leveraging_nlppretrained_models_for_document/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dldrbr</id><link href="https://www.reddit.com/r/LangChain/comments/1dldrbr/leveraging_nlppretrained_models_for_document/" /><updated>2024-06-21T20:26:44+00:00</updated><published>2024-06-21T20:26:44+00:00</published><title>Leveraging NLP/Pre-Trained Models for Document Comparison and Deviation Detection</title></entry><entry><author><name>/u/ExpressBalance2601</name><uri>https://www.reddit.com/user/ExpressBalance2601</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I was developing Chatbot for telegram Where i used to scrap contents from websites using langchain webBaseLoader&lt;/p&gt; &lt;p&gt;But the problem is, the data was too rough (eg: one content title combines with another) and some times the entire data may not be useful or the contents are in non-English language&lt;/p&gt; &lt;p&gt;But i need only the contents to be in proper format as much as possible&lt;/p&gt; &lt;p&gt;Any better possible way, that can improve content scraping from websites?&lt;/p&gt; &lt;p&gt;I found, some of the API are available they provide better content scraping, but I&amp;#39;m student, so i can&amp;#39;t invest on those Free API was not enough for my purpose as well&lt;/p&gt; &lt;p&gt;Thankyou for everybody in advance ❤️&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/ExpressBalance2601&quot;&gt; /u/ExpressBalance2601 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dl7ho6/chatbot_development_help/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dl7ho6/chatbot_development_help/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dl7ho6</id><link href="https://www.reddit.com/r/LangChain/comments/1dl7ho6/chatbot_development_help/" /><updated>2024-06-21T16:01:10+00:00</updated><published>2024-06-21T16:01:10+00:00</published><title>Chatbot development help</title></entry><entry><author><name>/u/HomunMage</name><uri>https://www.reddit.com/user/HomunMage</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;All resource on web all need OpenAI api key.&lt;/p&gt; &lt;p&gt;how to run local? last monther there is HuggingFace x LangChain&lt;/p&gt; &lt;p&gt;I tried to write some concept code but cannot find right way&lt;/p&gt; &lt;pre&gt;&lt;code&gt;from llama_index.core import SimpleDirectoryReader, VectorStoreIndex, ServiceContext from langchain_huggingface.embeddings import HuggingFaceEmbeddings from langchain_huggingface.llms import HuggingFacePipeline from langchain_huggingface import HuggingFacePipeline # Define your local models embedding_model_name = &amp;quot;sentence-transformers/all-mpnet-base-v2&amp;quot; llm_model_name = &amp;quot;StabilityAI/stablelm-tuned-alpha-3b&amp;quot; # Initialize the embedding model embed_model = HuggingFaceEmbeddings(model_name=embedding_model_name) # Initialize the LLM using from_model_id method llm = HuggingFacePipeline.from_model_id(model_id=llm_model_name, task=&amp;quot;text-generation&amp;quot;) # Create a service context service_context = ServiceContext.from_defaults(embed_model=embed_model, llm=llm) # Load data from the &amp;quot;data&amp;quot; directory reader = SimpleDirectoryReader(input_dir=&amp;quot;./data&amp;quot;, recursive=True) documents = reader.load_data() # Create an index from the loaded documents index = VectorStoreIndex.from_documents(documents, service_context=service_context) # Save the index to a file index.save_to_disk(&amp;#39;index.json&amp;#39;) &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/HomunMage&quot;&gt; /u/HomunMage &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dl6udt/how_to_rag_indexing_and_embedding_by_local_llama/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dl6udt/how_to_rag_indexing_and_embedding_by_local_llama/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dl6udt</id><link href="https://www.reddit.com/r/LangChain/comments/1dl6udt/how_to_rag_indexing_and_embedding_by_local_llama/" /><updated>2024-06-21T15:33:14+00:00</updated><published>2024-06-21T15:33:14+00:00</published><title>How to RAG Indexing and embedding by local llama index with langchain huggingface ?</title></entry><entry><author><name>/u/JAYBORICHA07</name><uri>https://www.reddit.com/user/JAYBORICHA07</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;So in my last project, I made a site that takes the URL of your landing page and gives you recommendations on what you should change in your landing page&amp;#39;s content. Now it was only for content, not for any visuals and I want to go one step further and implement the same for the visuals but don&amp;#39;t have any idea how I can do that.&lt;/p&gt; &lt;p&gt;Here is the link to the site: &lt;a href=&quot;https://landing-page-audit-pwa.vercel.app&quot;&gt;https://landing-page-audit-pwa.vercel.app&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/JAYBORICHA07&quot;&gt; /u/JAYBORICHA07 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dl0p4o/how_can_i_get_feedback_on_my_site_from_llm/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dl0p4o/how_can_i_get_feedback_on_my_site_from_llm/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dl0p4o</id><link href="https://www.reddit.com/r/LangChain/comments/1dl0p4o/how_can_i_get_feedback_on_my_site_from_llm/" /><updated>2024-06-21T10:24:33+00:00</updated><published>2024-06-21T10:24:33+00:00</published><title>How can i get feedback on my site from LLM</title></entry></feed>