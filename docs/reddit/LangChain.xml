<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/"><category term="LangChain" label="r/LangChain"/><updated>2024-04-06T11:51:11+00:00</updated><icon>https://www.redditstatic.com/icon.png/</icon><id>/r/LangChain.rss</id><link rel="self" href="https://www.reddit.com/r/LangChain.rss" type="application/atom+xml" /><link rel="alternate" href="https://www.reddit.com/r/LangChain" type="text/html" /><subtitle>LangChain is an open-source framework and developer toolkit that helps developers get LLM applications from prototype to production. It is available for Python and Javascript at https://www.langchain.com/.</subtitle><title>LangChain</title><entry><author><name>/u/zchaarm</name><uri>https://www.reddit.com/user/zchaarm</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;A place for members of &lt;a href=&quot;/r/LangChain&quot;&gt;r/LangChain&lt;/a&gt; to chat with each other&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/zchaarm&quot;&gt; /u/zchaarm &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_10ljho9</id><link href="https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/" /><updated>2023-01-26T04:33:05+00:00</updated><published>2023-01-26T04:33:05+00:00</published><title>r/LangChain Lounge</title></entry><entry><author><name>/u/Silver_Equivalent_58</name><uri>https://www.reddit.com/user/Silver_Equivalent_58</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I have a simple RAG pipeline, but now say the user is not satisfied with the response(basically a thumbs up or down), how can i incorporate this feedback to improve my RAG in a continuous manner? Thanks&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Silver_Equivalent_58&quot;&gt; /u/Silver_Equivalent_58 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bx74lo/how_to_incorporate_user_feedback_in_rag/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bx74lo/how_to_incorporate_user_feedback_in_rag/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bx74lo</id><link href="https://www.reddit.com/r/LangChain/comments/1bx74lo/how_to_incorporate_user_feedback_in_rag/" /><updated>2024-04-06T09:05:31+00:00</updated><published>2024-04-06T09:05:31+00:00</published><title>How to incorporate user feedback in RAG?</title></entry><entry><author><name>/u/sarthak_uchiha</name><uri>https://www.reddit.com/user/sarthak_uchiha</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I am building a RAG of patients clinical trial and over which preventing prompt injection also , first I am finding some data on clinical trials can some suggest ,where can I get a sample data like that &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/sarthak_uchiha&quot;&gt; /u/sarthak_uchiha &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bx8l2f/need_clinical_trials_dataset/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bx8l2f/need_clinical_trials_dataset/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bx8l2f</id><link href="https://www.reddit.com/r/LangChain/comments/1bx8l2f/need_clinical_trials_dataset/" /><updated>2024-04-06T10:43:46+00:00</updated><published>2024-04-06T10:43:46+00:00</published><title>Need clinical trials dataset</title></entry><entry><author><name>/u/yashdeep1929</name><uri>https://www.reddit.com/user/yashdeep1929</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I have a dataset of 400 resumes in .txt format. I want to build a model that can generate responses to specific candidate queries like &amp;#39;Tell me the skillset of XYZ,&amp;#39; but also handle generic queries like &amp;#39;Tell me the names of people who went to Ivy League schools.&amp;#39; While RAG using OpenAI works well for candidate-specific queries, it struggles with generic ones.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/yashdeep1929&quot;&gt; /u/yashdeep1929 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bwz6op/need_help_regarding_a_llm_project/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bwz6op/need_help_regarding_a_llm_project/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bwz6op</id><link href="https://www.reddit.com/r/LangChain/comments/1bwz6op/need_help_regarding_a_llm_project/" /><updated>2024-04-06T01:28:09+00:00</updated><published>2024-04-06T01:28:09+00:00</published><title>Need help regarding a LLM project</title></entry><entry><author><name>/u/Bubbly-Platypus-8602</name><uri>https://www.reddit.com/user/Bubbly-Platypus-8602</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;here the query of mongodb generated and query queried on mongodb response data is then visualized&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Bubbly-Platypus-8602&quot;&gt; /u/Bubbly-Platypus-8602 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bx3upb/required_open_source_librariespackage_in_python/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bx3upb/required_open_source_librariespackage_in_python/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bx3upb</id><link href="https://www.reddit.com/r/LangChain/comments/1bx3upb/required_open_source_librariespackage_in_python/" /><updated>2024-04-06T05:34:13+00:00</updated><published>2024-04-06T05:34:13+00:00</published><title>Required open source libraries/package in python for visualizing the data fetched from mongodb via prior prompt ,</title></entry><entry><author><name>/u/Important_Hamster171</name><uri>https://www.reddit.com/user/Important_Hamster171</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;m trying to work my way through the &lt;a href=&quot;https://github.com/langchain-ai/langgraph/blob/main/examples/multi_agent/hierarchical_agent_teams.ipynb&quot;&gt;Hierarchical Agent Teams&lt;/a&gt; example in the LangGraph documentation using LMStudio but am getting this error:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;OutputParserException: Could not parse function call: &amp;#39;function_call&amp;#39; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;When I run this block of code:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;for s in research_chain.stream( &amp;quot;when is Taylor Swift&amp;#39;s next tour?&amp;quot;, {&amp;quot;recursion_limit&amp;quot;: 100} ): if &amp;quot;__end__&amp;quot; not in s: print(s) print(&amp;quot;---&amp;quot;) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;function_call comes from this function at the bottom:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;def create_team_supervisor(llm: ChatOpenAI, system_prompt, members) -&amp;gt; str: &amp;quot;&amp;quot;&amp;quot;An LLM-based router.&amp;quot;&amp;quot;&amp;quot; options = [&amp;quot;FINISH&amp;quot;] + members function_def = { &amp;quot;name&amp;quot;: &amp;quot;route&amp;quot;, &amp;quot;description&amp;quot;: &amp;quot;Select the next role.&amp;quot;, &amp;quot;parameters&amp;quot;: { &amp;quot;title&amp;quot;: &amp;quot;routeSchema&amp;quot;, &amp;quot;type&amp;quot;: &amp;quot;object&amp;quot;, &amp;quot;properties&amp;quot;: { &amp;quot;next&amp;quot;: { &amp;quot;title&amp;quot;: &amp;quot;Next&amp;quot;, &amp;quot;anyOf&amp;quot;: [ {&amp;quot;enum&amp;quot;: options}, ], }, }, &amp;quot;required&amp;quot;: [&amp;quot;next&amp;quot;], }, } prompt = ChatPromptTemplate.from_messages( [ (&amp;quot;system&amp;quot;, system_prompt), MessagesPlaceholder(variable_name=&amp;quot;messages&amp;quot;), ( &amp;quot;system&amp;quot;, &amp;quot;Given the conversation above, who should act next?&amp;quot; &amp;quot; Or should we FINISH? Select one of: {options}&amp;quot;, ), ] ).partial(options=str(options), team_members=&amp;quot;, &amp;quot;.join(members)) return ( prompt | llm.bind_functions(functions=[function_def], function_call=&amp;quot;route&amp;quot;) | JsonOutputFunctionsParser() ) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Here&amp;#39;s my chat model:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;llm = ChatOpenAI(temperature=0.0, base_url=&amp;quot;http://localhost:1234/v1&amp;quot;, api_key=&amp;quot;not-needed&amp;quot;, model=&amp;quot;mistral-7b-instruct&amp;quot;) &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Important_Hamster171&quot;&gt; /u/Important_Hamster171 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bwq86t/langgraph_function_call_error_using_lmstudio/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bwq86t/langgraph_function_call_error_using_lmstudio/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bwq86t</id><link href="https://www.reddit.com/r/LangChain/comments/1bwq86t/langgraph_function_call_error_using_lmstudio/" /><updated>2024-04-05T19:07:05+00:00</updated><published>2024-04-05T19:07:05+00:00</published><title>LangGraph 'function_call' error using LMStudio</title></entry><entry><author><name>/u/Transit-Strike</name><uri>https://www.reddit.com/user/Transit-Strike</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Currently I’m training an LLM and that can only handle one input string at a time for summarization. Since it is running with map reduce, it’s also very slow splitting each text into small chunks of size 1024 tokens. I have access four GPUs and my plan is to define a training function that can create 4 GPUs. Load my data structures into batches containing 4 strings. And pass them all to a different GPU I tried that and my GPU utilization for each is still very low. Around 11%, same as last time. However last time each model had partial models loaded onto them. Is map reduce just that slow that it can’t fully utilize the GPU?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Transit-Strike&quot;&gt; /u/Transit-Strike &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bwqhj9/achieving_model_parallelism_and_n_langchain/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bwqhj9/achieving_model_parallelism_and_n_langchain/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bwqhj9</id><link href="https://www.reddit.com/r/LangChain/comments/1bwqhj9/achieving_model_parallelism_and_n_langchain/" /><updated>2024-04-05T19:18:08+00:00</updated><published>2024-04-05T19:18:08+00:00</published><title>Achieving model parallelism and n Langchain</title></entry><entry><author><name>/u/traderprof</name><uri>https://www.reddit.com/user/traderprof</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I did alredy the process, but i cannot register the tool to invoke it&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/traderprof&quot;&gt; /u/traderprof &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bwpi9u/how_to_create_a_custom_tool_in_langchain_i_get/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bwpi9u/how_to_create_a_custom_tool_in_langchain_i_get/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bwpi9u</id><link href="https://www.reddit.com/r/LangChain/comments/1bwpi9u/how_to_create_a_custom_tool_in_langchain_i_get/" /><updated>2024-04-05T18:36:55+00:00</updated><published>2024-04-05T18:36:55+00:00</published><title>How to create a custom tool in LangChain. i get this error. from langchain_core.tools import register_tool, tool ImportError: cannot import name 'register_tool' from 'langchain_core.tools' Does anybody know to solve this?</title></entry><entry><author><name>/u/iclickedca</name><uri>https://www.reddit.com/user/iclickedca</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;e.g. flashrank&amp;#39;s models?&lt;br/&gt; &lt;a href=&quot;https://python.langchain.com/docs/integrations/retrievers/flashrank-reranker/&quot;&gt;https://python.langchain.com/docs/integrations/retrievers/flashrank-reranker/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/iclickedca&quot;&gt; /u/iclickedca &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bwm4is/whats_your_favourite_reranker_any_best_for/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bwm4is/whats_your_favourite_reranker_any_best_for/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bwm4is</id><link href="https://www.reddit.com/r/LangChain/comments/1bwm4is/whats_your_favourite_reranker_any_best_for/" /><updated>2024-04-05T16:18:24+00:00</updated><published>2024-04-05T16:18:24+00:00</published><title>What's your favourite reranker? Any best for reranking chat history?</title></entry><entry><author><name>/u/BlindingLT</name><uri>https://www.reddit.com/user/BlindingLT</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I don&amp;#39;t agree with a lot of the hate LangChain gets. I actually quite like LCEL and the core/community distinctions they&amp;#39;ve made. LCEL is also well documented, so if I&amp;#39;m working with LCEL I know I won&amp;#39;t have to deal with outdated docs.&lt;/p&gt; &lt;p&gt;Having said that, I&amp;#39;m trying to understand why there are inconsistencies even in their most bread and butter classes. If I implement &lt;code&gt;ChatOpenAI&lt;/code&gt; and set the api key with &lt;code&gt;api_key&lt;/code&gt;, I would expect that &lt;code&gt;ChatAnthropic&lt;/code&gt; would work the same way. But nope - that&amp;#39;s &lt;code&gt;anthropic_api_key&lt;/code&gt;. If I set the timeout on &lt;code&gt;ChatOpenAI&lt;/code&gt; with &lt;code&gt;request_timeout&lt;/code&gt;, I would assume I could implement it the same way in &lt;code&gt;ChatAnthropic&lt;/code&gt;. But nope, that&amp;#39;s &lt;code&gt;default_request_timeout&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;I know these are minor annoyances, but they bother me because inconsistencies like these should be the first ones addressed, and I&amp;#39;d like to believe they care enough about their core-est of features to be diligent here.&lt;/p&gt; &lt;p&gt;I&amp;#39;m far from the world&amp;#39;s greatest developer, so perhaps there&amp;#39;s a good reason for the inconsistencies and I&amp;#39;m just missing it?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/BlindingLT&quot;&gt; /u/BlindingLT &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bw79ca/dear_langchain_why/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bw79ca/dear_langchain_why/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bw79ca</id><link href="https://www.reddit.com/r/LangChain/comments/1bw79ca/dear_langchain_why/" /><updated>2024-04-05T02:56:37+00:00</updated><published>2024-04-05T02:56:37+00:00</published><title>Dear LangChain, why?</title></entry><entry><author><name>/u/ZuckyFox</name><uri>https://www.reddit.com/user/ZuckyFox</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bwsgi7/explore_langchain_basics/&quot;&gt; &lt;img src=&quot;https://external-preview.redd.it/627lQQYd7OQMrFcfUEeTy8fr7pvyA-wXJGaqeWyi3k0.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=08d5bb045150177ccd3af9cd17f25ea0a5e58206&quot; alt=&quot;Explore Langchain basics&quot; title=&quot;Explore Langchain basics&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/ZuckyFox&quot;&gt; /u/ZuckyFox &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://youtu.be/veDJ3zKcWd4&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bwsgi7/explore_langchain_basics/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1bwsgi7</id><media:thumbnail url="https://external-preview.redd.it/627lQQYd7OQMrFcfUEeTy8fr7pvyA-wXJGaqeWyi3k0.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=08d5bb045150177ccd3af9cd17f25ea0a5e58206" /><link href="https://www.reddit.com/r/LangChain/comments/1bwsgi7/explore_langchain_basics/" /><updated>2024-04-05T20:39:45+00:00</updated><published>2024-04-05T20:39:45+00:00</published><title>Explore Langchain basics</title></entry><entry><author><name>/u/LARGE_LANGUE_MODEL</name><uri>https://www.reddit.com/user/LARGE_LANGUE_MODEL</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Can everybody let me have a question Regarding the problem of building a chatbot agent, if there are many tools, is there a way to export only the necessary tools and give them to the agent to use?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/LARGE_LANGUE_MODEL&quot;&gt; /u/LARGE_LANGUE_MODEL &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bwgtev/build_agent_chatbot/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bwgtev/build_agent_chatbot/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bwgtev</id><link href="https://www.reddit.com/r/LangChain/comments/1bwgtev/build_agent_chatbot/" /><updated>2024-04-05T12:30:08+00:00</updated><published>2024-04-05T12:30:08+00:00</published><title>Build agent chatbot</title></entry><entry><author><name>/u/wang_teng</name><uri>https://www.reddit.com/user/wang_teng</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hello,&lt;/p&gt; &lt;p&gt;I am currently working with langchain for document-related processing tasks, specifically utilizing the faiss.from_documents feature for indexing and similarity searches. I am interested in understanding what the default FAISS index type is when using faiss.from_documents without specifying any particular configuration. For instance, does it default to using PQIVF, LSH, or another type of index?&lt;/p&gt; &lt;p&gt;Does it only support inner product &amp;amp; L2 index?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/wang_teng&quot;&gt; /u/wang_teng &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bwkmwi/type_of_faiss_index/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bwkmwi/type_of_faiss_index/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bwkmwi</id><link href="https://www.reddit.com/r/LangChain/comments/1bwkmwi/type_of_faiss_index/" /><updated>2024-04-05T15:17:41+00:00</updated><published>2024-04-05T15:17:41+00:00</published><title>Type of Faiss index</title></entry><entry><author><name>/u/nhajji</name><uri>https://www.reddit.com/user/nhajji</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hello, every one I&amp;#39;m new with this so bare with me a little :)&lt;br/&gt; I&amp;#39;m trying to use the langchain SQL agent with my &lt;a href=&quot;https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1&quot;&gt;Mixtral-8x7B-Instruct-v0.1&lt;/a&gt; deployed with vLLM as an openai compatible server. I followed the documentation of langchain sql agent but it&amp;#39;s not working properly, there is always parsing problems, actions inputs and outputs empty, \nObserv always messes the llm output, ..., I didn&amp;#39;t get it to work not even once, I&amp;#39;m missing something and can&amp;#39;t figure out. Here is my code I appreciate any little help :&lt;/p&gt; &lt;pre&gt;&lt;code&gt;from fastapi import FastAPI from langchain_community.llms import VLLMOpenAI from langchain_community.utilities.sql_database import SQLDatabase from langchain_community.agent_toolkits import create_sql_agent from langchain.agents.agent_types import AgentType app = FastAPI() llm = VLLMOpenAI( openai_api_key=&amp;quot;0000&amp;quot;, openai_api_base=&amp;quot;http://0.0.0.0:8000/v1&amp;quot;, model_name=&amp;quot;mistralai/Mixtral-8x7B-Instruct-v0.1&amp;quot;, #model_kwargs={&amp;quot;stop&amp;quot;: [&amp;quot;.&amp;quot;]}, ) db = SQLDatabase.from_uri(&amp;quot;sqlite:///Chinook.db&amp;quot;) agent_executor = create_sql_agent(llm, db=db, agent_type=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True, handle_parsing_errors=True) @app.get(&amp;quot;/langchain/sqlite3/&amp;quot;) async def serve_llm_sqlite3(): agent_executor.invoke({&amp;quot;input&amp;quot;: &amp;quot;Describe the schema of the playlist table&amp;quot;}) &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/nhajji&quot;&gt; /u/nhajji &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bwkeql/sql_agent_with_mixtral/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bwkeql/sql_agent_with_mixtral/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bwkeql</id><link href="https://www.reddit.com/r/LangChain/comments/1bwkeql/sql_agent_with_mixtral/" /><updated>2024-04-05T15:08:22+00:00</updated><published>2024-04-05T15:08:22+00:00</published><title>SQL agent with mixtral</title></entry><entry><author><name>/u/D3NN152000</name><uri>https://www.reddit.com/user/D3NN152000</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi all,&lt;/p&gt; &lt;p&gt;I am relatively new to LangChain and ChatGPT, but for my work we want to use it for querying (single) documents. I have set everything up, using a stuff RetrievalQA chain with Azure OpenAI. It works pretty well, but I figured we could maybe improve the performance with some finetuning (for example, the documents are in Dutch, which does not help performance, but also the question answering in general). &lt;/p&gt; &lt;p&gt;Since the application queries documents, and LangChain adapts the sent prompts to include the relevant document sections, I figured finetuning would work best if it was used on _actual_ LangChain-generated prompts (and expected LangChain-formatted answers). We can get access to actual, human answered QA queries on certain documents. My question is how to set this up properly. I can find how to finetune using Azure OpenAI, but not how I can do this in a way where I can provide questions, answers AND context in the same way it would in the real application, meaning the way LangChain ends up sending the prompts to ChatGPT. Is there a way to provide questions and answers, obtain the context from the relevant sections in the corresponding pdf with LangChain (like it does in the normal automated procedure), and then format an input prompt and expected output with said questions, answers and context to record for finetuning?&lt;/p&gt; &lt;p&gt;Is this even a good idea to begin with, or am I better off looking in another direction? Any insights or code examples are very welcome!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/D3NN152000&quot;&gt; /u/D3NN152000 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bwikyf/finetuning_for_retrievalqa_chain/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bwikyf/finetuning_for_retrievalqa_chain/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bwikyf</id><link href="https://www.reddit.com/r/LangChain/comments/1bwikyf/finetuning_for_retrievalqa_chain/" /><updated>2024-04-05T13:51:46+00:00</updated><published>2024-04-05T13:51:46+00:00</published><title>Finetuning for RetrievalQA chain</title></entry><entry><author><name>/u/Adammm101</name><uri>https://www.reddit.com/user/Adammm101</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey all,&lt;/p&gt; &lt;p&gt;has anyone figured out how to calculate the tokens consumption when invoking chains via Langchain when using Claude models?&lt;/p&gt; &lt;p&gt;Sorry if it can be figured out easily, I just wasn&amp;#39;t able to find it.&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;We&amp;#39;re using the ChatAnthropic fn as our llm.&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;THanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Adammm101&quot;&gt; /u/Adammm101 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bwdr62/claude_tokens_consumption/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bwdr62/claude_tokens_consumption/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bwdr62</id><link href="https://www.reddit.com/r/LangChain/comments/1bwdr62/claude_tokens_consumption/" /><updated>2024-04-05T09:29:07+00:00</updated><published>2024-04-05T09:29:07+00:00</published><title>Claude Tokens consumption</title></entry><entry><author><name>/u/lypsoty112</name><uri>https://www.reddit.com/user/lypsoty112</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;ve been hearing a lot from co-students about how difficult langchain sometimes is to implement in a correct way. Because of this, I&amp;#39;ve created a project that simply follows the main functionalities I personally use in LLM-projects,from now 10 months practically only working in LangChain for projects. I&amp;#39;ve written this in 1 thursday evening before going to bed, so I&amp;#39;m not that sure about it, but any feedback is more than welcome!&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/lypsoty112/llm-project-skeleton?tab=readme-ov-file&quot;&gt;https://github.com/lypsoty112/llm-project-skeleton?tab=readme-ov-file&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/lypsoty112&quot;&gt; /u/lypsoty112 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bw0dke/i_made_a_github_repo_for_beginner_python_devs/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bw0dke/i_made_a_github_repo_for_beginner_python_devs/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bw0dke</id><link href="https://www.reddit.com/r/LangChain/comments/1bw0dke/i_made_a_github_repo_for_beginner_python_devs/" /><updated>2024-04-04T22:01:27+00:00</updated><published>2024-04-04T22:01:27+00:00</published><title>I made a GitHub repo for (beginner) Python devs using LangChain for LLM projects</title></entry><entry><author><name>/u/Automatic_Wheel7643</name><uri>https://www.reddit.com/user/Automatic_Wheel7643</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;pre&gt;&lt;code&gt;from langchain_core.runnables import RunnablePassthrough from langchain_core.output_parsers import StrOutputParser from langchain import PromptTemplate # Prompt template template = &amp;quot;&amp;quot;&amp;quot;Answer the question based only on the following context, which can include text and tables: {context} Question: {question} &amp;quot;&amp;quot;&amp;quot; #prompt = ChatPromptTemplate.from_template(template) prompt = PromptTemplate.from_template(template) # LLM model = Ollama(model=&amp;quot;llama2-uncensored&amp;quot;, callbacks=callbacks) # RAG pipeline chain = ( {&amp;quot;context&amp;quot;: retriever, &amp;quot;question&amp;quot;: RunnablePassthrough()} | prompt | model | StrOutputParser() ) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Automatic_Wheel7643&quot;&gt; /u/Automatic_Wheel7643 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bwa11t/how_to_print_the_context_that_is_in_prompt_of_rag/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bwa11t/how_to_print_the_context_that_is_in_prompt_of_rag/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bwa11t</id><link href="https://www.reddit.com/r/LangChain/comments/1bwa11t/how_to_print_the_context_that_is_in_prompt_of_rag/" /><updated>2024-04-05T05:18:45+00:00</updated><published>2024-04-05T05:18:45+00:00</published><title>how to print the context that is in prompt of RAG langchain in the following</title></entry><entry><author><name>/u/DBdev731</name><uri>https://www.reddit.com/user/DBdev731</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bvprhp/datastax_acquires_langflow_to_accelerate_making/&quot;&gt; &lt;img src=&quot;https://external-preview.redd.it/dBZTPxulPXNX_Ib3EzTQTv0u7LMh7DlOBPSBjd_0rO4.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8e2a3317ccaa76f72cf1d7ebe8e462a84b5c4861&quot; alt=&quot;DataStax Acquires Langflow to Accelerate Making AI Awesome | DataStax&quot; title=&quot;DataStax Acquires Langflow to Accelerate Making AI Awesome | DataStax&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/DBdev731&quot;&gt; /u/DBdev731 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.datastax.com/blog/datastax-acquires-langflow-to-accelerate-generative-ai-app-development?utm_medium=social_organic&amp;amp;utm_source=reddit&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bvprhp/datastax_acquires_langflow_to_accelerate_making/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1bvprhp</id><media:thumbnail url="https://external-preview.redd.it/dBZTPxulPXNX_Ib3EzTQTv0u7LMh7DlOBPSBjd_0rO4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=8e2a3317ccaa76f72cf1d7ebe8e462a84b5c4861" /><link href="https://www.reddit.com/r/LangChain/comments/1bvprhp/datastax_acquires_langflow_to_accelerate_making/" /><updated>2024-04-04T15:16:07+00:00</updated><published>2024-04-04T15:16:07+00:00</published><title>DataStax Acquires Langflow to Accelerate Making AI Awesome | DataStax</title></entry><entry><author><name>/u/paul_ds_berlin</name><uri>https://www.reddit.com/user/paul_ds_berlin</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;--- If this is violating a rule I am very sorry &amp;amp; please feel free to delete ---&lt;/p&gt; &lt;p&gt;Hi everyone - this a paid opportunity,&lt;/p&gt; &lt;p&gt;we are a Berlin based company currently building an LLM based research assistant. We are planning to go live with this till July, however I am the only person on our team working on the cognitive architecture and no-one but me understands a thing about it :D So I will def. need a second pair of eyes looking over what we are doing and as a sparrings partner. Below you find an overview of our project and the requirements.&lt;/p&gt; &lt;p&gt;Required:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Advanced proficiency in Python programming, including data science and machine learning libraries.&lt;/li&gt; &lt;li&gt;Experience in deploying LLM-based applications, best case RAG-Applications.&lt;/li&gt; &lt;li&gt;Experience with vector databases.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Overview of our current activities:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;Natural Language to SQL using LLM based agents &lt;/p&gt; &lt;ul&gt; &lt;li&gt;OpenAI function calls and LangChain-based SQL tools.&lt;/li&gt; &lt;li&gt;Purpose build databases for the agents needs, containing relevant data formatted closely to our business requirements.&lt;/li&gt; &lt;li&gt;Few Shot Example retrieval&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Self-Reflection and Plan and Solve architecture &lt;/p&gt; &lt;ul&gt; &lt;li&gt;Composing 1-2 page long reports using multiple collaborative agents.&lt;/li&gt; &lt;li&gt;Undertaking multiple iterations to generate step-by-step plans and addressing gaps in information. Based on these gaps, new natural language questions are formulated and directed to the SQL agent.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;LLMs are openAI only currently.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Monitoring and evaluation through Langsmith&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This will be an on-going project, so lets meet for a digital coffee chat first :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/paul_ds_berlin&quot;&gt; /u/paul_ds_berlin &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bvn6t9/looking_for_llm_consultant/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bvn6t9/looking_for_llm_consultant/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bvn6t9</id><link href="https://www.reddit.com/r/LangChain/comments/1bvn6t9/looking_for_llm_consultant/" /><updated>2024-04-04T13:30:39+00:00</updated><published>2024-04-04T13:30:39+00:00</published><title>Looking for LLM Consultant</title></entry><entry><author><name>/u/Visual_Ad8050</name><uri>https://www.reddit.com/user/Visual_Ad8050</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Visual_Ad8050&quot;&gt; /u/Visual_Ad8050 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bvy2ws/how_to_deploy_langgraph_using_langserve/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bvy2ws/how_to_deploy_langgraph_using_langserve/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bvy2ws</id><link href="https://www.reddit.com/r/LangChain/comments/1bvy2ws/how_to_deploy_langgraph_using_langserve/" /><updated>2024-04-04T20:35:41+00:00</updated><published>2024-04-04T20:35:41+00:00</published><title>How to deploy langgraph using langserve?</title></entry><entry><author><name>/u/julianponguta</name><uri>https://www.reddit.com/user/julianponguta</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;d like to know if anyone can give me a small example of how to use a vision model like gemini but through the openrouter api using langchain, or somewhere where I can get this documentation &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/julianponguta&quot;&gt; /u/julianponguta &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bw0wxx/example_of_using_an_openrouter_vision_model/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bw0wxx/example_of_using_an_openrouter_vision_model/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bw0wxx</id><link href="https://www.reddit.com/r/LangChain/comments/1bw0wxx/example_of_using_an_openrouter_vision_model/" /><updated>2024-04-04T22:23:02+00:00</updated><published>2024-04-04T22:23:02+00:00</published><title>Example of Using an Openrouter Vision Model</title></entry><entry><author><name>/u/Kiko28045</name><uri>https://www.reddit.com/user/Kiko28045</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi everybody!&lt;/p&gt; &lt;p&gt;I&amp;#39;m currently trying to figure out what are some common mistakes that people make when developing applications using LangChain. These could be related to security, efficiency or even readability issues.&lt;/p&gt; &lt;p&gt;Would love to hear what kinds of good/bad practices you have picked up on while working with LangChain. Also, if you could point me to any good resources on practices to avoid when using LangChain, I would be very appreciative!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Kiko28045&quot;&gt; /u/Kiko28045 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bvodo8/langchain_goodbad_practices/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bvodo8/langchain_goodbad_practices/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bvodo8</id><link href="https://www.reddit.com/r/LangChain/comments/1bvodo8/langchain_goodbad_practices/" /><updated>2024-04-04T14:21:25+00:00</updated><published>2024-04-04T14:21:25+00:00</published><title>LangChain Good/Bad Practices</title></entry><entry><author><name>/u/Delicious_Success303</name><uri>https://www.reddit.com/user/Delicious_Success303</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi all! I am working on a RAG application to which i gave a list of apis and 1-2 lines about that api. I query it and it should return the relevant api. The api list is in json format and i save that file as a txt file and generate embeddings. But the problem is its accuracy. Sometimes it gives proper answer and sometime s it says the api is not present in context. Any idea how to improve its accuracy. How do you guys prompt in RAG applications?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Delicious_Success303&quot;&gt; /u/Delicious_Success303 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bvxp42/rag_aplication/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bvxp42/rag_aplication/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bvxp42</id><link href="https://www.reddit.com/r/LangChain/comments/1bvxp42/rag_aplication/" /><updated>2024-04-04T20:20:29+00:00</updated><published>2024-04-04T20:20:29+00:00</published><title>RAG aplication</title></entry><entry><author><name>/u/DescriptionKind621</name><uri>https://www.reddit.com/user/DescriptionKind621</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Which simplest yet effective approaches other than LLMs will be a better approach to have a sentence similarity matching algorithm. I am looking at information schemas for having descriptions of columns. Want to get pinpoint column names based on queries having column descriptions ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/DescriptionKind621&quot;&gt; /u/DescriptionKind621 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bvp6lc/sentence_similarity_algorithms/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bvp6lc/sentence_similarity_algorithms/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bvp6lc</id><link href="https://www.reddit.com/r/LangChain/comments/1bvp6lc/sentence_similarity_algorithms/" /><updated>2024-04-04T14:53:22+00:00</updated><published>2024-04-04T14:53:22+00:00</published><title>Sentence Similarity algorithms</title></entry><entry><author><name>/u/cryptokaykay</name><uri>https://www.reddit.com/user/cryptokaykay</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bv4nzb/update_langtrace_launch_opensource_llm_monitoring/&quot;&gt; &lt;img src=&quot;https://a.thumbs.redditmedia.com/YiEW2V1SquZh7v1QB18kA-ptGQZiLOsjSUgzLYy4Xu8.jpg&quot; alt=&quot;Update: Langtrace Launch: Opensource LLM monitoring tool - achieving better cardinality compared to Langsmith.&quot; title=&quot;Update: Langtrace Launch: Opensource LLM monitoring tool - achieving better cardinality compared to Langsmith.&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;This is a follow up for: &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bnkvtv/update_langtrace_preview_opensource_llm/&quot;&gt;https://www.reddit.com/r/LangChain/comments/1bnkvtv/update_langtrace_preview_opensource_llm/&lt;/a&gt; &lt;/p&gt; &lt;p&gt;I am happy to finally launch &lt;strong&gt;Langtrace - an open source observability tool that collects and analyze traces in order to help you improve your LLM apps&lt;/strong&gt;. Langtrace has two components:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;SDK&lt;/strong&gt;: The SDK is a lightweight library that you can install and import into your project to collect traces.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Langtrace Dashboard&lt;/strong&gt;: The dashboard is a web-based interface where you can view and analyze your traces.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Attaching a couple of GIFs for your preview.&lt;/p&gt; &lt;p&gt;For context, we started this project internally a while back to solve our own problems. We are currently looking for feedback on how to improve this product and looking to boot strap a community around it. You can join our discord community using this link - &lt;a href=&quot;https://discord.com/invite/EaSATwtr4t&quot;&gt;https://discord.com/invite/EaSATwtr4t&lt;/a&gt; &lt;/p&gt; &lt;p&gt;There are a couple of ways to use this product:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;You can sign up using this link - &lt;a href=&quot;https://langtrace.ai/&quot;&gt;https://langtrace.ai/&lt;/a&gt; to the hosted version, generate an API key, install and initialize the SDK in your application with the API key to start sending traces. &lt;ol&gt; &lt;li&gt;&lt;strong&gt;The SDK installation and initialization is just 2 lines of code.&lt;/strong&gt;&lt;/li&gt; &lt;/ol&gt;&lt;/li&gt; &lt;li&gt;You can self host and use it within your own environment.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;You can find more details in our docs - &lt;a href=&quot;https://docs.langtrace.ai/introduction&quot;&gt;https://docs.langtrace.ai/introduction&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Open Source and Open Telemetry&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Entire code including the SDK and the web application is open source. You can check it out from here - &lt;a href=&quot;https://github.com/Scale3-Labs/langtrace&quot;&gt;https://github.com/Scale3-Labs/langtrace&lt;/a&gt; .&lt;/p&gt; &lt;p&gt;The spans generated by our SDKs adhere to &lt;strong&gt;open telemetry standards (OTEL)&lt;/strong&gt; which means, you can continue to use your existing observability backend and consume these traces by installing our SDKs.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Vendors supported&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;We support OpenAI, Anthropic, Langchain, LlamaIndex, ChromaDB, PineconeDB. We will continue to add more in the coming weeks.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Pricing (for the hosted version)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;It&amp;#39;s completely free to use at the moment. Since this is the first version, it is still rough around the edges and we are looking for feedback from the community to continue to improve and nail the experience. However, we may start to monetize the hosted version at some point at a reasonable cost. But, you can continue to use our open source version, self host and use it for free.&lt;/p&gt; &lt;p&gt;For more details, please do check out our launch blog post - &lt;a href=&quot;https://langtrace.ai/blog/introducing-langtrace&quot;&gt;https://langtrace.ai/blog/introducing-langtrace&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;Thank you all for continuing to engage with me over the past few weeks. It has been super fun building this project and we look forward to hearing all your feedback on our Discord.&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://i.redd.it/t29eh8rnxbsc1.gif&quot;&gt;https://i.redd.it/t29eh8rnxbsc1.gif&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://i.redd.it/k4ns4arnxbsc1.gif&quot;&gt;https://i.redd.it/k4ns4arnxbsc1.gif&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/cryptokaykay&quot;&gt; /u/cryptokaykay &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bv4nzb/update_langtrace_launch_opensource_llm_monitoring/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bv4nzb/update_langtrace_launch_opensource_llm_monitoring/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1bv4nzb</id><media:thumbnail url="https://a.thumbs.redditmedia.com/YiEW2V1SquZh7v1QB18kA-ptGQZiLOsjSUgzLYy4Xu8.jpg" /><link href="https://www.reddit.com/r/LangChain/comments/1bv4nzb/update_langtrace_launch_opensource_llm_monitoring/" /><updated>2024-04-03T21:20:19+00:00</updated><published>2024-04-03T21:20:19+00:00</published><title>Update: Langtrace Launch: Opensource LLM monitoring tool - achieving better cardinality compared to Langsmith.</title></entry></feed>