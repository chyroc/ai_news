<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/"><category term="LangChain" label="r/LangChain"/><updated>2023-12-06T22:38:42+00:00</updated><icon>https://www.redditstatic.com/icon.png/</icon><id>/r/LangChain.rss</id><link rel="self" href="https://www.reddit.com/r/LangChain.rss" type="application/atom+xml" /><link rel="alternate" href="https://www.reddit.com/r/LangChain" type="text/html" /><subtitle>LangChain is an open-source framework and developer toolkit that helps developers get LLM applications from prototype to production. It is available for Python and Javascript at https://www.langchain.com/.</subtitle><title>LangChain</title><entry><author><name>/u/zchaarm</name><uri>https://www.reddit.com/user/zchaarm</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;A place for members of &lt;a href=&quot;/r/LangChain&quot;&gt;r/LangChain&lt;/a&gt; to chat with each other&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/zchaarm&quot;&gt; /u/zchaarm &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_10ljho9</id><link href="https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/" /><updated>2023-01-26T04:33:05+00:00</updated><published>2023-01-26T04:33:05+00:00</published><title>r/LangChain Lounge</title></entry><entry><author><name>/u/johan_donquixote</name><uri>https://www.reddit.com/user/johan_donquixote</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Currently I am looping over chunks and getting keywords using prompt...&lt;/p&gt; &lt;p&gt;How do I combine the keywords from different chunks to get the most important keywords of the whole doc. &lt;/p&gt; &lt;p&gt;I was thinking of giving the summary of document(to understand context) as an input to the prompt along with all the keywords to get final output...&lt;/p&gt; &lt;p&gt;Any better method to do this?&lt;/p&gt; &lt;p&gt;TIA&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/johan_donquixote&quot;&gt; /u/johan_donquixote &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18cbvfj/i_want_to_extract_important_keywords_from_large/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18cbvfj/i_want_to_extract_important_keywords_from_large/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18cbvfj</id><link href="https://www.reddit.com/r/LangChain/comments/18cbvfj/i_want_to_extract_important_keywords_from_large/" /><updated>2023-12-06T19:24:55+00:00</updated><published>2023-12-06T19:24:55+00:00</published><title>I want to extract important keywords from large documents...</title></entry><entry><author><name>/u/mean-short-</name><uri>https://www.reddit.com/user/mean-short-</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I want to create an agent that is able to do RAG using langchain.&lt;br/&gt; I found this: &lt;a href=&quot;https://python.langchain.com/docs/use_cases/question_answering/conversational_retrieval_agents&quot;&gt;https://python.langchain.com/docs/use_cases/question_answering/conversational_retrieval_agents&lt;/a&gt;&lt;br/&gt; I can&amp;#39;t seem to get it to focus its search on the database alone, it still goes to its general knowledge to answer.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/mean-short-&quot;&gt; /u/mean-short- &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18cemoh/rag_with_agents/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18cemoh/rag_with_agents/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18cemoh</id><link href="https://www.reddit.com/r/LangChain/comments/18cemoh/rag_with_agents/" /><updated>2023-12-06T21:22:57+00:00</updated><published>2023-12-06T21:22:57+00:00</published><title>RAG with agents</title></entry><entry><author><name>/u/CantaloupeLeading646</name><uri>https://www.reddit.com/user/CantaloupeLeading646</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;i&amp;#39;m using chatGPT-4 for coding and i noticed it doesn&amp;#39;t use langchain properly. i mean that if i want chatGPT to implement a basic example using pytorch or sk-learn it does so without much hassle, but when it comes to a simple example with langchain it starts to show me rough estimates of how the code should look like and not actual runnable code. &lt;/p&gt; &lt;p&gt;I&amp;#39;m wondering, is there a way to bypass that or is it intentional? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/CantaloupeLeading646&quot;&gt; /u/CantaloupeLeading646 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18ccamh/chatgpt_doesnt_have_access_to_langchain/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18ccamh/chatgpt_doesnt_have_access_to_langchain/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18ccamh</id><link href="https://www.reddit.com/r/LangChain/comments/18ccamh/chatgpt_doesnt_have_access_to_langchain/" /><updated>2023-12-06T19:43:28+00:00</updated><published>2023-12-06T19:43:28+00:00</published><title>chatGPT doesn't have access to langchain</title></entry><entry><author><name>/u/jerry_10_</name><uri>https://www.reddit.com/user/jerry_10_</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18c70qt/libmagic_not_working_even_though_it_is_installed/&quot;&gt; &lt;img src=&quot;https://b.thumbs.redditmedia.com/z48mUrX7JIxSBVvUVvrsM6okNu_wzJk2qV3TRwScgAs.jpg&quot; alt=&quot;Libmagic not working, Even though it is installed&quot; title=&quot;Libmagic not working, Even though it is installed&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I want to make a project that reads URLs, makes embeddings, and stores them in a vector store. For this, I am using UnstructuredURLLoader from the langchain library. This library uses another library called libmagic. I have pip-installed python-libmagic and python-libmagic-bin, but it still shows me the following error. &lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/iwdqoleg5p4c1.png?width=1408&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6f748b7398405eed474d38741992a6fe57dd8ea2&quot;&gt;https://preview.redd.it/iwdqoleg5p4c1.png?width=1408&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6f748b7398405eed474d38741992a6fe57dd8ea2&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/jerry_10_&quot;&gt; /u/jerry_10_ &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18c70qt/libmagic_not_working_even_though_it_is_installed/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18c70qt/libmagic_not_working_even_though_it_is_installed/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_18c70qt</id><media:thumbnail url="https://b.thumbs.redditmedia.com/z48mUrX7JIxSBVvUVvrsM6okNu_wzJk2qV3TRwScgAs.jpg" /><link href="https://www.reddit.com/r/LangChain/comments/18c70qt/libmagic_not_working_even_though_it_is_installed/" /><updated>2023-12-06T15:56:23+00:00</updated><published>2023-12-06T15:56:23+00:00</published><title>Libmagic not working, Even though it is installed</title></entry><entry><author><name>/u/urlaklbek</name><uri>https://www.reddit.com/user/urlaklbek</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hello friends! I design my own langchain alternative for Go programming language and I&amp;#39;m trying to understand why Langchain support dynamic prompt templating? By that I mean ability to create prompt based on results from previous steps. Here&amp;#39;s some python-like pseudocode to make things clear:&lt;/p&gt; &lt;p&gt;&lt;code&gt;chain(step1,step2, step3(prompt=&amp;quot;bla bla bla... {step1Result}&amp;quot;))&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Could you guys please provide some examples? The more examples you have the better. Simple, complex... any of them! &lt;/p&gt; &lt;p&gt;--- &lt;/p&gt; &lt;p&gt;Please note that I have nothing against &amp;quot;simple chains&amp;quot; where we pass result from one step to the next one. What I&amp;#39;m not sure I get is real use cases for the ability to pass to the step results of &amp;quot;any previous steps&amp;quot;. Langchain seems to have some kind of global execution context of the chain that every step has access to. I wanna now whether I must add it to my lib or not. &lt;/p&gt; &lt;p&gt;Thank u!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/urlaklbek&quot;&gt; /u/urlaklbek &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18c2ovj/why_have_prompt_templates/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18c2ovj/why_have_prompt_templates/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18c2ovj</id><link href="https://www.reddit.com/r/LangChain/comments/18c2ovj/why_have_prompt_templates/" /><updated>2023-12-06T12:14:31+00:00</updated><published>2023-12-06T12:14:31+00:00</published><title>Why have Prompt Templates?</title></entry><entry><author><name>/u/jindo1412</name><uri>https://www.reddit.com/user/jindo1412</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;Currently, I want to build RAG chatbot for production. I already had my LLM API and I want to create a custom LLM and then use this in RetrievalQA.from_chain_type function. I don&amp;#39;t know whether Langchain support this in my case.&lt;/p&gt; &lt;p&gt;I read about this topic on reddit: &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/17v1rhv/integrating_llm_rest_api_into_a_langchain/&quot;&gt;https://www.reddit.com/r/LangChain/comments/17v1rhv/integrating_llm_rest_api_into_a_langchain/&lt;/a&gt; And in langchain document: &lt;a href=&quot;https://python.langchain.com/docs/modules/model_io/llms/custom_llm&quot;&gt;https://python.langchain.com/docs/modules/model_io/llms/custom_llm&lt;/a&gt;&lt;/p&gt; &lt;p&gt;But this still does not work when I apply the custom LLM to qa_chain. Below is my code, hope for the support from you, sorry for my language, english is not my mother tongue.&lt;/p&gt; &lt;p&gt;``` from pydantic import Extra import requests from typing import Any, List, Mapping, Optional&lt;/p&gt; &lt;p&gt;from langchain.callbacks.manager import CallbackManagerForLLMRun from langchain.llms.base import LLM&lt;/p&gt; &lt;p&gt;class LlamaLLM(LLM): llm_url = &amp;#39;https:/myhost/llama/api&amp;#39;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;class Config: extra = Extra.forbid @property def _llm_type(self) -&amp;gt; str: return &amp;quot;Llama2 7B&amp;quot; def _call( self, prompt: str, stop: Optional[List[str]] = None, run_manager: Optional[CallbackManagerForLLMRun] = None, **kwargs: Any, ) -&amp;gt; str: if stop is not None: raise ValueError(&amp;quot;stop kwargs are not permitted.&amp;quot;) payload = { &amp;quot;inputs&amp;quot;: prompt, &amp;quot;parameters&amp;quot;: {&amp;quot;max_new_tokens&amp;quot;: 100}, &amp;quot;token&amp;quot;: &amp;quot;abcdfejkwehr&amp;quot; } headers = {&amp;quot;Content-Type&amp;quot;: &amp;quot;application/json&amp;quot;} response = requests.post(self.llm_url, json=payload, headers=headers, verify=False) response.raise_for_status() # print(&amp;quot;API Response:&amp;quot;, response.json()) return response.json()[&amp;#39;generated_text&amp;#39;] # get the response from the API @property def _identifying_params(self) -&amp;gt; Mapping[str, Any]: &amp;quot;&amp;quot;&amp;quot;Get the identifying parameters.&amp;quot;&amp;quot;&amp;quot; return {&amp;quot;llmUrl&amp;quot;: self.llm_url} &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;```&lt;/p&gt; &lt;p&gt;&lt;code&gt; llm = LlamaLLM() &lt;/code&gt;&lt;/p&gt; &lt;p&gt;```&lt;/p&gt; &lt;h1&gt;Testing&lt;/h1&gt; &lt;p&gt;prompt = &amp;quot;[INST] Question: Who is Albert Einstein? \n Answer: [/INST]&amp;quot; result = llm._call(prompt) print(result)&lt;/p&gt; &lt;p&gt;Albert Einstein (1879-1955) was a German-born theoretical physicist who is widely regarded as one of the most influential scientists of the 20th century. He is best known for his theory of relativity, which revolutionized our understanding of space and time, and his famous equation E=mc². ```&lt;/p&gt; &lt;p&gt;```&lt;/p&gt; &lt;h1&gt;Build prompt&lt;/h1&gt; &lt;p&gt;from langchain.prompts import PromptTemplate template = &amp;quot;&amp;quot;&amp;quot;[INST] &amp;lt;&amp;lt;SYS&amp;gt;&amp;gt;&lt;/p&gt; &lt;p&gt;Answer the question base on the context below.&lt;/p&gt; &lt;p&gt;&amp;lt;&amp;lt;/SYS&amp;gt;&amp;gt;&lt;/p&gt; &lt;p&gt;Context: {context} Question: {question} Answer: [/INST]&amp;quot;&amp;quot;&amp;quot; QA_CHAIN_PROMPT = PromptTemplate(input_variables=[&amp;quot;context&amp;quot;, &amp;quot;question&amp;quot;],template=template,)&lt;/p&gt; &lt;h1&gt;Run chain&lt;/h1&gt; &lt;p&gt;from langchain.chains import RetrievalQA&lt;/p&gt; &lt;p&gt;qa_chain = RetrievalQA.from_chain_type(llm, verbose=True, # retriever=vectordb.as_retriever(), retriever=custom_retriever, return_source_documents=True, chain_type_kwargs={&amp;quot;prompt&amp;quot;: QA_CHAIN_PROMPT}) ```&lt;/p&gt; &lt;p&gt;``` question = &amp;quot;Is probability a class topic?&amp;quot; result = qa_chain({&amp;quot;query&amp;quot;: question}) result[&amp;quot;result&amp;quot;]&lt;/p&gt; &lt;p&gt;Encountered some errors. Please recheck your request! ```&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/jindo1412&quot;&gt; /u/jindo1412 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18btf1w/custom_llm_from_api_for_qa_chain/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18btf1w/custom_llm_from_api_for_qa_chain/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18btf1w</id><link href="https://www.reddit.com/r/LangChain/comments/18btf1w/custom_llm_from_api_for_qa_chain/" /><updated>2023-12-06T02:23:28+00:00</updated><published>2023-12-06T02:23:28+00:00</published><title>Custom LLM from API for QA chain</title></entry><entry><author><name>/u/sayanosis</name><uri>https://www.reddit.com/user/sayanosis</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Firstly, thank you so much for helping me with this. &lt;/p&gt; &lt;p&gt;I want to make a streamlit app which has RAG and Memory. This is how it looks: &lt;/p&gt; &lt;pre&gt;&lt;code&gt;_template = &amp;quot;&amp;quot;&amp;quot;Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language. Chat History: {chat_history} Follow Up Input: {question} Standalone question:&amp;quot;&amp;quot;&amp;quot; CONDENSE_QUESTION_PROMPT = PromptTemplate.from_template(_template) template = &amp;quot;&amp;quot;&amp;quot;Answer the question based only on the following context: {context} Question: {question} &amp;quot;&amp;quot;&amp;quot; ANSWER_PROMPT = ChatPromptTemplate.from_template(template) _inputs = RunnableParallel( standalone_question=RunnablePassthrough.assign( chat_history=lambda x: _format_chat_history(x[&amp;quot;chat_history&amp;quot;]) ) | CONDENSE_QUESTION_PROMPT | llmc | StrOutputParser(), ) _context = { &amp;quot;context&amp;quot;: itemgetter(&amp;quot;standalone_question&amp;quot;) | retriever | _combine_documents, &amp;quot;question&amp;quot;: lambda x: x[&amp;quot;standalone_question&amp;quot;], } conversational_qa_chain = _inputs | _context | ANSWER_PROMPT | llm &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;the _format_chat_history function looks like this&lt;/p&gt; &lt;pre&gt;&lt;code&gt;def _format_chat_history(chat_history: List[Tuple[str, str]]) -&amp;gt; str: # chat history is of format: # [ # (human_message_str, ai_message_str), # ... # ] # see below for an example of how it&amp;#39;s invoked buffer = &amp;quot;&amp;quot; for dialogue_turn in chat_history: human = &amp;quot;Human: &amp;quot; + dialogue_turn[0] ai = &amp;quot;Assistant: &amp;quot; + dialogue_turn[1] buffer += &amp;quot;\n&amp;quot; + &amp;quot;\n&amp;quot;.join([human, ai]) return buffer &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;My question is, streamlit already has messages stored in st.session_state.messages&lt;/p&gt; &lt;pre&gt;&lt;code&gt;st.session_state.messages &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;How to i pass this onto the chain to be condensed. Please help.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/sayanosis&quot;&gt; /u/sayanosis &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18bofgy/help_with_conversational_qa_chain_streamlit/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18bofgy/help_with_conversational_qa_chain_streamlit/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18bofgy</id><link href="https://www.reddit.com/r/LangChain/comments/18bofgy/help_with_conversational_qa_chain_streamlit/" /><updated>2023-12-05T22:32:23+00:00</updated><published>2023-12-05T22:32:23+00:00</published><title>Help with conversational_qa_chain - Streamlit Messages</title></entry><entry><author><name>/u/Temporary-Size7310</name><uri>https://www.reddit.com/user/Temporary-Size7310</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;pre&gt;&lt;code&gt;model_name = &amp;quot;jinaai/jina-embeddings-v2-small-en&amp;quot; model_kwargs = {&amp;quot;device&amp;quot;: &amp;quot;cuda&amp;quot;} encode_kwargs = {&amp;quot;normalize_embeddings&amp;quot;:True} embeddings = SentenceTransformerEmbeddings(model_name=model_name, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs ) store = InMemoryStore() #storage layer for parent documents child_text_splitter = RecursiveCharacterTextSplitter(chunk_size=400) parent_splitter = RecursiveCharacterTextSplitter(chunk_size=2000) vectordb = Chroma( embedding_function=embeddings, persist_directory=&amp;quot;./chroma_db_parent&amp;quot;, collection_name=&amp;quot;split_parents&amp;quot;, ) big_chunks_retrievr = ParentDocumentRetriever( vectorstore=vectordb, docstore=store, child_splitter=child_text_splitter, parent_splitter=parent_splitter, ) ---&amp;gt; 33 ParentDocumentRetriever( 34 vectorstore=vectordb, 35 docstore=store, TypeError: MultiVectorRetriever.__init__() got an unexpected keyword argument &amp;#39;child_splitter&amp;#39; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Relaunched my code today and it didn&amp;#39;t work anymore, any suggestion ? :/ &lt;/p&gt; &lt;p&gt;EDIT: There is an issue with langchain last release, re-installed 0.0.340 and relaunched it works.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Temporary-Size7310&quot;&gt; /u/Temporary-Size7310 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18bssnn/error_with_parentdocumentretriever_didnt/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18bssnn/error_with_parentdocumentretriever_didnt/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18bssnn</id><link href="https://www.reddit.com/r/LangChain/comments/18bssnn/error_with_parentdocumentretriever_didnt/" /><updated>2023-12-06T01:52:13+00:00</updated><published>2023-12-06T01:52:13+00:00</published><title>Error with ParentDocumentRetriever, didn't recognize child_splitter</title></entry><entry><author><name>/u/DannyBrownMz</name><uri>https://www.reddit.com/user/DannyBrownMz</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Anyone knows whether support for legacy langchain methods like sequential chain would still be continued(though it still is for now at least) despite the new addition LCEL? &lt;/p&gt; &lt;p&gt;Reason being that I find using Sequential chain and other types of chains used in Legacy Langchain quite easier to understand and implement than LCEL, plus it gives me better results.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/DannyBrownMz&quot;&gt; /u/DannyBrownMz &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18bhzhk/support_for_legacy/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18bhzhk/support_for_legacy/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18bhzhk</id><link href="https://www.reddit.com/r/LangChain/comments/18bhzhk/support_for_legacy/" /><updated>2023-12-05T18:00:38+00:00</updated><published>2023-12-05T18:00:38+00:00</published><title>Support for Legacy</title></entry><entry><author><name>/u/devinbost</name><uri>https://www.reddit.com/user/devinbost</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Most of the doc loaders assume a &amp;quot;one and done&amp;quot; process. &lt;/p&gt; &lt;p&gt;Anyone have suggestions on continually adding docs like for a RAG flow? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/devinbost&quot;&gt; /u/devinbost &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18bfxjm/anyone_have_suggestions_on_continuous_doc_loading/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18bfxjm/anyone_have_suggestions_on_continuous_doc_loading/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18bfxjm</id><link href="https://www.reddit.com/r/LangChain/comments/18bfxjm/anyone_have_suggestions_on_continuous_doc_loading/" /><updated>2023-12-05T16:31:12+00:00</updated><published>2023-12-05T16:31:12+00:00</published><title>Anyone have suggestions on continuous doc loading?</title></entry><entry><author><name>/u/anonymous_anki</name><uri>https://www.reddit.com/user/anonymous_anki</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I am trying to build a youtube assistant with the help of langchain and google palm api.&lt;/p&gt; &lt;p&gt;So, when I finally run my code, I am getting this error:&lt;/p&gt; &lt;pre&gt;&lt;code&gt; Traceback (most recent call last): File &amp;quot;/home/youtube_assitant/langchain_helper.py&amp;quot;, line 62, in &amp;lt;module&amp;gt; response, docs = get_response_from_query(vectordb, query) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File &amp;quot;/home/youtube_assitant/langchain_helper.py&amp;quot;, line 34, in get_response_from_query llm = google_palm(google_api_key=os.getenv(&amp;quot;GOOGLE_API_KEY&amp;quot;), temperature = 0) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ TypeError: &amp;#39;module&amp;#39; object is not callable &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Here is the entire code:&lt;/p&gt; &lt;pre&gt;&lt;code&gt; from langchain.document_loaders import YoutubeLoader from langchain.text_splitter import RecursiveCharacterTextSplitter from langchain.llms import google_palm from langchain.chains import LLMChain from langchain.prompts import PromptTemplate from langchain.vectorstores import FAISS from langchain.embeddings.google_palm import GooglePalmEmbeddings import os from dotenv import load_dotenv load_dotenv() embeddngs = GooglePalmEmbeddings() video_url = &amp;quot;https://www.youtube.com/watch?v=XxOh12Uhg08&amp;quot; # create_vectordb_from_youtube_url = cvfyu def create_vectordb_from_youtube_url(video_url: str) -&amp;gt; FAISS: loader = YoutubeLoader.from_youtube_url(video_url) transcript = loader.load() text_splitter = RecursiveCharacterTextSplitter(chunk_size = 1000, chunk_overlap = 100) docs = text_splitter.split_documents(transcript) db = FAISS.from_documents(docs, embeddngs) return db def get_response_from_query(db, query, k=4): # k is no. of docs that will be create docs = db.similarity_search(query, k = k) docs_page_content = &amp;quot; &amp;quot;.join([d.page_content for d in docs]) llm = google_palm(google_api_key=os.getenv(&amp;quot;GOOGLE_API_KEY&amp;quot;), temperature = 0) prompt = PromptTemplate( input_variables=[&amp;quot;question&amp;quot;, &amp;quot;docs&amp;quot;], template=&amp;quot;&amp;quot;&amp;quot; You are a helpful assistant that that can answer questions about youtube videos based on the video&amp;#39;s transcript. Answer the following question: {question} By searching the following video transcript: {docs} Only use the factual information from the transcript to answer the question. If you feel like you don&amp;#39;t have enough information to answer the question, say &amp;quot;I don&amp;#39;t know&amp;quot;. Your answers should be verbose and detailed. &amp;quot;&amp;quot;&amp;quot;, ) chain = LLMChain(llm=llm, prompt=prompt) response = chain.run(question=query, docs=docs_page_content) response = response.replace(&amp;quot;\n&amp;quot;,&amp;quot;&amp;quot;) return response, docs vectordb = create_vectordb_from_youtube_url(video_url) query = &amp;quot;What this video is about?&amp;quot; response, docs = get_response_from_query(vectordb, query) print(&amp;quot;Response: &amp;quot;, response) print(&amp;quot;Docs: &amp;quot;, docs) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;I have try many methods but nothing workout so far. I tried to google search it too but it didn&amp;#39;t work. Can someone please tell me how to fix this?? Thank you.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/anonymous_anki&quot;&gt; /u/anonymous_anki &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18bhdim/module_object_is_not_callable_in_langchain/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18bhdim/module_object_is_not_callable_in_langchain/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18bhdim</id><link href="https://www.reddit.com/r/LangChain/comments/18bhdim/module_object_is_not_callable_in_langchain/" /><updated>2023-12-05T17:34:02+00:00</updated><published>2023-12-05T17:34:02+00:00</published><title>&quot;module&quot; object is not callable in langchain</title></entry><entry><author><name>/u/overflow74</name><uri>https://www.reddit.com/user/overflow74</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I’m trying to build a customer service chatbot for a travel agency (book flights,hotels , answers questions about visa etc..) i want to use openAi api with gpt3.5 however i’m facing a difficulty with building a conversation pipeline. is there a framework (other than langchain) that could help with this project?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/overflow74&quot;&gt; /u/overflow74 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18b9uk9/chat_bot/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18b9uk9/chat_bot/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18b9uk9</id><link href="https://www.reddit.com/r/LangChain/comments/18b9uk9/chat_bot/" /><updated>2023-12-05T11:20:32+00:00</updated><published>2023-12-05T11:20:32+00:00</published><title>Chat bot</title></entry><entry><author><name>/u/techocompany25</name><uri>https://www.reddit.com/user/techocompany25</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I have a very structured DB that mostly contain numerical valuex (x: 55, y:77... etc). My use case is to chat naturally with the DB. Do I really need pgvector to do similarity search when the DB mostly contains numerical values? Would actually using pgvector bring less accurate results with this type of data?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/techocompany25&quot;&gt; /u/techocompany25 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18b80wx/is_pgvector_needed_for_a_structured_db_or_the_sql/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18b80wx/is_pgvector_needed_for_a_structured_db_or_the_sql/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18b80wx</id><link href="https://www.reddit.com/r/LangChain/comments/18b80wx/is_pgvector_needed_for_a_structured_db_or_the_sql/" /><updated>2023-12-05T09:07:45+00:00</updated><published>2023-12-05T09:07:45+00:00</published><title>Is PgVector needed for a structured DB or the sql agent is enough?</title></entry><entry><author><name>/u/AlkaliMedia</name><uri>https://www.reddit.com/user/AlkaliMedia</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi there, I made an assistant using &lt;/p&gt; &lt;p&gt;OpenAIAssistantRunnable and ran invoke, the response I get is&lt;/p&gt; &lt;p&gt;&lt;code&gt;[ThreadMessage(id=&amp;#39;msg_BtVZtxShzp5AEvFgK1mwyhme&amp;#39;, assistant_id=&amp;#39;asst_8HFshA1tzFfosxA6G7kzqpCt&amp;#39;, content=[MessageContentText(text=Text(annotations=[], value=&amp;#39;THIS IS WHAT I WANT.&amp;#39;), type=&amp;#39;text&amp;#39;)], created_at=1701792953, file_ids=[], metadata={}, object=&amp;#39;thread.message&amp;#39;, role=&amp;#39;assistant&amp;#39;, run_id=&amp;#39;run_12o5LTNgEFcw18zl9wiMfwb8&amp;#39;, thread_id=&amp;#39;thread_LzQyxf4I41GaTZFqryqjfWRv&amp;#39;)]&lt;/code&gt;&lt;/p&gt; &lt;p&gt;I cannot extract the value, whatever approach I try. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/AlkaliMedia&quot;&gt; /u/AlkaliMedia &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18bg3ve/cannot_parse_assitant_response/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18bg3ve/cannot_parse_assitant_response/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18bg3ve</id><link href="https://www.reddit.com/r/LangChain/comments/18bg3ve/cannot_parse_assitant_response/" /><updated>2023-12-05T16:38:51+00:00</updated><published>2023-12-05T16:38:51+00:00</published><title>Cannot Parse Assitant Response</title></entry><entry><author><name>/u/OpeningMarsupial7229</name><uri>https://www.reddit.com/user/OpeningMarsupial7229</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hello everyone I&amp;#39;m trying do an usecase where I can chat with CSV files,my CSV files is of 100k rows and 56 columns when I&amp;#39;m creating an CSV agent it is failing beacause of input token limit is exceeded and allowed limit is 4096,how do approach this problem please help&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/OpeningMarsupial7229&quot;&gt; /u/OpeningMarsupial7229 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18b6qjm/large_csv_files_with_llama/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18b6qjm/large_csv_files_with_llama/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18b6qjm</id><link href="https://www.reddit.com/r/LangChain/comments/18b6qjm/large_csv_files_with_llama/" /><updated>2023-12-05T07:32:28+00:00</updated><published>2023-12-05T07:32:28+00:00</published><title>Large CSV files with llama</title></entry><entry><author><name>/u/nderstand2grow</name><uri>https://www.reddit.com/user/nderstand2grow</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;ve come across many LLM frameworks: Langchain, LlamaIndex, LMQL, guidance, Marvin, Instructor, etc. There&amp;#39;s a lot of overlap between them and I don&amp;#39;t know if any of them actually adds a value to LLM workflows in a way that&amp;#39;s maintainable and robust. So far, I&amp;#39;ve been able to just build my own little libraries to use in some LLM applications (no RAG), but as I consider the more recent advancements in the field (guaranteed function calling, better RAG, agents and tool use, etc.), I wonder if using one of these frameworks would be a better approach compared to building everything on my own. I appreciate your thoughts and comments on this!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/nderstand2grow&quot;&gt; /u/nderstand2grow &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18anbjf/which_llm_frameworks_do_you_use_in_production_and/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18anbjf/which_llm_frameworks_do_you_use_in_production_and/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18anbjf</id><link href="https://www.reddit.com/r/LangChain/comments/18anbjf/which_llm_frameworks_do_you_use_in_production_and/" /><updated>2023-12-04T16:02:57+00:00</updated><published>2023-12-04T16:02:57+00:00</published><title>Which LLM framework(s) do you use in production and why?</title></entry><entry><author><name>/u/AgilePainting931</name><uri>https://www.reddit.com/user/AgilePainting931</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18aoewc/beacon_a_generative_ai_llmops_framework/&quot;&gt; &lt;img src=&quot;https://b.thumbs.redditmedia.com/KnhVw6iIV1V0y0-XCKePZUxzVUNRbxyJmi0-_DPTvaM.jpg&quot; alt=&quot;Beacon - A Generative AI LLMOps Framework&quot; title=&quot;Beacon - A Generative AI LLMOps Framework&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I am doing r&amp;amp;d in generative AI. I created a framework which has following functionality:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Data connectors - provides various data connectors like local documents (pdf, word), azure Blob storage, Azure MS SQL database, AWS S3 Bucket. System will take data from said structured and non-structured databases and insert into vector database. right now, we are using chromadb as our local vector database.&lt;/li&gt; &lt;li&gt;LLM - This section allows users to choose which llm provide they want to use. We give options like azure open ai, open ai and amazon bedrock. Users needs to choose one of these providers for LLM and embedding models.&lt;/li&gt; &lt;li&gt;Chat - user can start chatting in this section as system takes data from connectors, stores in vector db using embeddings and use LLM provider to answer user&amp;#39;s question.&lt;/li&gt; &lt;li&gt;Observability: We are using langsmith and showing observability charts and monitoring charts.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;I have attached screenshots of this tool.&lt;/p&gt; &lt;p&gt;Here is the link where you can sign up and start:&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;http://4.151.58.132/beacon&quot;&gt;http://4.151.58.132/beacon&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Please let me know your thoughts on how should i proceed further. What new things i can do here.&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/bhhbf75fpb4c1.png?width=1342&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=73924ba13cc0467f6c3b38809bf7423e57778278&quot;&gt;https://preview.redd.it/bhhbf75fpb4c1.png?width=1342&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=73924ba13cc0467f6c3b38809bf7423e57778278&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/7vqd295fpb4c1.png?width=1357&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=139a0bca58ffc2fbdfa11fa1c6c0d2a64abe8bd5&quot;&gt;https://preview.redd.it/7vqd295fpb4c1.png?width=1357&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=139a0bca58ffc2fbdfa11fa1c6c0d2a64abe8bd5&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/mf1tba5fpb4c1.png?width=1348&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6f43e03b19e7111b092b932143d599938d4c01a9&quot;&gt;https://preview.redd.it/mf1tba5fpb4c1.png?width=1348&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6f43e03b19e7111b092b932143d599938d4c01a9&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/lwz7nd5fpb4c1.png?width=1347&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e24fbd6b2383b59edad4bdc30bee6e149f2076f9&quot;&gt;https://preview.redd.it/lwz7nd5fpb4c1.png?width=1347&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e24fbd6b2383b59edad4bdc30bee6e149f2076f9&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/mii35b5fpb4c1.png?width=1356&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=da229817437a6481f0b4fda36cb6e8135dee9ac7&quot;&gt;https://preview.redd.it/mii35b5fpb4c1.png?width=1356&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=da229817437a6481f0b4fda36cb6e8135dee9ac7&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/AgilePainting931&quot;&gt; /u/AgilePainting931 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18aoewc/beacon_a_generative_ai_llmops_framework/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18aoewc/beacon_a_generative_ai_llmops_framework/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_18aoewc</id><media:thumbnail url="https://b.thumbs.redditmedia.com/KnhVw6iIV1V0y0-XCKePZUxzVUNRbxyJmi0-_DPTvaM.jpg" /><link href="https://www.reddit.com/r/LangChain/comments/18aoewc/beacon_a_generative_ai_llmops_framework/" /><updated>2023-12-04T16:52:31+00:00</updated><published>2023-12-04T16:52:31+00:00</published><title>Beacon - A Generative AI LLMOps Framework</title></entry><entry><author><name>/u/jim_andr</name><uri>https://www.reddit.com/user/jim_andr</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;seems that openAI document upload is better atm than many other solutions. Do we know what they use for embeddings?By web GPT-4 i mean openAI login to chatGPT.Locally, i mean call openAI API with gpt-4 as a model and same csv as RAG. &lt;/p&gt; &lt;p&gt;dataset is containing structured data from smartphone industry, brand model, ram, sttorage, price etc. I was able to ask questions like &amp;quot;cheapest model with 256gb of storage, etc)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/jim_andr&quot;&gt; /u/jim_andr &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18am1f9/i_tested_a_csv_upload_and_qa_to_web_gpt4_and/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18am1f9/i_tested_a_csv_upload_and_qa_to_web_gpt4_and/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18am1f9</id><link href="https://www.reddit.com/r/LangChain/comments/18am1f9/i_tested_a_csv_upload_and_qa_to_web_gpt4_and/" /><updated>2023-12-04T15:02:31+00:00</updated><published>2023-12-04T15:02:31+00:00</published><title>I tested a csv upload and Q&amp;A to web gpt-4 and worked like a charm. Tried to do the same locally with csv loader, chroma and langchain and results (Q&amp;A on the same dataset and GPT model - gpt4) were poor. I suspect i need to create better embeddings with chroma or any vector db. Any suggestions?</title></entry><entry><author><name>/u/devinbost</name><uri>https://www.reddit.com/user/devinbost</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;If you could have an execution framework for spinning up LangChain applications at runtime, how would you design it? One idea would be a YAML/config based approach that would use an orchestration layer to spin up Kubernetes pods. If we go that route, then the next question is determining what parts of langchain should be exposed vs what should be configurable via the config. Thoughts?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/devinbost&quot;&gt; /u/devinbost &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18alrin/execution_framework_for_langchain/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18alrin/execution_framework_for_langchain/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18alrin</id><link href="https://www.reddit.com/r/LangChain/comments/18alrin/execution_framework_for_langchain/" /><updated>2023-12-04T14:49:14+00:00</updated><published>2023-12-04T14:49:14+00:00</published><title>Execution framework for LangChain?</title></entry><entry><author><name>/u/IllustriousArt2202</name><uri>https://www.reddit.com/user/IllustriousArt2202</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;How do I Replace the existing Langchain REACT Agent in the L3AGI framework with the XAgent framework? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/IllustriousArt2202&quot;&gt; /u/IllustriousArt2202 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/189pkr1/integration_of_xagent_into_l3agi_framework/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/189pkr1/integration_of_xagent_into_l3agi_framework/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_189pkr1</id><link href="https://www.reddit.com/r/LangChain/comments/189pkr1/integration_of_xagent_into_l3agi_framework/" /><updated>2023-12-03T08:55:02+00:00</updated><published>2023-12-03T08:55:02+00:00</published><title>Integration of XAgent into L3AGI Framework</title></entry><entry><author><name>/u/Full_Sentence_3678</name><uri>https://www.reddit.com/user/Full_Sentence_3678</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/189gkp0/embodied_llms_for_robotics_code_in_comments/&quot;&gt; &lt;img src=&quot;https://external-preview.redd.it/NTRocnAxdWR6eTNjMXoQB95sJhOH8DH2fTHH41Ap3Y8STa1xy0MP2uEfLJ5e.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=287b0852c7d3c729420bbef12e3dea483cd73780&quot; alt=&quot;Embodied LLMs for robotics (code in comments)&quot; title=&quot;Embodied LLMs for robotics (code in comments)&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Full_Sentence_3678&quot;&gt; /u/Full_Sentence_3678 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://v.redd.it/jll5idt5zy3c1&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/189gkp0/embodied_llms_for_robotics_code_in_comments/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_189gkp0</id><media:thumbnail url="https://external-preview.redd.it/NTRocnAxdWR6eTNjMXoQB95sJhOH8DH2fTHH41Ap3Y8STa1xy0MP2uEfLJ5e.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=287b0852c7d3c729420bbef12e3dea483cd73780" /><link href="https://www.reddit.com/r/LangChain/comments/189gkp0/embodied_llms_for_robotics_code_in_comments/" /><updated>2023-12-02T23:55:10+00:00</updated><published>2023-12-02T23:55:10+00:00</published><title>Embodied LLMs for robotics (code in comments)</title></entry><entry><author><name>/u/BtownIU</name><uri>https://www.reddit.com/user/BtownIU</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi y&amp;#39;all,&lt;/p&gt; &lt;p&gt;Newbie to langchain here. Curious if there is a way to let a chain deal with different types of questions automatically. &lt;/p&gt; &lt;p&gt;Say when a prompt is something like &amp;quot;what&amp;#39;s your product XYZ?&amp;quot;, the user is usually ok with 15 seconds for a paragraph to be generated detailing the answer.&lt;/p&gt; &lt;p&gt;But a user may just want to get the links to self-help documents, like &amp;quot;give me the links to all your docs about XYZ&amp;quot;. And the speed is expected to be much faster.&lt;/p&gt; &lt;p&gt;Should I set up a chain that classifies a prompt into 2 different pipelines, where one pipeline would be something like a vector database search without RAG, the other one with RAG? Or are there tricks with LLM that can spew out structured answers much faster than unstructured textual answers? Thanks in advance&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/BtownIU&quot;&gt; /u/BtownIU &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/189ml0n/how_to_handle_prompts_asking_for_links_instead_of/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/189ml0n/how_to_handle_prompts_asking_for_links_instead_of/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_189ml0n</id><link href="https://www.reddit.com/r/LangChain/comments/189ml0n/how_to_handle_prompts_asking_for_links_instead_of/" /><updated>2023-12-03T05:29:46+00:00</updated><published>2023-12-03T05:29:46+00:00</published><title>How to handle prompts asking for links instead of textual answers?</title></entry><entry><author><name>/u/LeDebardeur</name><uri>https://www.reddit.com/user/LeDebardeur</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hello, I’m using sql agent, and I want to pass custom database tables infos ( column and tables infos ) Which format can I pass those and how ?&lt;/p&gt; &lt;p&gt;Thanks in advance&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/LeDebardeur&quot;&gt; /u/LeDebardeur &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/189k1lx/custom_sql_database_infos/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/189k1lx/custom_sql_database_infos/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_189k1lx</id><link href="https://www.reddit.com/r/LangChain/comments/189k1lx/custom_sql_database_infos/" /><updated>2023-12-03T03:05:55+00:00</updated><published>2023-12-03T03:05:55+00:00</published><title>Custom SQL database infos</title></entry><entry><author><name>/u/nebulum747</name><uri>https://www.reddit.com/user/nebulum747</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I was working on using gpt4all&amp;#39;s open AI-like API backend to see if I could start phasing out the actual openai API to some extent. &lt;/p&gt; &lt;p&gt;Essentially, I wanted to use Langchain&amp;#39;s ChatOpenAI(), but switch the OPENAI_BASE_URL, and put something random in for the key.&lt;/p&gt; &lt;p&gt;When I tried this with LLaMA models like mistral and snoozy, I get instant replies, but ones that aren&amp;#39;t useful. Something like this:&lt;/p&gt; &lt;p&gt;Me: hi!&lt;/p&gt; &lt;p&gt;AI: Echo: hi!&lt;/p&gt; &lt;p&gt;Me: how are you?&lt;/p&gt; &lt;p&gt;AI: Echo: how are you?&lt;/p&gt; &lt;p&gt;It just bounces the message I wrote back. Any tips on integrating in these models with OpenAI-like APIs? I&amp;#39;m trying to get it so that I can &amp;quot;plug and play&amp;quot; with models using the openai API standard (i&amp;#39;ll change the prompts, ofc)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/nebulum747&quot;&gt; /u/nebulum747 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1899yhr/does_langchain_support_openaiapi_compatible/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1899yhr/does_langchain_support_openaiapi_compatible/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1899yhr</id><link href="https://www.reddit.com/r/LangChain/comments/1899yhr/does_langchain_support_openaiapi_compatible/" /><updated>2023-12-02T18:34:31+00:00</updated><published>2023-12-02T18:34:31+00:00</published><title>Does Langchain support OpenAI-API compatible agents with it's ChatOpenAI, OpenAIFunctionsAgents, etc.?</title></entry><entry><author><name>/u/nebulum747</name><uri>https://www.reddit.com/user/nebulum747</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I was working on using gpt4all&amp;#39;s open AI-like API backend to see if I could start phasing out the actual openai API to some extent. &lt;/p&gt; &lt;p&gt;Essentially, I wanted to use Langchain&amp;#39;s ChatOpenAI(), but switch the OPENAI_BASE_URL, and put something random in for the key.&lt;/p&gt; &lt;p&gt;When I tried this with LLaMA models like mistral and snoozy, I get instant replies, but ones that aren&amp;#39;t useful. Something like this:&lt;/p&gt; &lt;p&gt;Me: hi!&lt;/p&gt; &lt;p&gt;AI: Echo: hi!&lt;/p&gt; &lt;p&gt;Me: how are you?&lt;/p&gt; &lt;p&gt;AI: Echo: how are you?&lt;/p&gt; &lt;p&gt;It just bounces the message I wrote back. Any tips on integrating in these models with OpenAI-like APIs? I&amp;#39;m trying to get it so that I can &amp;quot;plug and play&amp;quot; with models using the openai API standard (i&amp;#39;ll change the prompts, ofc)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/nebulum747&quot;&gt; /u/nebulum747 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1899yhe/does_langchain_support_openaiapi_compatible/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1899yhe/does_langchain_support_openaiapi_compatible/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1899yhe</id><link href="https://www.reddit.com/r/LangChain/comments/1899yhe/does_langchain_support_openaiapi_compatible/" /><updated>2023-12-02T18:34:30+00:00</updated><published>2023-12-02T18:34:30+00:00</published><title>Does Langchain support OpenAI-API compatible agents with it's ChatOpenAI, OpenAIFunctionsAgents, etc.?</title></entry></feed>