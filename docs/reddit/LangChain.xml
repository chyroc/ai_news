<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/"><category term="LangChain" label="r/LangChain"/><updated>2024-06-24T14:20:06+00:00</updated><icon>https://www.redditstatic.com/icon.png/</icon><id>/r/LangChain.rss</id><link rel="self" href="https://www.reddit.com/r/LangChain.rss" type="application/atom+xml" /><link rel="alternate" href="https://www.reddit.com/r/LangChain" type="text/html" /><subtitle>LangChain is an open-source framework and developer toolkit that helps developers get LLM applications from prototype to production. It is available for Python and Javascript at https://www.langchain.com/.</subtitle><title>LangChain</title><entry><author><name>/u/zchaarm</name><uri>https://www.reddit.com/user/zchaarm</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;A place for members of &lt;a href=&quot;/r/LangChain&quot;&gt;r/LangChain&lt;/a&gt; to chat with each other&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/zchaarm&quot;&gt; /u/zchaarm &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_10ljho9</id><link href="https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/" /><updated>2023-01-26T04:33:05+00:00</updated><published>2023-01-26T04:33:05+00:00</published><title>r/LangChain Lounge</title></entry><entry><author><name>/u/byrocuy</name><uri>https://www.reddit.com/user/byrocuy</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I’m working on a chatbot that interacts with an internal API, such as searching for items based on user queries. The output needs to be in a structured JSON format that I can pass to the front end. Here’s the desired structure:&lt;/p&gt; &lt;p&gt;&lt;code&gt;json { &amp;quot;type&amp;quot;: &amp;quot;message&amp;quot;, &amp;quot;message&amp;quot;: &amp;quot;Here is an item matching your search criteria&amp;quot;, &amp;quot;data&amp;quot;: [ { &amp;quot;item&amp;quot;: 1, &amp;quot;id&amp;quot;: &amp;quot;abc123&amp;quot; }, { &amp;quot;item&amp;quot;: 2, &amp;quot;id&amp;quot;: &amp;quot;def456&amp;quot; }, ... ] } &lt;/code&gt;&lt;/p&gt; &lt;p&gt;I’m struggling with extracting the assistant “message” and the item data (including the item ID) from the tools. I need to pass the data (e.g. item id) so then the front end can process it and display a custom view (card view) of items. My attempts to inject a Pydantic model and explicitly prompt to output a structured format haven’t been successful so far. &lt;/p&gt; &lt;p&gt;I am thinking to set up a specific node at the end of the graph to parse the assistant’s output, but I found that the assistant keeps discarding item details such as item id (even after I explicitly prompt not to do so). Is there a better approach to achieve this? Any advice or insights would be greatly appreciated!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/byrocuy&quot;&gt; /u/byrocuy &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dna58k/need_advice_in_structuring_json_output_in/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dna58k/need_advice_in_structuring_json_output_in/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dna58k</id><link href="https://www.reddit.com/r/LangChain/comments/1dna58k/need_advice_in_structuring_json_output_in/" /><updated>2024-06-24T10:21:26+00:00</updated><published>2024-06-24T10:21:26+00:00</published><title>Need advice in Structuring JSON Output in Langgraph for Chatbot</title></entry><entry><author><name>/u/sammopus</name><uri>https://www.reddit.com/user/sammopus</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I am using FastAPI with langchain I am using &lt;code&gt;aconvert_to_graph_documents&lt;/code&gt; which Asynchronously convert a sequence of documents into graph documents. &lt;/p&gt; &lt;p&gt;I am not able to use Langchain&amp;#39;s ChatAnthropic in async manner, I tried using semaphore but keep getting&lt;br/&gt; anthropic.RateLimitError saying Number of concurrent connections has exceeded your rate limit.&lt;/p&gt; &lt;p&gt;Is there no way set this to a reasonable number?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/sammopus&quot;&gt; /u/sammopus &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dn9ytb/number_of_concurrent_connections_has_exceeded/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dn9ytb/number_of_concurrent_connections_has_exceeded/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dn9ytb</id><link href="https://www.reddit.com/r/LangChain/comments/1dn9ytb/number_of_concurrent_connections_has_exceeded/" /><updated>2024-06-24T10:09:11+00:00</updated><published>2024-06-24T10:09:11+00:00</published><title>Number of concurrent connections has exceeded your rate limit with Anthropic with Langchain</title></entry><entry><author><name>/u/Notalabel_4566</name><uri>https://www.reddit.com/user/Notalabel_4566</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hello,&lt;/p&gt; &lt;p&gt;I have built an application at work which uses Angular, Django and SQL server. Currently all the user inputs are manual based meaning, either dropdown or text based inputs. Also, I provide recommendation in the project I want to integrate a LLM model so that I can automate the input module as well as recommendation one. Which one should i pick?&lt;br/&gt; Also, which LLM&amp;#39;s community is more than active and engages in Q&amp;amp;A&lt;/p&gt; &lt;p&gt;Upvote1Downvote0comments0 awardsShare&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Notalabel_4566&quot;&gt; /u/Notalabel_4566 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dnb9ia/please_help_me_pick_a_llm_model_for_my/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dnb9ia/please_help_me_pick_a_llm_model_for_my/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dnb9ia</id><link href="https://www.reddit.com/r/LangChain/comments/1dnb9ia/please_help_me_pick_a_llm_model_for_my/" /><updated>2024-06-24T11:32:18+00:00</updated><published>2024-06-24T11:32:18+00:00</published><title>Please help me pick a LLM model for my project(Angular+Dajngo+SQL Server)</title></entry><entry><author><name>/u/Rare_Confusion6373</name><uri>https://www.reddit.com/user/Rare_Confusion6373</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Aside from using better or larger models and employing fine-tuning, how else can we achieve improved responses from models?&lt;/p&gt; &lt;p&gt;p.s, I&amp;#39;m extremely new to this space, so any answers would be appreciated. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Rare_Confusion6373&quot;&gt; /u/Rare_Confusion6373 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dn7p4h/what_strategies_can_we_use_to_enhance_llm/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dn7p4h/what_strategies_can_we_use_to_enhance_llm/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dn7p4h</id><link href="https://www.reddit.com/r/LangChain/comments/1dn7p4h/what_strategies_can_we_use_to_enhance_llm/" /><updated>2024-06-24T07:24:59+00:00</updated><published>2024-06-24T07:24:59+00:00</published><title>What strategies can we use to enhance LLM responses besides fine-tuning and prompt engineering?</title></entry><entry><author><name>/u/suribe06</name><uri>https://www.reddit.com/user/suribe06</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;ve compiled a history of chats between a user and an AI assistant in JSON format. Here’s a snippet of how the chat data looks:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;&amp;quot;2024-06-23&amp;quot;: [ { &amp;quot;role&amp;quot;: &amp;quot;user&amp;quot;, &amp;quot;message&amp;quot;: &amp;quot;Hello! Can you suggest some tourist places to visit in Paris?&amp;quot;, &amp;quot;time&amp;quot;: &amp;quot;2024-06-23T15:30:00Z&amp;quot; }, { &amp;quot;role&amp;quot;: &amp;quot;assistant&amp;quot;, &amp;quot;message&amp;quot;: &amp;quot;Hello! Paris is a beautiful city with many wonderful places to visit. Here are some top tourist attractions: ...&amp;quot;, &amp;quot;time&amp;quot;: &amp;quot;2024-06-23T15:32:00Z&amp;quot; } ] &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;I’m planning to create a detailed knowledge graph, and I&amp;#39;m debating between two approaches for structuring the documents: creating a document for each interaction or aggregating all interactions for each date into a single document. Currently, I lean towards the former to capture more granularity in the analysis. Here’s how I’m setting up my documents using LangChain:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;from langchain.docstore.document import Document import json def load_conversations_from_json(file_path): docs = [] with open(file_path, &amp;#39;r&amp;#39;, encoding=&amp;#39;utf-8&amp;#39;) as file: chat_data = json.load(file) for date, messages in chat_data.items(): for message in messages: role = message[&amp;#39;role&amp;#39;] text = message[&amp;#39;message&amp;#39;] time = message[&amp;#39;time&amp;#39;] # Create a document with the conversation metadata = { &amp;quot;date&amp;quot;: date, &amp;quot;time&amp;quot;: time, &amp;quot;role&amp;quot;: role } docs.append(Document(page_content=text, metadata=metadata)) return docs conversations = load_conversations_from_json(&amp;#39;chat_history.json&amp;#39;) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;I’m considering using &lt;code&gt;LLMGraphTransformer&lt;/code&gt; from LangChain to convert these documents into graph documents. Do you recommend creating a specific prompt for this task? Here’s a simple prompt I’m considering:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;from langchain.prompts import ChatPromptTemplate kg_prompt = ChatPromptTemplate.from_template(&amp;quot;&amp;quot;&amp;quot; You are an AI that constructs knowledge graphs from chat histories. Your task is to identify key entities and the relationships between them based on the provided conversation. Please proceed with the extraction based on the conversation provided. &amp;quot;&amp;quot;&amp;quot;) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;How beneficial is it to use the &lt;code&gt;allowed_nodes&lt;/code&gt; and &lt;code&gt;allowed_relationships&lt;/code&gt; parameters in &lt;code&gt;LLMGraphTransformer&lt;/code&gt;? Which model would you recommend for the LLM? Currently, I’m using &lt;code&gt;gpt-4o&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;I’d appreciate any advice or insights on optimizing this process for creating a comprehensive knowledge graph. Thank you!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/suribe06&quot;&gt; /u/suribe06 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dn55j1/creating_a_knowledge_graph_from_chat_history/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dn55j1/creating_a_knowledge_graph_from_chat_history/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dn55j1</id><link href="https://www.reddit.com/r/LangChain/comments/1dn55j1/creating_a_knowledge_graph_from_chat_history/" /><updated>2024-06-24T04:35:05+00:00</updated><published>2024-06-24T04:35:05+00:00</published><title>Creating a Knowledge Graph from Chat History Using LangChain: Seeking Advice</title></entry><entry><author><name>/u/harshit_nariya</name><uri>https://www.reddit.com/user/harshit_nariya</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dn86fa/build_rag_in_10_lines_of_code_with_lyzr/&quot;&gt; &lt;img src=&quot;https://b.thumbs.redditmedia.com/Sh8ty2pedOlOCbFwCc1efB3PEGCwHPvQ1NQQbuCBnpI.jpg&quot; alt=&quot;Build RAG in 10 Lines of Code with Lyzr&quot; title=&quot;Build RAG in 10 Lines of Code with Lyzr&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/harshit_nariya&quot;&gt; /u/harshit_nariya &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;/r/AnyBodyCanAI/comments/1dn8338/build_rag_in_10_lines_of_code_with_lyzr/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dn86fa/build_rag_in_10_lines_of_code_with_lyzr/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1dn86fa</id><media:thumbnail url="https://b.thumbs.redditmedia.com/Sh8ty2pedOlOCbFwCc1efB3PEGCwHPvQ1NQQbuCBnpI.jpg" /><link href="https://www.reddit.com/r/LangChain/comments/1dn86fa/build_rag_in_10_lines_of_code_with_lyzr/" /><updated>2024-06-24T07:59:52+00:00</updated><published>2024-06-24T07:59:52+00:00</published><title>Build RAG in 10 Lines of Code with Lyzr</title></entry><entry><author><name>/u/Strange-Ant-4194</name><uri>https://www.reddit.com/user/Strange-Ant-4194</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Here is the function I&amp;#39;m using to execute tool calls:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;def single_tool_parser(model_output): tool_map = {tool.name: tool for tool in tools} chosen_tool = tool_map[model_output[&amp;quot;name&amp;quot;]] return itemgetter(&amp;quot;arguments&amp;quot;) | chosen_tool &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Method 1:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The above code works if i do something like this:&lt;/p&gt; &lt;p&gt;example_chain = prompt | llm | JsonOutputParser() | single_tool_parser&lt;/p&gt; &lt;p&gt;res=example_chain.invoke(..........)&lt;/p&gt; &lt;p&gt;print(res) //output of tool&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Method 2&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;However if i try to do this:&lt;/p&gt; &lt;p&gt;example_chain = prompt | llm | JsonOutputParser()&lt;/p&gt; &lt;p&gt;res=example_chain.invoke(..........)&lt;/p&gt; &lt;p&gt;res2=single _tool_parser(res)&lt;/p&gt; &lt;p&gt;Upon doing print(res2), here&amp;#39;s what i get:&lt;/p&gt; &lt;p&gt;first=RunnableLambda(itemgetter(&amp;#39;arguments&amp;#39;)) last=StructuredTool(name=&amp;#39;mute&amp;#39;, description=&amp;#39;Mute an incoming call\n : param caller: The name of the caller to be muted&amp;#39;, args_schema=&amp;lt;class &amp;#39;pydantic.v1.main.muteSchema&amp;#39;&amp;gt;, func=&amp;lt;function mute at 0x7f5b1403b380&amp;gt;)&lt;/p&gt; &lt;p&gt;How can i get method 2 to work?&lt;/p&gt; &lt;p&gt;Note: The tool I&amp;#39;m using is called &amp;#39;mute&amp;#39;. I&amp;#39;ve also tried doing res2=res | single_tool_parser, but i get an error saying: unsupported operand type(s) for |: &amp;#39;dict&amp;#39; and &amp;#39;function&amp;#39;.&lt;/p&gt; &lt;p&gt;Im using Llama3 with Ollama&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Strange-Ant-4194&quot;&gt; /u/Strange-Ant-4194 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dn6yaw/how_do_i_parse_tool_calls/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dn6yaw/how_do_i_parse_tool_calls/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dn6yaw</id><link href="https://www.reddit.com/r/LangChain/comments/1dn6yaw/how_do_i_parse_tool_calls/" /><updated>2024-06-24T06:32:17+00:00</updated><published>2024-06-24T06:32:17+00:00</published><title>How do i parse tool calls?</title></entry><entry><author><name>/u/derelict5432</name><uri>https://www.reddit.com/user/derelict5432</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Just started using RAG with LangChain the last couple of weeks for a project at work.&lt;/p&gt; &lt;p&gt;First pass, I used this tutorial: &lt;a href=&quot;https://python.langchain.com/v0.2/docs/tutorials/rag/&quot;&gt;https://python.langchain.com/v0.2/docs/tutorials/rag/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Instead of a webloader, I used a textloader to load a small text file, a help file for a custom software framework.&lt;/p&gt; &lt;p&gt;I ran it, queried the model, and it worked great. I was excited.&lt;/p&gt; &lt;p&gt;The full amount of data I want to reference is about 18K small text documents, about 179MB. I decided to work up to that, and just used about 10MB in about 1000 text documents. Query results were much worse.&lt;/p&gt; &lt;p&gt;In one specific case, I asked about a scenario description that was stored in a file called ea.txt. For troubleshooting, I increased the number of docs to be retrieved to 5 and added logging to show which docs were being retrieved.&lt;/p&gt; &lt;p&gt;The answer was wrong, and ed.txt was referenced three times, along with two other irrelevant docs. In the directory to be loaded, ed.txt directly follows ea.txt. How is RAG determining which docs to retrieve? The scenario I was asking about started with &amp;#39;ea&amp;#39; (e.g. &amp;#39;scenario ea4003&amp;#39;). Why would it pass over the file with the correct information, which contains strings that are much more similar to what I&amp;#39;m asking about? &lt;/p&gt; &lt;p&gt;And does anyone have any advice on how to improve performance? Thanks.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/derelict5432&quot;&gt; /u/derelict5432 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dmo3am/how_to_improve_rag_performance/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dmo3am/how_to_improve_rag_performance/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dmo3am</id><link href="https://www.reddit.com/r/LangChain/comments/1dmo3am/how_to_improve_rag_performance/" /><updated>2024-06-23T15:01:38+00:00</updated><published>2024-06-23T15:01:38+00:00</published><title>How to Improve RAG Performance</title></entry><entry><author><name>/u/link2ani</name><uri>https://www.reddit.com/user/link2ani</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Looking for a cost effective approach that doesn’t suck&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/link2ani&quot;&gt; /u/link2ani &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dmuxu9/recommendation_for_reranker_model_on_retrieved/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dmuxu9/recommendation_for_reranker_model_on_retrieved/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dmuxu9</id><link href="https://www.reddit.com/r/LangChain/comments/1dmuxu9/recommendation_for_reranker_model_on_retrieved/" /><updated>2024-06-23T20:06:10+00:00</updated><published>2024-06-23T20:06:10+00:00</published><title>Recommendation for re-ranker model on retrieved results?</title></entry><entry><author><name>/u/yaeha83</name><uri>https://www.reddit.com/user/yaeha83</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Although the plenty of integrations make our life easier if committed to langchain, it is disproportionally more difficult to optimize and configure their paramaters and especially the **kwargs. I understand that these usually refer to the underlying package that the langchain class wraps around.&lt;/p&gt; &lt;p&gt;All docs have vanilla params and when some further configuration is needed or even explored then things become a bit of a pain.&lt;/p&gt; &lt;p&gt;The only way so far to find what **kwargs are available for the integrations I use is to go deep in the langchain code to see whether these might be finally passed, read the wrapped package documentation and also do extensive google search as it is not just the kwargs but also the syntax to pass them on (eg dict).&lt;/p&gt; &lt;p&gt;I guess this is not a big deal for a seasoned developer, but is there an easier way to do this, especially in the LLM era?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/yaeha83&quot;&gt; /u/yaeha83 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dmtf4a/optimize_and_configure_integration_classes_wit/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dmtf4a/optimize_and_configure_integration_classes_wit/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dmtf4a</id><link href="https://www.reddit.com/r/LangChain/comments/1dmtf4a/optimize_and_configure_integration_classes_wit/" /><updated>2024-06-23T18:59:08+00:00</updated><published>2024-06-23T18:59:08+00:00</published><title>Optimize and configure integration classes wit **kwargs</title></entry><entry><author><name>/u/cwooters</name><uri>https://www.reddit.com/user/cwooters</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;m looking for some high-level advice around &lt;strong&gt;LangGraph&lt;/strong&gt; and was hoping that this community might have some creative ideas.&lt;/p&gt; &lt;p&gt;I&amp;#39;ve been playing with LangGraph and I love how it lets you control the flow of a conversation. But &lt;strong&gt;I&amp;#39;m struggling with how to design a graph in light of parallel tool calling&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;Background: So, let&amp;#39;s say I have a state that represents the first &amp;quot;stage&amp;quot; of a dialogue with an Agent. Once that stage is complete, I want the dialogue to move to the second stage. (Each stage is represented as a state.) I have a tool called &amp;quot;CompleteOrEscalate&amp;quot; (based on the LangGraph tutorials) that the LLM can use when it thinks that the task for stage 1 is complete. I also have a second tool called &amp;quot;ToFAQ&amp;quot; which can be used if the user asks a question that is not directly related to stage 1&amp;#39;s task. So, stage 1 can conditionally transition to two other stages/states, depending on what the user says. This works great, &lt;em&gt;most of the time&lt;/em&gt;.&lt;/p&gt; &lt;p&gt;But an issue arises when a user says something that causes the LLM to invoke more than one tool (i.e. the LLM is suggesting that we make more than one transition out of a state). For example, if the purpose of stage 1 is to confirm the user&amp;#39;s name, and the user says, &amp;quot;Yes, I&amp;#39;m John Smith. And I have a question about ...&amp;quot; That input both completes the task (confirming the user&amp;#39;s name) AND contains a question (requiring an FAQ response). So, with parallel tool calling enabled the LLM returns both tool calls (CompleteOrEscalate and ToFAQ). This is actually pretty cool, but I&amp;#39;m not sure how to handle this situation in the conditional transition?&lt;/p&gt; &lt;p&gt;I&amp;#39;ve considered turning off parallel tool calling. This would force the LLM to call only one tool at a time. But it seems like a waste of tokens/time not to allow the LLM to return &lt;em&gt;both/all&lt;/em&gt; tool calls.&lt;/p&gt; &lt;p&gt;Am I thinking about this all wrong? Is there a better way to handle this situation? TIA for any suggestions or thoughts you may have.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/cwooters&quot;&gt; /u/cwooters &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dmtcyn/looking_for_ideas_how_to_handle_parallel_tool/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dmtcyn/looking_for_ideas_how_to_handle_parallel_tool/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dmtcyn</id><link href="https://www.reddit.com/r/LangChain/comments/1dmtcyn/looking_for_ideas_how_to_handle_parallel_tool/" /><updated>2024-06-23T18:56:24+00:00</updated><published>2024-06-23T18:56:24+00:00</published><title>Looking for ideas: How to handle parallel tool calls in LangGraph?</title></entry><entry><author><name>/u/fra_bia91</name><uri>https://www.reddit.com/user/fra_bia91</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;m looking to implement a way for the users of my platform to upload CSV files and pass them to various LMs to analyze. I get how the process works with other files types, and I&amp;#39;ve already set up a RAG pipeline for pdf files. &lt;/p&gt; &lt;p&gt;However, with PDF files I can &amp;quot;simply&amp;quot; split it into chunks and generate embeddings with those (and later retrieve the most relevant ones), with CSV, since it&amp;#39;s mostly data that could relate to each other, I&amp;#39;m not sure how to proceed.&lt;/p&gt; &lt;p&gt;For example, which criteria should I use to split the document into chunks? And what about the retrieval? Are embeddings relevant for CSV files?&lt;/p&gt; &lt;p&gt;The main use case to RAG in this case -as compared to simply including the whole CSV as text in the prompt- is to save tokens, but is it possible to get decent results with RAG?&lt;/p&gt; &lt;p&gt;Thanks in advance&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/fra_bia91&quot;&gt; /u/fra_bia91 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dmj7p7/im_not_sure_i_understand_how_to_perform_rag_on/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dmj7p7/im_not_sure_i_understand_how_to_perform_rag_on/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dmj7p7</id><link href="https://www.reddit.com/r/LangChain/comments/1dmj7p7/im_not_sure_i_understand_how_to_perform_rag_on/" /><updated>2024-06-23T10:29:13+00:00</updated><published>2024-06-23T10:29:13+00:00</published><title>I'm not sure I understand how to perform RAG on CSV files...</title></entry><entry><author><name>/u/sarthakai</name><uri>https://www.reddit.com/user/sarthakai</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Knowledge graphs can improve your RAG accuracy if your documents contain interconnected concepts.&lt;/p&gt; &lt;p&gt;And you can create+search on KGs for your existing documents automatically by using the latest version of the knowledge-graph-rag library.&lt;/p&gt; &lt;p&gt;All in just 3 lines of code.&lt;/p&gt; &lt;p&gt;In this example, I use medical documents. Here&amp;#39;s how the library works:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Extract entities from the corpus (such as organs, diseases, therapies, etc)&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Extract the relationships between them (such as mitigation effect of therapies, accumulation of plaques, etc.)&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Create a knowledge graph from these representations using GPT 3.5 / Haiku&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;When a user sends a query, break it down into entities to be searched.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Search the KG and use the results in the context of the LLM call.&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Here’s the repo: &lt;a href=&quot;https://github.com/sarthakrastogi/graph-rag&quot;&gt;https://github.com/sarthakrastogi/graph-rag&lt;/a&gt;&lt;/p&gt; &lt;p&gt;If you&amp;#39;d like to contribute or have suggestions for features, please raise them on Github.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/sarthakai&quot;&gt; /u/sarthakai &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dmm13w/building_a_python_library_to_quickly_createsearch/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dmm13w/building_a_python_library_to_quickly_createsearch/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dmm13w</id><link href="https://www.reddit.com/r/LangChain/comments/1dmm13w/building_a_python_library_to_quickly_createsearch/" /><updated>2024-06-23T13:21:53+00:00</updated><published>2024-06-23T13:21:53+00:00</published><title>Building a Python library to quickly create+search knowledge graphs for RAG -- want to contribute?</title></entry><entry><author><name>/u/Active-Fuel-49</name><uri>https://www.reddit.com/user/Active-Fuel-49</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dmksbw/bridging_the_last_mile_in_langchain_application/&quot;&gt; &lt;img src=&quot;https://external-preview.redd.it/881AdqFUz0mznhLi2O_40GQW0p3T5Dv647jn0wc7VIU.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a4cc543a878eda2859d2cd4b88a41afb36d515b3&quot; alt=&quot;Bridging the Last Mile in LangChain Application Development&quot; title=&quot;Bridging the Last Mile in LangChain Application Development&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Active-Fuel-49&quot;&gt; /u/Active-Fuel-49 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://community.aws/content/2gYKTV25GGIqAzgRyAdYbYTtCTf/bridging-the-last-mile-in-langchain-application-development&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dmksbw/bridging_the_last_mile_in_langchain_application/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1dmksbw</id><media:thumbnail url="https://external-preview.redd.it/881AdqFUz0mznhLi2O_40GQW0p3T5Dv647jn0wc7VIU.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=a4cc543a878eda2859d2cd4b88a41afb36d515b3" /><link href="https://www.reddit.com/r/LangChain/comments/1dmksbw/bridging_the_last_mile_in_langchain_application/" /><updated>2024-06-23T12:13:19+00:00</updated><published>2024-06-23T12:13:19+00:00</published><title>Bridging the Last Mile in LangChain Application Development</title></entry><entry><author><name>/u/sarthakai</name><uri>https://www.reddit.com/user/sarthakai</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;So first, here&amp;#39;s what I understand of how they did it:&lt;/p&gt; &lt;p&gt;They made the KG by parsing customer support tickets into structured tree representations, preserving their internal relationships.&lt;/p&gt; &lt;p&gt;Tickets are linked based on contextual similarities, dependencies, and references — all of these make up a comprehensive graph.&lt;/p&gt; &lt;p&gt;Each node in the KG is embedded so they can do semantic search and retrieval.&lt;/p&gt; &lt;p&gt;The RAG QA system identifies relevant sub-graphs by doing traversal and searching by semantic similarity.&lt;/p&gt; &lt;p&gt;Then, it generates contextually aware answers from the KG, evaluating by MRR, which saw a significant improvement.&lt;/p&gt; &lt;p&gt;Paper: &lt;a href=&quot;https://arxiv.org/pdf/2404.17723&quot;&gt;https://arxiv.org/pdf/2404.17723&lt;/a&gt;&lt;/p&gt; &lt;p&gt;If you’d like to implement Graph RAG too, I’m creating a Python library which automatically creates this graph for the documents in your vectordb. It also makes it easy for you to retrieve relevant documents connected to the best matches.&lt;/p&gt; &lt;p&gt;If you&amp;#39;re interested in contributing or have suggestions please raise them on Github.&lt;/p&gt; &lt;p&gt;Here’s the repo for the library: &lt;a href=&quot;https://github.com/sarthakrastogi/graph-rag/tree/main&quot;&gt;https://github.com/sarthakrastogi/graph-rag/tree/main&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/sarthakai&quot;&gt; /u/sarthakai &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dlwc39/linkedin_used_graph_rag_to_cut_down_their_ticket/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dlwc39/linkedin_used_graph_rag_to_cut_down_their_ticket/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dlwc39</id><link href="https://www.reddit.com/r/LangChain/comments/1dlwc39/linkedin_used_graph_rag_to_cut_down_their_ticket/" /><updated>2024-06-22T14:00:46+00:00</updated><published>2024-06-22T14:00:46+00:00</published><title>LinkedIn used Graph RAG to cut down their ticket resolution time from 40 hrs to 15 hrs. Let's make a library to make it accessible to everyone?</title></entry><entry><author><name>/u/PurpleWho</name><uri>https://www.reddit.com/user/PurpleWho</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;m new to LangChain and slowly working my way through the docs. My intention is to build a chat interface that has a conversation with a user and then slowly fills out a form behind the scenes as answers come in. &lt;/p&gt; &lt;p&gt;Filling out the form directly is a lot of information upfront for the user whereas a chat interface lets me break the questions down into smaller chunks.&lt;/p&gt; &lt;p&gt;I&amp;#39;m trying to understand how I would use LangChain to do this. There are a lot of different moving parts to the framework and I was wondering if someone could point me in the right direction. That is, which modules I need to cover first or a relevant example of something similar.&lt;/p&gt; &lt;p&gt;Specific questions I have include:&lt;br/&gt; - How to keep costs down when polling the transcript to fill out the form. If i do this on every message submission it might get unnecessarily expensive. But if I do it too infrequently then the bot might end up asking questions it already has the answer to. Was hoping the framework had some best practices I could rely on in this regard.&lt;br/&gt; - How to redirect the redirect the bot&amp;#39;s focus based on which questions still need answering.&lt;br/&gt; - How to implement a system to determine if an answer is good enough or if I need to ask more follow up questions to get a more substantial answer.&lt;br/&gt; - How to detect when all the questions have been filled out so I can end the chat.&lt;/p&gt; &lt;p&gt;I&amp;#39;ll slowly figuring it out but any pointers would be much appreciated. &lt;/p&gt; &lt;p&gt;Also, are there other LangChain specific forums where I can ask these types of questions that anyone recommends?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/PurpleWho&quot;&gt; /u/PurpleWho &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dmbnet/using_a_chat_interface_to_help_people_fill_out_a/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dmbnet/using_a_chat_interface_to_help_people_fill_out_a/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dmbnet</id><link href="https://www.reddit.com/r/LangChain/comments/1dmbnet/using_a_chat_interface_to_help_people_fill_out_a/" /><updated>2024-06-23T02:10:59+00:00</updated><published>2024-06-23T02:10:59+00:00</published><title>Using a chat interface to help people fill out a form</title></entry><entry><author><name>/u/tuantruong84</name><uri>https://www.reddit.com/user/tuantruong84</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;As much as i like LangChain, there is some actual good points from this article &lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://www.octomind.dev/blog/why-we-no-longer-use-langchain-for-building-our-ai-agents&quot;&gt;https://www.octomind.dev/blog/why-we-no-longer-use-langchain-for-building-our-ai-agents&lt;/a&gt;&lt;/p&gt; &lt;p&gt;What you guys think ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/tuantruong84&quot;&gt; /u/tuantruong84 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dlu5t9/an_article_on_why_moving_away_from_langchain/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dlu5t9/an_article_on_why_moving_away_from_langchain/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dlu5t9</id><link href="https://www.reddit.com/r/LangChain/comments/1dlu5t9/an_article_on_why_moving_away_from_langchain/" /><updated>2024-06-22T12:03:37+00:00</updated><published>2024-06-22T12:03:37+00:00</published><title>An article on why moving away from langchain</title></entry><entry><author><name>/u/Minimum-You-9018</name><uri>https://www.reddit.com/user/Minimum-You-9018</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I just find that chat history in final request look like that - &lt;code&gt;&amp;lt;CHAT HISTORY&amp;gt; [HumanMessage(content=&amp;#39;Message one&amp;#39;), AIMessage(content=&amp;#39;Hey&amp;#39;), HumanMessage(content=&amp;#39;Message two&amp;#39;)] &amp;lt;/CHAT HISTORY&amp;gt;&lt;/code&gt;&lt;/p&gt; &lt;p&gt;i a bit confused, it how it should looks, is it correct? As i remember i was something like this before: &lt;code&gt;AI:message \n USER:Text Message&lt;/code&gt; &lt;/p&gt; &lt;p&gt;Can anyone clarify this for me?&lt;/p&gt; &lt;p&gt;P.S.: Does anyone have information on how models like OpenAI, Anthropic, or Gemini are trained to understand conversation history?&lt;/p&gt; &lt;p&gt;My research gives me this ideas:&lt;/p&gt; &lt;p&gt;&lt;code&gt; { &amp;quot;chat_history&amp;quot;: [ {&amp;quot;role&amp;quot;: &amp;quot;user&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;Message one&amp;quot;}, {&amp;quot;role&amp;quot;: &amp;quot;ai&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;Hey&amp;quot;}, {&amp;quot;role&amp;quot;: &amp;quot;user&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;Message two&amp;quot;} ] } &lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt; USER: Message one AI: Hey USER: Message two &lt;/code&gt;&lt;/p&gt; &lt;p&gt;``` &amp;lt;conversation&amp;gt; &amp;lt;user&amp;gt;Message one&amp;lt;/user&amp;gt; &amp;lt;ai&amp;gt;Hey&amp;lt;/ai&amp;gt; &amp;lt;user&amp;gt;Message two&amp;lt;/user&amp;gt; &amp;lt;/conversation&amp;gt;&lt;/p&gt; &lt;p&gt;```&lt;/p&gt; &lt;p&gt;Langchain use its own history structure for a reason?&lt;/p&gt; &lt;p&gt;This is how im execute it: &lt;code&gt; response = await self.llm_chain.ainvoke( {&amp;quot;input&amp;quot;: self.llm_input.content}, config={ &amp;quot;configurable&amp;quot;: { &amp;quot;user_id&amp;quot;: self.user_id, &amp;quot;session_type&amp;quot;: self.session_type, }, &amp;quot;callbacks&amp;quot;: [self.langfuse_handler], }, ) &lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;langchain = &amp;quot;&lt;sup&gt;0.2.5&amp;quot;&lt;/sup&gt; langchain-core = &amp;quot;&lt;sup&gt;0.2.9&amp;quot;&lt;/sup&gt; langchain-community = &amp;quot;&lt;sup&gt;0.2.5&amp;quot;&lt;/sup&gt;&lt;/em&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Minimum-You-9018&quot;&gt; /u/Minimum-You-9018 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dmeyfj/langchain_chat_history_data_structure_in_final/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dmeyfj/langchain_chat_history_data_structure_in_final/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dmeyfj</id><link href="https://www.reddit.com/r/LangChain/comments/1dmeyfj/langchain_chat_history_data_structure_in_final/" /><updated>2024-06-23T05:27:39+00:00</updated><published>2024-06-23T05:27:39+00:00</published><title>Langchain chat history data structure in final prompt</title></entry><entry><author><name>/u/CodingButStillAlive</name><uri>https://www.reddit.com/user/CodingButStillAlive</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I know that streamlit was popular, but neither optimized for chatbot interactivity, nor ready to set up for production.&lt;/p&gt; &lt;p&gt;I assume some TypeScript + REACT is state of the art, but I am a Data Scientist and no frontend developer.&lt;/p&gt; &lt;p&gt;Are there any new libraries that nicely integrate with LangGraph and also FastAPI?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/CodingButStillAlive&quot;&gt; /u/CodingButStillAlive &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dlrouj/what_is_the_best_python_library_for_chatbot_uis/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dlrouj/what_is_the_best_python_library_for_chatbot_uis/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dlrouj</id><link href="https://www.reddit.com/r/LangChain/comments/1dlrouj/what_is_the_best_python_library_for_chatbot_uis/" /><updated>2024-06-22T09:18:19+00:00</updated><published>2024-06-22T09:18:19+00:00</published><title>What is the best python library for chatbot UIs?</title></entry><entry><author><name>/u/AnEdgeLordWeeb</name><uri>https://www.reddit.com/user/AnEdgeLordWeeb</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey guys!&lt;br/&gt; Has anyone tried and managed to find a successful solution, as to how I can messages in LangGraph through the usage of FastAPI and React?&lt;br/&gt; I have multiple nodes in my LangGraph app, and each one is appending a message to the &amp;quot;messages&amp;quot; list attribute. I want these messages&amp;#39; content to be streamed as a single message in my React app, until I reach the END node.&lt;br/&gt; Does anyone have any idea as to how to do that?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/AnEdgeLordWeeb&quot;&gt; /u/AnEdgeLordWeeb &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dltljb/how_to_stream_messages_with_fastapi_and_react/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dltljb/how_to_stream_messages_with_fastapi_and_react/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dltljb</id><link href="https://www.reddit.com/r/LangChain/comments/1dltljb/how_to_stream_messages_with_fastapi_and_react/" /><updated>2024-06-22T11:29:52+00:00</updated><published>2024-06-22T11:29:52+00:00</published><title>How to stream messages with FastAPI and React? - LangGraph</title></entry><entry><author><name>/u/diptanuc</name><uri>https://www.reddit.com/user/diptanuc</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi folks, I often see questions about which open source pdf model or APIs are best for extraction from PDF. We attempt to help people make data-driven decisions by comparing the various models on their private documents.&lt;/p&gt; &lt;p&gt;We benchmarked several PDF models - Marker, EasyOCR, Unstructured and OCRMyPDF.&lt;/p&gt; &lt;p&gt;Marker is better than the others in terms of accuracy. EasyOCR comes second, and OCRMyPDF is pretty close.&lt;/p&gt; &lt;p&gt;You can run these benchmarks on your documents using our code - &lt;a href=&quot;https://github.com/tensorlakeai/indexify-extractors/tree/main/pdf/benchmark&quot;&gt;https://github.com/tensorlakeai/indexify-extractors/tree/main/pdf/benchmark&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The benchmark tool is using Indexify behind the scenes - &lt;a href=&quot;https://github.com/tensorlakeai/indexify&quot;&gt;https://github.com/tensorlakeai/indexify&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Indexify is a scalable unstructured data extraction engine for building multi-stage inference pipelines. The pipelines can handle extraction from 1000s of documents in parallel when deployed in a real cluster on the cloud.&lt;/p&gt; &lt;p&gt;I would love your feedback on what models and document layouts to benchmark next. &lt;/p&gt; &lt;p&gt;For some reason Reddit is marking this post as spam when I add pictures, so here is a link to the docs with some charts - &lt;a href=&quot;https://docs.getindexify.ai/usecases/pdf_extraction/#extractor-performance-analysis&quot;&gt;https://docs.getindexify.ai/usecases/pdf_extraction/#extractor-performance-analysis&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/diptanuc&quot;&gt; /u/diptanuc &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dlfth6/benchmarking_pdf_models_for_parsing_accuracy/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dlfth6/benchmarking_pdf_models_for_parsing_accuracy/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dlfth6</id><link href="https://www.reddit.com/r/LangChain/comments/1dlfth6/benchmarking_pdf_models_for_parsing_accuracy/" /><updated>2024-06-21T21:57:01+00:00</updated><published>2024-06-21T21:57:01+00:00</published><title>Benchmarking PDF models for parsing accuracy</title></entry><entry><author><name>/u/Informal-Victory8655</name><uri>https://www.reddit.com/user/Informal-Victory8655</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Informal-Victory8655&quot;&gt; /u/Informal-Victory8655 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dlsxte/how_to_use_rabbitmq_or_any_other_broker_with/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dlsxte/how_to_use_rabbitmq_or_any_other_broker_with/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dlsxte</id><link href="https://www.reddit.com/r/LangChain/comments/1dlsxte/how_to_use_rabbitmq_or_any_other_broker_with/" /><updated>2024-06-22T10:47:26+00:00</updated><published>2024-06-22T10:47:26+00:00</published><title>How to Use RabbitMQ or any other Broker with LangChain FastApi chatbot</title></entry><entry><author><name>/u/sarthakai</name><uri>https://www.reddit.com/user/sarthakai</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Suppose in your LLM you have the original weight matrix W of dimensions d x k.&lt;/p&gt; &lt;p&gt;Your traditional training process would update W directly -- that’s a huge number of parameters if d x k is large, needing a lot of compute.&lt;/p&gt; &lt;p&gt;So, we use Low-Rank Decomposition to break it down before weight update. Here’s how —We represent the weight update (Delta W) as a product of two lower-rank matrices A and B, such that Delta W = BA.&lt;/p&gt; &lt;p&gt;Here, A is a matrix of dimensions r x k and B is a matrix of dimensions d x r. And here, r (rank) is much smaller than both d and k.&lt;/p&gt; &lt;p&gt;Now, Matrix A is initialised with some random Gaussian values and matrix B is initialised with zeros.&lt;/p&gt; &lt;p&gt;Why? So that initially Delta W = BA can be 0.&lt;/p&gt; &lt;p&gt;Now comes the training process:&lt;/p&gt; &lt;p&gt;During weight update, only the smaller matrices A and B are updated — this reduces the number of parameters to be tuned by a huge margin.&lt;/p&gt; &lt;p&gt;The effective update to the original weight matrix W is Delta W = BA, which approximates the changes in W using fewer parameters.&lt;/p&gt; &lt;p&gt;Let’s compare the params to be updated before and after LoRA:&lt;/p&gt; &lt;p&gt;Earlier, the params to be updated were d x k (remember the dimensions of W).&lt;/p&gt; &lt;p&gt;But now, the no. of params is reduced to (d x r) + (r x k). This is much smaller because the rank r was taken to be much smaller than both d and k.&lt;/p&gt; &lt;p&gt;This is how low-rank approximation gives you efficient fine-tuning with this compact representation.&lt;/p&gt; &lt;p&gt;Training is faster and needs less compute and memory, while still capturing essential information from your fine-tuning dataset.&lt;/p&gt; &lt;p&gt;I also made a quick animation using Artifacts to explain (took like 10 secs):&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://www.linkedin.com/posts/sarthakrastogi_simply-explaining-how-lora-actually-works-activity-7209893533011333120-RSsz&quot;&gt;https://www.linkedin.com/posts/sarthakrastogi_simply-explaining-how-lora-actually-works-activity-7209893533011333120-RSsz&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/sarthakai&quot;&gt; /u/sarthakai &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dl53nn/simply_explaining_how_lora_actually_works_eli5/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dl53nn/simply_explaining_how_lora_actually_works_eli5/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dl53nn</id><link href="https://www.reddit.com/r/LangChain/comments/1dl53nn/simply_explaining_how_lora_actually_works_eli5/" /><updated>2024-06-21T14:17:39+00:00</updated><published>2024-06-21T14:17:39+00:00</published><title>Simply explaining how LoRA actually works (ELI5)</title></entry><entry><author><name>/u/goddamnit_1</name><uri>https://www.reddit.com/user/goddamnit_1</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dlaqn7/i_built_an_sql_agent_with_langchain_heres_my/&quot;&gt; &lt;img src=&quot;https://b.thumbs.redditmedia.com/yj28iUliBBMoZ3SIz1i2HUNtYl_ZhzBiXOQsjg5cL0c.jpg&quot; alt=&quot;I built an SQL Agent with Langchain - Here's my experience&quot; title=&quot;I built an SQL Agent with Langchain - Here's my experience&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;My agent writes queries to retrieve data from Sqlite Databases. This was my first time writing an agent with a good and serious usecase. The first framework i used for this was Langchain. &lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Very easy to implement: Its pretty convenient to import LLMs Gpt, Claude, Gemini. The documentation for it also clear.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Tools: This is my favourite part about the framework, writing tools and importing them is very easy and it helps in building for a lot of usecases.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Documentation can be improved since there are multiple versions and each time i click to the stable version, it goes back to the homepage.&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;a href=&quot;https://i.redd.it/1b8v8xv4vy7d1.gif&quot;&gt;https://i.redd.it/1b8v8xv4vy7d1.gif&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Here&amp;#39;s the &lt;a href=&quot;https://github.com/ComposioHQ/composio/tree/master/python/examples/sql_agent&quot;&gt;GITHUB LINK&lt;/a&gt; if you want to try it out.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/goddamnit_1&quot;&gt; /u/goddamnit_1 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dlaqn7/i_built_an_sql_agent_with_langchain_heres_my/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dlaqn7/i_built_an_sql_agent_with_langchain_heres_my/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1dlaqn7</id><media:thumbnail url="https://b.thumbs.redditmedia.com/yj28iUliBBMoZ3SIz1i2HUNtYl_ZhzBiXOQsjg5cL0c.jpg" /><link href="https://www.reddit.com/r/LangChain/comments/1dlaqn7/i_built_an_sql_agent_with_langchain_heres_my/" /><updated>2024-06-21T18:17:38+00:00</updated><published>2024-06-21T18:17:38+00:00</published><title>I built an SQL Agent with Langchain - Here's my experience</title></entry><entry><author><name>/u/mmkostov</name><uri>https://www.reddit.com/user/mmkostov</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I need to do RAG and web browsing. What other libraries can I use (except LangChain) that can achieve this functionality?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/mmkostov&quot;&gt; /u/mmkostov &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dlolna/langchain_alternatives_for_a_nextjs_project/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dlolna/langchain_alternatives_for_a_nextjs_project/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dlolna</id><link href="https://www.reddit.com/r/LangChain/comments/1dlolna/langchain_alternatives_for_a_nextjs_project/" /><updated>2024-06-22T05:42:33+00:00</updated><published>2024-06-22T05:42:33+00:00</published><title>LangChain alternatives for a Next.js project?</title></entry></feed>