<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/"><category term="LangChain" label="r/LangChain"/><updated>2024-06-06T03:20:50+00:00</updated><icon>https://www.redditstatic.com/icon.png/</icon><id>/r/LangChain.rss</id><link rel="self" href="https://www.reddit.com/r/LangChain.rss" type="application/atom+xml" /><link rel="alternate" href="https://www.reddit.com/r/LangChain" type="text/html" /><subtitle>LangChain is an open-source framework and developer toolkit that helps developers get LLM applications from prototype to production. It is available for Python and Javascript at https://www.langchain.com/.</subtitle><title>LangChain</title><entry><author><name>/u/zchaarm</name><uri>https://www.reddit.com/user/zchaarm</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;A place for members of &lt;a href=&quot;/r/LangChain&quot;&gt;r/LangChain&lt;/a&gt; to chat with each other&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/zchaarm&quot;&gt; /u/zchaarm &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_10ljho9</id><link href="https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/" /><updated>2023-01-26T04:33:05+00:00</updated><published>2023-01-26T04:33:05+00:00</published><title>r/LangChain Lounge</title></entry><entry><author><name>/u/mehul_gupta1997</name><uri>https://www.reddit.com/user/mehul_gupta1997</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/mehul_gupta1997&quot;&gt; /u/mehul_gupta1997 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;/r/ChatGPT/comments/1d98aa6/data_visualization_using_chatgpt_free/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d98eio/data_visualization_using_chatgpt_free/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1d98eio</id><link href="https://www.reddit.com/r/LangChain/comments/1d98eio/data_visualization_using_chatgpt_free/" /><updated>2024-06-06T03:07:41+00:00</updated><published>2024-06-06T03:07:41+00:00</published><title>Data visualization using ChatGPT (free)</title></entry><entry><author><name>/u/mean-short-</name><uri>https://www.reddit.com/user/mean-short-</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I am creating a RAG application but I am having this problem:&lt;/p&gt; &lt;p&gt;I have multiple files containing the companies projects lists along with their descriptions, used frameworks etc.&lt;/p&gt; &lt;p&gt;The type of question I want an answer for is: &amp;quot;Give me all the projects built using FastAPI&amp;quot; (as an example)&lt;/p&gt; &lt;p&gt;I am limited by top_k variable which means I do not get all the projects,&lt;/p&gt; &lt;p&gt;How would you solve this.&lt;/p&gt; &lt;p&gt;Thank you all &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/mean-short-&quot;&gt; /u/mean-short- &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d8uj3o/rag_how_to_answer_give_me_all_question/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d8uj3o/rag_how_to_answer_give_me_all_question/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1d8uj3o</id><link href="https://www.reddit.com/r/LangChain/comments/1d8uj3o/rag_how_to_answer_give_me_all_question/" /><updated>2024-06-05T16:43:37+00:00</updated><published>2024-06-05T16:43:37+00:00</published><title>RAG: How to answer &quot;give me all...&quot; question</title></entry><entry><author><name>/u/Massive_Building_952</name><uri>https://www.reddit.com/user/Massive_Building_952</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey Guys,&lt;/p&gt; &lt;p&gt;here is very intresting problem I have on hands --Any of you have any idea of a solution??&lt;/p&gt; &lt;p&gt;I need a AI Agnet to handle automatically the totality of the simulation i have developed..&lt;/p&gt; &lt;p&gt;I want to create an AI agent that can:&lt;/p&gt; &lt;p&gt;Understand Natural Language Queries:&lt;/p&gt; &lt;p&gt;The agent will interpret queries related to supply chain parameters (e.g., increasing costs, changing demand).&lt;/p&gt; &lt;p&gt;Translate Queries to JSON:&lt;/p&gt; &lt;p&gt;Convert the interpreted queries into a large, structured JSON format that your simulation system can use. --Pls note my JSOn can often go into 10,000 + lines&lt;/p&gt; &lt;p&gt;Run the Simulation: AI Agent must automatically do this..&lt;/p&gt; &lt;p&gt;Send the JSON input to your simulation system hosted on a server and run the simulation.&lt;/p&gt; &lt;p&gt;Output Financial Results:&lt;/p&gt; &lt;p&gt;Extract and present financial results such as revenue and COGS from the simulation&amp;#39;s output....Any suggestions as to how I can approach this are welcome...Thanks&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Massive_Building_952&quot;&gt; /u/Massive_Building_952 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d96jr5/ai_agent_to_manipulate_json/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d96jr5/ai_agent_to_manipulate_json/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1d96jr5</id><link href="https://www.reddit.com/r/LangChain/comments/1d96jr5/ai_agent_to_manipulate_json/" /><updated>2024-06-06T01:30:09+00:00</updated><published>2024-06-06T01:30:09+00:00</published><title>AI Agent to Manipulate JSON</title></entry><entry><author><name>/u/Alaya94</name><uri>https://www.reddit.com/user/Alaya94</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hello Langchain community,&lt;/p&gt; &lt;p&gt;I wanted to ask, based on your experience, what are the best open-source LLMs I can use to build complex agents? I&amp;#39;ve built an agent based on GPT-3.5, and it works fine, but now I want to migrate to open-source models. I&amp;#39;ve tried Llama3 8B, but it doesn&amp;#39;t seem to work very well. Could you please suggest a list based on performance and GPU cost?&lt;/p&gt; &lt;p&gt;Thank you!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Alaya94&quot;&gt; /u/Alaya94 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d8ri4z/best_open_source_models_to_build_complex_agents/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d8ri4z/best_open_source_models_to_build_complex_agents/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1d8ri4z</id><link href="https://www.reddit.com/r/LangChain/comments/1d8ri4z/best_open_source_models_to_build_complex_agents/" /><updated>2024-06-05T14:36:31+00:00</updated><published>2024-06-05T14:36:31+00:00</published><title>Best open source models to build complex agents:</title></entry><entry><author><name>/u/jandez97</name><uri>https://www.reddit.com/user/jandez97</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;m working on a personal project and my goal is the have multiples LLM available for the users, but I got one concern, how can I improve the size of the context of my LLM to avoid it from loosing track of the subject that&amp;#39;s being taking care of &lt;/p&gt; &lt;p&gt;Are there any techniques available for me to use or there&amp;#39;s any limitations inherited from the LLM itself that can stopped me from accomplished this?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/jandez97&quot;&gt; /u/jandez97 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d93xyb/theres_any_way_to_increase_the_context_of_a_llm/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d93xyb/theres_any_way_to_increase_the_context_of_a_llm/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1d93xyb</id><link href="https://www.reddit.com/r/LangChain/comments/1d93xyb/theres_any_way_to_increase_the_context_of_a_llm/" /><updated>2024-06-05T23:20:06+00:00</updated><published>2024-06-05T23:20:06+00:00</published><title>There's any way to increase the context of a LLM</title></entry><entry><author><name>/u/hwchase17</name><uri>https://www.reddit.com/user/hwchase17</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Today we released a DeepLearning class on LangGraph! &lt;a href=&quot;https://www.deeplearning.ai/short-courses/ai-agents-in-langgraph/&quot;&gt;https://www.deeplearning.ai/short-courses/ai-agents-in-langgraph/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I&amp;#39;m really excited about this (and wanted to share it here) for a few reasons. First, I&amp;#39;m incredbily bullish on LangGraph. We&amp;#39;re investing a lot in and seeing really good usage (including more questions about it here!). Second - the DeepLearning team is fantastic and the material we put together with them is always top notch&lt;/p&gt; &lt;p&gt;I hope people have some time to watch and try it out. As mentioned, we&amp;#39;re doing a bit push on LangGraph over the next month, and so if people have comments/feedback/questions I&amp;#39;d love to hear!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/hwchase17&quot;&gt; /u/hwchase17 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d8trdx/langgraph_deeplearning_class/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d8trdx/langgraph_deeplearning_class/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1d8trdx</id><link href="https://www.reddit.com/r/LangChain/comments/1d8trdx/langgraph_deeplearning_class/" /><updated>2024-06-05T16:11:18+00:00</updated><published>2024-06-05T16:11:18+00:00</published><title>LangGraph DeepLearning Class</title></entry><entry><author><name>/u/Environmental_Form14</name><uri>https://www.reddit.com/user/Environmental_Form14</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I have been doing retrieval by stacking vectors into a 2d matrix. To find similar documents, I multiply the 2d matrix by the other vector to find the highest similarity index. I then use that index to find the document.&lt;/p&gt; &lt;p&gt;I am currently planning to use embedded vectors for retrieval only. Is there an advantage of using vectorstores?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Environmental_Form14&quot;&gt; /u/Environmental_Form14 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d8uwcn/vectorstores_vs_2d_matrix_for_retrieval/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d8uwcn/vectorstores_vs_2d_matrix_for_retrieval/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1d8uwcn</id><link href="https://www.reddit.com/r/LangChain/comments/1d8uwcn/vectorstores_vs_2d_matrix_for_retrieval/" /><updated>2024-06-05T16:58:46+00:00</updated><published>2024-06-05T16:58:46+00:00</published><title>vectorstores vs 2d matrix for retrieval</title></entry><entry><author><name>/u/mrshmello1</name><uri>https://www.reddit.com/user/mrshmello1</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I would like to understand how a model is returning the right tool to call for a query, what&amp;#39;s happening behind the api call for function calling for open ai, mistral and other models.&lt;/p&gt; &lt;p&gt;Are they using a prompt with instructions on how select the right function to call? and when tools and query is passed via api, is the llm using that prompt to select the tool or what&amp;#39;s happening behind the call.&lt;/p&gt; &lt;p&gt;How do I achieve function calling using a open source model that I run locally using ollama. Thanks.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/mrshmello1&quot;&gt; /u/mrshmello1 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d8y7mq/how_does_function_calling_work_under_the_hood/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d8y7mq/how_does_function_calling_work_under_the_hood/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1d8y7mq</id><link href="https://www.reddit.com/r/LangChain/comments/1d8y7mq/how_does_function_calling_work_under_the_hood/" /><updated>2024-06-05T19:15:08+00:00</updated><published>2024-06-05T19:15:08+00:00</published><title>How does function calling work under the hood?</title></entry><entry><author><name>/u/Top_Tradition_7139</name><uri>https://www.reddit.com/user/Top_Tradition_7139</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Group below data type items into very tight semantic groups. Only group items together if they have exact semantic meaning or can be used interchangably with each other. Break data types into maximum groups as possbile. Return the groups in JSON format, ensuring that each key is exact to all the items in its list. Do not alter any characters.&lt;/p&gt; &lt;p&gt;Here is the list of words:&lt;/p&gt; &lt;p&gt;[&amp;#39;email_addr&amp;#39;, &amp;#39;email&amp;#39;, &amp;#39;email_address&amp;#39;, &amp;#39;address&amp;#39;, &amp;#39;email id&amp;#39;, &amp;#39;gmail&amp;#39;, &amp;#39;text&amp;#39;, &amp;#39;financial info&amp;#39;, &amp;#39;credit card number&amp;#39;, &amp;#39;cvv number&amp;#39;, &amp;quot;email address&amp;quot;, &amp;quot;eemmail&amp;quot;, &amp;quot;contact info&amp;quot;, &amp;quot;electronic mail&amp;quot;, &amp;#39;first&amp;#39;, &amp;#39;first_name&amp;#39;, &amp;#39;last name&amp;#39;, &amp;quot;bank details&amp;quot;, &amp;#39;firstName&amp;#39;, &amp;#39;christianName&amp;#39;, &amp;#39;christian_name&amp;#39;, &amp;#39;name&amp;#39;, &amp;quot;creadit card info&amp;quot;, &amp;quot;gsheets&amp;quot;, &amp;quot;google sheets&amp;quot;, &amp;quot;sheets&amp;quot;, &amp;quot;password&amp;quot;, &amp;quot;atm pin&amp;quot;, &amp;quot;atm card number&amp;quot;, &amp;quot;login details&amp;quot;, &amp;quot;vericyy&amp;quot;, &amp;quot;vericy&amp;quot;, &amp;quot;vericy.ai&amp;quot;]&lt;/p&gt; &lt;p&gt;I used llama3 70b from groq.com&lt;/p&gt; &lt;p&gt;This yields average results but atleast better than hierarchical clustering with openai embeddings.&lt;/p&gt; &lt;p&gt;What other methods beside zeronshot prompting can improve the clusters/groups while keeping costs to the minimum??&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Top_Tradition_7139&quot;&gt; /u/Top_Tradition_7139 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d8w8am/clustering_through_prompting/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d8w8am/clustering_through_prompting/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1d8w8am</id><link href="https://www.reddit.com/r/LangChain/comments/1d8w8am/clustering_through_prompting/" /><updated>2024-06-05T17:53:54+00:00</updated><published>2024-06-05T17:53:54+00:00</published><title>Clustering through prompting</title></entry><entry><author><name>/u/cryptokaykay</name><uri>https://www.reddit.com/user/cryptokaykay</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Learnings from doing Evaluations for LLM powered applications from my experience building with LLMs in the last few months: &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Evaluations for LLM powered applications is different from Model Evaluation leaderboards like Huggingface Open LLM leaderboard&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;If you are an enterprise leveraging or looking to leverage LLMs, spend very little time on model evaluation leaderboards. Pick the most powerful model to start with and invest in evaluating LLM responses in the context of your product and usecase. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Know your metrics&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;Ultimately what matters is whether your users are getting high quality product experience. This means it&amp;#39;s super important to look at your specific use case and determine the metrics that are best suited for your product. &lt;/p&gt; &lt;p&gt;Are you building a &lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;summarization tool? Manually evaluate the results and come up with your own thesis of what a good summary should look like that will solve the your user&amp;#39;s pain point. &lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;customer support agent chatbot? Look at a few responses and figure out what you care about the most and translate those to measurable metrics. &lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Its important to keep things simple and hyper optimize for your use case before jumping into measuring all the metrics that you found on the internet. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Write unit tests to capture basic assertions&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;Basic assertions includes things like,&lt;br/&gt; - looking for a specific word in every response.&lt;br/&gt; - making sure the generated response obeys a specific word count.&lt;br/&gt; - making sure the generated response costs less than $ x and uses less than n tokens. &lt;/p&gt; &lt;p&gt;These kinds of unit tests act as a first line of defence and will help you catch the basic issues quickly. If you are using python, you can use pytest to write these simple unit tests. There is no need to buy or adopt any fancy tools for this. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Use LLMs to evaluate the outputs&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;One of the popular approaches these days is to use a more powerful LLM to evaluate(or)grade the output of the LLM in use. This approach works well if you clearly know what metrics you care about which are often a bit subjective and specific to your use case. &lt;/p&gt; &lt;p&gt;The first step here is to identify a prompt that can be used for running a powerful LLM to grade the outputs. &lt;/p&gt; &lt;p&gt;There are nice opensource tools like Promptfoo and Inspect AI which already has built in support for model graded evaluations and unit tests which can be used as for starters. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Collect User Feedback&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;This is easier said than done. Especially for new products where there is not enough users to get quality feedback from to start with. But its important to make contact with reality as quickly as possible and get creative around getting this feedback - Ex: using it yourself, asking your network, friends and family to use it etc. &lt;/p&gt; &lt;p&gt;The goal here is to set up a system where you can diligently track the feedback and constantly tweak and iterate on the quality of the outputs. Establishing this feedback loop is extremely important. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Look at your data&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;No matter how many charts and visualizations you can create on top of your data, there is no proxy to looking at your data - both test and production data. In some cases, it may not be possible to do this when you are operating in a highly secure/private environment. But, you need to figure out a way to collect and look at all the LLM generations closely, especially in the early days. This will inform not just the quality of the outputs the users are experiencing, but also push you in the direction of identifying what metrics actually make sense for your use case. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Manually Evaluate&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;LLM based evaluations are not fool proof and you need to tweak and improve the prompts and grading scale continuously based on data. And the way to collect this data is by manually evaluating the outputs yourself. This will help you understand how far apart LLM evals is drifting from the real criteria that you want to evaluate against. It&amp;#39;s important to measure this drift and make sure LLM evals track closely with manual evals at most times. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Save your model parameters&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;Saving the model parameters you are using and tracking the responses along with the model parameters will help you with measuring the quality your product for that specific set of model parameters. This becomes useful when you are noticing a regression in the quality of when you are upgrading to a new model version or swapping out to a completely different model. &lt;/p&gt; &lt;p&gt;Leave your thoughts. I would love to hear about your experience managing your LLM powered product in terms of quality and accuracy. Also, I am also building a fully open source and open telemetry based tool called Langtrace AI to basically solve for the above problems. It&amp;#39;s super easy to setup with just 2 lines of code. Do check it out if you are interested.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/cryptokaykay&quot;&gt; /u/cryptokaykay &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d8euoz/learnings_from_doing_evaluations_for_llm_powered/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d8euoz/learnings_from_doing_evaluations_for_llm_powered/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1d8euoz</id><link href="https://www.reddit.com/r/LangChain/comments/1d8euoz/learnings_from_doing_evaluations_for_llm_powered/" /><updated>2024-06-05T02:11:48+00:00</updated><published>2024-06-05T02:11:48+00:00</published><title>Learnings from doing Evaluations for LLM powered applications</title></entry><entry><author><name>/u/TimeTravellingCat</name><uri>https://www.reddit.com/user/TimeTravellingCat</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d81pvi/opensource_low_code_platform_to_build_and/&quot;&gt; &lt;img src=&quot;https://external-preview.redd.it/d3NnYWNuNXN6azRkMQoZ5W3GjTdq7sq8sWHHdR4Qi5FBoBNxV3-SYCYEcMVj.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cc96b114ec6e6edcdf1387dcb3773799724ef389&quot; alt=&quot;Open-source low code platform to build and coordinate multi-agent teams&quot; title=&quot;Open-source low code platform to build and coordinate multi-agent teams&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/TimeTravellingCat&quot;&gt; /u/TimeTravellingCat &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://v.redd.it/dzjs6k5szk4d1&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d81pvi/opensource_low_code_platform_to_build_and/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1d81pvi</id><media:thumbnail url="https://external-preview.redd.it/d3NnYWNuNXN6azRkMQoZ5W3GjTdq7sq8sWHHdR4Qi5FBoBNxV3-SYCYEcMVj.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=cc96b114ec6e6edcdf1387dcb3773799724ef389" /><link href="https://www.reddit.com/r/LangChain/comments/1d81pvi/opensource_low_code_platform_to_build_and/" /><updated>2024-06-04T16:40:26+00:00</updated><published>2024-06-04T16:40:26+00:00</published><title>Open-source low code platform to build and coordinate multi-agent teams</title></entry><entry><author><name>/u/mehul_64</name><uri>https://www.reddit.com/user/mehul_64</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey guys, i am new to generative AI, it’s been a few days learning new things. I have a problem statement in hand. We have to evaluate a startup idea. We already have an evaluation checklist that has like 30 parameters on the basic of which we decide the feasibility of the idea. We have to build a model in which we prompt an idea and the input idea goes through various agents who are (business analysts, cofounder, VC). So it first goes to BA and then the result goes to cofounder and so on therefore getting perspective of all the agents. For starters i want to build the model with 3 agents. Once it passes through 3rd agent it gives the final result as an evaluation checklist (the same one i talked about above). &lt;/p&gt; &lt;p&gt;Now my question is how should i approach this problem and what would be the underlying concept used for building such a model? Also from where can i start ? FYI - i read a bit about genertive ai topics like embedding, fine tuning and a bit of langchain (built a simple agent) etc. Still exploring agentic AI. &lt;/p&gt; &lt;p&gt;Thanks in advance !! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/mehul_64&quot;&gt; /u/mehul_64 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d8pug0/how_to_get_started_with_the_problem_statement/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d8pug0/how_to_get_started_with_the_problem_statement/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1d8pug0</id><link href="https://www.reddit.com/r/LangChain/comments/1d8pug0/how_to_get_started_with_the_problem_statement/" /><updated>2024-06-05T13:23:45+00:00</updated><published>2024-06-05T13:23:45+00:00</published><title>How to get started with the problem statement?</title></entry><entry><author><name>/u/Danidre</name><uri>https://www.reddit.com/user/Danidre</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;One thing I notice isn&amp;#39;t discussed is the recoveries for failures on tools that require external connections, when in a long-running environment.&lt;/p&gt; &lt;p&gt;For example, the SQLToolKit that uses SQL Alchemy to connect to the database. &lt;/p&gt; &lt;p&gt;Eventually, you just get a &amp;quot;The database connection has been terminated.&amp;quot; Error, and there&amp;#39;s nothing built-in my default in any examples to account for things like these.&lt;/p&gt; &lt;p&gt;How would one suggest another goes about managing this, and things like these?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Danidre&quot;&gt; /u/Danidre &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d8p5c5/long_running_toolkits/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d8p5c5/long_running_toolkits/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1d8p5c5</id><link href="https://www.reddit.com/r/LangChain/comments/1d8p5c5/long_running_toolkits/" /><updated>2024-06-05T12:50:37+00:00</updated><published>2024-06-05T12:50:37+00:00</published><title>Long Running Toolkits</title></entry><entry><author><name>/u/Karlthagain</name><uri>https://www.reddit.com/user/Karlthagain</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I would like to install a chatbot/text generator on my Macmini for various uses: structured text generator from user-provided notes and a relative chatbot for consulting the generated texts.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Karlthagain&quot;&gt; /u/Karlthagain &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d8n48v/chatbot_and_text_generator_with/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d8n48v/chatbot_and_text_generator_with/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1d8n48v</id><link href="https://www.reddit.com/r/LangChain/comments/1d8n48v/chatbot_and_text_generator_with/" /><updated>2024-06-05T10:59:41+00:00</updated><published>2024-06-05T10:59:41+00:00</published><title>Chatbot and Text Generator with Mistral/LLAMA3+Ollama+Gradio+Langchain</title></entry><entry><author><name>/u/Extension-Ad5598</name><uri>https://www.reddit.com/user/Extension-Ad5598</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;In some tutorial that I saw online, it was mentioned that llama-index is faster than langchain when it comes to indexing the documents. Can someone explain me why this is the case and what does Ilamaindex use which makes it faster than langchain?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Extension-Ad5598&quot;&gt; /u/Extension-Ad5598 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d8le7w/why_is_llamaindex_faster_than_langchain/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d8le7w/why_is_llamaindex_faster_than_langchain/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1d8le7w</id><link href="https://www.reddit.com/r/LangChain/comments/1d8le7w/why_is_llamaindex_faster_than_langchain/" /><updated>2024-06-05T09:04:36+00:00</updated><published>2024-06-05T09:04:36+00:00</published><title>Why is llamaindex faster than langchain?</title></entry><entry><author><name>/u/ss1seekining</name><uri>https://www.reddit.com/user/ss1seekining</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I am using Langserve to create a langchain agent with tool calling. Till now all is working as expected and I am able to host it in AWS ECS with CDK also and learnt a lot in the process. But one thing I am stuck, in my chat, users will give their phone number and I will store the user as a lead in a crm I built using mongoDB. However, in the crm I want to show the chat history also.&lt;/p&gt; &lt;p&gt;For the chat history, I am using the mongodb integration and its working as expected when I pass the sessionId in the api calls.&lt;/p&gt; &lt;p&gt;Now I want to create a tool say `save_user` which will take the phone number as input. I tested dummy tool calls and its working. but in the save_user function, I want to save the phone number and the associated sessionId. I did not find any documentation or tutorial on how to achieve this.&lt;/p&gt; &lt;p&gt;This is the tool&lt;/p&gt; &lt;pre&gt;&lt;code&gt;@tool(&amp;quot;save_user&amp;quot;, args_schema=getUserInfoInput) def save_user(phone :str) -&amp;gt; dict: &amp;quot;&amp;quot;&amp;quot;Save the user detail&amp;quot;&amp;quot;&amp;quot; # I want the session_id of the chat here return &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;here is the tentative code I was using&lt;/p&gt; &lt;pre&gt;&lt;code&gt;# #!/usr/bin/env python from fastapi import FastAPI from langchain.agents import AgentExecutor from langchain.agents.format_scratchpad.openai_tools import ( format_to_openai_tool_messages, ) from langchain.agents.output_parsers.openai_tools import OpenAIToolsAgentOutputParser from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder from langchain_core.tools import tool from langchain_openai import ChatOpenAI from langserve import add_routes from langserve.pydantic_v1 import BaseModel, Field from langchain_core.runnables.history import RunnableWithMessageHistory from langchain_mongodb.chat_message_histories import MongoDBChatMessageHistory from langchain_openai import ChatOpenAI from langchain.agents import tool import os from pinecone import Pinecone from algoliasearch.search_client import SearchClient import openai import json from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder from langchain.agents import AgentExecutor from typing import Any from fastapi.middleware.cors import CORSMiddleware pinecone_client = Pinecone(api_key=os.environ[&amp;quot;PINECONE_API_KEY&amp;quot;]) pc_index = pinecone_client.Index(os.environ[&amp;quot;PINECONE_INDEX&amp;quot;]) algolia_client = SearchClient.create(os.environ[&amp;quot;ALGOLIA_APPLICATION_ID&amp;quot;], os.environ[&amp;quot;ALGOLIA_API_KEY&amp;quot;]) alg_index = algolia_client.init_index(os.environ[&amp;quot;ALGOLIA_INDEX&amp;quot;]) openai_client = openai.OpenAI(api_key=os.environ[&amp;#39;OPENAI_API_KEY&amp;#39;]) class getInfoInput(BaseModel): food: str = Field(description=&amp;quot;name of the food&amp;quot;) @tool(&amp;quot;get_food_price&amp;quot;, args_schema=getInfoInput) def get_food_price(food :str) -&amp;gt; dict: &amp;quot;&amp;quot;&amp;quot;Return cost of a food&amp;quot;&amp;quot;&amp;quot; # ideally this will call algolia and pinecone to do the search # but for this case, it does not matter return 100 tools = [get_food_price] class getUserInfoInput(BaseModel): phone: str = Field(description=&amp;quot;Users phone number&amp;quot;) @tool(&amp;quot;save_user&amp;quot;, args_schema=getUserInfoInput) def save_user(phone :str) -&amp;gt; dict: &amp;quot;&amp;quot;&amp;quot;Save the user detail&amp;quot;&amp;quot;&amp;quot; # I want the session_id of the chat here return tools = [get_food_price, save_user] system_prompt = &amp;quot;&amp;quot;&amp;quot; You are a waiter named Mia, People ask you about price of food and you reply &amp;quot;&amp;quot;&amp;quot; prompt = ChatPromptTemplate.from_messages( [ ( &amp;quot;system&amp;quot;, system_prompt, ), MessagesPlaceholder(variable_name=&amp;quot;chat_history&amp;quot;), (&amp;quot;user&amp;quot;, &amp;quot;{input}&amp;quot;), MessagesPlaceholder(variable_name=&amp;quot;agent_scratchpad&amp;quot;), ] ) llm = ChatOpenAI(model=&amp;quot;gpt-3.5-turbo&amp;quot;, temperature=0, streaming=True) llm_with_tools = llm.bind_tools(tools) agent = ( { &amp;quot;input&amp;quot;: lambda x: x[&amp;quot;input&amp;quot;], &amp;quot;agent_scratchpad&amp;quot;: lambda x: format_to_openai_tool_messages( x[&amp;quot;intermediate_steps&amp;quot;] ), &amp;quot;chat_history&amp;quot;: lambda x: x[&amp;quot;chat_history&amp;quot;], } | prompt | llm_with_tools | OpenAIToolsAgentOutputParser() ) agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True) agent_with_chat_history = RunnableWithMessageHistory( agent_executor, # This is needed because in most real world scenarios, a session id is needed # It isn&amp;#39;t really used here because we are using a simple in memory ChatMessageHistory lambda session_id: MongoDBChatMessageHistory( session_id=session_id, connection_string=os.environ[&amp;quot;MONGO_CONNECTION_STRING&amp;quot;], database_name=os.environ[&amp;quot;MONGO_DB&amp;quot;], collection_name=os.environ[&amp;quot;MONGO_COLLECTION&amp;quot;], ), input_messages_key=&amp;quot;input&amp;quot;, history_messages_key=&amp;quot;chat_history&amp;quot;, ) app = FastAPI( title=&amp;quot;LangChain Server&amp;quot;, version=&amp;quot;1.0&amp;quot;, description=&amp;quot;Spin up a simple api server using LangChain&amp;#39;s Runnable interfaces&amp;quot;, ) class Input(BaseModel): input: str # The field extra defines a chat widget. # Please see documentation about widgets in the main README. # The widget is used in the playground. # Keep in mind that playground support for agents is not great at the moment. # To get a better experience, you&amp;#39;ll need to customize the streaming output # for now. class Output(BaseModel): output: Any add_routes( app, agent_with_chat_history.with_types(input_type=Input, output_type=Output).with_config( {&amp;quot;run_name&amp;quot;: &amp;quot;agent&amp;quot;} ), ) # For health check, otherwise this will return 404 u/app.get(&amp;quot;/&amp;quot;) def get_root(): return {&amp;quot;message&amp;quot;: &amp;quot;FastAPI running in a Docker container&amp;quot;} # Set all CORS enabled origins app.add_middleware( CORSMiddleware, allow_origins=[&amp;quot;*&amp;quot;], allow_credentials=True, allow_methods=[&amp;quot;*&amp;quot;], allow_headers=[&amp;quot;*&amp;quot;], expose_headers=[&amp;quot;*&amp;quot;], ) if __name__ == &amp;quot;__main__&amp;quot;: import uvicorn uvicorn.run(app, host=&amp;quot;localhost&amp;quot;, port=8000) &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/ss1seekining&quot;&gt; /u/ss1seekining &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d8innr/how_to_get_access_of_sessionid_and_other_params/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d8innr/how_to_get_access_of_sessionid_and_other_params/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1d8innr</id><link href="https://www.reddit.com/r/LangChain/comments/1d8innr/how_to_get_access_of_sessionid_and_other_params/" /><updated>2024-06-05T05:50:12+00:00</updated><published>2024-06-05T05:50:12+00:00</published><title>How to get access of sessionId and other params which are passed in a config inside the tool of a langchain agent ?</title></entry><entry><author><name>/u/Apart-Damage143</name><uri>https://www.reddit.com/user/Apart-Damage143</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey, I have a Pinecone namespace with multiple documents, but as the number of documents increases, my retrieval performance is getting worse. I’m thinking of creating multiple retrievers that filter by document name to get relevant chunks from each document. I&amp;#39;m struggling to create an LCEL chain to achieve this. Does anyone know how to do this?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Apart-Damage143&quot;&gt; /u/Apart-Damage143 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d8b1k5/struggling_with_pinecone_retrieval_performance/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d8b1k5/struggling_with_pinecone_retrieval_performance/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1d8b1k5</id><link href="https://www.reddit.com/r/LangChain/comments/1d8b1k5/struggling_with_pinecone_retrieval_performance/" /><updated>2024-06-04T23:07:12+00:00</updated><published>2024-06-04T23:07:12+00:00</published><title>Struggling with Pinecone Retrieval Performance? Need Help with LCEL Chains!</title></entry><entry><author><name>/u/Fireche</name><uri>https://www.reddit.com/user/Fireche</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I am using the managed postgres DB by supabase to store my embeddings and so far I have used the PGVector provided by langchain: &lt;a href=&quot;https://python.langchain.com/v0.2/docs/integrations/vectorstores/pgvector/&quot;&gt;https://python.langchain.com/v0.2/docs/integrations/vectorstores/pgvector/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I see that langchain also provides a vectorstore class specfically tailored to supabase: &lt;a href=&quot;https://python.langchain.com/v0.2/docs/integrations/vectorstores/supabase/&quot;&gt;https://python.langchain.com/v0.2/docs/integrations/vectorstores/supabase/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;But I can&amp;#39;t tell what seems to be the difference except that instead of passing my Postgres Connection Env variables, I just simply instantiate a client. Is that all that is to it?&lt;/p&gt; &lt;p&gt;Does anyone know?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Fireche&quot;&gt; /u/Fireche &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d7y2ue/postgresvectorstore_vs_supabasevectorstore/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d7y2ue/postgresvectorstore_vs_supabasevectorstore/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1d7y2ue</id><link href="https://www.reddit.com/r/LangChain/comments/1d7y2ue/postgresvectorstore_vs_supabasevectorstore/" /><updated>2024-06-04T14:08:46+00:00</updated><published>2024-06-04T14:08:46+00:00</published><title>PostgresVectorStore vs SupabaseVectorStore</title></entry><entry><author><name>/u/Razeta101</name><uri>https://www.reddit.com/user/Razeta101</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hello, I have been working on a project that involves using langchain and RAG to write direct text quotes based on sourced texts.&lt;/p&gt; &lt;p&gt;The main objective is to create a vector database with PDF files from Google Academic and then utilize them to generate a user-requested text in APA format, which will output a paragraph containing direct text quotes.&lt;/p&gt; &lt;p&gt;For this project, I have been using langchain, local llm (phi3 and llama3 with ollama or llm studio), as well as Chat GPT 4 and Groq.&lt;/p&gt; &lt;p&gt;Unfortunately, I have encountered some difficulties as the models don&amp;#39;t seem to follow the instructions, especially when it comes to using direct text quotes, even when I include instructions on how to format them in APA style.&lt;/p&gt; &lt;p&gt;Therefore, I would appreciate any ideas or suggestions from the community on how I could achieve the desired result.&lt;/p&gt; &lt;p&gt;Thank you.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Razeta101&quot;&gt; /u/Razeta101 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d81hl4/rag_as_a_apa_direct_text_quotes_generator/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d81hl4/rag_as_a_apa_direct_text_quotes_generator/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1d81hl4</id><link href="https://www.reddit.com/r/LangChain/comments/1d81hl4/rag_as_a_apa_direct_text_quotes_generator/" /><updated>2024-06-04T16:30:37+00:00</updated><published>2024-06-04T16:30:37+00:00</published><title>RAG as a APA direct text quotes generator</title></entry><entry><author><name>/u/Unrealnooob</name><uri>https://www.reddit.com/user/Unrealnooob</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I am trying to use &lt;code&gt;GraphCypherQAChain&lt;/code&gt; with memory,&lt;/p&gt; &lt;p&gt;when I don&amp;#39;t use &lt;code&gt;return_intermediate_steps = False&lt;/code&gt; I am getting result, but when its true I am getting the error&lt;/p&gt; &lt;p&gt;Another scenario is when I give the output_key as intermediate_steps, it works, but I need the result, so I have given the &lt;code&gt;out_put key&lt;/code&gt; as result but then I am getting key error? - &lt;code&gt;return inputs[prompt_input_key], outputs[output_key] KeyError: &amp;#39;result&amp;#39;&lt;/code&gt;&lt;/p&gt; &lt;p&gt;i need both &lt;code&gt;result&lt;/code&gt; and &lt;code&gt;intermediate_steps&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Any one implemented this?&lt;/p&gt; &lt;p&gt;any help will be great!!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Unrealnooob&quot;&gt; /u/Unrealnooob &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d7uq7p/anyone_implemented_graphcypherqachain_with_memory/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d7uq7p/anyone_implemented_graphcypherqachain_with_memory/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1d7uq7p</id><link href="https://www.reddit.com/r/LangChain/comments/1d7uq7p/anyone_implemented_graphcypherqachain_with_memory/" /><updated>2024-06-04T11:23:32+00:00</updated><published>2024-06-04T11:23:32+00:00</published><title>Anyone implemented GraphCypherQAChain with memory?</title></entry><entry><author><name>/u/Bitman321</name><uri>https://www.reddit.com/user/Bitman321</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Let&amp;#39;s say I am a teacher, and I want my students to be able to interact with all the course material available. Is there a service that allows me to upload documents to a langchain chatbot and share it with others without having to host the chatbot myself?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Bitman321&quot;&gt; /u/Bitman321 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d7nf63/does_a_service_exist_that_allows_you_to_upload/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d7nf63/does_a_service_exist_that_allows_you_to_upload/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1d7nf63</id><link href="https://www.reddit.com/r/LangChain/comments/1d7nf63/does_a_service_exist_that_allows_you_to_upload/" /><updated>2024-06-04T03:21:10+00:00</updated><published>2024-06-04T03:21:10+00:00</published><title>Does a service exist that allows you to upload documents to a RAG and share the chatbot with others?</title></entry><entry><author><name>/u/Smooth_Incident6948</name><uri>https://www.reddit.com/user/Smooth_Incident6948</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;m a bit confused with the class of a graph object, from the LangServe overview page it says we can deploy runnables and chains as a rest API, so can we deploy langgraph?&lt;/p&gt; &lt;p&gt;If so, how should I config the Input class when I add_route? the question I have is, I am using the checkpointer for memory, so do I need to add a thread_id into the Input class for the client to call the API? I&amp;#39;m asking because to invoke the graph, we pass the thread_id in the second argument as a config dictionary, separate from the message, so I&amp;#39;m not sure how this works when we add_route.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Smooth_Incident6948&quot;&gt; /u/Smooth_Incident6948 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d7nr29/can_we_deploy_a_langgraph_graph_as_an_api/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d7nr29/can_we_deploy_a_langgraph_graph_as_an_api/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1d7nr29</id><link href="https://www.reddit.com/r/LangChain/comments/1d7nr29/can_we_deploy_a_langgraph_graph_as_an_api/" /><updated>2024-06-04T03:39:15+00:00</updated><published>2024-06-04T03:39:15+00:00</published><title>Can we deploy a LangGraph graph as an api endpoint with LangServe?</title></entry><entry><author><name>/u/gaharavara</name><uri>https://www.reddit.com/user/gaharavara</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I ask this because there landing page looks like a full blown product page with no clarity whatsover - it looks as if it is a commercial sass.&lt;/p&gt; &lt;p&gt;Are there any alternative projects which are less salesy ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/gaharavara&quot;&gt; /u/gaharavara &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d73efm/is_langchain_even_open_source_now/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d73efm/is_langchain_even_open_source_now/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1d73efm</id><link href="https://www.reddit.com/r/LangChain/comments/1d73efm/is_langchain_even_open_source_now/" /><updated>2024-06-03T12:38:41+00:00</updated><published>2024-06-03T12:38:41+00:00</published><title>is langchain even open source now</title></entry><entry><author><name>/u/bartselen</name><uri>https://www.reddit.com/user/bartselen</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi, I&amp;#39;m trying to create an LLM-based system that allows me to &amp;quot;interact&amp;quot; with my chat logs and draw conclusions from them. A few examples: * Find &amp;quot;sub&amp;quot;-conversations or messages in which a certain topic is talked about and perhaps even summarize all of those &amp;quot;sub&amp;quot;-conversations (and it&amp;#39;s important not to miss any, which is something I&amp;#39;ve been struggling with) * Find people in my group chats that talk a lot about certain topics And other similar prompts that I give the LLM.&lt;/p&gt; &lt;p&gt;Right now my solution is pretty simple. I processed all my messages with recursive character chunking and each message into a document, while using a Parent Document Retriever to get more context (in the future I plan on using semantic chunking to make the parent documents full sub-conversations). Those are then put into a vector database, and A basic conversational RAG agent (I currently use mixtral-instruct) with langchain. The conversations are injected as JSON as context.&lt;/p&gt; &lt;p&gt;First of all, I would love to know what approach you guys would take to this problem in each part of it - the embedding, the retrieval (is RAG even the solution here?), the context format, the agent, etc. I&amp;#39;m a newbie to LLMs and AI in general and I really want to hear the opinions of people with experience&lt;/p&gt; &lt;p&gt;Secondly, I have a few problems I&amp;#39;ve encountered with my setup, mainly: * Because I currently don&amp;#39;t split my documents semantically, I receive a big part of the conversation (up to 4k+ tokens). mixtral seems to, for some reason, ignore some of my context and only remembers it starting from some arbitrary index, unless I really shorten it down to several hundreds of tokens. I don&amp;#39;t understand why this happens as it should have a 32k context window. One solution I thought of was switching to llama3-8b (only 8k but might work better because it&amp;#39;s a better model). This is a huge roadblock for this project and I&amp;#39;d appreciate any help here * I&amp;#39;m not sure how to approach the not missing conversations part, as I&amp;#39;m always going to have a limited context. How do I make it so the agent continues fetching the rest of the convesations that were less relevant in the vector similarity search, like continually do it or something.&lt;/p&gt; &lt;p&gt;Thanks in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/bartselen&quot;&gt; /u/bartselen &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d7ghuk/rag_for_im_chat_logs/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d7ghuk/rag_for_im_chat_logs/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1d7ghuk</id><link href="https://www.reddit.com/r/LangChain/comments/1d7ghuk/rag_for_im_chat_logs/" /><updated>2024-06-03T21:43:51+00:00</updated><published>2024-06-03T21:43:51+00:00</published><title>RAG for IM chat logs</title></entry><entry><author><name>/u/Best_Day_3041</name><uri>https://www.reddit.com/user/Best_Day_3041</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Has anyone gotten langchain/pinecone running in Node.JS? I have seen a lot of examples and some fully developed github repositories showing a RAG implementation in Node, but none of them work. I&amp;#39;m following the examples exactly, but cannot get many of the imports to work, such as:&lt;/p&gt; &lt;p&gt;import { OpenAIEmbeddings } from &amp;#39;langchain/embeddings&amp;#39;;&lt;/p&gt; &lt;p&gt;import { PineconeStore } from &amp;#39;langchain/vectorstores&amp;#39;;&lt;/p&gt; &lt;p&gt;If the answer is NO, what is the best way to do the implementation with Python but write my server in Node.js? &lt;/p&gt; &lt;p&gt;Thanks&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Best_Day_3041&quot;&gt; /u/Best_Day_3041 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d7m4i0/langchainpinecone_in_nodejs/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d7m4i0/langchainpinecone_in_nodejs/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1d7m4i0</id><link href="https://www.reddit.com/r/LangChain/comments/1d7m4i0/langchainpinecone_in_nodejs/" /><updated>2024-06-04T02:12:56+00:00</updated><published>2024-06-04T02:12:56+00:00</published><title>Langchain/Pinecone in Node.JS</title></entry></feed>