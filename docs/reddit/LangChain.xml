<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/"><category term="LangChain" label="r/LangChain"/><updated>2024-04-25T07:37:22+00:00</updated><icon>https://www.redditstatic.com/icon.png/</icon><id>/r/LangChain.rss</id><link rel="self" href="https://www.reddit.com/r/LangChain.rss" type="application/atom+xml" /><link rel="alternate" href="https://www.reddit.com/r/LangChain" type="text/html" /><subtitle>LangChain is an open-source framework and developer toolkit that helps developers get LLM applications from prototype to production. It is available for Python and Javascript at https://www.langchain.com/.</subtitle><title>LangChain</title><entry><author><name>/u/zchaarm</name><uri>https://www.reddit.com/user/zchaarm</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;A place for members of &lt;a href=&quot;/r/LangChain&quot;&gt;r/LangChain&lt;/a&gt; to chat with each other&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/zchaarm&quot;&gt; /u/zchaarm &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_10ljho9</id><link href="https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/" /><updated>2023-01-26T04:33:05+00:00</updated><published>2023-01-26T04:33:05+00:00</published><title>r/LangChain Lounge</title></entry><entry><author><name>/u/patcher99</name><uri>https://www.reddit.com/user/patcher99</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ccjozm/openlit_preview_opentelemetrynative_llm/&quot;&gt; &lt;img src=&quot;https://b.thumbs.redditmedia.com/jllXvUlBtTTyc1aFLMyhzX7p3v8xZJMp2-Zweq5w5oQ.jpg&quot; alt=&quot;OpenLIT Preview: OpenTelemetry-native LLM Application Observability&quot; title=&quot;OpenLIT Preview: OpenTelemetry-native LLM Application Observability&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey folks! My friend and I were working on an LLM-based legal helper but got really stuck trying to figure out our tweaked GPT-3.5. So, we came up with a tool named Doku to keep an monitor on our LLM apps and make them more trusty. It got stars fairly quickly, but folks found it a bit tricky since they had to set up things before diving into the analysis.&lt;/p&gt; &lt;p&gt;Here&amp;#39;s what we did next:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;We switched our tech to OpenTelemetry for easier tracking.&lt;/li&gt; &lt;li&gt;We made it so you can see costs and how many tokens you&amp;#39;re using straight from your console – no extra Infra needed for basic debugging.&lt;/li&gt; &lt;li&gt;We decided to call it OpenLIT (short for Learning Interpretability Tool, shining a light on model behavior and data visualization, inspired by a term from &lt;a href=&quot;https://developers.google.com/machine-learning/glossary#learning-interpretability-tool-lit&quot;&gt;Google&lt;/a&gt;).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;We&amp;#39;ve just put out our new Python library, OpenLIT, in preview. You can check it out here: &lt;a href=&quot;https://pypi.org/project/openlit/&quot;&gt;https://pypi.org/project/openlit/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This library is provides OpenTelemetry Auto-Instrumentation for LLM Applications, It integrates monitoring for&lt;/p&gt; &lt;ul&gt; &lt;li&gt;LLM providers like OpenAI, Anthropic, HuggingFace, Cohere, and Mistral.&lt;/li&gt; &lt;li&gt;Vector DBs including Pinecone and ChromaDB.&lt;/li&gt; &lt;li&gt;Frameworks like LangChain&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This library will work even if you are using frameworks like LiteLLM!&lt;/p&gt; &lt;p&gt;It&amp;#39;s the first of its kind to align with the OTEL &lt;a href=&quot;https://github.com/open-telemetry/semantic-conventions/tree/main/docs/gen-ai&quot;&gt;Semcov for GenAI Applications&lt;/a&gt;, allowing you to forward all collected metrics to any OTEL-compatible backend.&lt;/p&gt; &lt;p&gt;We&amp;#39;re also working on an open-source, self-hosted UI - Attached a couple pictures for you to get a feel of it.&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/8toxeja56kwc1.png?width=2553&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6704818740f9e76f953e951351c06698a21396c9&quot;&gt;https://preview.redd.it/8toxeja56kwc1.png?width=2553&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6704818740f9e76f953e951351c06698a21396c9&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/k5b3cka56kwc1.png?width=2552&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=266b290b1548ff85ae5730bc7b81b855cfe71fd2&quot;&gt;https://preview.redd.it/k5b3cka56kwc1.png?width=2552&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=266b290b1548ff85ae5730bc7b81b855cfe71fd2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Follow our project here for updates - &lt;a href=&quot;https://github.com/openlit/openlit&quot;&gt;https://github.com/openlit/openlit&lt;/a&gt;. The stable release drops tomorrow for both the SDK and UI. Let me know if there&amp;#39;s something specific you’d love to see!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/patcher99&quot;&gt; /u/patcher99 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ccjozm/openlit_preview_opentelemetrynative_llm/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ccjozm/openlit_preview_opentelemetrynative_llm/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1ccjozm</id><media:thumbnail url="https://b.thumbs.redditmedia.com/jllXvUlBtTTyc1aFLMyhzX7p3v8xZJMp2-Zweq5w5oQ.jpg" /><link href="https://www.reddit.com/r/LangChain/comments/1ccjozm/openlit_preview_opentelemetrynative_llm/" /><updated>2024-04-25T05:06:40+00:00</updated><published>2024-04-25T05:06:40+00:00</published><title>OpenLIT Preview: OpenTelemetry-native LLM Application Observability</title></entry><entry><author><name>/u/ashpreetbedi</name><uri>https://www.reddit.com/user/ashpreetbedi</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Here&amp;#39;s an unedited video testing tools with llama3 running locally (at 1.5x speed). The good, bad and ugly. &lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://reddit.com/link/1ccdexb/video/a47qddncliwc1/player&quot;&gt;https://reddit.com/link/1ccdexb/video/a47qddncliwc1/player&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/ashpreetbedi&quot;&gt; /u/ashpreetbedi &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ccdexb/testing_local_llama3_at_function_calling_and_tool/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ccdexb/testing_local_llama3_at_function_calling_and_tool/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1ccdexb</id><link href="https://www.reddit.com/r/LangChain/comments/1ccdexb/testing_local_llama3_at_function_calling_and_tool/" /><updated>2024-04-24T23:48:18+00:00</updated><published>2024-04-24T23:48:18+00:00</published><title>🧙Testing local llama3 at function calling and tool use.</title></entry><entry><author><name>/u/WesEd178</name><uri>https://www.reddit.com/user/WesEd178</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I am building a RAG application from 400+ XML documents, half of the content are tables which I am converting to csv and then extracting all text from the xml tags. A document before being added to the retriever contains both text and csv. Currently I am using an ensemble retriever combining bm25, tfidf and vectorstore (FAISS, chunk_size=2000, overlap=100). I have around 4000 test questions for these documents along with human labeled ground truth for each question and I also have a reference to the document that contains the answer. Right now I am able to get 91 questions out of 100 correctly in a random sample. &lt;/p&gt; &lt;p&gt;model: gpt-4&lt;br/&gt; embeddings: OpenAI text-embedding-3-large retriever: ensemble (bm25, tfidf, FAISS(hunk_size=2000, overlap=100)) additional: -RAPTOR clustering -sort by date then reordered using Long-Context Reorder&lt;/p&gt; &lt;p&gt;Is this a good &amp;quot;accuracy&amp;quot;? How can I improve? Is there such thing as 100% accurate RAG? How are your RAG applications doing?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/WesEd178&quot;&gt; /u/WesEd178 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cclcns/how_accurate_are_your_rag_applications/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cclcns/how_accurate_are_your_rag_applications/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cclcns</id><link href="https://www.reddit.com/r/LangChain/comments/1cclcns/how_accurate_are_your_rag_applications/" /><updated>2024-04-25T06:51:10+00:00</updated><published>2024-04-25T06:51:10+00:00</published><title>How accurate are your RAG applications?</title></entry><entry><author><name>/u/Travolta1984</name><uri>https://www.reddit.com/user/Travolta1984</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;LangChain recently added Semantic Chunker as an option for splitting documents, and from my experience it performs better than RecursiveCharacterSplitter (although it&amp;#39;s more expensive due to the sentence embeddings). &lt;/p&gt; &lt;p&gt;One thing that I noticed though, is that there&amp;#39;s no pre-defined limit to the size of the result chunks: I have seen chunks that are just a couple of words (i.e. section headers), and also very long chunks (5k+ characters). Which makes total sense, given the logic: if all sentences in that chunk are semantically similar, they should all be grouped together, regardless of how long that chunk will be. But that can lead to issues downstream: document context gets too large for the LLM, or small chunks that add no context at all.&lt;/p&gt; &lt;p&gt;Based on that, I wrote my custom version of the Semantic Chunker that optionally respects the character count limit (both minimum and maximum). The logic I am using is: a chunk split happens when either the semantic distance between the sentences becomes too large and the chunk is at least &amp;lt;MIN\_SIZE&amp;gt; long, or when the chunk becomes larger than &amp;lt;MAX\_SIZE&amp;gt;.&lt;/p&gt; &lt;p&gt;My question to the community is: &lt;/p&gt; &lt;p&gt;- Does the above make sense? I feel like this approach can be useful, but it kind of goes against the idea of chunking your texts semantically.&lt;/p&gt; &lt;p&gt;- I thought about creating a PR to add this option to the official code. Has anyone contributed to LangChain&amp;#39;s repo? What has been your experience doing so?&lt;/p&gt; &lt;p&gt;Thanks.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Travolta1984&quot;&gt; /u/Travolta1984 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cca0h0/question_about_semantic_chunker/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cca0h0/question_about_semantic_chunker/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cca0h0</id><link href="https://www.reddit.com/r/LangChain/comments/1cca0h0/question_about_semantic_chunker/" /><updated>2024-04-24T21:21:33+00:00</updated><published>2024-04-24T21:21:33+00:00</published><title>Question about Semantic Chunker</title></entry><entry><author><name>/u/OfficeSalamander</name><uri>https://www.reddit.com/user/OfficeSalamander</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I am trying to use internet-search enabled bots, and I was wondering how you guys were doing it - I see that Serpdev and Tavily have Langchain integration - which of these two do you guys like? Or do you roll your own?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/OfficeSalamander&quot;&gt; /u/OfficeSalamander &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cc1dyq/how_are_you_guys_doing_internet_search/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cc1dyq/how_are_you_guys_doing_internet_search/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cc1dyq</id><link href="https://www.reddit.com/r/LangChain/comments/1cc1dyq/how_are_you_guys_doing_internet_search/" /><updated>2024-04-24T15:40:46+00:00</updated><published>2024-04-24T15:40:46+00:00</published><title>How are you guys doing internet search?</title></entry><entry><author><name>/u/BuildingLLMTools</name><uri>https://www.reddit.com/user/BuildingLLMTools</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey folks,&lt;/p&gt; &lt;p&gt;We&amp;#39;re building Langtrace, an open-source LLM App observability platform (&lt;a href=&quot;http://www.langtrace.ai&quot;&gt;www.langtrace.ai&lt;/a&gt;) and we recently built support for LlamaIndex, the go-to library for building retrieval-augmented generation (RAG) applications.&lt;/p&gt; &lt;p&gt;As builders, we know how frustrating it can be to optimize RAG apps (e.g. trying to figure out where the bottlenecks are, whether your retrieval strategy is effective, etc.) That&amp;#39;s why we&amp;#39;re building a tool that makes it easy to gain deeper insights and optimize performance, reliability, and user experience for your LLM apps.&lt;/p&gt; &lt;p&gt;With Langtrace and LlamaIndex, you can:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Get one-click observability for LlamaIndex-based RAG applications&lt;/li&gt; &lt;li&gt;Visualize latency breakdowns, context relevance, and resource utilization&lt;/li&gt; &lt;li&gt;Monitor and analyze traces, evals, metrics, and logs with OpenTelemetry&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Feel free to check out our &lt;a href=&quot;https://github.com/Scale3-Labs/langtrace&quot;&gt;repo&lt;/a&gt; for &lt;a href=&quot;https://github.com/Scale3-Labs/langtrace-docs/blob/main/langtrace-examples/llamaindex_essay/starter.py.ipynb&quot;&gt;examples&lt;/a&gt;, contribute, provide feedback, and join our &lt;a href=&quot;https://discord.com/invite/EaSATwtr4t&quot;&gt;community&lt;/a&gt;. More info on the integration with LlamaIndex &lt;a href=&quot;https://langtrace.ai/blog/langtrace-llamaindex-a-game-changing-combo-for-rag-developers&quot;&gt;here&lt;/a&gt; including a video demo. Looking forward to hearing of your feedback! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/BuildingLLMTools&quot;&gt; /u/BuildingLLMTools &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cc8fx4/solve_rag_app_optimization_puzzles_with_langtrace/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cc8fx4/solve_rag_app_optimization_puzzles_with_langtrace/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cc8fx4</id><link href="https://www.reddit.com/r/LangChain/comments/1cc8fx4/solve_rag_app_optimization_puzzles_with_langtrace/" /><updated>2024-04-24T20:18:16+00:00</updated><published>2024-04-24T20:18:16+00:00</published><title>Solve RAG App Optimization Puzzles with Langtrace + LlamaIndex</title></entry><entry><author><name>/u/Moochiberico</name><uri>https://www.reddit.com/user/Moochiberico</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I would like to know if this is possible, I&amp;#39;m fairly new to langchain, I want to split into text chunks by different paragraphs and after reading doc, still seem to be stuck on this one. Some help would be much appreciated. Thanks!&lt;/p&gt; &lt;p&gt;Edit: Nevermind, think I got it, posting it in case anyone else has this question.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;text_splitter = CharacterTextSplitter(separator=&amp;quot;\n&amp;quot;,chunk_size=400, chunk_overlap=20) &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Moochiberico&quot;&gt; /u/Moochiberico &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ccccw6/text_split_by_paragraphs/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ccccw6/text_split_by_paragraphs/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1ccccw6</id><link href="https://www.reddit.com/r/LangChain/comments/1ccccw6/text_split_by_paragraphs/" /><updated>2024-04-24T23:01:12+00:00</updated><published>2024-04-24T23:01:12+00:00</published><title>Text Split by paragraphs?</title></entry><entry><author><name>/u/ashpreetbedi</name><uri>https://www.reddit.com/user/ashpreetbedi</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I dont trust the benchmarks, so I recorded my very first test run. Completely unedited, each question asked for the first time. First impression is that it is good, very very good for its size. Sharing the code below.&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://reddit.com/link/1cbuqow/video/ay3us3wbmewc1/player&quot;&gt;https://reddit.com/link/1cbuqow/video/ay3us3wbmewc1/player&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/ashpreetbedi&quot;&gt; /u/ashpreetbedi &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cbuqow/initial_tests_rag_with_phi3/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cbuqow/initial_tests_rag_with_phi3/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cbuqow</id><link href="https://www.reddit.com/r/LangChain/comments/1cbuqow/initial_tests_rag_with_phi3/" /><updated>2024-04-24T10:26:44+00:00</updated><published>2024-04-24T10:26:44+00:00</published><title>Initial tests: RAG with Phi-3</title></entry><entry><author><name>/u/supreet02</name><uri>https://www.reddit.com/user/supreet02</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;While RAG is undeniably impressive, the process of creating a functional application with it can be daunting. There&amp;#39;s a significant amount to grasp regarding implementation and development practices, ranging from selecting the appropriate AI models for the specific use case to organizing data effectively to obtain the desired insights. While tools like LangChain and LlamaIndex exist to simplify the prototype design process, there has yet to be an accessible, ready-to-use open-source RAG template that incorporates best practices and offers modular support, allowing anyone to quickly and easily utilize it.&lt;/p&gt; &lt;p&gt;TrueFoundry has recently introduced a new open-source framework called &lt;a href=&quot;https://github.com/truefoundry/cognita&quot;&gt;&lt;strong&gt;Cognita&lt;/strong&gt;&lt;/a&gt;, which utilizes Retriever-Augmented Generation (RAG) technology to simplify the transition by providing robust, scalable solutions for deploying AI applications. AI development often begins in experimental environments such as Jupyter notebooks, which are useful for prototyping but not well-suited for production environments. However, Cognita aims to bridge this gap. Developed on top of Langchain and LlamaIndex, Cognita offers a structured and modular approach to AI application development. Each component of the RAG, from data handling to model deployment, is designed to be modular, API-driven, and extendable.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/supreet02&quot;&gt; /u/supreet02 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cbqzlr/how_to_quickly_build_and_deploy_scalable_rag/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cbqzlr/how_to_quickly_build_and_deploy_scalable_rag/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cbqzlr</id><link href="https://www.reddit.com/r/LangChain/comments/1cbqzlr/how_to_quickly_build_and_deploy_scalable_rag/" /><updated>2024-04-24T06:02:44+00:00</updated><published>2024-04-24T06:02:44+00:00</published><title>How to quickly build and deploy scalable RAG applications?</title></entry><entry><author><name>/u/sarthak_uchiha</name><uri>https://www.reddit.com/user/sarthak_uchiha</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I am working on a project of bill of material where a client has recieved a mail which contains the catalogue I&amp;#39;d the quantity of the catalogue and it&amp;#39;s description,... The data could be in normal text , in a table , or in a image of the body (not in attachments )&lt;/p&gt; &lt;p&gt;How should I tackle this , like image could be many and some irrelevant ones like logo of company and other than there might be possibility that a duplicate data may present in text and image , and how to handle the thread of email &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/sarthak_uchiha&quot;&gt; /u/sarthak_uchiha &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cc3m04/bill_of_material_need_some_pov/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cc3m04/bill_of_material_need_some_pov/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cc3m04</id><link href="https://www.reddit.com/r/LangChain/comments/1cc3m04/bill_of_material_need_some_pov/" /><updated>2024-04-24T17:07:37+00:00</updated><published>2024-04-24T17:07:37+00:00</published><title>Bill of material need some PoV</title></entry><entry><author><name>/u/UpskillingDS17</name><uri>https://www.reddit.com/user/UpskillingDS17</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi, I am thinking of creating a LLM based application where questions can be asked in excel files and the files are small to medium size less than 10 MB. What is the best way to approach this problem ? In my team there are consultants who have 0 to little background on coding and SQL, so this could be a great help to them. Thanks&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/UpskillingDS17&quot;&gt; /u/UpskillingDS17 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cbwajs/creating_data_analytics_qa_platform_using_llm/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cbwajs/creating_data_analytics_qa_platform_using_llm/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cbwajs</id><link href="https://www.reddit.com/r/LangChain/comments/1cbwajs/creating_data_analytics_qa_platform_using_llm/" /><updated>2024-04-24T11:57:48+00:00</updated><published>2024-04-24T11:57:48+00:00</published><title>Creating data analytics Q&amp;A platform using LLM</title></entry><entry><author><name>/u/MintDrake</name><uri>https://www.reddit.com/user/MintDrake</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;&lt;strong&gt;Code of pure implementation through POST to local ollama&lt;/strong&gt; &lt;a href=&quot;http://localhost:11434/api/chat&quot;&gt;&lt;strong&gt;http://localhost:11434/api/chat&lt;/strong&gt;&lt;/a&gt; &lt;strong&gt;(3.2s):&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;import aiohttp from dataclasses import dataclass, field from typing import List import time start_time = time.time() @dataclass class Message: role: str content: str @dataclass class ChatHistory: messages: List[Message] = field(default_factory=list) def add_message(self, message: Message): self.messages.append(message) @dataclass class RequestData: model: str messages: List[dict] stream: bool = False @classmethod def from_params(cls, model, system_message, history): messages = [ {&amp;quot;role&amp;quot;: &amp;quot;system&amp;quot;, &amp;quot;content&amp;quot;: system_message}, *[{&amp;quot;role&amp;quot;: msg.role, &amp;quot;content&amp;quot;: msg.content} for msg in history.messages], ] return cls(model=model, messages=messages, stream=False) class LocalLlm: def __init__(self, model=&amp;#39;llama3:8b&amp;#39;, history=None, system_message=&amp;quot;You are a helpful assistant&amp;quot;): self.model = model self.history = history or ChatHistory() self.system_message = system_message async def ask(self, input=&amp;quot;&amp;quot;): if input: self.history.add_message(Message(role=&amp;quot;user&amp;quot;, content=input)) data = RequestData.from_params(self.model, self.system_message, self.history) url = &amp;quot;http://localhost:11434/api/chat&amp;quot; async with aiohttp.ClientSession() as session: async with session.post(url, json=data.__dict__) as response: result = await response.json() print(result[&amp;quot;message&amp;quot;][&amp;quot;content&amp;quot;]) if result[&amp;quot;done&amp;quot;]: ai_response = result[&amp;quot;message&amp;quot;][&amp;quot;content&amp;quot;] self.history.add_message(Message(role=&amp;quot;assistant&amp;quot;, content=ai_response)) return ai_response else: raise Exception(&amp;quot;Error generating response&amp;quot;) if __name__ == &amp;quot;__main__&amp;quot;: chat_history = ChatHistory(messages=[ Message(role=&amp;quot;system&amp;quot;, content=&amp;quot;You are a crazy pirate&amp;quot;), Message(role=&amp;quot;user&amp;quot;, content=&amp;quot;Can you tell me a joke?&amp;quot;) ]) llm = LocalLlm(history=chat_history) import asyncio response = asyncio.run(llm.ask()) print(response) print(llm.history) print(&amp;quot;--- %s seconds ---&amp;quot; % (time.time() - start_time)) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;code&gt;--- 3.2285749912261963 seconds ---&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Lang chain equivalent (3.5 s):&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;from langchain_core.messages import HumanMessage, SystemMessage, AIMessage, BaseMessage from langchain_community.chat_models.ollama import ChatOllama from langchain.memory import ChatMessageHistory import time start_time = time.time() class LocalLlm: def __init__(self, model=&amp;#39;llama3:8b&amp;#39;, messages=ChatMessageHistory(), system_message=&amp;quot;You are a helpful assistant&amp;quot;, context_length = 8000): self.model = ChatOllama(model=model, system=system_message, num_ctx=context_length) self.history = messages def ask(self, input=&amp;quot;&amp;quot;): if input: self.history.add_user_message(input) response = self.model.invoke(self.history.messages) self.history.add_ai_message(response) return response if __name__ == &amp;quot;__main__&amp;quot;: chat = ChatMessageHistory() chat.add_messages([ SystemMessage(content=&amp;quot;You are a crazy pirate&amp;quot;), HumanMessage(content=&amp;quot;Can you tell me a joke?&amp;quot;) ]) print(chat) llm = LocalLlm(messages=chat) print(llm.ask()) print(llm.history.messages) print(&amp;quot;--- %s seconds ---&amp;quot; % (time.time() - start_time)) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;code&gt;--- 3.469588279724121 seconds ---&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;So it&amp;#39;s 3.2 vs 3.469(nice) so the difference so 0.3s difference is nothing.&lt;br/&gt; Made this post because was so upset over &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c6zktz/llms_frameworks_langchain_llamaindex_griptape/&quot;&gt;this post&lt;/a&gt; after getting to know langchain and finally coming up with some results. I think it&amp;#39;s true that it&amp;#39;s not very suitable for serious development, but it&amp;#39;s perfect for theory crafting and experimenting, but anyways you can just write your own abstractions which you know. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/MintDrake&quot;&gt; /u/MintDrake &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cbj7gg/i_tested_langchain_vs_vanilla_speed/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cbj7gg/i_tested_langchain_vs_vanilla_speed/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cbj7gg</id><link href="https://www.reddit.com/r/LangChain/comments/1cbj7gg/i_tested_langchain_vs_vanilla_speed/" /><updated>2024-04-23T23:19:36+00:00</updated><published>2024-04-23T23:19:36+00:00</published><title>I tested LANGCHAIN vs VANILLA speed</title></entry><entry><author><name>/u/Unrealnooob</name><uri>https://www.reddit.com/user/Unrealnooob</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi, So I have a rag application/chatbot, uses conversationalretrivalqa chain from Langchain, say if for questions like &amp;#39;Hi&amp;#39; and all retrieval is happening, and its returning random documents How do I make the llm answer directly without retrieval for questions like this.? And one more thing how do I implement a memory(longterm will be better) with conversationalretrivalqa.from_llm chain..whatever I tried is not working, I tried with the Runnablehistory but that screws up the retrieval Does anyone have any workaround on that.? Any help will be appreciated ,thanks &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Unrealnooob&quot;&gt; /u/Unrealnooob &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cbq1jt/how_to_make_llm_differentiate_whether_to_retrieve/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cbq1jt/how_to_make_llm_differentiate_whether_to_retrieve/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cbq1jt</id><link href="https://www.reddit.com/r/LangChain/comments/1cbq1jt/how_to_make_llm_differentiate_whether_to_retrieve/" /><updated>2024-04-24T05:04:47+00:00</updated><published>2024-04-24T05:04:47+00:00</published><title>How to make llm differentiate whether to retrieve or not</title></entry><entry><author><name>/u/mofusa16</name><uri>https://www.reddit.com/user/mofusa16</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey Redditors! 🙋‍♂️&lt;/p&gt; &lt;p&gt;I came up with the idea of summarizing text with various large language models (LLMs). I intend to develop this fully-fledged application (including a register page, login page, database etc.) using either Python, JavaScript, or both. Can you advise me on which framework would be most suitable for such an endeavor? I&amp;#39;m seeking recommendations on frameworks that excel in constructing this type of application. Some colleagues have proposed trying Flask, Gradio, or Django. Please share your insights on which framework would be optimal for this project, and kindly provide reasons to support your suggestion.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/mofusa16&quot;&gt; /u/mofusa16 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cbv3dv/seeking_advice_which_framework_is_best_suited_for/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cbv3dv/seeking_advice_which_framework_is_best_suited_for/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cbv3dv</id><link href="https://www.reddit.com/r/LangChain/comments/1cbv3dv/seeking_advice_which_framework_is_best_suited_for/" /><updated>2024-04-24T10:49:36+00:00</updated><published>2024-04-24T10:49:36+00:00</published><title>Seeking Advice: Which Framework is best suited for building GenAI Web App?</title></entry><entry><author><name>/u/QueRoub</name><uri>https://www.reddit.com/user/QueRoub</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I have built a RAG application with my own PDF documents.&lt;/p&gt; &lt;p&gt;Some of the answers are not correct, usually they are from wrong documents even if the right ones have been retrieved.&lt;/p&gt; &lt;p&gt;What is the right way to approach it?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/QueRoub&quot;&gt; /u/QueRoub &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cbrpms/how_to_finetune_the_answers_of_llm_in_a_rag/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cbrpms/how_to_finetune_the_answers_of_llm_in_a_rag/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cbrpms</id><link href="https://www.reddit.com/r/LangChain/comments/1cbrpms/how_to_finetune_the_answers_of_llm_in_a_rag/" /><updated>2024-04-24T06:50:23+00:00</updated><published>2024-04-24T06:50:23+00:00</published><title>How to fine-tune the answers of LLM in a RAG application</title></entry><entry><author><name>/u/AddendumLow4692</name><uri>https://www.reddit.com/user/AddendumLow4692</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi all,&lt;/p&gt; &lt;p&gt;First time using LangChain, I&amp;#39;m following &lt;a href=&quot;https://www.youtube.com/watch?v=BrsocJb-fAo&amp;amp;t=631s&quot;&gt;a guide&lt;/a&gt; and I&amp;#39;m getting this error, does anyone know what might be wrong? I&amp;#39;m using Pinecone along with this, I&amp;#39;m not sure if that makes a difference.&lt;/p&gt; &lt;p&gt;For my Pinecone API environment I&amp;#39;m using &amp;quot;us-east-1&amp;quot; - I&amp;#39;m unsure if this is the right format?&lt;/p&gt; &lt;p&gt;I&amp;#39;d be very grateful for any input!&lt;/p&gt; &lt;p&gt;Many thanks in advance :)&lt;/p&gt; &lt;p&gt;So this is my code:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;from langchain_community.vectorstores import DocArrayInMemorySearch vectorstore1 = DocArrayInMemorySearch.from_texts( [ &amp;quot;Mary&amp;#39;s sister is Susana&amp;quot;, &amp;quot;John and Tommy are brothers&amp;quot;, &amp;quot;Patricia likes white cars&amp;quot;, &amp;quot;Pedro&amp;#39;s mother is a teacher&amp;quot;, &amp;quot;Lucia drives an Audi&amp;quot;, &amp;quot;Mary has two siblings&amp;quot;, ], embedding=embeddings, ) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;And I&amp;#39;m getting this error:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;AttributeError Traceback (most recent call last) Cell In[58], line 3 1 from langchain_community.vectorstores import DocArrayInMemorySearch ----&amp;gt; 3 vectorstore1 = DocArrayInMemorySearch.from_texts( 4 [ 5 &amp;quot;Mary&amp;#39;s sister is Susana&amp;quot;, 6 &amp;quot;John and Tommy are brothers&amp;quot;, 7 &amp;quot;Patricia likes white cars&amp;quot;, 8 &amp;quot;Pedro&amp;#39;s mother is a teacher&amp;quot;, 9 &amp;quot;Lucia drives an Audi&amp;quot;, 10 &amp;quot;Mary has two siblings&amp;quot;, 11 ], 12 embedding=embeddings, 13 ) File ~\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\LocalCache\local-packages\Python312\site-packages\langchain_community\vectorstores\docarray\in_memory.py:68, in DocArrayInMemorySearch.from_texts(cls, texts, embedding, metadatas, **kwargs) 46 u/classmethod 47 def from_texts( 48 cls, (...) 52 **kwargs: Any, 53 ) -&amp;gt; DocArrayInMemorySearch: 54 &amp;quot;&amp;quot;&amp;quot;Create an DocArrayInMemorySearch store and insert data. 55 ... ---&amp;gt; 46 return Generic.__class_getitem__.__func__(cls, item) # type: ignore 47 # this do nothing that checking that item is valid type var or str 48 if not issubclass(item, BaseDoc): AttributeError: &amp;#39;builtin_function_or_method&amp;#39; object has no attribute &amp;#39;__func__&amp;#39; &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/AddendumLow4692&quot;&gt; /u/AddendumLow4692 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cbuikm/error_builtin_function_or_method_object_has_no/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cbuikm/error_builtin_function_or_method_object_has_no/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cbuikm</id><link href="https://www.reddit.com/r/LangChain/comments/1cbuikm/error_builtin_function_or_method_object_has_no/" /><updated>2024-04-24T10:11:57+00:00</updated><published>2024-04-24T10:11:57+00:00</published><title>Error: 'builtin_function_or_method' object has no attribute '__func__'</title></entry><entry><author><name>/u/Competitive-Ninja423</name><uri>https://www.reddit.com/user/Competitive-Ninja423</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I am working on project where I have multiple documents to process using output parser of Lang Chain, as I have Mutiple it takes time, so to reduce time I am planning to process each doc in parallel to reduce the time.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Competitive-Ninja423&quot;&gt; /u/Competitive-Ninja423 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cbqwg2/does_anybody_have_good_tutorial_or_page_or_repo/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cbqwg2/does_anybody_have_good_tutorial_or_page_or_repo/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cbqwg2</id><link href="https://www.reddit.com/r/LangChain/comments/1cbqwg2/does_anybody_have_good_tutorial_or_page_or_repo/" /><updated>2024-04-24T05:57:25+00:00</updated><published>2024-04-24T05:57:25+00:00</published><title>Does anybody have good tutorial or page or repo which targets the Runnable Parallels of Lang chain?</title></entry><entry><author><name>/u/Apart-Damage143</name><uri>https://www.reddit.com/user/Apart-Damage143</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Does PydanticOutputParser only function with ChatGPT from OpenAI, or does it extend its support to other large language models (LLMs) as well? &lt;/p&gt; &lt;p&gt;I&amp;#39;m particularly interested in using it with models available through groq and wondering if anyone has explored this compatibility. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Apart-Damage143&quot;&gt; /u/Apart-Damage143 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cbr66a/question_pydanticoutputparser_compatibility/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cbr66a/question_pydanticoutputparser_compatibility/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cbr66a</id><link href="https://www.reddit.com/r/LangChain/comments/1cbr66a/question_pydanticoutputparser_compatibility/" /><updated>2024-04-24T06:14:44+00:00</updated><published>2024-04-24T06:14:44+00:00</published><title>Question: PydanticOutputParser Compatibility Beyond ChatOpenAI?</title></entry><entry><author><name>/u/phantom69_ftw</name><uri>https://www.reddit.com/user/phantom69_ftw</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Basically if I have a pdf of 100 pages and to answer my question I need 30 diff chunks across diff pages. Now if my top_k is set to 20. How will this ever be possible?&lt;/p&gt; &lt;p&gt;Like in general, isn&amp;#39;t this a issue with RAGs? How can I know how many chunks are needed to answer a question? If it&amp;#39;s less than whatever topk I set, it&amp;#39;s fine. But what if there are more?&lt;/p&gt; &lt;p&gt;Is this a limitation of RAG? If no, how to solve for this? If yes, what other ways can I explore?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/phantom69_ftw&quot;&gt; /u/phantom69_ftw &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cbcln9/how_to_solve_if_relevant_docs_max_top_k/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cbcln9/how_to_solve_if_relevant_docs_max_top_k/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cbcln9</id><link href="https://www.reddit.com/r/LangChain/comments/1cbcln9/how_to_solve_if_relevant_docs_max_top_k/" /><updated>2024-04-23T18:52:55+00:00</updated><published>2024-04-23T18:52:55+00:00</published><title>How to solve if relevant docs &gt; max top_k ?</title></entry><entry><author><name>/u/Dry-Magician1415</name><uri>https://www.reddit.com/user/Dry-Magician1415</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I am working with LangGraph and used the multi agent collaboration example (&lt;a href=&quot;https://github.com/langchain-ai/langgraph/blob/main/examples/multi_agent/multi-agent-collaboration.ipynb&quot;&gt;github&lt;/a&gt;). To the create_agent(...) helper method, I added the following so that the response from LLMs would match a format I can use.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;parser = JsonOutputParser(pydantic_object=MyResponse) ... return prompt | llm.bind_functions(functions) | parser &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This works and provides a python dict that is easy to work with. So far so good.&lt;/p&gt; &lt;p&gt;The issue is when the LLM wants to use the Tavily search tool which is failing at &lt;code&gt;result = agent.invoke(state)&lt;/code&gt; with &lt;strong&gt;OutputParserException(&amp;#39;Invalid json output: &amp;#39;)&lt;/strong&gt; because the llm obviously wants to call Tavily which has its own shape being something like that:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;tavily_search_results_json&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Which obviously doesnt conform to the MyResponse class, but the parser is kicking in anyway.&lt;/p&gt; &lt;p&gt;I guess I can make a Researcher agent which doesnt have the JsonOutputParser and a Worker that doesn&amp;#39;t have the Tavily tool but, I figure there must be a way to get JsonOutputParser to work with tools like Tavily. I mean they can&amp;#39;t just be outright incompatible (i.e. if you have an agent with a parser, it can&amp;#39;t have the tavily tool).&lt;/p&gt; &lt;p&gt;This is my full create_agent function if anybody knows what it should look like in terms of getting the parsers and tools to play nice:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;def create_task_agent(llm, tools, system_message: str): &amp;quot;&amp;quot;&amp;quot;Create an agent.&amp;quot;&amp;quot;&amp;quot; functions = [convert_to_openai_function(t) for t in tools] functions.append(convert_pydantic_to_openai_function(TaskResponse)) prompt = ChatPromptTemplate.from_messages( [ ( &amp;quot;system&amp;quot;, &amp;quot;some message&amp;quot;, ), MessagesPlaceholder(variable_name=&amp;quot;messages&amp;quot;), ] ) parser = JsonOutputParser(pydantic_object=TaskResponse) prompt = prompt.partial(system_message=system_message) prompt = prompt.partial(task_response=&amp;quot;TaskResponse&amp;quot;) prompt = prompt.partial(tool_names=&amp;quot;, &amp;quot;.join([tool.name for tool in tools])) prompt = prompt.partial(format_instructions=parser) return prompt | llm.bind_functions(functions) | parser &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Dry-Magician1415&quot;&gt; /u/Dry-Magician1415 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cbodkf/jsonoutputparser_conflicting_with_tavily/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cbodkf/jsonoutputparser_conflicting_with_tavily/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cbodkf</id><link href="https://www.reddit.com/r/LangChain/comments/1cbodkf/jsonoutputparser_conflicting_with_tavily/" /><updated>2024-04-24T03:30:42+00:00</updated><published>2024-04-24T03:30:42+00:00</published><title>JsonOutputParser conflicting with Tavily</title></entry><entry><author><name>/u/MarkusWeierstrass</name><uri>https://www.reddit.com/user/MarkusWeierstrass</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey all. I&amp;#39;m building a simple chatbot that will let users hold their own documents to use in RAG; basically I just want them to be able to ask questions related to what&amp;#39;s on their own files. I&amp;#39;m using LangChain of course, using postgres with the pgvector extension. &lt;/p&gt; &lt;p&gt;My question is, what&amp;#39;s the most optimized way to design the documents table(s) in order for users to only be able to search their own files? Do you create separate doc tables for each user? Do you filter through metadata or some other technique? Metadata filtering in particular doesn&amp;#39;t look like it&amp;#39;d be too optimized, so I&amp;#39;m just looking into how best to think about storing and retrieving from a vector store for this use case. I don&amp;#39;t want the bot to be able to find the answer in another user&amp;#39;s files.&lt;/p&gt; &lt;p&gt;Or am I just thinking about the whole thing in the wrong way and is there a better way to structure all this? &lt;/p&gt; &lt;p&gt;Thanks a lot in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/MarkusWeierstrass&quot;&gt; /u/MarkusWeierstrass &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cbdk3m/how_to_structure_the_vector_store_and_retrieval/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cbdk3m/how_to_structure_the_vector_store_and_retrieval/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cbdk3m</id><link href="https://www.reddit.com/r/LangChain/comments/1cbdk3m/how_to_structure_the_vector_store_and_retrieval/" /><updated>2024-04-23T19:31:11+00:00</updated><published>2024-04-23T19:31:11+00:00</published><title>How to structure the vector store and retrieval for user files RAG?</title></entry><entry><author><name>/u/ravediamond000</name><uri>https://www.reddit.com/user/ravediamond000</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hello guys,&lt;/p&gt; &lt;p&gt;Just wrote a new blog post explaining Langchain LCEL in a easier manner: &lt;a href=&quot;https://www.metadocs.co/&quot;&gt;link&lt;/a&gt;.&lt;br/&gt; I really love LCEL (feels a little like functional programing right !?) and wanted to try to explain it in a simpler way.&lt;br/&gt; Enjoy.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/ravediamond000&quot;&gt; /u/ravediamond000 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cb01ch/langchain_lcel_explained_the_easy_way/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cb01ch/langchain_lcel_explained_the_easy_way/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cb01ch</id><link href="https://www.reddit.com/r/LangChain/comments/1cb01ch/langchain_lcel_explained_the_easy_way/" /><updated>2024-04-23T09:12:48+00:00</updated><published>2024-04-23T09:12:48+00:00</published><title>Langchain LCEL explained the easy way</title></entry><entry><author><name>/u/StrayyLight</name><uri>https://www.reddit.com/user/StrayyLight</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;m using llamaindex for a multilingual database retriever system and using claude as the provider. I&amp;#39;m interested in integrating external apis( function calling) and knowledge graphs.&lt;/p&gt; &lt;p&gt;Separately it&amp;#39;d also be helpful to have the ability to manage states within a conversation and langgraph seems to meet the criteria.&lt;/p&gt; &lt;p&gt;Should I switch to langchain and rewrite my early stage code? Does langchain function calling work well with Claude? does llamaindex offer langgraph like abilities or good integration with neo4j?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/StrayyLight&quot;&gt; /u/StrayyLight &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cbfeci/langchain_vs_llamaindex/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cbfeci/langchain_vs_llamaindex/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cbfeci</id><link href="https://www.reddit.com/r/LangChain/comments/1cbfeci/langchain_vs_llamaindex/" /><updated>2024-04-23T20:44:00+00:00</updated><published>2024-04-23T20:44:00+00:00</published><title>Langchain vs llamaindex</title></entry><entry><author><name>/u/Money_Mycologist4939</name><uri>https://www.reddit.com/user/Money_Mycologist4939</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cb68b9/langsmith_render_of_retrieved_documents/&quot;&gt; &lt;img src=&quot;https://b.thumbs.redditmedia.com/JmafAij99N282f2vvy_q1p_h8FB_SzZHa5Ipo8uMwPg.jpg&quot; alt=&quot;Langsmith render of retrieved documents&quot; title=&quot;Langsmith render of retrieved documents&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I have been using langsmith for controling the retrieving step of my rag application. And it&amp;#39;s nice because it has a render that format the raw langchain docs in a more readable format. &lt;/p&gt; &lt;p&gt;The problem is that since I changed the format of my langchain docs, adding more metadata, this feature does not work anymore. Do anyone has got any advice on what&amp;#39;s the right format compatible to the render??&lt;/p&gt; &lt;p&gt;now: &lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/7leugfcup8wc1.png?width=1266&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f7738461ab2add7e587aeb9aed415a5b2e1f6c20&quot;&gt;https://preview.redd.it/7leugfcup8wc1.png?width=1266&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f7738461ab2add7e587aeb9aed415a5b2e1f6c20&lt;/a&gt;&lt;/p&gt; &lt;p&gt;BEFORE:&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/8s86iy6xp8wc1.png?width=1287&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=302f1f3b2923f428288c59c29f31327d5d9b6db0&quot;&gt;https://preview.redd.it/8s86iy6xp8wc1.png?width=1287&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=302f1f3b2923f428288c59c29f31327d5d9b6db0&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Money_Mycologist4939&quot;&gt; /u/Money_Mycologist4939 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cb68b9/langsmith_render_of_retrieved_documents/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cb68b9/langsmith_render_of_retrieved_documents/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1cb68b9</id><media:thumbnail url="https://b.thumbs.redditmedia.com/JmafAij99N282f2vvy_q1p_h8FB_SzZHa5Ipo8uMwPg.jpg" /><link href="https://www.reddit.com/r/LangChain/comments/1cb68b9/langsmith_render_of_retrieved_documents/" /><updated>2024-04-23T14:35:42+00:00</updated><published>2024-04-23T14:35:42+00:00</published><title>Langsmith render of retrieved documents</title></entry><entry><author><name>/u/furyacer</name><uri>https://www.reddit.com/user/furyacer</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I am trying to build a chatbot using RAG and LangChain that will update the PDFs based on the user prompt and the pdfs will be stored in a db (chromedb) that will be connected to the chatbot. I&amp;#39;m planning to use OpenAI for chunking and indexing information that will be analyzed by the bot. &lt;/p&gt; &lt;p&gt;It will be helpful if anyone can tell me how to proceed further with this. I have only found projects and repos which focus on QA chatbots so I just want to extend this project to include this functionality.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/furyacer&quot;&gt; /u/furyacer &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cb35fa/chatbot_using_rag_to_update_pdfs/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cb35fa/chatbot_using_rag_to_update_pdfs/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cb35fa</id><link href="https://www.reddit.com/r/LangChain/comments/1cb35fa/chatbot_using_rag_to_update_pdfs/" /><updated>2024-04-23T12:18:55+00:00</updated><published>2024-04-23T12:18:55+00:00</published><title>Chat-bot using RAG to update PDFs</title></entry></feed>