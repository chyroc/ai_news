<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/"><category term="LangChain" label="r/LangChain"/><updated>2024-05-09T16:08:17+00:00</updated><icon>https://www.redditstatic.com/icon.png/</icon><id>/r/LangChain.rss</id><link rel="self" href="https://www.reddit.com/r/LangChain.rss" type="application/atom+xml" /><link rel="alternate" href="https://www.reddit.com/r/LangChain" type="text/html" /><subtitle>LangChain is an open-source framework and developer toolkit that helps developers get LLM applications from prototype to production. It is available for Python and Javascript at https://www.langchain.com/.</subtitle><title>LangChain</title><entry><author><name>/u/zchaarm</name><uri>https://www.reddit.com/user/zchaarm</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;A place for members of &lt;a href=&quot;/r/LangChain&quot;&gt;r/LangChain&lt;/a&gt; to chat with each other&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/zchaarm&quot;&gt; /u/zchaarm &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_10ljho9</id><link href="https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/" /><updated>2023-01-26T04:33:05+00:00</updated><published>2023-01-26T04:33:05+00:00</published><title>r/LangChain Lounge</title></entry><entry><author><name>/u/chaitu9701</name><uri>https://www.reddit.com/user/chaitu9701</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;My org has this usecase to build a rag to answer questions. In rag works great given that I given it a lot of instructions(prompts). One demand that rag isn&amp;#39;t able to fullfill is to never mention document reference in the response.&lt;/p&gt; &lt;p&gt;Eg. 1) The document does not mention how to ... 2) you can view the steps on page 60 in the document.&lt;/p&gt; &lt;p&gt;Any prompt suggestions to overcome this particular scenario. The rag should never share the source of its response &lt;/p&gt; &lt;p&gt;My pipeline&lt;/p&gt; &lt;p&gt;1) Pdf Document Langchain &lt;/p&gt; &lt;p&gt;2) Qdrant for retriever&lt;/p&gt; &lt;p&gt;3) Chat gpt3.5 turbo 16k for llm&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/chaitu9701&quot;&gt; /u/chaitu9701 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cnxfnz/rag_response_always_includes_reference_to_document/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cnxfnz/rag_response_always_includes_reference_to_document/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cnxfnz</id><link href="https://www.reddit.com/r/LangChain/comments/1cnxfnz/rag_response_always_includes_reference_to_document/" /><updated>2024-05-09T13:34:29+00:00</updated><published>2024-05-09T13:34:29+00:00</published><title>Rag response always includes reference to document</title></entry><entry><author><name>/u/Alarming-East1193</name><uri>https://www.reddit.com/user/Alarming-East1193</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;I&amp;#39;m performing prompt engineering for my Phi3-mini-4K-instruct model and I&amp;#39;m using Anything LLM for my front end application.&lt;/p&gt; &lt;p&gt;The thing is i want my model to only answer query from the context data provided (PDFs) and Don&amp;#39;t give any answers from his own knowledge or external source. The prompt I&amp;#39;m giving is:&lt;/p&gt; &lt;p&gt;&amp;quot; &amp;quot; &amp;quot;You are an assistant for question answering tasks. use the following pieces of retrieved context to answer the question. If the answer isn&amp;#39;t present in the knowledge base, refrain from providing an answer based on your own knowledge. Instead of answer to such question, indicate that relevant information isn&amp;#39;t available. Use three sentences maximum to keep the answer concise&amp;quot; &amp;quot; &amp;quot;&lt;/p&gt; &lt;p&gt;After this promopt I&amp;#39;m still getting answers for the questions which are irrelevant and from outside of the knowledge base provided.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Alarming-East1193&quot;&gt; /u/Alarming-East1193 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cntf8t/prompt_engineering_on_phi3mini4kinstruct_model/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cntf8t/prompt_engineering_on_phi3mini4kinstruct_model/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cntf8t</id><link href="https://www.reddit.com/r/LangChain/comments/1cntf8t/prompt_engineering_on_phi3mini4kinstruct_model/" /><updated>2024-05-09T09:54:38+00:00</updated><published>2024-05-09T09:54:38+00:00</published><title>Prompt Engineering on Phi3-mini-4K-instruct Model</title></entry><entry><author><name>/u/powderarc</name><uri>https://www.reddit.com/user/powderarc</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey y‚Äôall üôåüèº I‚Äôve been using some prompt management tools (Humanloop and Braintrust Data) for a few of my recent projects. Overall, they‚Äôre powerful tools but I‚Äôve hit a few snags that make me wonder if a better tool can be built. &lt;/p&gt; &lt;p&gt;I&amp;#39;m really interested in hearing about others&amp;#39; experiences with similar tools, so if you‚Äôre willing to share, that would be awesome! ü´∂üèº &lt;/p&gt; &lt;ul&gt; &lt;li&gt;What tool are you using? &lt;/li&gt; &lt;li&gt;How much does it cost you? &lt;/li&gt; &lt;li&gt;What kind of issues have you run into while using this tool?&lt;/li&gt; &lt;li&gt;Are there specific features that you feel are lacking? &lt;/li&gt; &lt;li&gt;If you could build a wish list of features, what would they be? üåü&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/powderarc&quot;&gt; /u/powderarc &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1co09cc/limitations_with_existing_prompt_management_tools/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1co09cc/limitations_with_existing_prompt_management_tools/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1co09cc</id><link href="https://www.reddit.com/r/LangChain/comments/1co09cc/limitations_with_existing_prompt_management_tools/" /><updated>2024-05-09T15:39:36+00:00</updated><published>2024-05-09T15:39:36+00:00</published><title>Limitations with existing prompt management tools?</title></entry><entry><author><name>/u/Informal-Victory8655</name><uri>https://www.reddit.com/user/Informal-Victory8655</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cnu7zh/hi_guyswhat_is_the_use_of_parameter_k_in/&quot;&gt; &lt;img src=&quot;https://b.thumbs.redditmedia.com/6rhoXyDpNWbgMOkDF20HATQNnc_QYPsXlAv9TbixPUY.jpg&quot; alt=&quot;Hi guys,what is the use of parameter K in ConversationEntityMemory?&quot; title=&quot;Hi guys,what is the use of parameter K in ConversationEntityMemory?&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;&lt;a href=&quot;https://api.python.langchain.com/en/latest/memory/langchain.memory.entity.ConversationEntityMemory.html&quot;&gt;https://api.python.langchain.com/en/latest/memory/langchain.memory.entity.ConversationEntityMemory.html&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/ufmg5yvirdzc1.png?width=488&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ec24354784b8f84a445c4965f6064b9ed0cd838a&quot;&gt;https://preview.redd.it/ufmg5yvirdzc1.png?width=488&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ec24354784b8f84a445c4965f6064b9ed0cd838a&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Informal-Victory8655&quot;&gt; /u/Informal-Victory8655 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cnu7zh/hi_guyswhat_is_the_use_of_parameter_k_in/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cnu7zh/hi_guyswhat_is_the_use_of_parameter_k_in/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1cnu7zh</id><media:thumbnail url="https://b.thumbs.redditmedia.com/6rhoXyDpNWbgMOkDF20HATQNnc_QYPsXlAv9TbixPUY.jpg" /><link href="https://www.reddit.com/r/LangChain/comments/1cnu7zh/hi_guyswhat_is_the_use_of_parameter_k_in/" /><updated>2024-05-09T10:46:02+00:00</updated><published>2024-05-09T10:46:02+00:00</published><title>Hi guys,what is the use of parameter K in ConversationEntityMemory?</title></entry><entry><author><name>/u/Omervx</name><uri>https://www.reddit.com/user/Omervx</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cntvmi/having_a_hard_time_with_templates/&quot;&gt; &lt;img src=&quot;https://external-preview.redd.it/o8AUCLRdlCnANPZeJBedQZzE7o4EKpgnQNYkyDtKBP8.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=74efa463daa026a2d0c19f410b6af492e5d26877&quot; alt=&quot;Having a hard time with templates &quot; title=&quot;Having a hard time with templates &quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey everyone, I&amp;#39;m diving into LangChain and AI for the first time, so please bear with me as I navigate through this learning curve. &lt;/p&gt; &lt;p&gt;I&amp;#39;ve managed to create a small CLI bot with memory, and you can check out the GitHub link here: [GitHub Link] &lt;a href=&quot;https://github.com/oovaa/ChatPDF/blob/main/exper%2Fcommandr.js&quot;&gt;https://github.com/oovaa/ChatPDF/blob/main/exper%2Fcommandr.js&lt;/a&gt;. &lt;/p&gt; &lt;p&gt;However, I&amp;#39;m encountering an issue where the bot interprets my name input as a command rather than part of the conversation. It seems to struggle with understanding the context of the conversation. I&amp;#39;d really appreciate some guidance on how to fix this. Thanks in advance for any help you can provide!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Omervx&quot;&gt; /u/Omervx &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://github.com/oovaa/ChatPDF/blob/main/exper%2Fcommandr.js&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cntvmi/having_a_hard_time_with_templates/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1cntvmi</id><media:thumbnail url="https://external-preview.redd.it/o8AUCLRdlCnANPZeJBedQZzE7o4EKpgnQNYkyDtKBP8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=74efa463daa026a2d0c19f410b6af492e5d26877" /><link href="https://www.reddit.com/r/LangChain/comments/1cntvmi/having_a_hard_time_with_templates/" /><updated>2024-05-09T10:23:58+00:00</updated><published>2024-05-09T10:23:58+00:00</published><title>Having a hard time with templates</title></entry><entry><author><name>/u/Putrid_Spinach3961</name><uri>https://www.reddit.com/user/Putrid_Spinach3961</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I have access to gpt 4 models in azure openai platform, can i convert react 18.2.0 code to typescript 4.9.5 using langchain and azure openai gpt 4 model. I know langchain is not necessary for conversion, but the data cut off for gpt 4 model is 2021 and the latest version might not be used for train the gpt4 model. So do i have any option to use any langchain tool like chains or agent for this conversion as the model might need external agent support.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Putrid_Spinach3961&quot;&gt; /u/Putrid_Spinach3961 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cnslbe/react_to_typescript/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cnslbe/react_to_typescript/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cnslbe</id><link href="https://www.reddit.com/r/LangChain/comments/1cnslbe/react_to_typescript/" /><updated>2024-05-09T08:55:47+00:00</updated><published>2024-05-09T08:55:47+00:00</published><title>React to Typescript</title></entry><entry><author><name>/u/MoronSlayer42</name><uri>https://www.reddit.com/user/MoronSlayer42</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Building an LLM app and using Unstructured for parsing data. From the vector store, have can I create a conversational agent that has 2 tools for intent classification? I want to create an agent so that according to user query in my application, the backend either returns a conversational output (chat) + shows sources or for some other type of user queries it returns only documents (no chat) akin to a generic Google search. After I create these two tools, I also want additional tools for the agent to recognize whether the user query is a simple greeting or whether there is any abusive language in the query. Any approaches, suggestions or examples would be helpful.&lt;/p&gt; &lt;p&gt;Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/MoronSlayer42&quot;&gt; /u/MoronSlayer42 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cnqsxj/langchain_agents_tools_for_intent_classification/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cnqsxj/langchain_agents_tools_for_intent_classification/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cnqsxj</id><link href="https://www.reddit.com/r/LangChain/comments/1cnqsxj/langchain_agents_tools_for_intent_classification/" /><updated>2024-05-09T06:47:15+00:00</updated><published>2024-05-09T06:47:15+00:00</published><title>Langchain agents - tools for intent classification</title></entry><entry><author><name>/u/MediocreMolasses9542</name><uri>https://www.reddit.com/user/MediocreMolasses9542</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cn4pyt/i_made_a_tool_that_allows_you_to_searchchat_with/&quot;&gt; &lt;img src=&quot;https://external-preview.redd.it/NDNhdDhxcmxnN3pjMW2L8kqVelOF3uY9ZoVdnni9DgED4Czo_rSma0qXHZfi.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=973474be24330b917d0e3a16cac97b40fe56f481&quot; alt=&quot;I made a tool that allows you to search/chat with the LangChain codebase&quot; title=&quot;I made a tool that allows you to search/chat with the LangChain codebase&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/MediocreMolasses9542&quot;&gt; /u/MediocreMolasses9542 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://v.redd.it/axiobrrlg7zc1&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cn4pyt/i_made_a_tool_that_allows_you_to_searchchat_with/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1cn4pyt</id><media:thumbnail url="https://external-preview.redd.it/NDNhdDhxcmxnN3pjMW2L8kqVelOF3uY9ZoVdnni9DgED4Czo_rSma0qXHZfi.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=973474be24330b917d0e3a16cac97b40fe56f481" /><link href="https://www.reddit.com/r/LangChain/comments/1cn4pyt/i_made_a_tool_that_allows_you_to_searchchat_with/" /><updated>2024-05-08T13:35:26+00:00</updated><published>2024-05-08T13:35:26+00:00</published><title>I made a tool that allows you to search/chat with the LangChain codebase</title></entry><entry><author><name>/u/pikaLuffy</name><uri>https://www.reddit.com/user/pikaLuffy</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;To my fellow experts, I am having trouble to extract tables from PDF. I know there are some packages out there that claim to do the job, but I can‚Äôt seem to get good results from it. Moreover, my work laptop kinda restrict on installation of softwares and the most I can do is download open source library package. Wondering if there are any straightforward ways on how to do that ? Or I have to a rite the code from scratch to process the tables but there seem to be many types of tables I need to consider. &lt;/p&gt; &lt;p&gt;Here are the packages I tried and the reasons why they didn‚Äôt work. &lt;/p&gt; &lt;ol&gt; &lt;li&gt;Pymupdf- messy table formatting, can misinterpret title of the page as column headers&lt;/li&gt; &lt;li&gt;Tabula/pdfminer- same performance as Pymupdf &lt;/li&gt; &lt;li&gt;Camelot- I can‚Äôt seem to get it to work given that it needs to download Ghostscript and tkinter, which require admin privilege which is blocked in my work laptop. &lt;/li&gt; &lt;li&gt;Unstructured- complicated setup as require a lot of dependencies and they are hard to set up &lt;/li&gt; &lt;li&gt;Llamaparse from llama: need cloud api key which is blocked &lt;/li&gt; &lt;/ol&gt; &lt;p&gt;I tried converting pdf to html but can‚Äôt seem to identify the tables very well. &lt;/p&gt; &lt;p&gt;Please help a beginner ü•∫&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/pikaLuffy&quot;&gt; /u/pikaLuffy &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cn0z11/extract_tables_from_pdf_for_rag/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cn0z11/extract_tables_from_pdf_for_rag/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cn0z11</id><link href="https://www.reddit.com/r/LangChain/comments/1cn0z11/extract_tables_from_pdf_for_rag/" /><updated>2024-05-08T10:10:38+00:00</updated><published>2024-05-08T10:10:38+00:00</published><title>Extract tables from PDF for RAG</title></entry><entry><author><name>/u/xandie985</name><uri>https://www.reddit.com/user/xandie985</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;During runtime, I can see, what chain is being executed. I need that information being displayed for further steps. Do you know how can I access the output text while the code is being executed?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/xandie985&quot;&gt; /u/xandie985 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cncubh/how_can_i_access_the_output_while_the_code_is/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cncubh/how_can_i_access_the_output_while_the_code_is/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cncubh</id><link href="https://www.reddit.com/r/LangChain/comments/1cncubh/how_can_i_access_the_output_while_the_code_is/" /><updated>2024-05-08T19:17:00+00:00</updated><published>2024-05-08T19:17:00+00:00</published><title>How can I access the output while the code is running?</title></entry><entry><author><name>/u/Different_Star9899</name><uri>https://www.reddit.com/user/Different_Star9899</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;So, I made an information extraction system where basically, when I upload a technical data sheet of a construction material through streamlit, the LLM generates a text string in .csv format containing the attributes of the material that I defined to extract through the prompts (which are already embedded so it&amp;#39;s not a Q&amp;amp;A system). And I linked the response with Gspread so that the string is automatically exported to google sheets in correct order. &lt;/p&gt; &lt;p&gt;I tested and the prototype is working as intended but the problem is with the evaluation of the system. Since it&amp;#39;s part of a thesis project, I have to demonstrate how well the proposed system is performing based on certain metrics, but I am finding difficulty in looking for a quantitively evaluated method that suits this use case scenario. What I want to do is to compare the performances of different LLMs that are being used for the generation as well as assessing the retrieval portion of the system.&lt;/p&gt; &lt;p&gt;Obviously, I&amp;#39;m not well-versed in this area so any help is appreciated. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Different_Star9899&quot;&gt; /u/Different_Star9899 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cn6lau/evaluation_for_rag_for_extraction_and_restricted/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cn6lau/evaluation_for_rag_for_extraction_and_restricted/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cn6lau</id><link href="https://www.reddit.com/r/LangChain/comments/1cn6lau/evaluation_for_rag_for_extraction_and_restricted/" /><updated>2024-05-08T14:56:25+00:00</updated><published>2024-05-08T14:56:25+00:00</published><title>Evaluation for RAG for extraction and restricted responses</title></entry><entry><author><name>/u/MrMapleFarmer</name><uri>https://www.reddit.com/user/MrMapleFarmer</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hello AI peeps, I need some help/advice. I‚Äôm building a fairly comprehensive chatbot which includes a RAG QnA component. All knowledge base data is in an Airtable, where each row/record is another piece of knowledge. &lt;/p&gt; &lt;p&gt;The plan is to vectorize the knowledge base to Pinecone via Flowise Upsert and then retrieve with OpenAI Embeddings. &lt;/p&gt; &lt;p&gt;The main issue is that I can‚Äôt figure out how to use the columns as seperate metadata keys instead of all being vectorized in 1 piece. Is there an easy solution to accomplish this? Is there a better approach overall to convert the data from Airtable into a RAG knowledge base? Any help would be appreciated! I mentioned Flowise because it‚Äôs the simplest way to use Langchain.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/MrMapleFarmer&quot;&gt; /u/MrMapleFarmer &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cnhb5a/using_airtable_data_as_a_vector_database_for/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cnhb5a/using_airtable_data_as_a_vector_database_for/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cnhb5a</id><link href="https://www.reddit.com/r/LangChain/comments/1cnhb5a/using_airtable_data_as_a_vector_database_for/" /><updated>2024-05-08T22:24:44+00:00</updated><published>2024-05-08T22:24:44+00:00</published><title>Using Airtable data as a vector database for Chatbot Knowledge Base</title></entry><entry><author><name>/u/Sad-Anywhere-2204</name><uri>https://www.reddit.com/user/Sad-Anywhere-2204</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I am working on a ReAct agent that will have a couple of pre-defined tools to perform specific actions BUT we need to have some kind of &amp;quot;default&amp;quot; or &amp;quot;else&amp;quot; tool, what I mean is: if non of the pre-defined tools is selected by the agent then it will try to answer the user query using the &amp;quot;else&amp;quot; tool, the idea is that there are some pre-defined and well known actions that will be executed by the agent when tue user query matches those fine, but if there is not a good match we still want the agent to be able to come up with the best answer possible(inbstead of something like: I cannot answer this question because I don&amp;#39;t have a tool for it). Any ideas? I&amp;#39;m thinking on something as a&lt;br/&gt; &lt;code&gt;GeneralHandlerTool(BaseTool):&lt;/code&gt;&lt;br/&gt; &lt;code&gt;def _run():&lt;/code&gt;&lt;br/&gt; &lt;code&gt;....&lt;/code&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Sad-Anywhere-2204&quot;&gt; /u/Sad-Anywhere-2204 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cngb56/create_a_default_or_else_tool_for_react_agent/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cngb56/create_a_default_or_else_tool_for_react_agent/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cngb56</id><link href="https://www.reddit.com/r/LangChain/comments/1cngb56/create_a_default_or_else_tool_for_react_agent/" /><updated>2024-05-08T21:43:59+00:00</updated><published>2024-05-08T21:43:59+00:00</published><title>create a &quot;default&quot; or &quot;else&quot; tool for ReAct agent</title></entry><entry><author><name>/u/VRoid</name><uri>https://www.reddit.com/user/VRoid</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Current LangGraph is just libraries for multiple Agents functionality built on Langchain but it can be more useful to have GUI within LangFlow. Any attempt to expand LangFlow with LangGraph? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/VRoid&quot;&gt; /u/VRoid &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cn21xp/any_langflow_update_planned_for_langgraph/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cn21xp/any_langflow_update_planned_for_langgraph/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cn21xp</id><link href="https://www.reddit.com/r/LangChain/comments/1cn21xp/any_langflow_update_planned_for_langgraph/" /><updated>2024-05-08T11:19:02+00:00</updated><published>2024-05-08T11:19:02+00:00</published><title>Any LangFlow update planned for LangGraph?</title></entry><entry><author><name>/u/Future-Outcome3167</name><uri>https://www.reddit.com/user/Future-Outcome3167</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I was going through a &lt;a href=&quot;https://www.analyticsvidhya.com/blog/2023/10/a-comprehensive-guide-to-using-chains-in-langchain/&quot;&gt;tutorial&lt;/a&gt; about router chains. Is there a method to fetch the name of the executed chain in the output, such as &amp;quot;math&amp;quot;. How can I retrieve the name of the executed chain? Also, is there a way to store the output shown below when the verbosity is set to True, into a variable?&lt;br/&gt; &lt;a href=&quot;https://imgur.com/a/axKBUfO&quot;&gt;link to image&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Future-Outcome3167&quot;&gt; /u/Future-Outcome3167 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cndgig/obtaining_the_name_of_the_chain_in_the_output/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cndgig/obtaining_the_name_of_the_chain_in_the_output/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cndgig</id><link href="https://www.reddit.com/r/LangChain/comments/1cndgig/obtaining_the_name_of_the_chain_in_the_output/" /><updated>2024-05-08T19:43:20+00:00</updated><published>2024-05-08T19:43:20+00:00</published><title>Obtaining the name of the chain in the output during runtime</title></entry><entry><author><name>/u/Flaky_Assistant8371</name><uri>https://www.reddit.com/user/Flaky_Assistant8371</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;0&lt;/p&gt; &lt;p&gt;I used the Python LangChain UnstructuredURLLoader to retrieve all our products on the company website for RAG purposes. The products were on different pages in the company website.&lt;/p&gt; &lt;p&gt;UnstructuredURLLoader was able to retrieve the products in multiple Document objects before they were chunked, embedded and stored in the vector database.&lt;/p&gt; &lt;p&gt;With the OpenAI LLM and RAG module, I asked the AI, &lt;strong&gt;&amp;quot;How many products in the company A?&amp;quot; AI replied &amp;quot;There are 11 products. You should check the company A website for more info...&amp;quot;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;If I asked &amp;quot;Please list all the products in the company A&amp;quot;, AI replied the list of the 11 products only.&lt;/p&gt; &lt;p&gt;The problem is, there are more than 11 products. Why can&amp;#39;t LLM read and aggregate the products in the Documents to count and to return all of the products?&lt;/p&gt; &lt;p&gt;Is there any context hint or prompt to tell LLM to read and return all products? Is it because of the chunking process?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Flaky_Assistant8371&quot;&gt; /u/Flaky_Assistant8371 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cmzazp/langchain_with_openai_not_return_full_products_in/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cmzazp/langchain_with_openai_not_return_full_products_in/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cmzazp</id><link href="https://www.reddit.com/r/LangChain/comments/1cmzazp/langchain_with_openai_not_return_full_products_in/" /><updated>2024-05-08T08:10:57+00:00</updated><published>2024-05-08T08:10:57+00:00</published><title>LangChain with OpenAI not return full products in RAG QnA</title></entry><entry><author><name>/u/SmoothRolla</name><uri>https://www.reddit.com/user/SmoothRolla</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey all&lt;/p&gt; &lt;p&gt;Im fairly new to langchain and langgraph and have a question about changing state attributes in conditional edge nodes &lt;/p&gt; &lt;p&gt;i have this code, where im deciding if i like the answer, if i dont, i would like to return the state to return to, but also manipulate a state attribute&lt;/p&gt; &lt;p&gt;def decide_if_answer_acceptable_node(state: GraphState):&lt;br/&gt; &amp;quot;&amp;quot;&amp;quot;&lt;br/&gt; Determines if answer is acceptable &lt;/p&gt; &lt;p&gt;Args&lt;br/&gt; state (dict): The current state of the graph &lt;/p&gt; &lt;p&gt;Returns:&lt;br/&gt; str: Binary decision for the next node to call&lt;br/&gt; &amp;quot;&amp;quot;&amp;quot; &lt;/p&gt; &lt;p&gt;if state[&amp;quot;answerok&amp;quot;] == False or state[&amp;quot;answerok&amp;quot;] == &amp;#39;False&amp;#39;:&lt;br/&gt; state[&amp;quot;answer&amp;quot;] = &amp;quot;not OK&amp;quot; # &amp;lt;--- can i alter state attributes here?&lt;br/&gt; return &amp;quot;noanswer&amp;quot;&lt;br/&gt; else:&lt;br/&gt; return &amp;quot;answer&amp;quot; &lt;/p&gt; &lt;p&gt;And its linked like so: &lt;/p&gt; &lt;p&gt;workflow.add_conditional_edges(&lt;br/&gt; &amp;quot;answer_grader_llm_node&amp;quot;,&lt;br/&gt; decide_if_answer_acceptable_node,&lt;br/&gt; {&lt;br/&gt; &amp;quot;noanswer&amp;quot;: END,&lt;br/&gt; &amp;quot;answer&amp;quot;: END&lt;br/&gt; },&lt;br/&gt; ) &lt;/p&gt; &lt;p&gt;I understand i could blank the answer in the &amp;quot;noanswer&amp;quot; node, but i would like to understand if its possible to set this in the conditional edge function so i can keep my code more compact?&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;Thanks! &lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/SmoothRolla&quot;&gt; /u/SmoothRolla &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cn7cjy/changing_state_attributes_in_langgraph/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cn7cjy/changing_state_attributes_in_langgraph/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cn7cjy</id><link href="https://www.reddit.com/r/LangChain/comments/1cn7cjy/changing_state_attributes_in_langgraph/" /><updated>2024-05-08T15:26:59+00:00</updated><published>2024-05-08T15:26:59+00:00</published><title>changing state attributes in langgraph conditional edge?</title></entry><entry><author><name>/u/theferalmonkey</name><uri>https://www.reddit.com/user/theferalmonkey</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cmyi9k/discussion_declaratively_orchestrate_your_code/&quot;&gt; &lt;img src=&quot;https://external-preview.redd.it/sAUbNyH098cBBqicB9rc8ofLEQDpkmgZKE1tSibcy5k.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6631c24e2bdd21f4aa327a412b8faec0969e28a2&quot; alt=&quot;Discussion: Declaratively orchestrate your code instead of using LCEL &quot; title=&quot;Discussion: Declaratively orchestrate your code instead of using LCEL &quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi All,&lt;/p&gt; &lt;p&gt;I&amp;#39;d be curious to discuss what peoples&amp;#39; thoughts would be on the following API to express their LLM workflows in place of LCEL. LangChain has the kitchen sink of things, so useful for that, but I haven&amp;#39;t been fond of LCEL...&lt;/p&gt; &lt;p&gt;&lt;strong&gt;LCEL&lt;/strong&gt; - it&amp;#39;s terse, but it pains me to come back to the code each time to figure out what it&amp;#39;s going on. Then if I want to do anything complex it gets worse. Simple example from the docs:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;from langchain_core.output_parsers import StrOutputParser from langchain_core.prompts import ChatPromptTemplate from langchain_core.runnables import RunnablePassthrough from langchain_openai import ChatOpenAI prompt = ChatPromptTemplate.from_template( &amp;quot;Tell me a short joke about {topic}&amp;quot;) output_parser = StrOutputParser() model = ChatOpenAI(model=&amp;quot;gpt-3.5-turbo&amp;quot;) chain = ( {&amp;quot;topic&amp;quot;: RunnablePassthrough()} | prompt | model | output_parser ) if __name__ == &amp;quot;__main__&amp;quot;: print(chain.invoke(&amp;quot;ice cream&amp;quot;)) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;What about this &lt;strong&gt;declarative API&lt;/strong&gt;, using a framework called &lt;a href=&quot;https://github.com/dagworks-inc/hamilton/&quot;&gt;Hamilton&lt;/a&gt; (note: I&amp;#39;m one of the authors)- it&amp;#39;s more verbose, but I can always clearly see how things connect and make modifications -- Hamilton knows which function to call when stitching things together based on the function name and function input arguments -- as you write functions you &amp;quot;declare&amp;quot; what they are and what they require.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;# hamilton_invoke.py from typing import List import openai def llm_client() -&amp;gt; openai.OpenAI: return openai.OpenAI() def joke_prompt(topic: str) -&amp;gt; str: return f&amp;quot;Tell me a short joke about {topic}&amp;quot; def joke_messages(joke_prompt: str) -&amp;gt; List[dict]: return [{&amp;quot;role&amp;quot;: &amp;quot;user&amp;quot;, &amp;quot;content&amp;quot;: joke_prompt}] def joke_response(llm_client: openai.OpenAI, joke_messages: List[dict]) -&amp;gt; str: response = llm_client.chat.completions.create( model=&amp;quot;gpt-3.5-turbo&amp;quot;, messages=joke_messages, ) return response.choices[0].message.content if __name__ == &amp;quot;__main__&amp;quot;: import hamilton_invoke from hamilton import driver dr = ( driver.Builder() .with_modules(hamilton_invoke) .build() ) dr.display_all_functions(&amp;quot;hamilton-invoke.png&amp;quot;) # see image below print(dr.execute([&amp;quot;joke_response&amp;quot;], inputs={&amp;quot;topic&amp;quot;: &amp;quot;ice cream&amp;quot;})) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This image (generated by Hamilton) represents how Hamilton stitches together the code to then run it&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/tq5ms3ltj5zc1.png?width=702&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=048f54f953ab50996e459b93d034771d9a943c7c&quot;&gt;Result of dr.display_all_functions(\&amp;quot;hamilton-invoke.png\&amp;quot;)&lt;/a&gt;&lt;/p&gt; &lt;p&gt;To see more comparisons (e.g. conditionally swapping anthropic for openai) &lt;a href=&quot;https://hamilton.dagworks.io/en/latest/code-comparisons/langchain/&quot;&gt;click here&lt;/a&gt;. For code that is both Hamilton &amp;amp; LangChain &lt;a href=&quot;https://hub.dagworks.io/docs/DAGWorks/conversational_rag/&quot;&gt;see this example&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Now I wouldn&amp;#39;t use Hamilton for a simple function call -- much like I wouldn&amp;#39;t use LangChain for that either.&lt;/p&gt; &lt;p&gt;I&amp;#39;m interested in discussing thoughts and opinions to see if there&amp;#39;s (a) appetite for this style of API, and (b) therefore should we integrate more closely with LangChain. Cheers!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/theferalmonkey&quot;&gt; /u/theferalmonkey &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cmyi9k/discussion_declaratively_orchestrate_your_code/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cmyi9k/discussion_declaratively_orchestrate_your_code/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1cmyi9k</id><media:thumbnail url="https://external-preview.redd.it/sAUbNyH098cBBqicB9rc8ofLEQDpkmgZKE1tSibcy5k.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=6631c24e2bdd21f4aa327a412b8faec0969e28a2" /><link href="https://www.reddit.com/r/LangChain/comments/1cmyi9k/discussion_declaratively_orchestrate_your_code/" /><updated>2024-05-08T07:12:45+00:00</updated><published>2024-05-08T07:12:45+00:00</published><title>Discussion: Declaratively orchestrate your code instead of using LCEL</title></entry><entry><author><name>/u/Top_Raccoon_1493</name><uri>https://www.reddit.com/user/Top_Raccoon_1493</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;If we are using a GPU for running the LLama2/LLama3 model, which library should I use? LLama CPP or Ctransformers? I&amp;#39;m a bit confused about both of these libraries. Can anyone please clear my doubt?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Top_Raccoon_1493&quot;&gt; /u/Top_Raccoon_1493 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cn08m1/choosing_between_llama_cpp_and_ctransformers_for/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cn08m1/choosing_between_llama_cpp_and_ctransformers_for/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cn08m1</id><link href="https://www.reddit.com/r/LangChain/comments/1cn08m1/choosing_between_llama_cpp_and_ctransformers_for/" /><updated>2024-05-08T09:18:24+00:00</updated><published>2024-05-08T09:18:24+00:00</published><title>Choosing Between LLama CPP and Ctransformers for GPU-based LLama2/LLama3 Model Execution</title></entry><entry><author><name>/u/ramirez_tn</name><uri>https://www.reddit.com/user/ramirez_tn</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I am using Langchain to load PDF files and ask questions using RetrievalQA but when I ask to generate a solution or be creative it does not .It looks like it is limited to the content of the provided files only. Is there a limitation for RertievalQA or just an issue with my prompts ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/ramirez_tn&quot;&gt; /u/ramirez_tn &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cn3d2x/how_to_make_llm_answers_more_creative_and_find/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cn3d2x/how_to_make_llm_answers_more_creative_and_find/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cn3d2x</id><link href="https://www.reddit.com/r/LangChain/comments/1cn3d2x/how_to_make_llm_answers_more_creative_and_find/" /><updated>2024-05-08T12:29:23+00:00</updated><published>2024-05-08T12:29:23+00:00</published><title>How to make LLM answers more creative and find answers from the internet</title></entry><entry><author><name>/u/RoboCoachTech</name><uri>https://www.reddit.com/user/RoboCoachTech</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cmquwv/using_langchain_agents_to_create_a_multiagent/&quot;&gt; &lt;img src=&quot;https://external-preview.redd.it/GklbHOLRONUSqoLxfRvIdxg-bpVWervSnhvxAvD8mnc.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c6e26ad3b1025c14d13502efeae32c2669bf11c4&quot; alt=&quot;Using LangChain agents to create a multi-agent platform that creates robot softwares&quot; title=&quot;Using LangChain agents to create a multi-agent platform that creates robot softwares&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;When using LLMs for your generative AI needs, it&amp;#39;s best to think of the LLM as a person rather than as a traditional AI engine. You can train and tune an LLM and give it memory to create an agent. The LLM-agent can act like a domain-expert for whatever domain you&amp;#39;ve trained and equipped it for. Using one agent to solve a complex problem is not the optimum solution. Much like how a project manager breaks a complex project into different tasks and assigns different individuals with different skills and trainings to manage each task, a multi-agent solution, where each agent has different capabilities and trainings, can be applied to a complex problem. &lt;/p&gt; &lt;p&gt;In our case, we want to automatically generate the entire robot software (for any given robot description) in ROS (Robot Operating System); In order to do so, first, we need to understand the overall design of the robot (a.k.a the ROS graph) and then for each ROS node we need to know if the LLM should generate the code, or if the LLM can fetch a suitable code from online open-source repositories (a.k.a. RAG: Retrieval Augmented Generation). Each of these steps can be handled by different agents which have different sets of tools at their disposal. The following figure shows how we are doing this:&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/qcvb8y98c3zc1.png?width=1570&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5f4072288e470fd9e2d946e471f35e4c2dff1f94&quot;&gt;Robot software generation using four collaborating agents each responsible for a different part of the problem, each equipped with different toolsets.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This is a free and open-source tool that we have released. We named it &lt;a href=&quot;https://github.com/RoboCoachTechnologies/ROScribe&quot;&gt;ROScribe&lt;/a&gt;. Please checkout our &lt;a href=&quot;https://github.com/RoboCoachTechnologies/ROScribe&quot;&gt;repository&lt;/a&gt; for more information and give us a star if you like what you see. :) &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/RoboCoachTech&quot;&gt; /u/RoboCoachTech &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cmquwv/using_langchain_agents_to_create_a_multiagent/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cmquwv/using_langchain_agents_to_create_a_multiagent/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1cmquwv</id><media:thumbnail url="https://external-preview.redd.it/GklbHOLRONUSqoLxfRvIdxg-bpVWervSnhvxAvD8mnc.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=c6e26ad3b1025c14d13502efeae32c2669bf11c4" /><link href="https://www.reddit.com/r/LangChain/comments/1cmquwv/using_langchain_agents_to_create_a_multiagent/" /><updated>2024-05-08T00:05:42+00:00</updated><published>2024-05-08T00:05:42+00:00</published><title>Using LangChain agents to create a multi-agent platform that creates robot softwares</title></entry><entry><author><name>/u/Guizkane</name><uri>https://www.reddit.com/user/Guizkane</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey everyone! I recently wrote a blog post about &lt;a href=&quot;http://affiliate.ai/&quot;&gt;Affiliate.ai&lt;/a&gt;, a chat-based affiliate marketing analytics tool we&amp;#39;ve been working on. It simplifies the analytics process, letting you ask natural language questions and get insights, reports, and even spreadsheets delivered right within Microsoft Teams or Slack.&lt;/p&gt; &lt;p&gt;But the interesting part (for this audience, at least) is how it works under the hood. Here&amp;#39;s a breakdown of some key elements:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Intent Discernment with Function Calling:&lt;/strong&gt; We use simple function calling to quickly determine whether a user wants data or is just chatting, ensuring the bot stays focused.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;LLM-Powered Named Entity Recognition:&lt;/strong&gt; Instead of complex pipelines, we feed the LLM a list of advertisers and let it figure out the matches‚Äì surprisingly effective!&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Query Reconstruction for Context:&lt;/strong&gt; Understanding context is tricky. We use a dedicated module to rewrite queries based on chat history.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Parallelization for Speed:&lt;/strong&gt; We run multiple potential routes simultaneously, speeding up response times dramatically.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Interested in the specifics? The full blog post has more details (link below). If you&amp;#39;re building similar GenAI apps, I&amp;#39;d love to hear about your approaches and techniques!&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://www.affiliate.ai/post/a-technical-deepdive-into-affiliate-ai&quot;&gt;https://www.affiliate.ai/post/a-technical-deepdive-into-affiliate-ai&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Guizkane&quot;&gt; /u/Guizkane &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cn04dj/deep_dive_building_affiliateai_a_genaipowered/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cn04dj/deep_dive_building_affiliateai_a_genaipowered/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cn04dj</id><link href="https://www.reddit.com/r/LangChain/comments/1cn04dj/deep_dive_building_affiliateai_a_genaipowered/" /><updated>2024-05-08T09:09:59+00:00</updated><published>2024-05-08T09:09:59+00:00</published><title>Deep Dive: Building Affiliate.ai, a GenAI-Powered Affiliate Marketing Analytics Tool</title></entry><entry><author><name>/u/Basil2BulgarSlayer</name><uri>https://www.reddit.com/user/Basil2BulgarSlayer</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I am working on a Nextjs demo app that needs to use inference on a custom LLM I will train. When I deploy it, I‚Äôm planning on using Baseten but for local development I am now considering using Lanchain in Node (as opposed to setting up a Flask server to handle inference and stream the responses back). Has anyone used it before? Is it a total disaster? know it‚Äôs not going to be as good as the Python version but maybe it‚Äôs good enough for my situation. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Basil2BulgarSlayer&quot;&gt; /u/Basil2BulgarSlayer &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cmz6uh/node_js_support/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cmz6uh/node_js_support/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cmz6uh</id><link href="https://www.reddit.com/r/LangChain/comments/1cmz6uh/node_js_support/" /><updated>2024-05-08T08:02:19+00:00</updated><published>2024-05-08T08:02:19+00:00</published><title>Node JS Support</title></entry><entry><author><name>/u/Organic_Manner359</name><uri>https://www.reddit.com/user/Organic_Manner359</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I just saw that Langchain is now a legacy provider. How can i still use Langchain with the Vercel AI SDK for my NextJS apps in a futureproof way. On the website it says, that the legacy providers are not recommended for new projects.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Organic_Manner359&quot;&gt; /u/Organic_Manner359 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cmkcsb/langchain_is_legacy_in_vercel_ai_sdk_how_to_still/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cmkcsb/langchain_is_legacy_in_vercel_ai_sdk_how_to_still/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cmkcsb</id><link href="https://www.reddit.com/r/LangChain/comments/1cmkcsb/langchain_is_legacy_in_vercel_ai_sdk_how_to_still/" /><updated>2024-05-07T19:29:22+00:00</updated><published>2024-05-07T19:29:22+00:00</published><title>Langchain is legacy in Vercel AI SDK, how to still use Langchain in a stable and futureproof way?</title></entry><entry><author><name>/u/moonbunR</name><uri>https://www.reddit.com/user/moonbunR</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Here is a simple code snippet on how to use the Cycls chatbot library&lt;/p&gt; &lt;pre&gt;&lt;code&gt;main.py from cycls import App app = App(secret=&amp;quot;sk-secret&amp;quot;, handler=&amp;quot;@handler-name&amp;quot;) @app def entry_point(context): # Capture the received message received_message = context.message.content.text # Reply back with a simple message context.send.text(f&amp;quot;Received message: {received_message}&amp;quot;) app.publish() &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This is a simplified example but when you run &lt;a href=&quot;http://main.py&quot;&gt;main.py&lt;/a&gt;, the chatbot immediately gets deployed with a public url and a chat interface. This has helped me a huge deal with testing while developing chatbots.&lt;br/&gt; Here are the docs: &lt;a href=&quot;https://docs.cycls.com/getting-started&quot;&gt;https://docs.cycls.com/getting-started&lt;/a&gt; &lt;a href=&quot;https://docs.cycls.com/getting-started&quot;&gt; &lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/moonbunR&quot;&gt; /u/moonbunR &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cmdxyi/python_library_to_deploy_llm_chat_bots_fast/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cmdxyi/python_library_to_deploy_llm_chat_bots_fast/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cmdxyi</id><link href="https://www.reddit.com/r/LangChain/comments/1cmdxyi/python_library_to_deploy_llm_chat_bots_fast/" /><updated>2024-05-07T14:59:16+00:00</updated><published>2024-05-07T14:59:16+00:00</published><title>Python library to deploy LLM chat bots fast?</title></entry></feed>