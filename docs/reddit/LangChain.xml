<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/"><category term="LangChain" label="r/LangChain"/><updated>2024-06-21T07:10:08+00:00</updated><icon>https://www.redditstatic.com/icon.png/</icon><id>/r/LangChain.rss</id><link rel="self" href="https://www.reddit.com/r/LangChain.rss" type="application/atom+xml" /><link rel="alternate" href="https://www.reddit.com/r/LangChain" type="text/html" /><subtitle>LangChain is an open-source framework and developer toolkit that helps developers get LLM applications from prototype to production. It is available for Python and Javascript at https://www.langchain.com/.</subtitle><title>LangChain</title><entry><author><name>/u/zchaarm</name><uri>https://www.reddit.com/user/zchaarm</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;A place for members of &lt;a href=&quot;/r/LangChain&quot;&gt;r/LangChain&lt;/a&gt; to chat with each other&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/zchaarm&quot;&gt; /u/zchaarm &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_10ljho9</id><link href="https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/" /><updated>2023-01-26T04:33:05+00:00</updated><published>2023-01-26T04:33:05+00:00</published><title>r/LangChain Lounge</title></entry><entry><author><name>/u/fleeced-artichoke</name><uri>https://www.reddit.com/user/fleeced-artichoke</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi all,&lt;br/&gt; I am creating a streamlit RAG app to allow tech-support agents to get information from service manuals without having to read them. I&amp;#39;m using an Azure OpenAI gpt-4o LLM in conjunction with an Azure AI Search retriever. The responses I&amp;#39;m getting are good.&lt;/p&gt; &lt;p&gt;I am wanting to implement a feature in the app where each response contains citations to the retrieved documents. In an ideal world, the user would be able to click on the citations to bring up the specific pages in the service manual PDFs where the retrieved documents are.&lt;/p&gt; &lt;p&gt;I have read the documentations relating to citations ( &lt;a href=&quot;https://python.langchain.com/v0.2/docs/how_to/qa_citations/&quot;&gt;https://python.langchain.com/v0.2/docs/how_to/qa_citations/&lt;/a&gt; ), but none of the approaches outlined in the article work for my app. Does anyone have any ideas on how to accomplish what I&amp;#39;m trying to do?&lt;/p&gt; &lt;p&gt;For reference, much of my app uses the code in this how-to guide: &lt;a href=&quot;https://python.langchain.com/v0.2/docs/tutorials/qa_chat_history/&quot;&gt;https://python.langchain.com/v0.2/docs/tutorials/qa_chat_history/&lt;/a&gt; . For responses to user queries, I am invoking the conversational_rag_chain outlined in that guide.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/fleeced-artichoke&quot;&gt; /u/fleeced-artichoke &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dkjxos/create_citations_in_rag_streamlit_app/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dkjxos/create_citations_in_rag_streamlit_app/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dkjxos</id><link href="https://www.reddit.com/r/LangChain/comments/1dkjxos/create_citations_in_rag_streamlit_app/" /><updated>2024-06-20T19:16:47+00:00</updated><published>2024-06-20T19:16:47+00:00</published><title>Create Citations in RAG Streamlit App</title></entry><entry><author><name>/u/hihowudoin1</name><uri>https://www.reddit.com/user/hihowudoin1</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;We just launched an exciting project and would love to hear your thoughts and feedback! Here&amp;#39;s the scoop:&lt;/p&gt; &lt;p&gt;Project Details:Our open-source initiative focuses on integrating advanced search technologies under one roof. By harnessing gradient boosting (xgboost) machine learning techniques, we combine Keyword-based searches, Vector databases, and Machine Learning rerankers for optimal performance.&lt;/p&gt; &lt;p&gt;Performance Benchmark:According to our tests on the MSMARCO dataset, Denser Retriever has achieved an impressive 13.07% relative gain in NDCG@10 compared to leading vector search baselines of similar model sizes.&lt;/p&gt; &lt;p&gt;Here are the Key Features:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Github repo:&lt;a href=&quot;https://github.com/denser-org/denser-retriever/tree/main&quot;&gt; Denser Retriever&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Blog:&lt;a href=&quot;https://denser.ai/blog/denser-retriever/&quot;&gt; Learn more about Denser Retriever&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Documentation:&lt;a href=&quot;https://retriever.denser.ai/&quot;&gt; Denser Retriever Documentation&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Looking forward to hear your thoughts.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/hihowudoin1&quot;&gt; /u/hihowudoin1 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dk0ukb/seeking_feedback_on_denser_retriever_for_advanced/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dk0ukb/seeking_feedback_on_denser_retriever_for_advanced/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dk0ukb</id><link href="https://www.reddit.com/r/LangChain/comments/1dk0ukb/seeking_feedback_on_denser_retriever_for_advanced/" /><updated>2024-06-20T02:25:33+00:00</updated><published>2024-06-20T02:25:33+00:00</published><title>Seeking Feedback on Denser Retriever for Advanced GenAI RAG Performance</title></entry><entry><author><name>/u/jabr7</name><uri>https://www.reddit.com/user/jabr7</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi, im new to vector databases and im currently using chroma db with langchain and Azure embeddings for llms, i have been using it for a low ammount of documents, like a few hundreds, but now i have a case where i have to embed 400k documents with 1500 characters each (each is an article of a law).&lt;/p&gt; &lt;p&gt;I managed to compute and index all the embeddings, but as soon as i try to load it from the disk (the file is 3.3gb) the docker container fails with an out of memory (its failing at 9GB of memory), my questions where:&lt;/p&gt; &lt;p&gt;If my file on disk is 3,3gb, how much RAM memory do i have to have to instantiate it? more or less obviously:&lt;/p&gt; &lt;p&gt;def create_vectorStore():&lt;br/&gt; embeddings = AzureOpenAIEmbeddings(&lt;br/&gt; model=&amp;quot;text-embedding-3-small&amp;quot;,&lt;br/&gt; azure_deployment=&amp;quot;text-embedding-3-small&amp;quot;,&lt;br/&gt; openai_api_version=&amp;quot;2024-02-01&amp;quot;,&lt;br/&gt; )&lt;br/&gt; &lt;br/&gt; chromaVectorStore = Chroma(&lt;br/&gt; collection_name=&amp;quot;cv_collection&amp;quot;,&lt;br/&gt; embedding_function=embeddings,&lt;br/&gt; persist_directory=&amp;quot;data/chroma_vector_store&amp;quot;&lt;br/&gt; ) &lt;/p&gt; &lt;p&gt;record_manager = SQLRecordManager(&lt;br/&gt; namespace=&amp;quot;chroma/cv_collection&amp;quot;,&lt;br/&gt; db_url=&amp;quot;sqlite:///record_manager_cache.sql&amp;quot;,&lt;br/&gt; ) &lt;/p&gt; &lt;p&gt;record_manager.create_schema() &lt;/p&gt; &lt;p&gt;return chromaVectorStore, record_manager&lt;/p&gt; &lt;p&gt;Does it change if i use the &lt;a href=&quot;https://hub.docker.com/layers/chromadb/chroma/latest/images/sha256-0b84e8a5d8a9305690a8fd9beba871a3af708bf9cfbae16de839027005798f06&quot;&gt;chroma docker container&lt;/a&gt;?&lt;/p&gt; &lt;p&gt;Any tips to manage this ammount of data in a vector database and how to scale it?&lt;/p&gt; &lt;p&gt;Thank you for the responses&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/jabr7&quot;&gt; /u/jabr7 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dkkhlb/how_to_scale_a_vector_database_using_langchain/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dkkhlb/how_to_scale_a_vector_database_using_langchain/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dkkhlb</id><link href="https://www.reddit.com/r/LangChain/comments/1dkkhlb/how_to_scale_a_vector_database_using_langchain/" /><updated>2024-06-20T19:40:18+00:00</updated><published>2024-06-20T19:40:18+00:00</published><title>How to scale a vector database using langchain? Langchain and ChromaDB</title></entry><entry><author><name>/u/PomegranateFun9900</name><uri>https://www.reddit.com/user/PomegranateFun9900</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Most people know about the UI that Streamlit provides for Langchain Agents, but I am looking for a more custom solution, so I can get more control over the UI.&lt;/p&gt; &lt;p&gt;There are some solution like &lt;a href=&quot;https://docs.nlkit.com/nlux&quot;&gt;NLux&lt;/a&gt; for React, but it still does not support agents and tool calling. Does anyone know of any solution? I want to stream and display tool calls also along with the LLM outputs. Langserve events streaming looks like a nightmare to develop over.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/PomegranateFun9900&quot;&gt; /u/PomegranateFun9900 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dkkck6/custom_streamlitlike_ui_for_langchain_agents/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dkkck6/custom_streamlitlike_ui_for_langchain_agents/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dkkck6</id><link href="https://www.reddit.com/r/LangChain/comments/1dkkck6/custom_streamlitlike_ui_for_langchain_agents/" /><updated>2024-06-20T19:34:17+00:00</updated><published>2024-06-20T19:34:17+00:00</published><title>Custom Streamlit-like UI for Langchain Agents</title></entry><entry><author><name>/u/Ukpersfidev</name><uri>https://www.reddit.com/user/Ukpersfidev</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi, &lt;/p&gt; &lt;p&gt;I&amp;#39;m using the XMLOutputParser, and it works 90% of the time when not using streaming, but when I do, it fails 2/3 of the time with a ` Text data outside of root node.` error.&lt;/p&gt; &lt;p&gt;I&amp;#39;ve taken a look at the actual text generated and it seems to be valid XML, so i&amp;#39;m wondering if the issue could be with the parser instead, of if the first chunk isn&amp;#39;t a complete XML tag or something.&lt;/p&gt; &lt;p&gt;Has anyone else faced this issue?&lt;/p&gt; &lt;p&gt;Heres my code for context:&lt;/p&gt; &lt;pre&gt;&lt;code&gt; const model = new ChatAnthropic({ temperature: 1, model: ANTHROPIC_MODELS.SONNET, apiKey: env.ANTHROPIC_API_KEY, }); const parser = new XMLOutputParser({ tags: [&amp;#39;company&amp;#39;, &amp;#39;name&amp;#39;, &amp;#39;year&amp;#39;, &amp;#39;description&amp;#39;], }); const systemMessage = ` You are an AI assistant tasked with extracting information from a document. The user will provide you with the text You should extract the company name, the year it was founded and a brief description of the company (a max of 10 words). ${parser.getFormatInstructions()} Example: &amp;lt;company&amp;gt; &amp;lt;name&amp;gt;Company name&amp;lt;/name&amp;gt; &amp;lt;year&amp;gt;Year founded&amp;lt;/year&amp;gt; &amp;lt;description&amp;gt;Company description&amp;lt;/description&amp;gt; &amp;lt;/company&amp;gt; `; const prompt = ChatPromptTemplate.fromMessages([ [&amp;#39;system&amp;#39;, systemMessage], [&amp;#39;user&amp;#39;, text], ]); const chain = prompt.pipe(model).pipe(parser); const stream = await chain.stream({}); for await (const chunk of stream) { console.log(&amp;#39;-------&amp;#39;); console.log(chunk); console.log(&amp;#39;-------&amp;#39;); } &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Ukpersfidev&quot;&gt; /u/Ukpersfidev &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dkgh71/parsing_fails_when_streaming/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dkgh71/parsing_fails_when_streaming/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dkgh71</id><link href="https://www.reddit.com/r/LangChain/comments/1dkgh71/parsing_fails_when_streaming/" /><updated>2024-06-20T16:52:57+00:00</updated><published>2024-06-20T16:52:57+00:00</published><title>Parsing fails when streaming</title></entry><entry><author><name>/u/NoIdeaAbaout</name><uri>https://www.reddit.com/user/NoIdeaAbaout</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I am using LangChain, a simple agent to search the internet. I am trying everything ReaCT, the problem is that the model finds the answer, but then still keeps asking questions by itself and continues a meaningless chain. Whatever I try the model when found the answer it thinks it is incorrect or incomplete and keep going. If I do not limit the iterations it keep doing without stopping. I do not want to use OpenAI, I have tried also Mistral but with similar results. If you know a better model, or way to do, I will be really thankful&lt;/p&gt; &lt;p&gt;&lt;code&gt;from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;from langchain_huggingface import HuggingFacePipeline&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;from langchain.agents import load_tools, AgentExecutor, initialize_agent&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;from langchain_core.prompts import PromptTemplate&lt;/code&gt;&lt;/p&gt; &lt;p&gt;,&lt;code&gt;# Load the model and tokenizer&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;model_id = &amp;quot;microsoft/Phi-3-mini-4k-instruct&amp;quot;&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;tokenizer = AutoTokenizer.from_pretrained(model_id)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;model = AutoModelForCausalLM.from_pretrained(&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;model_id,&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;load_in_4bit=True,&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;# Define the text generation pipeline using HuggingFace transformers&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;pipe = pipeline(&amp;quot;text-generation&amp;quot;, model=model,&lt;/code&gt; &lt;/p&gt; &lt;p&gt;&lt;code&gt;tokenizer=tokenizer, max_new_tokens=500,&lt;/code&gt; &lt;/p&gt; &lt;p&gt;&lt;code&gt;top_k=50, temperature=0.1,&lt;/code&gt; &lt;/p&gt; &lt;p&gt;&lt;code&gt;do_sample=True)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;# Wrap the pipeline in a HuggingFacePipeline object&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;llm = HuggingFacePipeline(pipeline=pipe)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;# Load the necessary tools&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;tools = load_tools([&amp;quot;ddg-search&amp;quot;], llm=llm)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;# Define the prompt template with explicit stop instructions&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;template = &amp;#39;&amp;#39;&amp;#39;Answer the following question as best as you can. You have access to the following tools:&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;{tools}&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;Use the following format:&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;Question: {input}&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;Thought: You should think about what action to take&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;Action: the action to take, should be one of [{tool_names}]&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;Action Input: the input to the action&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;Observation: the result of the action&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;Thought: I now know the final answer&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;Final Answer: the final answer to the original input question&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;Do not answer/ask any other questions.&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;Once got the information provide the Final Answer.&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;please, stop once you have provided the Final Answer.&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;Begin!&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;Question: {input}&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;&amp;#39;&amp;#39;&amp;#39;&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;# Create a PromptTemplate from the template&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;prompt = PromptTemplate.from_template(template)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;# Initialize the agent using initialize_agent&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;agent = initialize_agent(&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;tools=tools,&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;llm=llm,&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;agent=&amp;quot;zero-shot-react-description&amp;quot;,&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;prompt=prompt,&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;verbose=True,&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;handle_parsing_errors=True,&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;max_iterations=1,&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;stop_sequence=&amp;quot;Final Answer:&amp;quot;&lt;/code&gt; &lt;/p&gt; &lt;p&gt;&lt;code&gt;)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;# Define the query&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;query = &amp;quot;What is the capital of France?&amp;quot;&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;# Execute the query&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;response = agent.run(query)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;print(response)&lt;/code&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/NoIdeaAbaout&quot;&gt; /u/NoIdeaAbaout &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dkejk7/use_langchain_with_open_source_llm_to_search/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dkejk7/use_langchain_with_open_source_llm_to_search/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dkejk7</id><link href="https://www.reddit.com/r/LangChain/comments/1dkejk7/use_langchain_with_open_source_llm_to_search/" /><updated>2024-06-20T15:32:10+00:00</updated><published>2024-06-20T15:32:10+00:00</published><title>Use LangChain with open source LLM to search internet</title></entry><entry><author><name>/u/HappyDataGuy</name><uri>https://www.reddit.com/user/HappyDataGuy</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I am using langchain ReAct agent with tools. The thing is there is a lot of wasted effort because the agent want to call tools which are not even present. Due to this the agent reaches max iteration without calling the tool which are present. Is there any better way to build these agents? or is there any research on better type of agents for this?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/HappyDataGuy&quot;&gt; /u/HappyDataGuy &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dk73vw/what_is_better_way_of_creating_react_agent_or_are/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dk73vw/what_is_better_way_of_creating_react_agent_or_are/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dk73vw</id><link href="https://www.reddit.com/r/LangChain/comments/1dk73vw/what_is_better_way_of_creating_react_agent_or_are/" /><updated>2024-06-20T09:02:47+00:00</updated><published>2024-06-20T09:02:47+00:00</published><title>What is better way of creating ReAct agent or are there any alternatives to it?</title></entry><entry><author><name>/u/user-1318</name><uri>https://www.reddit.com/user/user-1318</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Has anyone worked with caching in RAG in production. What database you used? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/user-1318&quot;&gt; /u/user-1318 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dk70yt/has_anyone_worked_with_caching_in_rag_chatbot_for/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dk70yt/has_anyone_worked_with_caching_in_rag_chatbot_for/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dk70yt</id><link href="https://www.reddit.com/r/LangChain/comments/1dk70yt/has_anyone_worked_with_caching_in_rag_chatbot_for/" /><updated>2024-06-20T08:57:14+00:00</updated><published>2024-06-20T08:57:14+00:00</published><title>Has anyone worked with caching in RAG chatbot for production?</title></entry><entry><author><name>/u/ChallengeOk6437</name><uri>https://www.reddit.com/user/ChallengeOk6437</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I am using Cohere reranker right now and it is really good. I want to know if there is anything else which is as good or better and open source?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/ChallengeOk6437&quot;&gt; /u/ChallengeOk6437 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1djsnov/best_open_source_reranker_for_rag/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1djsnov/best_open_source_reranker_for_rag/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1djsnov</id><link href="https://www.reddit.com/r/LangChain/comments/1djsnov/best_open_source_reranker_for_rag/" /><updated>2024-06-19T20:06:51+00:00</updated><published>2024-06-19T20:06:51+00:00</published><title>Best Open Source RE-RANKER for RAG??!!</title></entry><entry><author><name>/u/tf1155</name><uri>https://www.reddit.com/user/tf1155</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I just want to ask if someone is already using LangChain in production, which basically means that during the development process, nothing could stop you from deploying and maintaining it.&lt;/p&gt; &lt;p&gt;I always assumed that LangChain is a project suitable for academic projects and/or beginners as a good starting point. And now, since I also suggested it to our team, two colleagues are against Langchain because they share the same assumption with me and still stick with it.&lt;/p&gt; &lt;p&gt;I am asking here for arguments pro and con :) thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/tf1155&quot;&gt; /u/tf1155 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dju7yb/using_langchain_in_production/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dju7yb/using_langchain_in_production/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dju7yb</id><link href="https://www.reddit.com/r/LangChain/comments/1dju7yb/using_langchain_in_production/" /><updated>2024-06-19T21:12:54+00:00</updated><published>2024-06-19T21:12:54+00:00</published><title>Using LangChain in production</title></entry><entry><author><name>/u/Unique-Drink-9916</name><uri>https://www.reddit.com/user/Unique-Drink-9916</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi. I am facing issue while using the Tavily search tool example in langchain. I get SSLError while invoking the tool. Can someone guide me to fix this?&lt;/p&gt; &lt;p&gt;Error is SSLError (MaxRetryError...)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Unique-Drink-9916&quot;&gt; /u/Unique-Drink-9916 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dk74g9/sslerror_when_using_tavilysearchresults/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dk74g9/sslerror_when_using_tavilysearchresults/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dk74g9</id><link href="https://www.reddit.com/r/LangChain/comments/1dk74g9/sslerror_when_using_tavilysearchresults/" /><updated>2024-06-20T09:04:00+00:00</updated><published>2024-06-20T09:04:00+00:00</published><title>SSLError when using TavilySearchResults</title></entry><entry><author><name>/u/sarthakai</name><uri>https://www.reddit.com/user/sarthakai</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Google just announced Context Caching in the Gemini API — it allows you to store and reuse input tokens for repetitive requests.&lt;/p&gt; &lt;p&gt;Many LLM tasks have extensive system prompts laying down instructions and initial context.&lt;/p&gt; &lt;p&gt;If these are cached, they wouldn’t have to be encoded all over again every time, saving on costs and latency.&lt;/p&gt; &lt;p&gt;Tokens are cached for a specified duration (TTL), after which they are automatically deleted.&lt;/p&gt; &lt;p&gt;Costs depend on the number of tokens cached and their storage duration, and efficiency would be higher for prompts with context used across many LLM calls.&lt;/p&gt; &lt;p&gt;Docs: &lt;a href=&quot;https://ai.google.dev/gemini-api/docs/caching?lang=python&quot;&gt;https://ai.google.dev/gemini-api/docs/caching?lang=python&lt;/a&gt;&lt;/p&gt; &lt;p&gt;You can learn more about AI here: &lt;a href=&quot;https://linktr.ee/sarthakrastogi&quot;&gt;https://linktr.ee/sarthakrastogi&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/sarthakai&quot;&gt; /u/sarthakai &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1djjgia/apparently_geminis_context_caching_can_cut_your/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1djjgia/apparently_geminis_context_caching_can_cut_your/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1djjgia</id><link href="https://www.reddit.com/r/LangChain/comments/1djjgia/apparently_geminis_context_caching_can_cut_your/" /><updated>2024-06-19T13:41:00+00:00</updated><published>2024-06-19T13:41:00+00:00</published><title>Apparently Gemini's context caching can cut your LLM cost and latency to half</title></entry><entry><author><name>/u/_zero2hundred</name><uri>https://www.reddit.com/user/_zero2hundred</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi all! I’m a newbie developer still learning while trying to get an experimental idea off the ground. &lt;/p&gt; &lt;p&gt;My project involves giving LLMs access to various tools to retrieve data as well as a large vector database. &lt;/p&gt; &lt;p&gt;I want to get a product out as soon as possible while having a scalable codebase for further improvements. I feel like LangChain is much more comprehensive and will be useful for improving my application. On the other hand, Phidata make it so easy to create agents, set up tools and RAG and create multi-agent architectures that I’m leaning towards using it for the first version. &lt;/p&gt; &lt;p&gt;As I’m a beginner, I wanted to get your thoughts on which one I should use to begin with and how to think about stuff like this going forward.&lt;/p&gt; &lt;p&gt;Are there any disadvantages to using such frameworks? Will it cause roadblocks?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/_zero2hundred&quot;&gt; /u/_zero2hundred &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1djzh8e/using_langchain_vs_an_agent_framework_like/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1djzh8e/using_langchain_vs_an_agent_framework_like/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1djzh8e</id><link href="https://www.reddit.com/r/LangChain/comments/1djzh8e/using_langchain_vs_an_agent_framework_like/" /><updated>2024-06-20T01:15:59+00:00</updated><published>2024-06-20T01:15:59+00:00</published><title>Using LangChain vs an agent framework like Phidata for a prototype</title></entry><entry><author><name>/u/chermi</name><uri>https://www.reddit.com/user/chermi</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I was running through what I thought would be a simple tutorial: &lt;a href=&quot;https://python.langchain.com/v0.2/docs/tutorials/chatbot/&quot;&gt;https://python.langchain.com/v0.2/docs/tutorials/chatbot/&lt;/a&gt;, but am having some trouble that seems to indicate they have removed a module referenced in the tutorial.&lt;/p&gt; &lt;p&gt;In the section on managing history &lt;a href=&quot;https://python.langchain.com/v0.2/docs/tutorials/chatbot/#managing-conversation-history&quot;&gt;https://python.langchain.com/v0.2/docs/tutorials/chatbot/#managing-conversation-history&lt;/a&gt; we are supposed to use this &amp;quot;trim_messages&amp;quot; module. I tried importing as in&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;from langchain_core.messages import SystemMessage, trim_messages&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;and got &lt;/p&gt; &lt;pre&gt;&lt;code&gt;cannot import name &amp;#39;trim_messages&amp;#39; from &amp;#39;langchain_core.messages&amp;#39; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;. Diving into the API documentation, I noticed that the &amp;quot;trim_messages&amp;quot; package doesn&amp;#39;t exist.&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://api.python.langchain.com/en/latest/core_api_reference.html#module-langchain_core.messages&quot;&gt;https://api.python.langchain.com/en/latest/core_api_reference.html#module-langchain_core.messages&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This seems like a pretty obvious mistake so I assume I&amp;#39;m doing something wrong. Have any of you got trim_messages to work? Any help would be appreciated. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/chermi&quot;&gt; /u/chermi &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1djw5oq/broken_tutorial/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1djw5oq/broken_tutorial/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1djw5oq</id><link href="https://www.reddit.com/r/LangChain/comments/1djw5oq/broken_tutorial/" /><updated>2024-06-19T22:36:32+00:00</updated><published>2024-06-19T22:36:32+00:00</published><title>Broken Tutorial?</title></entry><entry><author><name>/u/AnEdgeLordWeeb</name><uri>https://www.reddit.com/user/AnEdgeLordWeeb</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey guys!&lt;br/&gt; How would you recommend me to approach the frontend and backend side of such an app? I&amp;#39;m done with creating the chatbot logic through LangGraph, and now I want to implement said logic in a UI where responses can be displayed in a beautiful manner to the user.&lt;br/&gt; What do you think about the idea of using Streamlit for frontend and FastAPI for backend? Also, regarding the backend, I have found an API called &amp;quot;LangCorn&amp;quot; that is said to leverage the power of FastAPI, has anyone worked with it before? Would you recommend me using it instead of the traditional FastAPI framework?&lt;br/&gt; Through this combo, will I be able to add functionalities such as auth, memory persistence and streaming tokens?&lt;br/&gt; Thank you!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/AnEdgeLordWeeb&quot;&gt; /u/AnEdgeLordWeeb &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1djz34y/tips_for_creating_a_simple_webbased_app_for_my/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1djz34y/tips_for_creating_a_simple_webbased_app_for_my/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1djz34y</id><link href="https://www.reddit.com/r/LangChain/comments/1djz34y/tips_for_creating_a_simple_webbased_app_for_my/" /><updated>2024-06-20T00:55:22+00:00</updated><published>2024-06-20T00:55:22+00:00</published><title>Tips for creating a simple web-based app for my chatbot's logic</title></entry><entry><author><name>/u/Due-Pitch-8779</name><uri>https://www.reddit.com/user/Due-Pitch-8779</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Everything looks ok to me from a prompt perspective.. but get this error when using RetrievalQA.invoke &lt;/p&gt; &lt;h1&gt;argument needs to be of type (SquadExample, dict)&lt;/h1&gt; &lt;p&gt;Any Suggestions how to fix this ? thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Due-Pitch-8779&quot;&gt; /u/Due-Pitch-8779 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1djyvwr/rag_using_langchain/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1djyvwr/rag_using_langchain/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1djyvwr</id><link href="https://www.reddit.com/r/LangChain/comments/1djyvwr/rag_using_langchain/" /><updated>2024-06-20T00:45:01+00:00</updated><published>2024-06-20T00:45:01+00:00</published><title>RAG Using LangChain</title></entry><entry><author><name>/u/ImGallo</name><uri>https://www.reddit.com/user/ImGallo</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi, I&amp;#39;m working on a project involving the use of language models (LLMs). I need to perform tasks such as text classification, extraction of sections of interest, entity recognition, and formatting the output in a specific way. From what I&amp;#39;ve read, I might initially be able to achieve this with FewShots, but given the scope of the project, I think I might need to do some fine tuning. I might be a bit presumptuous, as I&amp;#39;m just starting to use this technology. Could anyone recommend any tutorials or resources for learning about Langchain and LLMs?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/ImGallo&quot;&gt; /u/ImGallo &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1djyu3k/recommendations_for_using_llms_in_text_processing/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1djyu3k/recommendations_for_using_llms_in_text_processing/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1djyu3k</id><link href="https://www.reddit.com/r/LangChain/comments/1djyu3k/recommendations_for_using_llms_in_text_processing/" /><updated>2024-06-20T00:42:23+00:00</updated><published>2024-06-20T00:42:23+00:00</published><title>Recommendations for Using LLMs in Text Processing Projects</title></entry><entry><author><name>/u/kid_90</name><uri>https://www.reddit.com/user/kid_90</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Recently I built a chatbot with OpenAI Assistant API. Its doing what we require it to do which brought me to thinking whats the point of langchain or maybe I dont understand it well.&lt;/p&gt; &lt;p&gt;For example, I have custom knowledge base, I upload it to OpenAI Vector Store, connect it to my Assistant and I have a chatbot. Where does langchain come in this?&lt;/p&gt; &lt;p&gt;Or if I upload my knowledge base to any vector database for example, Pinecone, then connect it with OpenAI API, I&amp;#39;d still get a chatbot.&lt;/p&gt; &lt;p&gt;Please help me understand langchain on a deeper level.&lt;/p&gt; &lt;p&gt;Would really appreciate this. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/kid_90&quot;&gt; /u/kid_90 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1djtbtj/still_cant_grasp_the_idea_of_langchain_with/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1djtbtj/still_cant_grasp_the_idea_of_langchain_with/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1djtbtj</id><link href="https://www.reddit.com/r/LangChain/comments/1djtbtj/still_cant_grasp_the_idea_of_langchain_with/" /><updated>2024-06-19T20:35:25+00:00</updated><published>2024-06-19T20:35:25+00:00</published><title>Still cant grasp the idea of Langchain with OpenAI Assistant</title></entry><entry><author><name>/u/bucketheadfan13</name><uri>https://www.reddit.com/user/bucketheadfan13</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;ve looked into completely using Google Big Query to store, embed, and vector search the results since they now offer Vector Searches&lt;/p&gt; &lt;p&gt;Does anyone have any experience doing this with Google Big Query alone?&lt;/p&gt; &lt;p&gt;Would it be better to just import the data into something line Pinecone and use LangChain to chunk/query?&lt;/p&gt; &lt;p&gt;Or could I also just use LangChain with Google Big Query?&lt;/p&gt; &lt;p&gt;Also not sure if I should be chunking the data, or how chunking would work if I needed it to be on an item by item basis&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/bucketheadfan13&quot;&gt; /u/bucketheadfan13 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1djm57k/whats_the_best_way_to_chunk_store_and_query/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1djm57k/whats_the_best_way_to_chunk_store_and_query/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1djm57k</id><link href="https://www.reddit.com/r/LangChain/comments/1djm57k/whats_the_best_way_to_chunk_store_and_query/" /><updated>2024-06-19T15:35:46+00:00</updated><published>2024-06-19T15:35:46+00:00</published><title>What's the best way to chunk, store and, query extremely large datasets where the data is in a CSV/SQL type format (item by item basis with name, description, etc., not a large text file)</title></entry><entry><author><name>/u/dccpt</name><uri>https://www.reddit.com/user/dccpt</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1djkolz/zep_longterm_memory_free_plan_upgraded_to_10k/&quot;&gt; &lt;img src=&quot;https://external-preview.redd.it/LW_D9uWwns4LU9ujOrFRUZxvcoo09iNqWTmeA5II1BQ.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2054db40fb5ea832bf5fcd3e98ecee071501124a&quot; alt=&quot;Zep Long-term Memory: Free Plan Upgraded to 10K Messages&quot; title=&quot;Zep Long-term Memory: Free Plan Upgraded to 10K Messages&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi all - we received some friendly criticism on this subreddit a while back about Zep&amp;#39;s Free Plan limit of 1K messages per month. We&amp;#39;ve heard y&amp;#39;all and have increased the monthly limit 10x to 10K messages. You can sign up here: &lt;a href=&quot;https://www.getzep.com/&quot;&gt;https://www.getzep.com/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We also recently &lt;a href=&quot;https://app.getzep.com/playground&quot;&gt;released a Playground&lt;/a&gt;, allowing you experiment with Zep&amp;#39;s long-term memory features without writing any code.&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://i.redd.it/g2kv7ue7hj7d1.gif&quot;&gt;https://i.redd.it/g2kv7ue7hj7d1.gif&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Getting Started&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Learn about &lt;a href=&quot;https://help.getzep.com/concepts&quot;&gt;key Zep concepts&lt;/a&gt; such as Sessions, Facts, and more.&lt;/li&gt; &lt;li&gt;Experiment with &lt;a href=&quot;https://app.getzep.com/playground&quot;&gt;Zep in the Playground&lt;/a&gt; and &lt;a href=&quot;https://help.getzep.com/building-prompt&quot;&gt;learn how to build LLM prompts&lt;/a&gt; with Zep.&lt;/li&gt; &lt;li&gt;Install Zep&amp;#39;s &lt;a href=&quot;https://help.getzep.com/sdks&quot;&gt;Python, TypeScript, or Go SDKs&lt;/a&gt; and add long-term memory to your application.&lt;/li&gt; &lt;li&gt;Learn how to use &lt;a href=&quot;https://help.getzep.com/langchain/overview&quot;&gt;Zep with LangChain LCEL&lt;/a&gt;.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Let me know if you have any questions!&lt;/p&gt; &lt;p&gt;-Daniel&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/dccpt&quot;&gt; /u/dccpt &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1djkolz/zep_longterm_memory_free_plan_upgraded_to_10k/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1djkolz/zep_longterm_memory_free_plan_upgraded_to_10k/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1djkolz</id><media:thumbnail url="https://external-preview.redd.it/LW_D9uWwns4LU9ujOrFRUZxvcoo09iNqWTmeA5II1BQ.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=2054db40fb5ea832bf5fcd3e98ecee071501124a" /><link href="https://www.reddit.com/r/LangChain/comments/1djkolz/zep_longterm_memory_free_plan_upgraded_to_10k/" /><updated>2024-06-19T14:34:27+00:00</updated><published>2024-06-19T14:34:27+00:00</published><title>Zep Long-term Memory: Free Plan Upgraded to 10K Messages</title></entry><entry><author><name>/u/Strange-Ant-4194</name><uri>https://www.reddit.com/user/Strange-Ant-4194</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;m trying to make llama 3 identify whether or not the user is in a meeting by feeding it this prompt:&lt;/p&gt; &lt;p&gt;&amp;lt;|begin_of_text|&amp;gt;&amp;lt;|start_header_id|&amp;gt;system&amp;lt;|end_header_id|&amp;gt; You are a helpful agent who will answer the user&amp;#39;s question to the best of your abilities. You are NOT allowed to return blank results. &lt;/p&gt; &lt;p&gt;Return ONLY Strings &lt;/p&gt; &lt;p&gt;&amp;lt;|eot_id|&amp;gt;&amp;lt;|start_header_id|&amp;gt;user&amp;lt;|end_header_id|&amp;gt; &lt;/p&gt; &lt;p&gt;Here is the context:&lt;/p&gt; &lt;p&gt;Here is the &amp;#39;contact list&amp;#39; containing the names of contacts and their respective numbers, and the &amp;#39;relationship list&amp;#39; containing their relationships to the user:&lt;/p&gt; &lt;p&gt;Contact list:&lt;/p&gt; &lt;p&gt;1.Priya, +911234567890&lt;/p&gt; &lt;p&gt;2.Kau, +910987654321&lt;/p&gt; &lt;p&gt;3.Laksh, +912234567890&lt;/p&gt; &lt;p&gt;4.Agilan, +919987654321&lt;/p&gt; &lt;p&gt;5.Srikar, +913234567890&lt;/p&gt; &lt;p&gt;6.Prahlad, +918987654321&lt;/p&gt; &lt;p&gt;Relationship list:&lt;/p&gt; &lt;p&gt;1.Priya is &amp;quot;Wife&amp;quot;&lt;/p&gt; &lt;p&gt;2.Kau is &amp;quot;Boss&amp;quot;&lt;/p&gt; &lt;p&gt;3.Laksh is &amp;quot;Brother&amp;quot;&lt;/p&gt; &lt;p&gt;4.Agilan is &amp;quot;Son&amp;quot;&lt;/p&gt; &lt;p&gt;5.Srikar is &amp;quot;Sister&amp;quot;&lt;/p&gt; &lt;p&gt;6.Prahlad is &amp;quot;Daughter&amp;quot;&lt;/p&gt; &lt;p&gt;Here is the calendar:&lt;/p&gt; &lt;p&gt;Meeting1 from 11:06-12:54&lt;/p&gt; &lt;p&gt;Meeting2 from 13:00-15:00&lt;/p&gt; &lt;p&gt;Meeting3 from 15:25-18:00 &lt;/p&gt; &lt;p&gt;Current time: {curt}&lt;/p&gt; &lt;p&gt;Determine: &lt;/p&gt; &lt;p&gt;1.Whether or not the user is in a meeting(use the calendar). If the current time comes after the start time of any meeting and before the end time of that same meeting. the user is in a meeting, else if the current time is before the start time of all meetings or after the end time of all meetings, the user is not in a meeting). TAKE INTO ACCOUNT the EXACT HOURS AND MINUTES of the meeting timings and current time. Even a difference of one minute must be taken into account. &lt;/p&gt; &lt;p&gt;Summarise your findings. &lt;/p&gt; &lt;p&gt;&amp;lt;|eot_id|&amp;gt;&amp;lt;|start_header_id|&amp;gt;assistant&amp;lt;|end_header_id|&amp;gt;&lt;/p&gt; &lt;p&gt;This is just one of many prompts that i&amp;#39;ve given to llama3. In each of them, it always gets it wrong for certain timings.&lt;/p&gt; &lt;p&gt;Note: The contact list and relationship list is not being used for this particular task. It&amp;#39;s just that they all belong to the same file are displayed together.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Strange-Ant-4194&quot;&gt; /u/Strange-Ant-4194 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1djfibu/how_do_i_prompt_llama38b_to_work_accurately_for/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1djfibu/how_do_i_prompt_llama38b_to_work_accurately_for/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1djfibu</id><link href="https://www.reddit.com/r/LangChain/comments/1djfibu/how_do_i_prompt_llama38b_to_work_accurately_for/" /><updated>2024-06-19T10:05:04+00:00</updated><published>2024-06-19T10:05:04+00:00</published><title>How do i prompt llama3:8b to work accurately for this specific task?</title></entry><entry><author><name>/u/Ukpersfidev</name><uri>https://www.reddit.com/user/Ukpersfidev</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I don&amp;#39;t have access to the chunks when using the `StructuredOutputParser(zodSchema)`, streaming works fine when using the text parser, but not this one.&lt;/p&gt; &lt;p&gt;My parsing needs are quite simple, I just need a an array with each variation of generated content, does anyone know which parser I can use for this?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Ukpersfidev&quot;&gt; /u/Ukpersfidev &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1djimi6/streaming_structured_output/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1djimi6/streaming_structured_output/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1djimi6</id><link href="https://www.reddit.com/r/LangChain/comments/1djimi6/streaming_structured_output/" /><updated>2024-06-19T13:01:38+00:00</updated><published>2024-06-19T13:01:38+00:00</published><title>Streaming structured output?</title></entry><entry><author><name>/u/Informal-Victory8655</name><uri>https://www.reddit.com/user/Informal-Victory8655</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi, I&amp;#39;m working on a project for learning purposes, to build a langchain app where user can upload files (for now .pdf files) and do QA with uploaded files using pinecone vector database.&lt;/p&gt; &lt;p&gt;Now, I want to know what should I do to add a functionality to display all the files uploaded by a user at any time. and at any time user can limit his QA to specific files he select from all the files he uploaded. Now to do this using pinecone I first have to get all the vectors ids and for each vector id, I need to get the source in metadata and find unique of the sources, what if the no of vectors is very large and user has uploaded many files, so this method is not feasible at all.&lt;/p&gt; &lt;p&gt;what can I do to save the list of all files uploaded by a user somewhere and instantly get the list of files for any user.&lt;/p&gt; &lt;p&gt;I&amp;#39;m considering production perspective while learning these things. I don&amp;#39;t wanna store anything locally. Consider only frontend would be deployed somewhere. &lt;/p&gt; &lt;p&gt;Is there any thing simple to use like pinecone but for only user specific information retrieval purposes.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Informal-Victory8655&quot;&gt; /u/Informal-Victory8655 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1djhxc4/an_app_to_allow_user_to_upload_files_for_now_pdf/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1djhxc4/an_app_to_allow_user_to_upload_files_for_now_pdf/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1djhxc4</id><link href="https://www.reddit.com/r/LangChain/comments/1djhxc4/an_app_to_allow_user_to_upload_files_for_now_pdf/" /><updated>2024-06-19T12:27:30+00:00</updated><published>2024-06-19T12:27:30+00:00</published><title>An app to allow user to upload files (for now .pdf) and do QA from the uploaded files. Expert Advice Needed.</title></entry><entry><author><name>/u/Lethal_Protector_404</name><uri>https://www.reddit.com/user/Lethal_Protector_404</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;m building a chatbot that can perform multiple actions, with each action managed by a separate agent tailored to a specific use case. Initially, I created a query router using an LLM chain to determine the appropriate agent for a given query. However, as the number of agents has grown, the static query router with if-else conditions is becoming inefficient and unmanageable. I&amp;#39;m seeking guidance on how to improve the query routing mechanism to handle a large number of agents more efficiently. Any suggestions or best practices would be greatly appreciated.&lt;/p&gt; &lt;p&gt;Thanks in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Lethal_Protector_404&quot;&gt; /u/Lethal_Protector_404 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1djhpg4/looking_for_a_dynamic_approach_for_query_router/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1djhpg4/looking_for_a_dynamic_approach_for_query_router/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1djhpg4</id><link href="https://www.reddit.com/r/LangChain/comments/1djhpg4/looking_for_a_dynamic_approach_for_query_router/" /><updated>2024-06-19T12:15:58+00:00</updated><published>2024-06-19T12:15:58+00:00</published><title>Looking for a Dynamic approach for Query Router</title></entry><entry><author><name>/u/93simoon</name><uri>https://www.reddit.com/user/93simoon</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I&amp;#39;ve been trying to get the &lt;a href=&quot;https://github.com/langchain-ai/langgraph-example&quot;&gt;langgraph api &lt;/a&gt;to work on my Windows machine, but I&amp;#39;ve hit a frustrating roadblock. Here&amp;#39;s what&amp;#39;s been happening:&lt;/p&gt; &lt;p&gt;I&amp;#39;ve got Docker Desktop up and running. Next step was to fire up &lt;code&gt;langgraph&lt;/code&gt;. After installing &lt;code&gt;langgraph-cli&lt;/code&gt; and setting up my Python environment, I ran &lt;code&gt;langgraph up&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;However I was greeted with this error that seem to be related to the process not being ran on a unix system. The error stack looks like this:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Traceback (most recent call last): File &amp;quot;c:\workspace\python\delfi\.venv\Lib\site-packages\langgraph_cli\exec.py&amp;quot;, line 64, in subp_exec loop.add_signal_handler(signal.SIGINT, signal_handler) File &amp;quot;C:\Users\user\AppData\Local\Programs\Python\Python311\Lib\asyncio\events.py&amp;quot;, line 574, in add_signal_handler raise NotImplementedError NotImplementedError During handling of the above exception, another exception occurred: Traceback (most recent call last): File &amp;quot;&amp;lt;frozen runpy&amp;gt;&amp;quot;, line 198, in _run_module_as_main File &amp;quot;&amp;lt;frozen runpy&amp;gt;&amp;quot;, line 88, in _run_code File &amp;quot;c:\workspace\python\delfi\.venv\Scripts\langgraph.exe\__main__.py&amp;quot;, line 7, in &amp;lt;module&amp;gt; File &amp;quot;c:\workspace\python\delfi\.venv\Lib\site-packages\click\core.py&amp;quot;, line 1157, in __call__ return self.main(*args, **kwargs) ^^^^^^^^^^^^^^^^^^^^^^^^^^ File &amp;quot;c:\workspace\python\delfi\.venv\Lib\site-packages\click\core.py&amp;quot;, line 1078, in main rv = self.invoke(ctx) ^^^^^^^^^^^^^^^^ File &amp;quot;c:\workspace\python\delfi\.venv\Lib\site-packages\click\core.py&amp;quot;, line 1688, in invoke return _process_result(sub_ctx.command.invoke(sub_ctx)) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File &amp;quot;c:\workspace\python\delfi\.venv\Lib\site-packages\click\core.py&amp;quot;, line 1434, in invoke return ctx.invoke(self.callback, **ctx.params) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File &amp;quot;c:\workspace\python\delfi\.venv\Lib\site-packages\click\core.py&amp;quot;, line 783, in invoke return __callback(*args, **kwargs) ^^^^^^^^^^^^^^^^^^^^^^^^^^^ File &amp;quot;c:\workspace\python\delfi\.venv\Lib\site-packages\langgraph_cli\cli.py&amp;quot;, line 183, in up capabilities = langgraph_cli.docker.check_capabilities(runner) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File &amp;quot;c:\workspace\python\delfi\.venv\Lib\site-packages\langgraph_cli\docker.py&amp;quot;, line 83, in check_capabilities stdout, _ = runner.run(subp_exec(&amp;quot;docker&amp;quot;, &amp;quot;info&amp;quot;, &amp;quot;-f&amp;quot;, &amp;quot;json&amp;quot;, collect=True)) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File &amp;quot;C:\Users\user\AppData\Local\Programs\Python\Python311\Lib\asyncio\runners.py&amp;quot;, line 118, in run return self._loop.run_until_complete(task) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File &amp;quot;C:\Users\user\AppData\Local\Programs\Python\Python311\Lib\asyncio\base_events.py&amp;quot;, line 650, in run_until_complete return future.result() ^^^^^^^^^^^^^^^ File &amp;quot;c:\workspace\python\delfi\.venv\Lib\site-packages\langgraph_cli\exec.py&amp;quot;, line 103, in subp_exec os.killpg(os.getpgid(proc.pid), signal.SIGINT) ^^^^^^^^^ AttributeError: module &amp;#39;os&amp;#39; has no attribute &amp;#39;killpg&amp;#39;. Did you mean: &amp;#39;kill&amp;#39;? Exception ignored in: &amp;lt;function BaseSubprocessTransport.__del__ at 0x000001EA5ECC2E80&amp;gt; Traceback (most recent call last): File &amp;quot;C:\Users\user\AppData\Local\Programs\Python\Python311\Lib\asyncio\base_subprocess.py&amp;quot;, line 126, in __del__ File &amp;quot;C:\Users\user\AppData\Local\Programs\Python\Python311\Lib\asyncio\base_subprocess.py&amp;quot;, line 104, in close File &amp;quot;C:\Users\user\AppData\Local\Programs\Python\Python311\Lib\asyncio\proactor_events.py&amp;quot;, line 108, in close File &amp;quot;C:\Users\user\AppData\Local\Programs\Python\Python311\Lib\asyncio\base_events.py&amp;quot;, line 758, in call_soon File &amp;quot;C:\Users\user\AppData\Local\Programs\Python\Python311\Lib\asyncio\base_events.py&amp;quot;, line 519, in _check_closed RuntimeError: Event loop is closed &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;I&amp;#39;m running Python 3.11 on Windows 11. The github page doesn&amp;#39;t mention anything about this being linux exclusive.&lt;/p&gt; &lt;p&gt;Has anyone else encountered similar issues or found a workaround? Your insights would be immensely helpful!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/93simoon&quot;&gt; /u/93simoon &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1djha74/issue_running_langgraph_api_on_windows/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1djha74/issue_running_langgraph_api_on_windows/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1djha74</id><link href="https://www.reddit.com/r/LangChain/comments/1djha74/issue_running_langgraph_api_on_windows/" /><updated>2024-06-19T11:53:20+00:00</updated><published>2024-06-19T11:53:20+00:00</published><title>Issue running langgraph api on Windows</title></entry></feed>