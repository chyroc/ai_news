<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/"><category term="LangChain" label="r/LangChain"/><updated>2024-07-15T05:38:10+00:00</updated><icon>https://www.redditstatic.com/icon.png/</icon><id>/r/LangChain.rss</id><link rel="self" href="https://www.reddit.com/r/LangChain.rss" type="application/atom+xml" /><link rel="alternate" href="https://www.reddit.com/r/LangChain" type="text/html" /><subtitle>LangChain is an open-source framework and developer toolkit that helps developers get LLM applications from prototype to production. It is available for Python and Javascript at https://www.langchain.com/.</subtitle><title>LangChain</title><entry><author><name>/u/zchaarm</name><uri>https://www.reddit.com/user/zchaarm</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;A place for members of &lt;a href=&quot;/r/LangChain&quot;&gt;r/LangChain&lt;/a&gt; to chat with each other&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/zchaarm&quot;&gt; /u/zchaarm &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_10ljho9</id><link href="https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/" /><updated>2023-01-26T04:33:05+00:00</updated><published>2023-01-26T04:33:05+00:00</published><title>r/LangChain Lounge</title></entry><entry><author><name>/u/Kv-boii</name><uri>https://www.reddit.com/user/Kv-boii</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;m currently exploring options to reduce the response time/ inference time through multithreading the prompts, so is it possible and what are some other ways to execute multiple prompts at a same time.&lt;br/&gt; Currently I&amp;#39;m working with the phi-3 models in google colab free, i&amp;#39;m always encountering a error related to a parameter &amp;#39;num_new_tokens&amp;#39; and the prompts are not executing.&lt;/p&gt; &lt;p&gt;If anyone has already implemented multithreading or multiprocessing of LLMs can you explain or provide a link to the resource&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Kv-boii&quot;&gt; /u/Kv-boii &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e3mqf3/is_multi_threading_possible_for_llms_integrated/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e3mqf3/is_multi_threading_possible_for_llms_integrated/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1e3mqf3</id><link href="https://www.reddit.com/r/LangChain/comments/1e3mqf3/is_multi_threading_possible_for_llms_integrated/" /><updated>2024-07-15T05:14:57+00:00</updated><published>2024-07-15T05:14:57+00:00</published><title>Is multi threading possible for llms integrated from the vllm framework?</title></entry><entry><author><name>/u/sks8100</name><uri>https://www.reddit.com/user/sks8100</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey guys so I had a question. Have a simple api that in an input, calls an LLM for processing (I use llama3 with grok at the moment) and returns an output. When I run it on my computer it runs fine. Takes a few seconds but runs. Most of this is fancy prompting so I haven’t fine tuned the model or anything at the moment. It’s just chained prompts and LLMs&lt;/p&gt; &lt;p&gt;I tried to deploy it on render to test out but I keep getting a memory error given it’s taken more than 512MB given that is renders free tier. &lt;/p&gt; &lt;p&gt;Does anybody know where else I can try this out? Another server or even a real cheap one? &lt;/p&gt; &lt;p&gt;As for long term I need to move passed grok and deploy my own LLM using olama or just move to open AI. I haven’t moved yet because I find the output to be better with llama3 vs OpenAI 3.5….4o is too cost prohibitive at the moment &lt;/p&gt; &lt;p&gt;Any advice? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/sks8100&quot;&gt; /u/sks8100 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e3lu2v/deploy_api_with_llm_backend/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e3lu2v/deploy_api_with_llm_backend/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1e3lu2v</id><link href="https://www.reddit.com/r/LangChain/comments/1e3lu2v/deploy_api_with_llm_backend/" /><updated>2024-07-15T04:20:37+00:00</updated><published>2024-07-15T04:20:37+00:00</published><title>Deploy Api with LLM backend</title></entry><entry><author><name>/u/giagara</name><uri>https://www.reddit.com/user/giagara</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I made a RAG system with Langchain just becasuse a coworker did a POC with it, and i&amp;#39;ve structured it much better. At the moment i use pgvector as vectorstore adding some filter to it based on metadata, pass the document to the stuff chain and get a streaming answer.&lt;/p&gt; &lt;p&gt;Now the business requires some new features, like parent document retriever, calculating costs, hybrid search/reranking, etc.. that i find very tough using Langchain instead of making all vanilla.&lt;/p&gt; &lt;p&gt;At the moment i just place a postgres query, call openAI to rephrase my question and history, and call openAI again to get the answer. Is nothing that fancy, isn&amp;#39;t it?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/giagara&quot;&gt; /u/giagara &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e2vx5m/why_should_i_keep_using_langchain/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e2vx5m/why_should_i_keep_using_langchain/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1e2vx5m</id><link href="https://www.reddit.com/r/LangChain/comments/1e2vx5m/why_should_i_keep_using_langchain/" /><updated>2024-07-14T06:57:00+00:00</updated><published>2024-07-14T06:57:00+00:00</published><title>Why should i keep using langchain?</title></entry><entry><author><name>/u/Rough_Grapefruit1900</name><uri>https://www.reddit.com/user/Rough_Grapefruit1900</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e2xzbu/stupid_question_which_tool_langchain_team_use_to/&quot;&gt; &lt;img src=&quot;https://b.thumbs.redditmedia.com/CrRC1zbzhfsXvbhN48SORaeZg8aNaHaJQb7JLMzXK3c.jpg&quot; alt=&quot;stupid question : which tool LangChain team use to draw their schema ?&quot; title=&quot;stupid question : which tool LangChain team use to draw their schema ?&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Love LangChain schematic, which tool they use ? Do you have any recommandation to draw easily agentic/LLM app architecture ?&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/905gadmjbgcd1.png?width=2893&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=dbc2601a5ad8917901a4ab23c88a60959681411f&quot;&gt;https://preview.redd.it/905gadmjbgcd1.png?width=2893&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=dbc2601a5ad8917901a4ab23c88a60959681411f&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Rough_Grapefruit1900&quot;&gt; /u/Rough_Grapefruit1900 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e2xzbu/stupid_question_which_tool_langchain_team_use_to/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e2xzbu/stupid_question_which_tool_langchain_team_use_to/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1e2xzbu</id><media:thumbnail url="https://b.thumbs.redditmedia.com/CrRC1zbzhfsXvbhN48SORaeZg8aNaHaJQb7JLMzXK3c.jpg" /><link href="https://www.reddit.com/r/LangChain/comments/1e2xzbu/stupid_question_which_tool_langchain_team_use_to/" /><updated>2024-07-14T09:15:57+00:00</updated><published>2024-07-14T09:15:57+00:00</published><title>stupid question : which tool LangChain team use to draw their schema ?</title></entry><entry><author><name>/u/ML_DL_RL</name><uri>https://www.reddit.com/user/ML_DL_RL</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Super excited to share that our iOS app is live for beta testers. In case you want to join please visit us at: &lt;a href=&quot;https://myreflection.ai/&quot;&gt;https://myreflection.ai/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;MyReflection is a memory preservation agent on steroids, encompassing images, audios, and journals. Imagine interacting with these memories, reminiscing, and exploring them. It&amp;#39;s like a mirror allowing you to further reflect on your thoughts, ideas, or experiences. Through these memories, we enable our users to create a digital interactive twin of themselves later on.&lt;/p&gt; &lt;p&gt;This was built keeping user security and privacy on top of our list. Please give it a test drive would love to hear your feedback. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/ML_DL_RL&quot;&gt; /u/ML_DL_RL &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e37ksf/memory_preservation_using_ai_beta_testing_ios_app/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e37ksf/memory_preservation_using_ai_beta_testing_ios_app/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1e37ksf</id><link href="https://www.reddit.com/r/LangChain/comments/1e37ksf/memory_preservation_using_ai_beta_testing_ios_app/" /><updated>2024-07-14T17:15:03+00:00</updated><published>2024-07-14T17:15:03+00:00</published><title>Memory Preservation using AI (Beta testing iOS App)</title></entry><entry><author><name>/u/RepresentativeAge297</name><uri>https://www.reddit.com/user/RepresentativeAge297</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I recently started with Python and the project I built first was a simple project based on the rag model. High level is I used a book and parsed it , created chunks and stored the embeddings in the chromadb (which is a vector db). &lt;/p&gt; &lt;p&gt;Based on user query finds the most relevant first matching chunk and passes it to llm with this info as context and user query. The llm(gemini-pro) generates the response &lt;/p&gt; &lt;p&gt;Doubt:&lt;/p&gt; &lt;p&gt;Are these type of projects are even good projects as I&amp;#39;ve just completed my graduation 2 weeks ago in computer science. Does this type of projects are even good for fresher? What more interesting things I should do and how do I measure that the my rag model produces better results compared to directly asking the llm same query. What are the parameters the length, Grammer or etc of the generated response &lt;/p&gt; &lt;p&gt;I&amp;#39;ve just started this and there might be some very basic or dumb doubts from my side. I&amp;#39;m looking for some direction I could go on with Gen-ai &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/RepresentativeAge297&quot;&gt; /u/RepresentativeAge297 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e2zh63/beginner_using_langchain_having_some_doubts/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e2zh63/beginner_using_langchain_having_some_doubts/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1e2zh63</id><link href="https://www.reddit.com/r/LangChain/comments/1e2zh63/beginner_using_langchain_having_some_doubts/" /><updated>2024-07-14T10:53:07+00:00</updated><published>2024-07-14T10:53:07+00:00</published><title>Beginner using langchain having some doubts</title></entry><entry><author><name>/u/OrioMax</name><uri>https://www.reddit.com/user/OrioMax</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Has anyone worked on or have any resources regarding using the self query retrieval mechanism with langchain agent and got exact response from agent without any hallucinations or not able get required documents from vectordb like problems.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/OrioMax&quot;&gt; /u/OrioMax &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e32ffw/langchain_agent_with_self_query_retrieval/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e32ffw/langchain_agent_with_self_query_retrieval/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1e32ffw</id><link href="https://www.reddit.com/r/LangChain/comments/1e32ffw/langchain_agent_with_self_query_retrieval/" /><updated>2024-07-14T13:33:49+00:00</updated><published>2024-07-14T13:33:49+00:00</published><title>Langchain Agent with self query retrieval.</title></entry><entry><author><name>/u/hi87</name><uri>https://www.reddit.com/user/hi87</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I created some documents to test out the performance of the vector store and embedding, and when using LangChain batch method to process multiple queries at the same time I get different results even for pretty straightforward queries. I have two documents each contains info about golf and tennis. The query to the retriever is simply retriever.batch([Tennis, Golf]).&lt;/p&gt; &lt;p&gt;The response changes from one call to the next. Sometimes the batch returns content related to only golf or sometimes only tennis. I thought maybe it’s because the similarity closeness of the sports in vector space so I added a third document with “cats,dogs” etc and randomly the batch method sometimes returns this document as the most relevant. &lt;/p&gt; &lt;p&gt;Extremely confused. Could it be an issue with the embeddings?&lt;/p&gt; &lt;p&gt;UPDATE: I tried another vector db (Chromadb) instead of Weaviate and the issue was resolved. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/hi87&quot;&gt; /u/hi87 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e2hnoq/are_vector_embeddings_similarity_search/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e2hnoq/are_vector_embeddings_similarity_search/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1e2hnoq</id><link href="https://www.reddit.com/r/LangChain/comments/1e2hnoq/are_vector_embeddings_similarity_search/" /><updated>2024-07-13T18:46:58+00:00</updated><published>2024-07-13T18:46:58+00:00</published><title>Are vector embeddings / similarity search non-deterministic?</title></entry><entry><author><name>/u/Tuxedotux83</name><uri>https://www.reddit.com/user/Tuxedotux83</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I wrote a custom search tool class which does not use Google, but I am still unable to get the web research functionality to work since it keeps demanding a google api key?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Tuxedotux83&quot;&gt; /u/Tuxedotux83 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e2w4zx/do_googleapi_a_requirement_for_the_web_research/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e2w4zx/do_googleapi_a_requirement_for_the_web_research/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1e2w4zx</id><link href="https://www.reddit.com/r/LangChain/comments/1e2w4zx/do_googleapi_a_requirement_for_the_web_research/" /><updated>2024-07-14T07:10:47+00:00</updated><published>2024-07-14T07:10:47+00:00</published><title>Do GoogleAPI a requirement for the web research class ?</title></entry><entry><author><name>/u/Vivid-Force8042</name><uri>https://www.reddit.com/user/Vivid-Force8042</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;While implementing RAG in a chatbot pipeline, what are the common strategies for dealing with questions that are not relevant to the context?&lt;br/&gt; When asking questions about the context stored in my local DB the retriever gets the relevant data and the LLM generates the right answer. However, if I ask questions like &amp;quot;Who are you&amp;quot;, or &amp;quot;How are you?&amp;quot; I get hallucinations because the prompt contains under the hood also retrieved context.&lt;/p&gt; &lt;p&gt;I tried specifying in the system prompt that if the question is not relevant to the context, say I don&amp;#39;t know but it didn&amp;#39;t help.&lt;/p&gt; &lt;p&gt;Thank you all for your help&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Vivid-Force8042&quot;&gt; /u/Vivid-Force8042 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e2grkh/nonrelevant_questions_to_the_context_in_chatbot/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e2grkh/nonrelevant_questions_to_the_context_in_chatbot/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1e2grkh</id><link href="https://www.reddit.com/r/LangChain/comments/1e2grkh/nonrelevant_questions_to_the_context_in_chatbot/" /><updated>2024-07-13T18:07:28+00:00</updated><published>2024-07-13T18:07:28+00:00</published><title>Non-relevant questions to the context in chatbot</title></entry><entry><author><name>/u/DhairyaRaj13</name><uri>https://www.reddit.com/user/DhairyaRaj13</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;We are an ai chatbot company, I am to add a phone calling layer over the chatbot , I am looking for techstack for such usecase. I am using python for that. Want everything to be customised, Speech to Text , text to speech.&lt;/p&gt; &lt;p&gt;Currently I am to use Twillio. But I want to have full control over speech . To transcribe &amp;amp;etc i aim to use in house model &lt;/p&gt; &lt;p&gt;Suggest resources.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/DhairyaRaj13&quot;&gt; /u/DhairyaRaj13 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e25aed/ai_phone_calling_agent/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e25aed/ai_phone_calling_agent/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1e25aed</id><link href="https://www.reddit.com/r/LangChain/comments/1e25aed/ai_phone_calling_agent/" /><updated>2024-07-13T08:15:40+00:00</updated><published>2024-07-13T08:15:40+00:00</published><title>AI phone calling agent</title></entry><entry><author><name>/u/bferreira85</name><uri>https://www.reddit.com/user/bferreira85</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi! I would like to get suggestions on how to implement the following using langgraph: What i have today: i have a langgraph workflow containing one &amp;quot;supervisor&amp;quot; agent and I 2 specialised agents. When the user inputs a request the supervisor decides what agent to send the work to until it decides the work is finished. What i want to do now is: -for one of these agents I want to make it a two step operation. So the agent receives the request and then displays to the user information about what he is about to do. The user can then give an ok of ask for some tweak. Then the agent finishes the job. Another possibility here is that the agent decides it doesn&amp;#39;t has all the info and reaches to the user to ask for additional info (in that case he will also reach the user to confirm the operation). I imagine I can achieve this with breakpoints and collecting human input, but since I&amp;#39;ve never did it before i am a bit confusing on how the states and workflow should look like. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/bferreira85&quot;&gt; /u/bferreira85 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e276vp/langgraph_what_implementation_strategy_would_you/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e276vp/langgraph_what_implementation_strategy_would_you/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1e276vp</id><link href="https://www.reddit.com/r/LangChain/comments/1e276vp/langgraph_what_implementation_strategy_would_you/" /><updated>2024-07-13T10:24:21+00:00</updated><published>2024-07-13T10:24:21+00:00</published><title>Langgraph: what implementation strategy would you recommend for this case?</title></entry><entry><author><name>/u/Affectionate_Nose_39</name><uri>https://www.reddit.com/user/Affectionate_Nose_39</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I am currently working on a project where I need to process nested queries using Langchain. Specifically, I have a requirement where an LLM evaluates a rule for example address should contain company name, street name, pin code, city name, and state. The outer chain should wait for all the internal chains to process, and then the outer chain should be evaluated.&lt;/p&gt; &lt;p&gt;I have been trying to implement this using Land Chain runnables, but I am facing some challenges in getting it to work as expected. Could anyone provide some guidance or examples on how to properly structure the Land Chain runnables to achieve this nested query processing behavior?&lt;/p&gt; &lt;p&gt;Any help, code snippets, or resources would be greatly appreciated. Thank you in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Affectionate_Nose_39&quot;&gt; /u/Affectionate_Nose_39 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e2acs1/need_help_with_using_langchain_runnables/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e2acs1/need_help_with_using_langchain_runnables/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1e2acs1</id><link href="https://www.reddit.com/r/LangChain/comments/1e2acs1/need_help_with_using_langchain_runnables/" /><updated>2024-07-13T13:24:07+00:00</updated><published>2024-07-13T13:24:07+00:00</published><title>Need help with using Langchain runnables ..</title></entry><entry><author><name>/u/Altruistic-Box5900</name><uri>https://www.reddit.com/user/Altruistic-Box5900</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi, I am building an AI agent with Claude 3.5. My functions are not being invoked by the agent. My understanding was that Langchain automatically calls the agent, but it is not being called. I do, however, am receiving as a model response that function needs to be called. Here&amp;#39;s the model response and my code:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;&amp;gt; Entering new AgentExecutor chain... content: Certainly content: , content: ****. I content: &amp;#39;ll create content: a new I content: **** for you content: using content: the available content: function content: . content: toolu_01G44ahCcrqSaXgiUWrvX85L content: None [{&amp;#39;text&amp;#39;: &amp;quot;Certainly, *****. I&amp;#39;ll create ******* for you using the available function.&amp;quot;, &amp;#39;type&amp;#39;: &amp;#39;text&amp;#39;, &amp;#39;index&amp;#39;: 0}, {&amp;#39;id&amp;#39;: &amp;#39;toolu_01G44ahCcrqSaXgiUWrvX85L&amp;#39;, &amp;#39;input&amp;#39;: {}, &amp;#39;name&amp;#39;: &amp;#39;*******&amp;#39;, &amp;#39;type&amp;#39;: &amp;#39;tool_use&amp;#39;, &amp;#39;index&amp;#39;: 1, &amp;#39;partial_json&amp;#39;: &amp;#39;&amp;#39;}] &amp;gt; Finished chain. llm_with_tools = llm.bind_tools(tools) agent = ( { &amp;quot;input&amp;quot;: lambda x: x[&amp;quot;input&amp;quot;], &amp;quot;agent_scratchpad&amp;quot;: lambda x: format_to_openai_tool_messages( x[&amp;quot;intermediate_steps&amp;quot;] ), &amp;quot;chat_history&amp;quot;: lambda x: x[&amp;quot;chat_history&amp;quot;], } | prompt | llm_with_tools | OpenAIToolsAgentOutputParser() ) agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True) &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Altruistic-Box5900&quot;&gt; /u/Altruistic-Box5900 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e1r8rk/tool_calling_with_claude_35_not_working/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e1r8rk/tool_calling_with_claude_35_not_working/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1e1r8rk</id><link href="https://www.reddit.com/r/LangChain/comments/1e1r8rk/tool_calling_with_claude_35_not_working/" /><updated>2024-07-12T20:10:02+00:00</updated><published>2024-07-12T20:10:02+00:00</published><title>Tool calling with Claude 3.5 not working</title></entry><entry><author><name>/u/WarriorA</name><uri>https://www.reddit.com/user/WarriorA</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hello dear swarm intelligence practitioner! I have come to you in a time of great need.&lt;/p&gt; &lt;p&gt;Imagine you have a document about some topic. Lets say the GoldenGate Bridge. And this document has multiple wonderfully written paragraphs about this Bridge. This includes materials used, the timeframe of when it was built, and much much more.&lt;br/&gt; This document then goes on writing about California and San Francisco.&lt;/p&gt; &lt;p&gt;A few paragraphs down maybe a few sentences about the pacific ocean.&lt;/p&gt; &lt;p&gt;And last but not least this: &amp;quot;It was designed by Joseph Strauss with the help of architect Irving Morrow and engineers Charles Alton Ellis and Leon Moisseiff. Construction was carried out by the McClintic-Marshall Construction Company, a subsidiary of Bethlehem Steel Corporation.&amp;quot;&lt;/p&gt; &lt;p&gt;This article is chunked using common techniques and embedded into a vector index. Now, this RAG System will be used to answer Questions. We retrieve k docs from this index that are similar to the query, and rerank them to top_k documents to generate an answer. This pipeline works with almost all queries and documents.&lt;/p&gt; &lt;p&gt;The problem here would be to answer the question &amp;quot;&lt;strong&gt;Who built the Golden Gate Bridge?&amp;quot;&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;The retrieval process would find these chunks, and confidently (and correctly) rank them as the most relevant for our question. However, the chunk with the information we are actually looking for will not be retrieved, as it is not at all similar to the query.&lt;/p&gt; &lt;p&gt;How can this question be answered, while maintaining performance?&lt;/p&gt; &lt;ol&gt; &lt;li&gt;When finding a high scoring chunk (e.g: Golden Gate Bridge chunk from the top of the article) we fetch the document in its entirety, and &lt;strong&gt;provide the entire article&lt;/strong&gt; to the generator LLM. Problem here is, that we have to either make a decision to do that based on the query, or do it every time. Both are compromises I want to avoid.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Bigger chunks&lt;/strong&gt; would include the information. Problem here is, that we can&amp;#39;t really generalize. What if there is more content in between? This won&amp;#39;t work as reliably as I would like.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Summarize chunks&lt;/strong&gt; to make them smaller and be able to fit all of the context into the generator LLM. This has the same problem as Approach 1, about making a decision to instead of just going with the chunk to answer the query, we do the extra step of retrieving the entire article instead of(or its summary) before returning from the retrieval pipeline.&lt;/li&gt; &lt;li&gt;Use a &lt;strong&gt;LLM as a Reranker&lt;/strong&gt;. This LLM-Reranker would (hopefully) understand to return the relevant chunk (Joseph Strauss), instead of the similar chunks (Golden Gate Bridge). This would again imply, that we have this decision making process based on which Reranker should be applied.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Things that could help, but also don&amp;#39;t really work in a more general approach:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Hybrid Retrieval&lt;/strong&gt;: Embedding and Keywordbased retrieval will still most likely not find the relevant chunk. And even if, the Reranking process would drop it again.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Using NER/POS/TF-IDF keywords&lt;/strong&gt; alongside the text chunks seems do run into similar problems&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Query Expansion and Reformulation&lt;/strong&gt;: Similar Problem as above. Also I have tried this approach and including the Augmented Queries in the Rerank step results in other QA-Tasks to perform worse. IMO only the original query should be involved in reranking. I use LLM-Generated Queries for SimilaritySearch already, but then rerank those results with the original query.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Two Stage Retrieval&lt;/strong&gt;: Retrieve GoldenGate chunks -&amp;gt; get all chunks of this article -&amp;gt; find relevant chunks from this preselection of article-chunks. How can we do the last step without involving a slow LLM and also deciding to go into this TwoStage retrieval in the first place?&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Different Chunk Size&lt;/strong&gt;? Would probably work, but what if the document is just a little bit longer, and again we have the information in another chunk that the Bridge-Chunk? Doesn&amp;#39;t generalize well.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;In the current pipeline, even if the relevant chunk (Joseph Strauss) would appear in the retrieved chunks, lets say via hybrid-search using BM25, keyword matching or other strategies, this chunk would be dropped by the Reranker, because it is not similar to the query.&lt;/p&gt; &lt;p&gt;In conclusion, all approaches I can think of, include some sort of LLM to make a decision before returning the chunks to the generatorLLM and/or another decision based on all chunks of this article.&lt;/p&gt; &lt;p&gt;What are other approaches one could apply here? What is a best practice? How do others handle this problem?&lt;/p&gt; &lt;p&gt;I am looking for an approach that is robust and fast enough to work in a production grade system. Ideally some sort of transformer model similar to a reranker, that instead of returning similar matches it returns relevant matches.&lt;/p&gt; &lt;p&gt;No matter how similar all our chunks are to the Golden Gate Bridge and SanFrancisco and the pacific, the actual desired chunk is not similar. How can we handle this?&lt;/p&gt; &lt;p&gt;&lt;strong&gt;This approach seems the most promising to me, but also seems like a really daunting task?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Train a model to distinguish between &amp;quot;similar&amp;quot; and &amp;quot;relevant&amp;quot; content using contrastive learning techniques.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Create training pairs that include similar but irrelevant chunks as negative examples, and dissimilar but relevant chunks as positive examples.&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/WarriorA&quot;&gt; /u/WarriorA &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e1cdhp/retrieve_chunks_which_are_not_similar_but_still/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e1cdhp/retrieve_chunks_which_are_not_similar_but_still/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1e1cdhp</id><link href="https://www.reddit.com/r/LangChain/comments/1e1cdhp/retrieve_chunks_which_are_not_similar_but_still/" /><updated>2024-07-12T08:18:23+00:00</updated><published>2024-07-12T08:18:23+00:00</published><title>Retrieve Chunks which are not similar but still relevant</title></entry><entry><author><name>/u/Able_Scholar_2420</name><uri>https://www.reddit.com/user/Able_Scholar_2420</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;The model is about to extract the data from the pdf based bunch of questions. I tried different quantized model. But every time I tried i failed to make an output. The model giving me schema it self (jsonoutputparser) &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Able_Scholar_2420&quot;&gt; /u/Able_Scholar_2420 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e1jutc/please_suggest_me_better_open_source_model_for/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e1jutc/please_suggest_me_better_open_source_model_for/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1e1jutc</id><link href="https://www.reddit.com/r/LangChain/comments/1e1jutc/please_suggest_me_better_open_source_model_for/" /><updated>2024-07-12T15:02:46+00:00</updated><published>2024-07-12T15:02:46+00:00</published><title>Please Suggest me better open source model for getting json output (Rag operation).</title></entry><entry><author><name>/u/adlx</name><uri>https://www.reddit.com/user/adlx</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;m interested in adding reranking in our RAG application (rerank the chunks retrieved from a pinecone index similarity search).&lt;/p&gt; &lt;p&gt;We are using models deployed in Azure, mostly OpenAI. Ive seen Cohere offers a reranking model, but it&amp;#39;s not available in Azure not AWS Bedrock.&lt;/p&gt; &lt;p&gt;Do you know any approach that would be possible with models we could consume as service (API) on Azure, AWS bedrock or Google Cloud? (we don&amp;#39;t have any entreprise relationship with Cohere yet so I think any alternative would be faster to deploy). &lt;/p&gt; &lt;p&gt;Thanks in advance for any pointer. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/adlx&quot;&gt; /u/adlx &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e1i30i/reranker_models_experience/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e1i30i/reranker_models_experience/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1e1i30i</id><link href="https://www.reddit.com/r/LangChain/comments/1e1i30i/reranker_models_experience/" /><updated>2024-07-12T13:45:58+00:00</updated><published>2024-07-12T13:45:58+00:00</published><title>Reranker models experience</title></entry><entry><author><name>/u/The404Dude</name><uri>https://www.reddit.com/user/The404Dude</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I need to create an agent that can query a sql database to answer questions. I started playing with agents and function calling in langchain. Following the documentation, I put together a very simple code (see below) but it is definitely not working as intended. &lt;/p&gt; &lt;p&gt;I ask a very direct question&lt;br/&gt; &lt;code&gt;what is my resource consumption for the past 30 days?&lt;/code&gt;&lt;/p&gt; &lt;p&gt;and it invokes my tool with the wrong parameters - see date ranges&lt;/p&gt; &lt;p&gt;&lt;code&gt;Invoking: \&lt;/code&gt;get_resource_consumption` with `{&amp;#39;customer_id&amp;#39;: 1, &amp;#39;start_date&amp;#39;: &amp;#39;2023-09-21&amp;#39;, &amp;#39;end_date&amp;#39;: &amp;#39;2023-10-21&amp;#39;}``&lt;/p&gt; &lt;p&gt;and each time I run my app, it comes up with a different date range.&lt;/p&gt; &lt;p&gt;what am I doing wrong?&lt;/p&gt; &lt;pre&gt;&lt;code&gt;from langchain_core.tools import tool from langchain import hub from langchain_openai import ChatOpenAI from langchain.agents import create_tool_calling_agent from langchain_core.prompts import ChatPromptTemplate llm = ChatOpenAI(model=&amp;quot;gpt-4o&amp;quot;) @tool def get_resource_consumption(customer_id, start_date, end_date): &amp;quot;&amp;quot;&amp;quot;Get the resource consumption for a specific customer between two dates. Args: customer_id: The unique identifier of the customer. start_date: The start date of the period. end_date: The end date of the period. &amp;quot;&amp;quot;&amp;quot; return 100 tools = [get_resource_consumption] prompt = ChatPromptTemplate.from_messages( [ (&amp;quot;system&amp;quot;, &amp;quot;You are a helpful assistant that helps users with their questions about resource consumption.&amp;quot;), (&amp;quot;placeholder&amp;quot;, &amp;quot;{chat_history}&amp;quot;), (&amp;quot;human&amp;quot;, &amp;quot;{input}&amp;quot;), (&amp;quot;placeholder&amp;quot;, &amp;quot;{agent_scratchpad}&amp;quot;), ] ) prompt.pretty_print() agent = create_tool_calling_agent(llm, tools, prompt) from langchain.agents import AgentExecutor agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True) agent_executor.invoke({&amp;quot;input&amp;quot;: &amp;quot;what is my resource consumption for the past 30 days? customer_id = 1&amp;quot;}) &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/The404Dude&quot;&gt; /u/The404Dude &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e1h5c6/asking_for_specific_date_range_and_getting/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e1h5c6/asking_for_specific_date_range_and_getting/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1e1h5c6</id><link href="https://www.reddit.com/r/LangChain/comments/1e1h5c6/asking_for_specific_date_range_and_getting/" /><updated>2024-07-12T13:02:40+00:00</updated><published>2024-07-12T13:02:40+00:00</published><title>Asking for specific date range and getting something totally different. What am I doing wrong?</title></entry><entry><author><name>/u/MZuc</name><uri>https://www.reddit.com/user/MZuc</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;ve heard so many AI teams ask this question, I decided to sum up my take on this in a short post. Let me know what you guys think.&lt;/p&gt; &lt;p&gt;The way I see it, the first step is to change how you identify and approach problems. Too often, teams use vague terms like “it &lt;strong&gt;&lt;em&gt;feels&lt;/em&gt;&lt;/strong&gt; like” or “it &lt;strong&gt;&lt;em&gt;seems&lt;/em&gt;&lt;/strong&gt; like” instead of specific metrics, like “the feedback score for this type of request improved by 20%.”&lt;/p&gt; &lt;p&gt;When you&amp;#39;re developing a new AI-driven RAG application, the process tends to be chaotic. There are too many priorities and not enough time to tackle them all. Even if you could, you&amp;#39;re not sure how to enhance your RAG system. You sense that there&amp;#39;s a &amp;quot;right path&amp;quot; – a set of steps that would lead to maximum growth in the shortest time. There are a myriad of great trendy RAG libraries, pipelines, and tools out there but you don&amp;#39;t know which will work on &lt;em&gt;your&lt;/em&gt; documents and &lt;em&gt;your&lt;/em&gt; usecase (&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dr5kki/the_most_important_thing_to_build_great_rag_system/&quot;&gt;as mentioned in another Reddit post that inspired this one&lt;/a&gt;).&lt;/p&gt; &lt;p&gt;I discuss this whole topic in more detail in my &lt;a href=&quot;https://pashpashpash.substack.com/p/why-does-my-rag-suck-and-how-do-i&quot;&gt;Substack article&lt;/a&gt; including specific advice for pre-launch and post-launch, but in a nutshell, when starting any RAG system &lt;strong&gt;&lt;em&gt;you need to capture valuable metrics like cosine similarity, user feedback, and reranker scores - for every retrieval, right from the start&lt;/em&gt;&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;Basically, in an ideal scenario, you will end up with an &lt;strong&gt;observability table&lt;/strong&gt; that looks like this:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;retrieval_id&lt;/strong&gt; (some unique identifier for every piece of retrieved context)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;query_id&lt;/strong&gt; (unique id for the input query/question/message that RAG was used to answer)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;cosine similarity score&lt;/strong&gt; (null for non-vector retrieval e.g. elastic search)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;reranker relevancy score&lt;/strong&gt; (highly recommended for ALL kinds of retrieval, including vector and traditional text search like elastic)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;timestamp&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;retrieved_context&lt;/strong&gt; (optional, but nice to have for QA purposes) &lt;ul&gt; &lt;li&gt;e.g. &lt;code&gt;&amp;quot;The New York City Subway [...]&amp;quot;&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;user_feedback&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;e.g. &lt;code&gt;false (thumbs down)&lt;/code&gt; or &lt;code&gt;true (thumbs up)&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Once you start collecting and storing these super powerful observability metrics, you can begin analyzing production performance. We can &lt;a href=&quot;https://x.com/jxnlco/status/1803899526723387895&quot;&gt;categorize this analysis into two main areas&lt;/a&gt;:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Topics: This refers to the content and context of the data, which can be represented by the way words are structured or the embeddings used in search queries. You can use topic modeling to better understand the types of responses your system handles. &lt;ul&gt; &lt;li&gt;E.g. People talking about their family, or their hobbies, etc.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Capabilities (Agent Tools/Functions): This pertains to the functional aspects of the queries, such as: &lt;ul&gt; &lt;li&gt;Direct conversation requests (e.g., &lt;em&gt;“Remind me what we talked about when we discussed my neighbor&amp;#39;s dogs barking all the time.”&lt;/em&gt;)&lt;/li&gt; &lt;li&gt;Time-sensitive queries (e.g., &lt;em&gt;“Show me the latest X”&lt;/em&gt; or &lt;em&gt;“Show me the most recent Y.”&lt;/em&gt;)&lt;/li&gt; &lt;li&gt;Metadata-specific inquiries (e.g., &lt;em&gt;“What date was our last conversation?”&lt;/em&gt;), which might require specific filters or keyword matching that go beyond simple text embeddings.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;By applying clustering techniques to these topics and capabilities (I cover this in more depth in my &lt;a href=&quot;https://pashpashpash.substack.com/p/tackling-the-challenge-of-document&quot;&gt;previous article on K-Means clusterization&lt;/a&gt;), you can:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Group similar queries/questions together and categorize them by topic e.g. &lt;em&gt;“Product availability questions”&lt;/em&gt; or capability e.g. &lt;em&gt;“Requests to search previous conversations”&lt;/em&gt;.&lt;/li&gt; &lt;li&gt;Calculate the frequency and distribution of these groups.&lt;/li&gt; &lt;li&gt;Assess the average performance scores for each group.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This data-driven approach allows you to prioritize system enhancements based on actual user needs and system performance. For instance:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;If person-entity-retrieval commands a significant portion of query volume (say 60%) and shows high satisfaction rates (90% thumbs up) with minimal cosine distance, this area may not need further refinement.&lt;/li&gt; &lt;li&gt;Conversely, queries like &amp;quot;What date was our last conversation&amp;quot; might show poor results, indicating a limitation of our current functional capabilities. If such queries constitute a small fraction (e.g., 2%) of total volume, it might be more strategic to temporarily exclude these from the system’s capabilities (&lt;em&gt;“I forget, honestly!”&lt;/em&gt; or &lt;em&gt;“Do you think I&amp;#39;m some kind of calendar!?”&lt;/em&gt;), thus improving overall system performance. &lt;ul&gt; &lt;li&gt;Handling these exclusions gracefully significantly improves user experience. &lt;ul&gt; &lt;li&gt;When appropriate, Use humor and personality to your advantage instead of saying &lt;em&gt;“I cannot answer this right now.”&lt;/em&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;TL;DR:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Getting your RAG system from “sucks” to “good” isn&amp;#39;t about magic solutions or trendy libraries. The first step is to implement strong observability practices to continuously analyze and improve performance. Cluster collected data into topics &amp;amp; capabilities to have a clear picture of how people are using your product and where it falls short. Prioritize enhancements based on real usage and remember, a touch of personality can go a long way in handling limitations.&lt;/p&gt; &lt;p&gt;For a more detailed treatment of this topic, check out my &lt;a href=&quot;https://pashpashpash.substack.com/p/why-does-my-rag-suck-and-how-do-i&quot;&gt;article here&lt;/a&gt;. I&amp;#39;d love to hear your thoughts on this, please let me know if there are any other good metrics or considerations to keep in mind!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/MZuc&quot;&gt; /u/MZuc &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e0rsou/why_does_my_rag_suck_and_how_do_i_make_it_good/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e0rsou/why_does_my_rag_suck_and_how_do_i_make_it_good/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1e0rsou</id><link href="https://www.reddit.com/r/LangChain/comments/1e0rsou/why_does_my_rag_suck_and_how_do_i_make_it_good/" /><updated>2024-07-11T15:33:21+00:00</updated><published>2024-07-11T15:33:21+00:00</published><title>&quot;Why does my RAG suck and how do I make it good&quot;</title></entry><entry><author><name>/u/gr8satvik</name><uri>https://www.reddit.com/user/gr8satvik</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e1av5p/react_prompting/&quot;&gt; &lt;img src=&quot;https://b.thumbs.redditmedia.com/BYIL8aSbC2_LeHXd4Q6UQIZ8vRUQHvhKZK-CA6vLNFY.jpg&quot; alt=&quot;ReAct Prompting&quot; title=&quot;ReAct Prompting&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;My LLM is hallaucinating when I ask it a very simple question&lt;br/&gt; using Langchain tools, here wikipedia.&lt;br/&gt; I am using gemini-1.5-flash&lt;br/&gt; tried llama 70b and gemini pro&lt;br/&gt; but still it hallucinates if not same but easy questions.&lt;br/&gt; I am using React Prompt Template&lt;br/&gt; which goes like (2nd image)&lt;/p&gt; &lt;p&gt;My LLM doesn&amp;#39;t even reach the observation step it starts hallucinating after action Input&lt;br/&gt; I tried even changing params of my tools by changing the max_retrieval etc...&lt;br/&gt; Please help...&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/gmrydfxw81cd1.png?width=1614&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9df61fc7f740dab11233da781e3740a871274a9a&quot;&gt;https://preview.redd.it/gmrydfxw81cd1.png?width=1614&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9df61fc7f740dab11233da781e3740a871274a9a&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/ohj4e34a91cd1.png?width=1145&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=966fb6707cf3e9183aa52b093f29f42c0c2192b4&quot;&gt;https://preview.redd.it/ohj4e34a91cd1.png?width=1145&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=966fb6707cf3e9183aa52b093f29f42c0c2192b4&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/gr8satvik&quot;&gt; /u/gr8satvik &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e1av5p/react_prompting/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e1av5p/react_prompting/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1e1av5p</id><media:thumbnail url="https://b.thumbs.redditmedia.com/BYIL8aSbC2_LeHXd4Q6UQIZ8vRUQHvhKZK-CA6vLNFY.jpg" /><link href="https://www.reddit.com/r/LangChain/comments/1e1av5p/react_prompting/" /><updated>2024-07-12T06:38:10+00:00</updated><published>2024-07-12T06:38:10+00:00</published><title>ReAct Prompting</title></entry><entry><author><name>/u/Plane_Past129</name><uri>https://www.reddit.com/user/Plane_Past129</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Is there any chance of building RAG for real-time data that was stored in SQL databases. Specifically, can we use Unstructured there?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Plane_Past129&quot;&gt; /u/Plane_Past129 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e1ba2l/rag_for_realtime_data_sql/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e1ba2l/rag_for_realtime_data_sql/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1e1ba2l</id><link href="https://www.reddit.com/r/LangChain/comments/1e1ba2l/rag_for_realtime_data_sql/" /><updated>2024-07-12T07:04:45+00:00</updated><published>2024-07-12T07:04:45+00:00</published><title>RAG for real-time data (SQL)</title></entry><entry><author><name>/u/northwolf56</name><uri>https://www.reddit.com/user/northwolf56</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e0twcd/my_serverless_visual_langgraph_editor/&quot;&gt; &lt;img src=&quot;https://b.thumbs.redditmedia.com/UJ085UairrlLGvO3kv9DnoNb9gVW-_hN6_J8iNlVyME.jpg&quot; alt=&quot;My Serverless Visual LangGraph Editor&quot; title=&quot;My Serverless Visual LangGraph Editor&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/northwolf56&quot;&gt; /u/northwolf56 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/gallery/1e0twcd&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e0twcd/my_serverless_visual_langgraph_editor/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1e0twcd</id><media:thumbnail url="https://b.thumbs.redditmedia.com/UJ085UairrlLGvO3kv9DnoNb9gVW-_hN6_J8iNlVyME.jpg" /><link href="https://www.reddit.com/r/LangChain/comments/1e0twcd/my_serverless_visual_langgraph_editor/" /><updated>2024-07-11T17:01:43+00:00</updated><published>2024-07-11T17:01:43+00:00</published><title>My Serverless Visual LangGraph Editor</title></entry><entry><author><name>/u/Informal-Victory8655</name><uri>https://www.reddit.com/user/Informal-Victory8655</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;m having error when using checkpointer in graph.&lt;/p&gt; &lt;p&gt;| ValueError: Checkpointer requires one or more of the following &amp;#39;configurable&amp;#39; keys: [&amp;#39;thread_id&amp;#39;, &amp;#39;thread_ts&amp;#39;]&lt;/p&gt; &lt;p&gt;.&lt;/p&gt; &lt;p&gt;How to pass configuration with thread_id here to graph?&lt;/p&gt; &lt;p&gt;add_routes(&lt;/p&gt; &lt;p&gt;app,&lt;/p&gt; &lt;p&gt;(graph | RunnableLambda(output_parsing_for_playground)).with_types(input_type=InputChat, output_type=str),&lt;/p&gt; &lt;p&gt;playground_type=&amp;quot;chat&amp;quot;&lt;/p&gt; &lt;p&gt;)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Informal-Victory8655&quot;&gt; /u/Informal-Victory8655 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e1d466/how_to_pass_thread_id_using_config_when_serving/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e1d466/how_to_pass_thread_id_using_config_when_serving/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1e1d466</id><link href="https://www.reddit.com/r/LangChain/comments/1e1d466/how_to_pass_thread_id_using_config_when_serving/" /><updated>2024-07-12T09:08:43+00:00</updated><published>2024-07-12T09:08:43+00:00</published><title>how to pass thread_id using config when serving langgraph(with checkpointer) through langgserve chain. Please help</title></entry><entry><author><name>/u/mehul_gupta1997</name><uri>https://www.reddit.com/user/mehul_gupta1997</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/mehul_gupta1997&quot;&gt; /u/mehul_gupta1997 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;/r/ArtificialInteligence/comments/1e18y5b/localgemma_for_loading_gemma2_models_locally/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e190m1/localgemma_for_loading_gemma2_models_locally/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1e190m1</id><link href="https://www.reddit.com/r/LangChain/comments/1e190m1/localgemma_for_loading_gemma2_models_locally/" /><updated>2024-07-12T04:45:29+00:00</updated><published>2024-07-12T04:45:29+00:00</published><title>Local-Gemma for loading Gemma2 models locally</title></entry><entry><author><name>/u/Odd-Bonus-4075</name><uri>https://www.reddit.com/user/Odd-Bonus-4075</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I am working on a project where I need to extract information of a very long document.&lt;br/&gt; Using a local LLM (gemma2) through ollama.&lt;/p&gt; &lt;p&gt;If using a vector store, Is there a way I can make sure my LLM query searches all pages or chunks of the vector store?&lt;/p&gt; &lt;p&gt;Is it perhaps, better to use brute force and do chunks of the text to pass the LLM the compile the results? &lt;/p&gt; &lt;p&gt;Any relevant information on this would be much appreciated. Many thanks&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Odd-Bonus-4075&quot;&gt; /u/Odd-Bonus-4075 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e1aziw/search_a_long_document/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e1aziw/search_a_long_document/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1e1aziw</id><link href="https://www.reddit.com/r/LangChain/comments/1e1aziw/search_a_long_document/" /><updated>2024-07-12T06:45:57+00:00</updated><published>2024-07-12T06:45:57+00:00</published><title>Search a long document</title></entry></feed>