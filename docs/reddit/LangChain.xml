<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/"><category term="LangChain" label="r/LangChain"/><updated>2024-01-27T22:01:12+00:00</updated><icon>https://www.redditstatic.com/icon.png/</icon><id>/r/LangChain.rss</id><link rel="self" href="https://www.reddit.com/r/LangChain.rss" type="application/atom+xml" /><link rel="alternate" href="https://www.reddit.com/r/LangChain" type="text/html" /><subtitle>LangChain is an open-source framework and developer toolkit that helps developers get LLM applications from prototype to production. It is available for Python and Javascript at https://www.langchain.com/.</subtitle><title>LangChain</title><entry><author><name>/u/zchaarm</name><uri>https://www.reddit.com/user/zchaarm</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;A place for members of &lt;a href=&quot;/r/LangChain&quot;&gt;r/LangChain&lt;/a&gt; to chat with each other&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/zchaarm&quot;&gt; /u/zchaarm &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_10ljho9</id><link href="https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/" /><updated>2023-01-26T04:33:05+00:00</updated><published>2023-01-26T04:33:05+00:00</published><title>r/LangChain Lounge</title></entry><entry><author><name>/u/dima11235813</name><uri>https://www.reddit.com/user/dima11235813</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I use webpilot and vox script all the time and wondering if there are examples of achieving this with LangChain.&lt;/p&gt; &lt;p&gt;I especially like both because I can provide a url and get all that info in context, which seems like even ChatGPT 4 with Bing can&amp;#39;t be relied on to do it consistently.&lt;/p&gt; &lt;p&gt;I have some examples in this article I generated.&lt;br/&gt; &lt;a href=&quot;https://www.learninternetgrow.com/real-time-search-with-llms/&quot;&gt;https://www.learninternetgrow.com/real-time-search-with-llms/&lt;/a&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/dima11235813&quot;&gt; /u/dima11235813 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1achbg7/has_anyone_found_an_example_of_coupling_langchain/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1achbg7/has_anyone_found_an_example_of_coupling_langchain/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1achbg7</id><link href="https://www.reddit.com/r/LangChain/comments/1achbg7/has_anyone_found_an_example_of_coupling_langchain/" /><updated>2024-01-27T18:25:11+00:00</updated><published>2024-01-27T18:25:11+00:00</published><title>Has anyone found an example of coupling LangChain with external URL requests?</title></entry><entry><author><name>/u/techycatx</name><uri>https://www.reddit.com/user/techycatx</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;i&amp;#39;m new to langchain and javascript, and i am struggling to properly implement the transform function that will be passed to the transform chain.&lt;/p&gt; &lt;p&gt;this is my transform function&lt;/p&gt; &lt;p&gt;&lt;code&gt; const extractAndRemoveCodeSnippets = (inputs) =&amp;gt; { const text = inputs[&amp;quot;main_llm_response&amp;quot;]; const pattern = /&lt;/code&gt;([\s\S]*?)&lt;code&gt;/g; const codeSnippets = text.match(pattern) || []; const cleanedText = text.replace(pattern, &amp;quot;&amp;quot;); return { &amp;quot;cleaned_text&amp;quot;: cleanedText, &amp;quot;code&amp;quot;: codeSnippets }; }; &lt;/code&gt;&lt;/p&gt; &lt;p&gt;and this how i pass it to the transform chain &lt;/p&gt; &lt;p&gt;``` const cleanTextChain = new TransformChain({ inputVariables: [&amp;quot;main_llm_response&amp;quot;], outputVariables: [&amp;quot;cleaned_text&amp;quot;, &amp;quot;code&amp;quot;], transformFunc: extractAndRemoveCodeSnippets });&lt;/p&gt; &lt;p&gt;``` and this is the error i am recieving &lt;/p&gt; &lt;p&gt;&lt;code&gt; TypeError: this.transformFunc is not a function &lt;/code&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/techycatx&quot;&gt; /u/techycatx &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1achbyj/need_help_with_using_the_transform_chain_in/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1achbyj/need_help_with_using_the_transform_chain_in/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1achbyj</id><link href="https://www.reddit.com/r/LangChain/comments/1achbyj/need_help_with_using_the_transform_chain_in/" /><updated>2024-01-27T18:25:45+00:00</updated><published>2024-01-27T18:25:45+00:00</published><title>need help with using the transform chain in langchain.js</title></entry><entry><author><name>/u/cambridgecoder415</name><uri>https://www.reddit.com/user/cambridgecoder415</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;pre&gt;&lt;code&gt;from langchain.chains import RetrievalQA from langchain.chat_models import ChatOpenAI from langchain.document_loaders import CSVLoader from langchain.vectorstores import DocArrayInMemorySearch from IPython.display import display, Markdown from langchain.llms import OpenAI file = &amp;#39;OutdoorClothingCatalog_1000.csv&amp;#39; loader = CSVLoader(file_path=file) print(loader) from langchain.indexes import VectorstoreIndexCreator index = VectorstoreIndexCreator( vectorstore_cls=DocArrayInMemorySearch ).from_loaders([loader]) query =&amp;quot;Please list all your shirts with sun protection \ in a table in markdown and summarize each one.&amp;quot; from langchain_openai import ChatOpenAI model_name = &amp;#39;gpt-3.5-turbo-instruct&amp;#39; llm_replacement_model = ChatOpenAI(temperature=0.0, model=model_name, openai_api_key=openai.api_key) response = index.query(query, llm=llm_replacement_model &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;--------------------------------------------------------------------------- ValidationError Traceback (most recent call last) Cell In[22], line 5 3 model_name = &amp;#39;gpt-3.5-turbo-instruct&amp;#39; 4 llm_replacement_model = ChatOpenAI(temperature=0.0, model=model_name, openai_api_key=openai.api_key) ----&amp;gt; 5 response = index.query(query, llm=llm_replacement_model) File ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/langchain/indexes/vectorstore.py:46, in VectorStoreIndexWrapper.query(self, question, llm, retriever_kwargs, **kwargs) 42 retriever_kwargs = retriever_kwargs or {} 43 chain = RetrievalQA.from_chain_type( 44 llm, retriever=self.vectorstore.as_retriever(**retriever_kwargs), **kwargs 45 ) ---&amp;gt; 46 return chain.run(question) File ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:145, in deprecated.&amp;lt;locals&amp;gt;.deprecate.&amp;lt;locals&amp;gt;.warning_emitting_wrapper(*args, **kwargs) 143 warned = True 144 emit_warning() --&amp;gt; 145 return wrapped(*args, **kwargs) File ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/langchain/chains/base.py:538, in Chain.run(self, callbacks, tags, metadata, *args, **kwargs) 536 if len(args) != 1: 537 raise ValueError(&amp;quot;`run` supports only one positional argument.&amp;quot;) --&amp;gt; 538 return self(args[0], callbacks=callbacks, tags=tags, metadata=metadata)[ 539 _output_key 540 ] ... Field required [type=missing, input_value={&amp;#39;embedding&amp;#39;: [0.00682570..., -0.02392816262769903]}, input_type=dict] For further information visit https://errors.pydantic.dev/2.5/v/missing metadata Field required [type=missing, input_value={&amp;#39;embedding&amp;#39;: [0.00682570..., -0.02392816262769903]}, input_type=dict] For further information visit https://errors.pydantic.dev/2.5/v/missing &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;Help, this is from a course I&amp;#39;m taking on Deep learning&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/cambridgecoder415&quot;&gt; /u/cambridgecoder415 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1aceuxq/query_csv_code_not_working_help/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1aceuxq/query_csv_code_not_working_help/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1aceuxq</id><link href="https://www.reddit.com/r/LangChain/comments/1aceuxq/query_csv_code_not_working_help/" /><updated>2024-01-27T16:38:41+00:00</updated><published>2024-01-27T16:38:41+00:00</published><title>Query CSV code not working help!</title></entry><entry><author><name>/u/Dealwap1337</name><uri>https://www.reddit.com/user/Dealwap1337</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;How can I ensure that the order of extracted documents in Langchain is maintained? I have a RAG app that allows users to query documents, but I&amp;#39;ve noticed that some numbered data is extracted in the wrong order. &lt;/p&gt; &lt;p&gt;For example, the documents may contain numbered items from 1 to 50, but when the final result is returned, the 2nd item may appear last and the 50th item may appear first. I need to maintain the same order as it appears in the original document.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Dealwap1337&quot;&gt; /u/Dealwap1337 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ac80ys/how_to_maintain_extracted_document_order/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ac80ys/how_to_maintain_extracted_document_order/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1ac80ys</id><link href="https://www.reddit.com/r/LangChain/comments/1ac80ys/how_to_maintain_extracted_document_order/" /><updated>2024-01-27T10:31:31+00:00</updated><published>2024-01-27T10:31:31+00:00</published><title>How to Maintain extracted Document Order</title></entry><entry><author><name>/u/Datenschieber</name><uri>https://www.reddit.com/user/Datenschieber</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I am a noob experimenting with LLMs and i am looking for a reliable method for merging / combining two texts into one credible sounding merged text. Does that take a langchain based RAG or am i simply too stupid to engineer the right prompt in a mixtral or something similar? :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Datenschieber&quot;&gt; /u/Datenschieber &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1acbcqn/how_do_i_merge_combine_two_texts_into_one/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1acbcqn/how_do_i_merge_combine_two_texts_into_one/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1acbcqn</id><link href="https://www.reddit.com/r/LangChain/comments/1acbcqn/how_do_i_merge_combine_two_texts_into_one/" /><updated>2024-01-27T13:55:58+00:00</updated><published>2024-01-27T13:55:58+00:00</published><title>How do i merge / combine two texts into one?</title></entry><entry><author><name>/u/Slow-Associate-127</name><uri>https://www.reddit.com/user/Slow-Associate-127</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi everyone.... Recently I have got a project where I need to retrieve some financial documents using RAG....The process is 1. The query will be around 20 to 30 page long pdf document. 2. The document contains some information requirements...Those requirements will be stored in my db....based on the requirements of the pdf I need to fetch the existing documents from the db....&lt;/p&gt; &lt;p&gt;Can anyone please help me out with this.... Thanks in advance&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Slow-Associate-127&quot;&gt; /u/Slow-Associate-127 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ac32g9/document_retrieval_using_llms_from_long_documents/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ac32g9/document_retrieval_using_llms_from_long_documents/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1ac32g9</id><link href="https://www.reddit.com/r/LangChain/comments/1ac32g9/document_retrieval_using_llms_from_long_documents/" /><updated>2024-01-27T05:06:53+00:00</updated><published>2024-01-27T05:06:53+00:00</published><title>Document retrieval using llm's from long documents</title></entry><entry><author><name>/u/jubjub07</name><uri>https://www.reddit.com/user/jubjub07</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;As the title says, I&amp;#39;m working to enable an app I wrote that generates SQL to allow it to work from a locally served LLM instead of one in the cloud. This is a requirement for a couple of customers, so I&amp;#39;ve been experimenting...&lt;/p&gt; &lt;p&gt;I&amp;#39;m using the create_sql_agent function... I&amp;#39;m supplying the prompts as well as some custom tools that feed in appropriate metadata about the database. Against OpenAI or AzureOpenAI it works fine.&lt;/p&gt; &lt;p&gt;When I run against Ollama, I&amp;#39;ve tried a bunch of models - SQLcoder, LLama70b, Mixtral8x7b, etc. And I get the same result... I can watch the agent work away (via my own debugging as well as LangSmith).. &lt;/p&gt; &lt;p&gt;The agent runs typically follow the same general path as the OpenAI runs, with one exception - I can see the final SQL statement generated, but after executing the statement and getting a perfectly fine answer (and identifying it as such) it goes into a kind of loop and never exits the chain. &lt;/p&gt; &lt;p&gt;I&amp;#39;ve toyed with supplying &amp;quot;stop=&amp;quot; tokens etc, but I just don&amp;#39;t see what&amp;#39;s going on.&lt;/p&gt; &lt;p&gt;There&amp;#39;s so many moving pieces that I thought someone might have an idea where to look... &lt;/p&gt; &lt;p&gt;It&amp;#39;s possible that it&amp;#39;s a prompt format issue - I know the prompt format between the Llama models and OpenAI models are pretty different... with special tokens, etc. but I&amp;#39;m utterly unclear how that would change things, since up to the point it should &amp;quot;exit&amp;quot; and return the final answer it works pretty well.&lt;/p&gt; &lt;p&gt;I&amp;#39;ll add that I created a custom &amp;quot;handler&amp;quot; to extract the final sql statement it creates, since the sql agent returns the &amp;quot;answer&amp;quot; rather than the SQL statement. My handler is looking for invocation of the sql_db_query tool which indicates that the sql has been generated and is being sent off for execution - I grab the string at that point and save it for later... that works in openai, but not in any of the other models I&amp;#39;ve tried.&lt;/p&gt; &lt;p&gt;I do get some different results with different models...&lt;/p&gt; &lt;p&gt;Most have given the above general issue - generating a good answer, but then never stopping.&lt;/p&gt; &lt;p&gt;mixtral:8x7b-instruct-v0.1-q5_K_M - was weird. It didn&amp;#39;t seem to want to use the tools I provided and sort of skipped some steps. It generated SQL that had no relation to the actual database - so it wasn&amp;#39;t using any of the tools - then when it came time to run the SQL it DID generate, it messed up that name of the sql_db_query tool:&lt;/p&gt; &lt;p&gt;&amp;quot;{&amp;#39;requested_tool_name&amp;#39;: &amp;#39;sql\\_db\\_query&amp;#39;, &amp;#39;available_tool_names&amp;#39;: [&amp;#39;sql_db_query&amp;#39;, &amp;#39;sql_db_schema&amp;#39;, &amp;#39;sql_db_list_tables&amp;#39;, &amp;#39;sql_db_query_checker&amp;#39;, &amp;#39;sql_get_few_shot_examples&amp;#39;, &amp;#39;sql_get_column_descriptions&amp;#39;, &amp;#39;get_column_to_table_cross_reference&amp;#39;, &amp;#39;get_hierarchies&amp;#39;, &amp;#39;sql_get_table_descriptions&amp;#39;]}&amp;quot;&lt;/p&gt; &lt;p&gt;No other model did this... &lt;/p&gt; &lt;p&gt;SO, I&amp;#39;m wondering if anyone else has had the same struggle, or can point me in some direction to figure this out... it feels like I&amp;#39;m so very close, just missing one key ingredient!&lt;/p&gt; &lt;p&gt;Thanks!&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/jubjub07&quot;&gt; /u/jubjub07 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1abwoh6/so_close_switching_from_openai_models_to_local/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1abwoh6/so_close_switching_from_openai_models_to_local/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1abwoh6</id><link href="https://www.reddit.com/r/LangChain/comments/1abwoh6/so_close_switching_from_openai_models_to_local/" /><updated>2024-01-26T23:46:28+00:00</updated><published>2024-01-26T23:46:28+00:00</published><title>So close... Switching from Openai models to local models served by Ollama</title></entry><entry><author><name>/u/Ill_Bodybuilder3499</name><uri>https://www.reddit.com/user/Ill_Bodybuilder3499</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;JinaAi just released an Embeddingmodel with a context size of 8k. I was wondering what are the advantages of Long Context Embedding models for a Rag Use Case?&lt;/p&gt; &lt;p&gt;Happy for discussion!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Ill_Bodybuilder3499&quot;&gt; /u/Ill_Bodybuilder3499 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ac7t19/what_ist_the_advantage_of_long_context_embedding/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ac7t19/what_ist_the_advantage_of_long_context_embedding/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1ac7t19</id><link href="https://www.reddit.com/r/LangChain/comments/1ac7t19/what_ist_the_advantage_of_long_context_embedding/" /><updated>2024-01-27T10:16:10+00:00</updated><published>2024-01-27T10:16:10+00:00</published><title>What ist the advantage of Long Context Embedding Models for Rag</title></entry><entry><author><name>/u/rkubc</name><uri>https://www.reddit.com/user/rkubc</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hello everyone,&lt;/p&gt; &lt;p&gt;I&amp;#39;m working on a project using Langchain involving information extraction from PDF documents, each ranging from 5-10 pages. My primary goal is to extract information based on approximately 26 keywords, which I load from a CSV file.&lt;/p&gt; &lt;p&gt;Here&amp;#39;s an overview of my current setup:&lt;/p&gt; &lt;p&gt;Basic Language Chain Components: I&amp;#39;m using a loader, a recursive splitter set to 500 with an overlap of 300, and an Ensemble Retriever BM25)and MultiQuery Retriever setup with FAISS, where each component has a .5 weight for each.&lt;/p&gt; &lt;p&gt;FAISS Library Usage: In the FAISS library, I&amp;#39;m employing the from_documents method and a retriever.&lt;/p&gt; &lt;p&gt;OpenAI Integration: After combining the top 2 chunks for each keyword, I&amp;#39;m feeding the data into OpenAI with prompt.&lt;/p&gt; &lt;p&gt;However, I&amp;#39;m encountering an issue where the results are about 8-10% hallucinated or inaccurate.&lt;/p&gt; &lt;p&gt;I&amp;#39;m looking for advice on how to improve the accuracy of my information extraction process. Are there any specific parameters or techniques in better chunking, FAISS or with OpenAI that I should consider consider or tweaking? Any insights or suggestions would be greatly appreciated!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/rkubc&quot;&gt; /u/rkubc &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1abvyps/overcoming_810_inaccuracy_in_pdf_keyword/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1abvyps/overcoming_810_inaccuracy_in_pdf_keyword/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1abvyps</id><link href="https://www.reddit.com/r/LangChain/comments/1abvyps/overcoming_810_inaccuracy_in_pdf_keyword/" /><updated>2024-01-26T23:14:36+00:00</updated><published>2024-01-26T23:14:36+00:00</published><title>Overcoming 8-10% Inaccuracy in PDF Keyword Extraction Using Faiss and OpenAI – Need Advice</title></entry><entry><author><name>/u/redd-dev</name><uri>https://www.reddit.com/user/redd-dev</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I am currently using Mixtral 8x7B Instruct v0.1 - GPTQ and was wondering what is currently the best open source LLM to use to output SQL code?&lt;/p&gt; &lt;p&gt;Would really appreciate any input on this. Many thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/redd-dev&quot;&gt; /u/redd-dev &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1abiozm/what_is_the_best_open_source_llm_for_outputting/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1abiozm/what_is_the_best_open_source_llm_for_outputting/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1abiozm</id><link href="https://www.reddit.com/r/LangChain/comments/1abiozm/what_is_the_best_open_source_llm_for_outputting/" /><updated>2024-01-26T13:45:42+00:00</updated><published>2024-01-26T13:45:42+00:00</published><title>What is the best open source LLM for outputting SQL code</title></entry><entry><author><name>/u/gollum1632</name><uri>https://www.reddit.com/user/gollum1632</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hello everyone, I am just starting with RAG.&lt;/p&gt; &lt;p&gt;I want to retrieve building regulations ( max height, area, etc) information from pdfs using a llm. &lt;/p&gt; &lt;p&gt;the pdfs contains both text and data. I am currently using PypdfLoader from langchain to load the pdfs. Splitting using recursive character split, embedding using open ai and storing it in chroma db.&lt;/p&gt; &lt;p&gt;Sometimes the llm doesn’t retrive the information correctly. don’t know if it is because of the parsing of pdfs. &lt;/p&gt; &lt;p&gt;Any help in improving the system is greatly appreciated. Thanks.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/gollum1632&quot;&gt; /u/gollum1632 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1abuv1f/need_help_improving_rag/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1abuv1f/need_help_improving_rag/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1abuv1f</id><link href="https://www.reddit.com/r/LangChain/comments/1abuv1f/need_help_improving_rag/" /><updated>2024-01-26T22:27:26+00:00</updated><published>2024-01-26T22:27:26+00:00</published><title>Need help improving RAG</title></entry><entry><author><name>/u/Electronic-Letter592</name><uri>https://www.reddit.com/user/Electronic-Letter592</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I recently heard about promptflow, a tool to create end to end LLM processes. Is it more a competitor to langchain, can I combine both, or what is the difference to langchain?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Electronic-Letter592&quot;&gt; /u/Electronic-Letter592 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1abpzp2/can_i_combine_langchain_with_prompflow/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1abpzp2/can_i_combine_langchain_with_prompflow/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1abpzp2</id><link href="https://www.reddit.com/r/LangChain/comments/1abpzp2/can_i_combine_langchain_with_prompflow/" /><updated>2024-01-26T19:01:15+00:00</updated><published>2024-01-26T19:01:15+00:00</published><title>Can I combine LangChain with PrompFlow?</title></entry><entry><author><name>/u/Euloghtos</name><uri>https://www.reddit.com/user/Euloghtos</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hello , lets say that we prompt the user to add documents , which the app chunks , embeds and adds to a vector database. &lt;/p&gt; &lt;p&gt;How can we avoid inserting the same document twice? Is there any way it can be done with langchain?&lt;/p&gt; &lt;p&gt;I really cant find a way around.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Euloghtos&quot;&gt; /u/Euloghtos &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1abkaif/avoid_duplicates_inside_a_chroma_db/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1abkaif/avoid_duplicates_inside_a_chroma_db/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1abkaif</id><link href="https://www.reddit.com/r/LangChain/comments/1abkaif/avoid_duplicates_inside_a_chroma_db/" /><updated>2024-01-26T15:00:54+00:00</updated><published>2024-01-26T15:00:54+00:00</published><title>Avoid duplicates inside a chroma db</title></entry><entry><author><name>/u/jubjub07</name><uri>https://www.reddit.com/user/jubjub07</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;So I read the blogpost from OpenAI on their new v3 embeddings models, and decided to try them in my app... I get an error... so I simplified to the 4 line example from the langchain docs, which results in the same error: &lt;/p&gt; &lt;p&gt;from langchain_openai import OpenAIEmbeddings&lt;/p&gt; &lt;pre&gt;&lt;code&gt;embeddings = OpenAIEmbeddings(model=&amp;quot;text-embedding-3-large&amp;quot;) query_result = embeddings.embed_query(&amp;quot;Some test text!!!&amp;quot;) print(query_result[0:5]) % python3 t.py Warning: model not found. Using cl100k_base encoding. [-0.0076643440879791275, 0.023873071539678815, -0.01047247064095197, 0.006004269555153845, 0.0286252864226502] &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Which seems odd, since I can execute the curl example from the openai docs, no problem:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;% curl https://api.openai.com/v1/embeddings \  ✔  11:34:33 -H &amp;quot;Content-Type: application/json&amp;quot; \ -H &amp;quot;Authorization: Bearer xxxxx&amp;quot; \ -d &amp;#39;{ &amp;quot;input&amp;quot;: &amp;quot;Your text string goes here&amp;quot;, &amp;quot;model&amp;quot;: &amp;quot;text-embedding-3-large&amp;quot; }&amp;#39; ... -0.02287454, -0.02287454, -0.022449568 ] } ], &amp;quot;model&amp;quot;: &amp;quot;text-embedding-3-large&amp;quot;, &amp;quot;usage&amp;quot;: { &amp;quot;prompt_tokens&amp;quot;: 5, &amp;quot;total_tokens&amp;quot;: 5 } } &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;When I refer to the langchain embedding docs, they&amp;#39;ve already been updated to show the new embeddings... BUT they are showing the same error RIGHT IN THEIR DOCs as well.&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;from langchain_openai import OpenAIEmbeddings embeddings = OpenAIEmbeddings(model=&amp;quot;text-embedding-3-large&amp;quot;) text = &amp;quot;This is a test document.&amp;quot; Embed documents doc_result = embeddings.embed_documents([text]) Warning: model not found. Using cl100k_base encoding. doc_result[0][:5] [-0.014380056377383358, -0.027191711627651764, -0.020042716111860304, 0.057301379620345545, -0.022267658631828974] &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Am i missing something??? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/jubjub07&quot;&gt; /u/jubjub07 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1abrtev/new_openai_v3_embeddings_are_they_working_with/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1abrtev/new_openai_v3_embeddings_are_they_working_with/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1abrtev</id><link href="https://www.reddit.com/r/LangChain/comments/1abrtev/new_openai_v3_embeddings_are_they_working_with/" /><updated>2024-01-26T20:19:00+00:00</updated><published>2024-01-26T20:19:00+00:00</published><title>New openai v3 embeddings - are they working with langchain or not?</title></entry><entry><author><name>/u/ep3gotts</name><uri>https://www.reddit.com/user/ep3gotts</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I have a QA-style few-shot style agent and it&amp;#39;s prompt goes like this:&lt;br/&gt; &amp;gt; You&amp;#39;re a helpful assistant who&amp;#39;s an expert in X, (explanation of its audience, task and output guardrails/format).&lt;/p&gt; &lt;p&gt;&amp;gt; Examples:&lt;/p&gt; &lt;p&gt;&amp;gt; Question: xxx&lt;/p&gt; &lt;p&gt;&amp;gt; Answer:&lt;/p&gt; &lt;p&gt;&amp;gt; ```yyy``` &lt;/p&gt; &lt;p&gt;Tokens utilization is pretty steep because of this large context that&amp;#39;s being sent in full over and over again for every request using Chat Completions API( &lt;a href=&quot;https://platform.openai.com/docs/guides/text-generation/chat-completions-api&quot;&gt;https://platform.openai.com/docs/guides/text-generation/chat-completions-api&lt;/a&gt; )&lt;/p&gt; &lt;p&gt;Is it possible to switch instead to Assistants API( &lt;a href=&quot;https://platform.openai.com/docs/assistants/overview/agents&quot;&gt;https://platform.openai.com/docs/assistants/overview/agents&lt;/a&gt; ) for such a task so that context is sent only as 1st system message and all questions are just appended to that thread? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/ep3gotts&quot;&gt; /u/ep3gotts &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1abheko/switch_to_assistants_api_from_chat_completions_to/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1abheko/switch_to_assistants_api_from_chat_completions_to/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1abheko</id><link href="https://www.reddit.com/r/LangChain/comments/1abheko/switch_to_assistants_api_from_chat_completions_to/" /><updated>2024-01-26T12:39:14+00:00</updated><published>2024-01-26T12:39:14+00:00</published><title>Switch to Assistants API from Chat completions to save on shared context bandwidth?</title></entry><entry><author><name>/u/int2me</name><uri>https://www.reddit.com/user/int2me</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;What is the difference between these two approaches for importing OpenAI chat models?&amp;quot;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;from&lt;/em&gt;&lt;/strong&gt; &lt;strong&gt;langchain_openai&lt;/strong&gt; &lt;strong&gt;&lt;em&gt;import&lt;/em&gt;&lt;/strong&gt; &lt;strong&gt;ChatOpenAI&lt;/strong&gt;&lt;br/&gt; &lt;strong&gt;&lt;em&gt;from&lt;/em&gt;&lt;/strong&gt; &lt;strong&gt;langchain.chat_models&lt;/strong&gt; &lt;strong&gt;&lt;em&gt;import&lt;/em&gt;&lt;/strong&gt; &lt;strong&gt;ChatOpenAI&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Does langchain-openai use the latest release of OpenAI or do they use the same version?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/int2me&quot;&gt; /u/int2me &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1abomxb/langchain_chatmodel_import_from_2_different/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1abomxb/langchain_chatmodel_import_from_2_different/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1abomxb</id><link href="https://www.reddit.com/r/LangChain/comments/1abomxb/langchain_chatmodel_import_from_2_different/" /><updated>2024-01-26T18:03:30+00:00</updated><published>2024-01-26T18:03:30+00:00</published><title>Langchain ChatModel import from 2 different library?</title></entry><entry><author><name>/u/travel-nerd-05</name><uri>https://www.reddit.com/user/travel-nerd-05</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I was trying out X-shot prompting with Langchain but couldn&amp;#39;t find some good relevant examples of how to form the prompt template for this. Specifically how should an example or examples be inserted into the prompt template and merged with proper instructions. Any useful examples or turtorials?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/travel-nerd-05&quot;&gt; /u/travel-nerd-05 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1abo6l5/xshot_prompting_with_langchain/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1abo6l5/xshot_prompting_with_langchain/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1abo6l5</id><link href="https://www.reddit.com/r/LangChain/comments/1abo6l5/xshot_prompting_with_langchain/" /><updated>2024-01-26T17:44:55+00:00</updated><published>2024-01-26T17:44:55+00:00</published><title>X-Shot prompting with Langchain</title></entry><entry><author><name>/u/gswithai</name><uri>https://www.reddit.com/user/gswithai</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi there! As you may know I often post here about my latest &lt;a href=&quot;https://www.gettingstarted.ai/tag/langchain&quot;&gt;LangChain tutorials and articles&lt;/a&gt;. I was recently introduced to Embedchain, a Python library built on top of LangChain that takes care of your RAG needs in a few lines of Python code. It basically does all of the following for you right out-of-the-box:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Sets up local Chroma DB&lt;/li&gt; &lt;li&gt;Takes in data source (URL, YouTube, PDF, etc…)&lt;/li&gt; &lt;li&gt;Makes chunks out of the data&lt;/li&gt; &lt;li&gt;Converts the chunks to embeddings&lt;/li&gt; &lt;li&gt;Stores the embeddings on the vector database&lt;/li&gt; &lt;li&gt;Performs similarity search&lt;/li&gt; &lt;li&gt;Query a large language model&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Long story short, I took it for a spin and &lt;a href=&quot;https://www.gettingstarted.ai/what-is-the-difference-between-embedchain-and-langchain/&quot;&gt;wrote about it here&lt;/a&gt; specifically, comparing it with LangChain.&lt;/p&gt; &lt;p&gt;If you‘re looking for a tool that lets you build a RAG app quickly, give it a try. You may find it suitable for your use case.&lt;/p&gt; &lt;p&gt;Let me know what you think!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/gswithai&quot;&gt; /u/gswithai &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/19fhmbv/langchain_is_awesome_but_have_you_tried_embedchain/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/19fhmbv/langchain_is_awesome_but_have_you_tried_embedchain/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_19fhmbv</id><link href="https://www.reddit.com/r/LangChain/comments/19fhmbv/langchain_is_awesome_but_have_you_tried_embedchain/" /><updated>2024-01-25T19:31:24+00:00</updated><published>2024-01-25T19:31:24+00:00</published><title>LangChain is awesome, but have you tried Embedchain?</title></entry><entry><author><name>/u/Money_Mycologist4939</name><uri>https://www.reddit.com/user/Money_Mycologist4939</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I found this blog articles on &lt;a href=&quot;https://blog.langchain.dev/building-chat-langchain-2/&quot;&gt;https://blog.langchain.dev/building-chat-langchain-2/&lt;/a&gt;, about building a Rag Chatbot using langchain expression language. Looking at the entire code from the github repo, I am not pretty sure to understand what does this chain do:&lt;br/&gt; (&lt;/p&gt; &lt;p&gt;{&lt;/p&gt; &lt;p&gt;&amp;quot;question&amp;quot;: RunnableLambda(itemgetter(&amp;quot;question&amp;quot;)).with_config(&lt;/p&gt; &lt;p&gt;run_name=&amp;quot;Itemgetter:question&amp;quot;&lt;/p&gt; &lt;p&gt;),&lt;/p&gt; &lt;p&gt;&amp;quot;chat_history&amp;quot;: RunnableLambda(serialize_history).with_config(&lt;/p&gt; &lt;p&gt;run_name=&amp;quot;SerializeHistory&amp;quot;&lt;/p&gt; &lt;p&gt;),&lt;/p&gt; &lt;p&gt;}&lt;/p&gt; &lt;p&gt;| _context&lt;/p&gt; &lt;p&gt;| response_synthesizer&lt;/p&gt; &lt;p&gt;) &lt;/p&gt; &lt;p&gt;It seems like: is getting the question from a dictionary and pass it to the context etc..., but I can&amp;#39;t see what&amp;#39;s the input to the serialize_history function. I mean should not be chat_history since it&amp;#39;s the key assigned to the result of the function, it&amp;#39;s not the input to the function right? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Money_Mycologist4939&quot;&gt; /u/Money_Mycologist4939 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1abf7eo/rag_chatbot_with_lcel/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1abf7eo/rag_chatbot_with_lcel/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1abf7eo</id><link href="https://www.reddit.com/r/LangChain/comments/1abf7eo/rag_chatbot_with_lcel/" /><updated>2024-01-26T10:20:18+00:00</updated><published>2024-01-26T10:20:18+00:00</published><title>RAG Chatbot with LCEL</title></entry><entry><author><name>/u/throwawayrandomvowel</name><uri>https://www.reddit.com/user/throwawayrandomvowel</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hello,&lt;/p&gt; &lt;p&gt;I&amp;#39;m working on a project like this for another dataset, but I don&amp;#39;t see why I can&amp;#39;t apply it to my own repo. I love using GPT for coding productivity, but one of the limitations is in api-type environments with multiple files and dependencies flying around - it&amp;#39;s difficult to share all the relevant information to your agent or chatbot. &lt;/p&gt; &lt;p&gt;Is there anything like this currently available? This isn&amp;#39;t exactly rocket scient to start. If not, I&amp;#39;ll start working on it&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/throwawayrandomvowel&quot;&gt; /u/throwawayrandomvowel &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/19fgnin/ragd_repo_and_multiagent_chatbot_for_advanced/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/19fgnin/ragd_repo_and_multiagent_chatbot_for_advanced/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_19fgnin</id><link href="https://www.reddit.com/r/LangChain/comments/19fgnin/ragd_repo_and_multiagent_chatbot_for_advanced/" /><updated>2024-01-25T18:50:52+00:00</updated><published>2024-01-25T18:50:52+00:00</published><title>RAG'd Repo and multi-agent chatbot for advanced codebase support?</title></entry><entry><author><name>/u/Gullible-Being-8595</name><uri>https://www.reddit.com/user/Gullible-Being-8595</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I am working on a project where I want to use LLMs (GPT4) to summarize the data from a website. First, I want to scrap all the data from the all the links available in that website and then I want to use GPT4 to summarize the data. I can get the summarization part done easily but how can I build a scrapper which can scrap the data from a website. For example;&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;If the website is a restaurant website then I would want to go over all the links in that site. It will be an iterative process and somehow I need to keep the already visited/scrapped links so that I shouldn&amp;#39;t scrap it again. &lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;Is there any framework or already built library/API available for this purpose?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Gullible-Being-8595&quot;&gt; /u/Gullible-Being-8595 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/19fadp1/advance_scrapping_with_llm/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/19fadp1/advance_scrapping_with_llm/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_19fadp1</id><link href="https://www.reddit.com/r/LangChain/comments/19fadp1/advance_scrapping_with_llm/" /><updated>2024-01-25T14:19:16+00:00</updated><published>2024-01-25T14:19:16+00:00</published><title>Advance Scrapping with LLM</title></entry><entry><author><name>/u/whir_of_invention</name><uri>https://www.reddit.com/user/whir_of_invention</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;m working on a project that involves querying a database using natural language, and I&amp;#39;m trying to decide between using Langchain SQL and the GPT-4 SQL function. From my understanding, Langchain SQL specializes in converting natural language into SQL queries, which seems ideal for direct database interaction. On the other hand, gpt4 can also generate sqls directly with function calling. So it seems there is no need to use Langchain anymore.&lt;/p&gt; &lt;p&gt;I&amp;#39;d appreciate insights or experiences from anyone who has used either Langchain SQL or GPT-4 for similar purposes.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/whir_of_invention&quot;&gt; /u/whir_of_invention &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/19ffzyk/choosing_between_langchain_sql_and_gpt4_sql/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/19ffzyk/choosing_between_langchain_sql_and_gpt4_sql/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_19ffzyk</id><link href="https://www.reddit.com/r/LangChain/comments/19ffzyk/choosing_between_langchain_sql_and_gpt4_sql/" /><updated>2024-01-25T18:22:45+00:00</updated><published>2024-01-25T18:22:45+00:00</published><title>Choosing Between Langchain SQL and GPT-4 SQL Function</title></entry><entry><author><name>/u/3RiversAINexus</name><uri>https://www.reddit.com/user/3RiversAINexus</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;m looking for the best LLM currently for agentic behavior in langchain. By best, I mean the most consistent with the least parsing errors.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/3RiversAINexus&quot;&gt; /u/3RiversAINexus &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/19fnp1g/what_is_the_best_local_llm_for_agentic_behavior/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/19fnp1g/what_is_the_best_local_llm_for_agentic_behavior/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_19fnp1g</id><link href="https://www.reddit.com/r/LangChain/comments/19fnp1g/what_is_the_best_local_llm_for_agentic_behavior/" /><updated>2024-01-25T23:57:00+00:00</updated><published>2024-01-25T23:57:00+00:00</published><title>What is the best local LLM for agentic behavior?</title></entry><entry><author><name>/u/romeinday1</name><uri>https://www.reddit.com/user/romeinday1</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Let us say we have a simple spec like this below:&lt;/p&gt; &lt;p&gt;set my_property to 1, when condition1 is met&lt;/p&gt; &lt;p&gt;set my_property to 2, when condition2 is met&lt;/p&gt; &lt;p&gt;set my_property to 3, when condition3 and condition33 are both met&lt;/p&gt; &lt;p&gt;set my_property to 4, when condition4 is met&lt;/p&gt; &lt;p&gt;Then this is python code below:&lt;/p&gt; &lt;p&gt;if condition1:&lt;/p&gt; &lt;p&gt;my_property to 1&lt;/p&gt; &lt;p&gt;elif condition2:&lt;/p&gt; &lt;p&gt;my_property to -2929 # people making error here&lt;/p&gt; &lt;p&gt;elif condition3 and condition33:&lt;/p&gt; &lt;p&gt;my_property to 3&lt;/p&gt; &lt;p&gt;elif ...&lt;/p&gt; &lt;p&gt;This is an extremely simple example. I am wondering if langchain or other popular LLM library can somehow help us engineers achieve this to make our lives easier. I know langchain can somehow compare two similar (let us say earning report pdf) but not sure if it can help, even just a little bit, to spot any mistaken typo between spec and implementation?&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/romeinday1&quot;&gt; /u/romeinday1 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/19fma04/anyway_to_compare_a_spec_word_or_excel_with_its/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/19fma04/anyway_to_compare_a_spec_word_or_excel_with_its/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_19fma04</id><link href="https://www.reddit.com/r/LangChain/comments/19fma04/anyway_to_compare_a_spec_word_or_excel_with_its/" /><updated>2024-01-25T22:53:36+00:00</updated><published>2024-01-25T22:53:36+00:00</published><title>Anyway to compare a spec (word or excel) with its programming implementation for any gap/error</title></entry><entry><author><name>/u/Sad_Reporter910</name><uri>https://www.reddit.com/user/Sad_Reporter910</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;System Prompt with ConversationalRetrievalChain: I am a newb playing with open-ai, chat-gpt, and langchain. Using open-ai api&amp;#39;s directly, I am able to set a system prompt to prepare the llm to respond to all all my interactions with the initial system prompt instructions. Now I am using langchain&amp;#39;s ConversationalRetrievalChain to try to interact with custom content. I have it working, but I would like to give the underlying llm (gpt-4) a system prompt as I do using the open-ai api&amp;#39;s directly. How can I do this? e.g. &amp;quot;You are a helpful sale agent named Joe that works for ACME CO. Try to respond with links to products sold by ACME CO&amp;quot;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Sad_Reporter910&quot;&gt; /u/Sad_Reporter910 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/19fgxzb/how_do_i_get_conversationalretrievalchain_to_use/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/19fgxzb/how_do_i_get_conversationalretrievalchain_to_use/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_19fgxzb</id><link href="https://www.reddit.com/r/LangChain/comments/19fgxzb/how_do_i_get_conversationalretrievalchain_to_use/" /><updated>2024-01-25T19:03:06+00:00</updated><published>2024-01-25T19:03:06+00:00</published><title>How do I get ConversationalRetrievalChain to use a System Prompt?</title></entry></feed>