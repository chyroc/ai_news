<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/"><category term="LangChain" label="r/LangChain"/><updated>2024-05-14T05:32:13+00:00</updated><icon>https://www.redditstatic.com/icon.png/</icon><id>/r/LangChain.rss</id><link rel="self" href="https://www.reddit.com/r/LangChain.rss" type="application/atom+xml" /><link rel="alternate" href="https://www.reddit.com/r/LangChain" type="text/html" /><subtitle>LangChain is an open-source framework and developer toolkit that helps developers get LLM applications from prototype to production. It is available for Python and Javascript at https://www.langchain.com/.</subtitle><title>LangChain</title><entry><author><name>/u/zchaarm</name><uri>https://www.reddit.com/user/zchaarm</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;A place for members of &lt;a href=&quot;/r/LangChain&quot;&gt;r/LangChain&lt;/a&gt; to chat with each other&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/zchaarm&quot;&gt; /u/zchaarm &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_10ljho9</id><link href="https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/" /><updated>2023-01-26T04:33:05+00:00</updated><published>2023-01-26T04:33:05+00:00</published><title>r/LangChain Lounge</title></entry><entry><author><name>/u/mehul_gupta1997</name><uri>https://www.reddit.com/user/mehul_gupta1997</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;DSPy is a breakthrough Generative AI package that helps in automatic prompt tuning. How is it different from LangChain? Find in this video &lt;a href=&quot;https://youtu.be/3QbiUEWpO0E?si=4oOXx6olUv-7Bdr9&quot;&gt;https://youtu.be/3QbiUEWpO0E?si=4oOXx6olUv-7Bdr9&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/mehul_gupta1997&quot;&gt; /u/mehul_gupta1997 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cril3q/langchain_vs_dspy_key_differences_explained/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cril3q/langchain_vs_dspy_key_differences_explained/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cril3q</id><link href="https://www.reddit.com/r/LangChain/comments/1cril3q/langchain_vs_dspy_key_differences_explained/" /><updated>2024-05-14T03:30:13+00:00</updated><published>2024-05-14T03:30:13+00:00</published><title>LangChain vs DSPy Key differences explained</title></entry><entry><author><name>/u/help-me-grow</name><uri>https://www.reddit.com/user/help-me-grow</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1crduue/11_ways_to_mix_and_match_tools_to_build_ai_agents/&quot;&gt; &lt;img src=&quot;https://a.thumbs.redditmedia.com/e_NtZPkFf4DlJeare7KHf4ZF14L0R5M6XskpL_ZZFG0.jpg&quot; alt=&quot;11 Ways to Mix and Match Tools to Build AI Agents&quot; title=&quot;11 Ways to Mix and Match Tools to Build AI Agents&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/help-me-grow&quot;&gt; /u/help-me-grow &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://github.com/ytang07/ai_agents_cookbooks&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1crduue/11_ways_to_mix_and_match_tools_to_build_ai_agents/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1crduue</id><media:thumbnail url="https://a.thumbs.redditmedia.com/e_NtZPkFf4DlJeare7KHf4ZF14L0R5M6XskpL_ZZFG0.jpg" /><link href="https://www.reddit.com/r/LangChain/comments/1crduue/11_ways_to_mix_and_match_tools_to_build_ai_agents/" /><updated>2024-05-13T23:32:35+00:00</updated><published>2024-05-13T23:32:35+00:00</published><title>11 Ways to Mix and Match Tools to Build AI Agents</title></entry><entry><author><name>/u/liquidus08</name><uri>https://www.reddit.com/user/liquidus08</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;ve created a (RAG) system that returns relevant documents. I am now looking to enhance the user experience by displaying images associated with these documents using their documents IDS.&lt;/p&gt; &lt;p&gt;So my system is: doc retrieval, get document ids, load relevant images using IDs, display images.&lt;/p&gt; &lt;p&gt;Anyone knows how this can be done? I understand multimodal is an option but I don&amp;#39;t want to search across images. Just want to display images using document ids of returned docs&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/liquidus08&quot;&gt; /u/liquidus08 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1crkgp0/displaying_images_using_document_ids_in_a_rag/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1crkgp0/displaying_images_using_document_ids_in_a_rag/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1crkgp0</id><link href="https://www.reddit.com/r/LangChain/comments/1crkgp0/displaying_images_using_document_ids_in_a_rag/" /><updated>2024-05-14T05:20:50+00:00</updated><published>2024-05-14T05:20:50+00:00</published><title>Displaying Images using document IDs in a RAG System</title></entry><entry><author><name>/u/Honest-Worth3677</name><uri>https://www.reddit.com/user/Honest-Worth3677</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=xDOm_G7Cyac&quot;&gt;Hugging Face + Langchain+ Upwork | How to Solve Real World AI Job in UPWORK (youtube.com)&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Honest-Worth3677&quot;&gt; /u/Honest-Worth3677 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1crjkxf/hugging_face_langchain_upwork_how_to_solve_real/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1crjkxf/hugging_face_langchain_upwork_how_to_solve_real/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1crjkxf</id><link href="https://www.reddit.com/r/LangChain/comments/1crjkxf/hugging_face_langchain_upwork_how_to_solve_real/" /><updated>2024-05-14T04:27:10+00:00</updated><published>2024-05-14T04:27:10+00:00</published><title>Hugging Face + Langchain+ Upwork | How to Solve Real World AI Job in UPWORK</title></entry><entry><author><name>/u/emersounds</name><uri>https://www.reddit.com/user/emersounds</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;I&amp;#39;m working on a project using Google Cloud and I&amp;#39;m exploring two approaches to create an agent:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Option 1: Reasoning Engine with Langchain&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;This approach involves using Langchain&amp;#39;s Reasoning Engine with my own tools connected through a REST API. I envision a frontend built with Next.js that communicates with a FastAPI backend using WebSockets. The backend would then query the Reasoning Engine using the Vertex AI SDK, which would reason with my Langchain Agent.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Option 2: Langchain &amp;amp; Langserve on Google Cloud Run&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Alternatively, I could create a standalone agent entirely in Langserve. This agent would handle memory, tools, and queries to the Gemini API for completion. Communication with the frontend would likely involve a REST API (but I&amp;#39;m open to suggestions).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;My Dilemma:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;I&amp;#39;m unsure which approach is better suited for my needs.&lt;/li&gt; &lt;li&gt;My REST API is private and deployed on AWS.&lt;/li&gt; &lt;li&gt;I&amp;#39;m open to recommendations on building an agent with Google Cloud, particularly regarding WebSockets (mandatory or optional) and overall architecture.&lt;/li&gt; &lt;li&gt;Does Langserve even support WebSockets?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Any insights or suggestions from the community would be greatly appreciated!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/emersounds&quot;&gt; /u/emersounds &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1crezed/building_an_agent_with_google_cloud_langserve_on/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1crezed/building_an_agent_with_google_cloud_langserve_on/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1crezed</id><link href="https://www.reddit.com/r/LangChain/comments/1crezed/building_an_agent_with_google_cloud_langserve_on/" /><updated>2024-05-14T00:26:56+00:00</updated><published>2024-05-14T00:26:56+00:00</published><title>Building an Agent with Google Cloud (Langserve on GC Run vs. Vertex AI Reasoning Engine)</title></entry><entry><author><name>/u/aryanmadhavverma</name><uri>https://www.reddit.com/user/aryanmadhavverma</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cqzjq0/experimenting_with_langchain_langgraph_and/&quot;&gt; &lt;img src=&quot;https://b.thumbs.redditmedia.com/s5LwlsknW0tpECrvQHNAcOTUzwaZmriSp4A7dv2TGGk.jpg&quot; alt=&quot;Experimenting with Langchain, Langgraph, and Snowflake to Build a Product Copilot POC&quot; title=&quot;Experimenting with Langchain, Langgraph, and Snowflake to Build a Product Copilot POC&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;In a recent hackweek, a colleague and I decided to explore the integration of natural language processing and data visualization by building a prototype agent that interfaces directly with Snowflake. Our goal was to create a tool that could automatically interpret intent, fetch relevant data, and generate visual insights, starting with trends and funnels.&lt;/p&gt; &lt;p&gt;Here’s what we’ve implemented so far:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Trend visualization&lt;/li&gt; &lt;li&gt;Funnel analysis&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Looking ahead, we’re excited to expand the tool&amp;#39;s capabilities to include:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Retention reports&lt;/li&gt; &lt;li&gt;User cohort analysis&lt;/li&gt; &lt;li&gt;Metric alerts&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This project is very much a work-in-progress, and we&amp;#39;re keen on refining and enhancing its functionalities. We want this tool to be a helpful assistant for product managers who rely on Snowflake for data insights.&lt;/p&gt; &lt;p&gt;For a closer look, check out the video demo we posted on our LinkedIn. &lt;a href=&quot;https://www.linkedin.com/posts/shubhankarsrivastava_analytics-agents-in-snowflake-product-activity-7194598110440955904-wi51?utm_source=share&amp;amp;utm_medium=member_desktop&quot;&gt;Here&amp;#39;s the link to our LinkedIn post with the video demo.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Attached is an image showing how we structured the architecture of our agent. I’m eager to hear any feedback or ideas from this community!&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/c2kj0psg670d1.png?width=795&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=13f65407c9cd4f8295a5c9c7826451c51492446e&quot;&gt;https://preview.redd.it/c2kj0psg670d1.png?width=795&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=13f65407c9cd4f8295a5c9c7826451c51492446e&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/aryanmadhavverma&quot;&gt; /u/aryanmadhavverma &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cqzjq0/experimenting_with_langchain_langgraph_and/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cqzjq0/experimenting_with_langchain_langgraph_and/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1cqzjq0</id><media:thumbnail url="https://b.thumbs.redditmedia.com/s5LwlsknW0tpECrvQHNAcOTUzwaZmriSp4A7dv2TGGk.jpg" /><link href="https://www.reddit.com/r/LangChain/comments/1cqzjq0/experimenting_with_langchain_langgraph_and/" /><updated>2024-05-13T13:39:11+00:00</updated><published>2024-05-13T13:39:11+00:00</published><title>Experimenting with Langchain, Langgraph, and Snowflake to Build a Product Copilot POC</title></entry><entry><author><name>/u/CantaloupeLeading646</name><uri>https://www.reddit.com/user/CantaloupeLeading646</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;hi,&lt;/p&gt; &lt;p&gt;i&amp;#39;m struggling to find an answer to this question - &lt;/p&gt; &lt;p&gt;i&amp;#39;m writing a small application that utilizes RAG with a mistral model, the main script looks something like this: &lt;/p&gt; &lt;p&gt;&lt;code&gt;from langchain_community.document_loaders import TextLoader&lt;/code&gt;&lt;br/&gt; &lt;code&gt;from langchain_community.document_loaders import PyPDFLoader&lt;/code&gt;&lt;br/&gt; &lt;code&gt;from langchain_mistralai.chat_models import ChatMistralAI&lt;/code&gt;&lt;br/&gt; &lt;code&gt;from langchain_mistralai.embeddings import MistralAIEmbeddings&lt;/code&gt;&lt;br/&gt; &lt;code&gt;from langchain_community.vectorstores import FAISS&lt;/code&gt;&lt;br/&gt; &lt;code&gt;from langchain.text_splitter import RecursiveCharacterTextSplitter&lt;/code&gt;&lt;br/&gt; &lt;code&gt;from langchain.chains.combine_documents import create_stuff_documents_chain&lt;/code&gt;&lt;br/&gt; &lt;code&gt;from langchain_core.prompts import ChatPromptTemplate&lt;/code&gt;&lt;br/&gt; &lt;code&gt;from langchain.chains import create_retrieval_chain&lt;/code&gt;&lt;br/&gt; &lt;code&gt;from langchain.storage import LocalFileStore&lt;/code&gt;&lt;br/&gt; &lt;code&gt;from langchain.embeddings import CacheBackedEmbeddings&lt;/code&gt;&lt;br/&gt; &lt;code&gt;from time import time&lt;/code&gt;&lt;br/&gt; &lt;code&gt;from dotenv import load_dotenv&lt;/code&gt;&lt;br/&gt; &lt;code&gt;import os&lt;/code&gt; &lt;/p&gt; &lt;p&gt;&lt;code&gt;if __name__ == &amp;quot;__main__&amp;quot;:&lt;/code&gt;&lt;br/&gt; &lt;code&gt;load_dotenv()&lt;/code&gt;&lt;br/&gt; &lt;code&gt;api_key = os.getenv(&amp;quot;MISTRAL_KEY&amp;quot;)&lt;/code&gt;&lt;br/&gt; &lt;code&gt;loader = PyPDFLoader(&amp;quot;paper1.pdf&amp;quot;)&lt;/code&gt;&lt;br/&gt; &lt;code&gt;text_splitter = RecursiveCharacterTextSplitter(chunk_size = 2000, chunk_overlap = 300)&lt;/code&gt;&lt;br/&gt; &lt;code&gt;docs = loader.load_and_split()&lt;/code&gt;&lt;br/&gt; &lt;code&gt;# Split text into chunks&lt;/code&gt;&lt;br/&gt; &lt;code&gt;documents = text_splitter.split_documents(docs)&lt;/code&gt;&lt;br/&gt; &lt;code&gt;# Define the embedding model&lt;/code&gt;&lt;br/&gt; &lt;code&gt;embeddings = MistralAIEmbeddings(model=&amp;quot;mistral-embed&amp;quot;, mistral_api_key=api_key)&lt;/code&gt;&lt;br/&gt; &lt;code&gt;store = LocalFileStore(&amp;quot;./cache/&amp;quot;)&lt;/code&gt;&lt;br/&gt; &lt;code&gt;cached_embedder = CacheBackedEmbeddings.from_bytes_store(&lt;/code&gt;&lt;br/&gt; &lt;code&gt;embeddings, store, namespace=embeddings.model&lt;/code&gt;&lt;br/&gt; &lt;code&gt;)&lt;/code&gt;&lt;br/&gt; &lt;code&gt;# Create the vector store&lt;/code&gt;&lt;br/&gt; &lt;code&gt;initial_time = time()&lt;/code&gt;&lt;br/&gt; &lt;code&gt;vector = FAISS.from_documents(documents, cached_embedder)&lt;/code&gt;&lt;br/&gt; &lt;code&gt;# Define a retriever interface&lt;/code&gt;&lt;br/&gt; &lt;code&gt;retriever = vector.as_retriever()&lt;/code&gt;&lt;br/&gt; &lt;code&gt;# Define LLM&lt;/code&gt;&lt;br/&gt; &lt;code&gt;model = ChatMistralAI(mistral_api_key=api_key)&lt;/code&gt;&lt;br/&gt; &lt;code&gt;# Define prompt template&lt;/code&gt;&lt;br/&gt; &lt;code&gt;prompt = ChatPromptTemplate.from_template(&amp;quot;&amp;quot;&amp;quot;Answer the following question based only on the provided context:&lt;/code&gt; &lt;/p&gt; &lt;p&gt;&lt;code&gt;&amp;lt;context&amp;gt;&lt;/code&gt;&lt;br/&gt; &lt;code&gt;{context}&lt;/code&gt;&lt;br/&gt; &lt;code&gt;&amp;lt;/context&amp;gt;&lt;/code&gt; &lt;/p&gt; &lt;p&gt;&lt;code&gt;Question: {input}&amp;quot;&amp;quot;&amp;quot;)&lt;/code&gt; &lt;/p&gt; &lt;p&gt;&lt;code&gt;# Create a retrieval chain to answer questions&lt;/code&gt;&lt;br/&gt; &lt;code&gt;document_chain = create_stuff_documents_chain(model, prompt)&lt;/code&gt;&lt;br/&gt; &lt;code&gt;retrieval_chain = create_retrieval_chain(retriever, document_chain)&lt;/code&gt;&lt;br/&gt; &lt;code&gt;response = retrieval_chain.invoke({&amp;quot;input&amp;quot;: &amp;quot;What were the two main things the author worked on before college?&amp;quot;})&lt;/code&gt;&lt;br/&gt; &lt;code&gt;print(response[&amp;quot;answer&amp;quot;])&lt;/code&gt;&lt;/p&gt; &lt;p&gt;is there any function i can run from a main script to track my usage in dollars?&lt;br/&gt; everytime the script runs it will print out how many dollars i have left or something like that. &lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/CantaloupeLeading646&quot;&gt; /u/CantaloupeLeading646 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cr5usv/tracking_token_usage_programatically_in_python/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cr5usv/tracking_token_usage_programatically_in_python/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cr5usv</id><link href="https://www.reddit.com/r/LangChain/comments/1cr5usv/tracking_token_usage_programatically_in_python/" /><updated>2024-05-13T18:01:32+00:00</updated><published>2024-05-13T18:01:32+00:00</published><title>tracking token usage programatically in python</title></entry><entry><author><name>/u/jiggerjog</name><uri>https://www.reddit.com/user/jiggerjog</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi y&amp;#39;all! I&amp;#39;m trying to get some feedback from those using langchain with OpenAI APIs. What areas are you guys seeing the most difficulty with? I would love to hear about your experience! Feel free to mention more details in the comments section about your specific usecase&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://www.reddit.com/poll/1cravwx&quot;&gt;View Poll&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/jiggerjog&quot;&gt; /u/jiggerjog &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cravwx/what_is_the_hardest_part_of_integrating_langchain/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cravwx/what_is_the_hardest_part_of_integrating_langchain/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cravwx</id><link href="https://www.reddit.com/r/LangChain/comments/1cravwx/what_is_the_hardest_part_of_integrating_langchain/" /><updated>2024-05-13T21:23:33+00:00</updated><published>2024-05-13T21:23:33+00:00</published><title>what is the hardest part of integrating langchain with openai?</title></entry><entry><author><name>/u/echopurpose</name><uri>https://www.reddit.com/user/echopurpose</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;m trying to understand custom tools and agents. &lt;/p&gt; &lt;p&gt;I tried to make a simple example, with three tools to choose from. Then I ask it for money. Mostly it fails in one of a few ways:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;It identifies &amp;#39;get_money&amp;#39; as the function to call, but then it says it is not valid and tries to call it again&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;It just loops calling &amp;#39;get_money&amp;#39; over and over again&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;It calls &amp;#39;get_money&amp;#39; but then decides it needs to do something with the money and that &amp;#39;get_bricks&amp;#39; seems like a good things to do. &lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;I&amp;#39;m using llama3 as my LLM.&lt;/p&gt; &lt;p&gt;Can you give me any pointers? Code snippet and example of looping behavior below.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;@tool def get_bricks(query: str) -&amp;gt; str: &amp;quot;&amp;quot;&amp;quot;Returns bricks&amp;quot;&amp;quot;&amp;quot; return &amp;quot;bricks&amp;quot; @tool def get_money(query: str) -&amp;gt; int: &amp;quot;&amp;quot;&amp;quot;Returns money.&amp;quot;&amp;quot;&amp;quot; return &amp;quot;money&amp;quot; @tool def get_word_length(word: str) -&amp;gt; int: &amp;quot;&amp;quot;&amp;quot;Returns the length of a word.&amp;quot;&amp;quot;&amp;quot; return len(word) tools = [get_word_length, get_money, get_bricks] prompt = hub.pull(&amp;quot;hwchase17/react&amp;quot;) agent = create_react_agent(llm, tools, prompt) agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True) agent_executor.invoke( { &amp;quot;input&amp;quot;: &amp;quot;Can you get me some money?&amp;quot; } ) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Which results in something like this:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;python minimal_tool_bot.py &amp;gt; Entering new AgentExecutor chain... I&amp;#39;d be happy to help! Thought: Hmm, it looks like I need to take an action to get the money. Let me think... Action: get_money Action Input: None (since get_money doesn&amp;#39;t require any inputmoneyI&amp;#39;m glad you asked! Thought: Now that I have the money, let&amp;#39;s see how we can use it. Action: None (for now) The money seems like a nice chunk of change! Let&amp;#39;s think about what to do next... Thought: What should we do with this money? Action: get_bricks Action Input: None (since get_bricks doesn&amp;#39;t require any inputNone (for now) The money seems like a nice chunk of change! Let&amp;#39;s think about what to do next... Thought: What should we do with this money? Action: get_bricks is not a valid tool, try one of [get_word_length, get_money, get_bricks].Let&amp;#39;s continue! Thought: Oops, it looks like I made a mistake! Action: get_word_length Action Input: None (since get_word_length doesn&amp;#39;t require any input53Question: Can you get me some money? Thought: I&amp;#39;d be happy to help! Thought: Hmm, it looks like I need to take an action to get the money. Let me think... Action: get_money Action Input: None (since get_money doesn&amp;#39;t require any inputmoneyI&amp;#39;ll continue from where you left off. Question: What does the length of &amp;quot;Can you get me some money?&amp;quot; mean? Thought: I think I can use this information to help answer my original question... Action: get_word_length &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/echopurpose&quot;&gt; /u/echopurpose &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cr3lav/unexpected_and_erroneous_results_when_trying_to/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cr3lav/unexpected_and_erroneous_results_when_trying_to/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cr3lav</id><link href="https://www.reddit.com/r/LangChain/comments/1cr3lav/unexpected_and_erroneous_results_when_trying_to/" /><updated>2024-05-13T16:29:36+00:00</updated><published>2024-05-13T16:29:36+00:00</published><title>Unexpected and erroneous results when trying to use a react agent with minimal tools</title></entry><entry><author><name>/u/Username_Availabe1</name><uri>https://www.reddit.com/user/Username_Availabe1</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;We as a team are starting to work on the product but are still trying to decide whether to use the open-source LLMs and vectorDB hosted on AWS or go with the OpenAI and Pinecone?&lt;/p&gt; &lt;p&gt;Can you please suggest what are the pros and cons in both situations? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Username_Availabe1&quot;&gt; /u/Username_Availabe1 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cquz8v/open_source_vs_closed_source/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cquz8v/open_source_vs_closed_source/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cquz8v</id><link href="https://www.reddit.com/r/LangChain/comments/1cquz8v/open_source_vs_closed_source/" /><updated>2024-05-13T09:20:07+00:00</updated><published>2024-05-13T09:20:07+00:00</published><title>Open source vs Closed source?</title></entry><entry><author><name>/u/Glittering-Bear5748</name><uri>https://www.reddit.com/user/Glittering-Bear5748</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;i have multiple collection(tables like employee,hr,deptartment) of user data and i want build RAG chat bot with citation and prompt using langchain&lt;br/&gt; can you please provide detail steps to perform it don&amp;#39;t provide reference documents&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Glittering-Bear5748&quot;&gt; /u/Glittering-Bear5748 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cr4z66/rag_using_langchain/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cr4z66/rag_using_langchain/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cr4z66</id><link href="https://www.reddit.com/r/LangChain/comments/1cr4z66/rag_using_langchain/" /><updated>2024-05-13T17:26:00+00:00</updated><published>2024-05-13T17:26:00+00:00</published><title>RAG using Langchain</title></entry><entry><author><name>/u/UnhappyAd2901</name><uri>https://www.reddit.com/user/UnhappyAd2901</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I try to write a assistant prompt to llama 3 but it doesn&amp;#39;t recognize the tool at all, does any one can give me an example to it. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/UnhappyAd2901&quot;&gt; /u/UnhappyAd2901 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cr48vz/llama3_assistant_prompt/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cr48vz/llama3_assistant_prompt/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cr48vz</id><link href="https://www.reddit.com/r/LangChain/comments/1cr48vz/llama3_assistant_prompt/" /><updated>2024-05-13T16:56:47+00:00</updated><published>2024-05-13T16:56:47+00:00</published><title>Llama3 Assistant Prompt</title></entry><entry><author><name>/u/cryptokaykay</name><uri>https://www.reddit.com/user/cryptokaykay</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I have been tinkering with DSPy and thought I will share my 2 cents here for anyone who is planning to explore it:&lt;/p&gt; &lt;p&gt;The core idea behind DSPy are two things:&lt;/p&gt; &lt;ol&gt; &lt;li&gt; ⁠Separate programming from prompting&lt;/li&gt; &lt;li&gt; ⁠incorporate some of the best practice prompting techniques under the hood and expose it as a “signature”&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Imagine working on a RAG. Today, the typical approach is to write some retrieval and pass the results to a language model for natural language generation. But, after the first pass, you realize it’s not perfect and you need to iterate and improve it. Typically, there are 2 levers to pull:&lt;/p&gt; &lt;ol&gt; &lt;li&gt; ⁠Document Chunking, insertion and Retrieval strategy&lt;/li&gt; &lt;li&gt; ⁠Language model settings and prompt engineering&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Now, you try a few things, maybe document the performance in a google sheet, iterate and arrive at an ideal set of variables that gives max accuracy.&lt;/p&gt; &lt;p&gt;Now, let’s say after a month, model upgrades, and all of a sudden the accuracy of your RAG regresses. Again you are back to square one, cos you don’t know what to optimize now - retrieval or model? You see what the problem is with this approach? This is a very open ended, monolithic, brittle and unstructured way to optimize and build language model based applications.&lt;/p&gt; &lt;p&gt;This is precisely the problem DSPy is trying to solve. Whatever you can achieve with DSPy can be achieved with native prompt engineering and program composition techniques but it is purely dependent on the programmers skill. But DSPy provides native constructs which anyone can learn and use for trying different techniques in a systematic manner.&lt;/p&gt; &lt;p&gt;DSPy the concept:&lt;/p&gt; &lt;p&gt;Separate prompting from programming and signatures&lt;/p&gt; &lt;p&gt;DSPy does not do any magic with the language model. It just uses a bunch of prompt templates behind the scenes and exposes them as signatures. Ex: when you write a signature like ‘context, question -&amp;gt; answer’, DSPy adds a typical RAG prompt before it makes the call to the LLM. But DSPy also gives you nice features like module settings, assertion based backtracking and automatic prompt optimization.&lt;/p&gt; &lt;p&gt;Basically, you can do something like this with DSPy,&lt;/p&gt; &lt;p&gt;“Given a context and question, answer the following question. Make sure the answer is only “yes” or “no””. If the language model responds with anything else, traditionally we prompt engineer our way to fix it. In DSPy, you can assert the answer for “yes” or “no” and if the assertion fails, DSPy will backtrack automatically, update the prompt to say something like, “this is not a correct answer- {previous_answer} and always only respond with a “yes” or “no”” and makes another language model call which improves the LLMs response because of this newly optimized prompt. In addition, you can also incorporate things like multi hops in your retrieval where you can do something like “retrieve -&amp;gt; generate queries and then retrieve again using the generated queries” for n times and build up a larger context to answer the original question.&lt;/p&gt; &lt;p&gt;Obviously, this can also be done using usual prompt engineering and programming techniques, but the framework exposes native easy to use settings and constructs to do these things more naturally. DSPy as a concept really shines when you are composing a pipeline of language model calls where prompt engineering the entire pipeline or even module wise can lead to a brittle Pipeline.&lt;/p&gt; &lt;p&gt;DSPy the Framework:&lt;/p&gt; &lt;p&gt;Now coming to the framework which is built in python, I think the framework as it stands today is&lt;/p&gt; &lt;ol&gt; &lt;li&gt; ⁠Not production ready&lt;/li&gt; &lt;li&gt; ⁠Buggy and poorly implemented&lt;/li&gt; &lt;li&gt; ⁠Lacks proper documentation&lt;/li&gt; &lt;li&gt; ⁠Poorly designed&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;To me it felt like a rushed implementation with little thought for design thinking, testing and programming principles. The framework code is very hard to understand with a lot of meta programming and data structure parsing and construction going behind the scenes that are scary to run in production.&lt;/p&gt; &lt;p&gt;This is a huge deterrent for anyone trying to learn and use this framework. But, I am sure the creators are thinking about all this and are working to reengineer the framework. There’s also a typescript implementation of this framework that is fairly less popular but has a much better and cleaner design and codebase:&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/dosco/llm-client/&quot;&gt;https://github.com/dosco/llm-client/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;My final thought about this framework is, it’s a promising concept, but it does not change anything about what we already know about LLMs. Also, hiding prompts as templates does not mean prompt engineering is going away, someone still needs to “engineer” the prompts the framework uses and imo the framework should expose these templates and give control back to the developers that way, the vision of separate programming and prompting co exists with giving control not only to program but also to prompt.&lt;/p&gt; &lt;p&gt;Finally, I was able to understand all this by running DSPy programs and visualizing the LLM calls and what prompts it’s adding using my open source tool - &lt;a href=&quot;https://github.com/Scale3-Labs/langtrace&quot;&gt;https://github.com/Scale3-Labs/langtrace&lt;/a&gt; . Do check it out and let me know if you have any feedback.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/cryptokaykay&quot;&gt; /u/cryptokaykay &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cqexk6/thoughts_on_dspy/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cqexk6/thoughts_on_dspy/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cqexk6</id><link href="https://www.reddit.com/r/LangChain/comments/1cqexk6/thoughts_on_dspy/" /><updated>2024-05-12T18:50:23+00:00</updated><published>2024-05-12T18:50:23+00:00</published><title>Thoughts on DSPy</title></entry><entry><author><name>/u/snonsense</name><uri>https://www.reddit.com/user/snonsense</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I&amp;#39;ve been working on a solution for incident management for our internal team recently, and I wanted to share my ideas with all of you to gather your thoughts and suggestions. Here&amp;#39;s what I have in mind:&lt;/p&gt; &lt;p&gt;Incident Viewing: Our system will allow viewing all recorded incidents, along with their current statuses, qualifications, and unique IDs for easy tracking.&lt;/p&gt; &lt;p&gt;Incident Modification: the LLM will have the ability to modify existing incidents by changing priority, assigned team, or even the description for better accuracy.&lt;/p&gt; &lt;p&gt;User Communication: A commenting feature will be integrated so that we can exchange information directly with the concerned users, adding transparency and facilitating swift issue resolution&lt;/p&gt; &lt;p&gt;Any suggestions on how to do that with or without langchain?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/snonsense&quot;&gt; /u/snonsense &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cqzelj/langchain_for_classification/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cqzelj/langchain_for_classification/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cqzelj</id><link href="https://www.reddit.com/r/LangChain/comments/1cqzelj/langchain_for_classification/" /><updated>2024-05-13T13:32:34+00:00</updated><published>2024-05-13T13:32:34+00:00</published><title>Langchain for classification ?</title></entry><entry><author><name>/u/giobirkelund</name><uri>https://www.reddit.com/user/giobirkelund</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;When sending confidential, and highly sensitive data in rag search, I believe everything needs to be encrypted, so that even me, as the database operator, doesn&amp;#39;t have access to the data.&lt;/p&gt; &lt;p&gt;This must be a common use-case, as any company doing rag search on sensitive data has this problem. So I wonder, does anyone know how to do RAG search for sensitive data? &lt;/p&gt; &lt;p&gt;I would imagine you need to encrypt the embeddings, but how do you do the cosine similarity search on encrypted data? Seems like a tricky problem. I&amp;#39;m currently using mongodb atlas vector store, but they don&amp;#39;t offer search on encrypted data.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/giobirkelund&quot;&gt; /u/giobirkelund &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cqr72f/rag_search_on_sensitive_data/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cqr72f/rag_search_on_sensitive_data/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cqr72f</id><link href="https://www.reddit.com/r/LangChain/comments/1cqr72f/rag_search_on_sensitive_data/" /><updated>2024-05-13T04:56:26+00:00</updated><published>2024-05-13T04:56:26+00:00</published><title>RAG Search on sensitive data?</title></entry><entry><author><name>/u/EngineeringFree313</name><uri>https://www.reddit.com/user/EngineeringFree313</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hello,&lt;/p&gt; &lt;p&gt;I am trying to build customer support chat-bot with the goal of answering questions related to some training center courses. I have pdf document per each course containing all the information like course content, tools, price etc..&lt;br/&gt; If I load all documents into one index on a vector db, the returned chunks from db using similarity search given user&amp;#39;s query will probably related to other courses. for example, if the customer asks for a price of some course, All price chunks of all courses will be similar and a price of different course will probably be the answer which is wrong.&lt;br/&gt; So What is the standard way to solve this problem and target the right document only relevant to a specific course regardless all other courses documents.&lt;br/&gt; Also, I might in some cases need to retrieve data from multiple documents or all documents in case of a general question from the user like &amp;quot;List all courses&amp;quot; for example. In this case, I need all courses documents to answer this question.&lt;/p&gt; &lt;p&gt;I am building my project using langchain and gpt-3.5.&lt;/p&gt; &lt;p&gt;Thanks in advance.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/EngineeringFree313&quot;&gt; /u/EngineeringFree313 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cqu5di/customer_support_rag_chatbot/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cqu5di/customer_support_rag_chatbot/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cqu5di</id><link href="https://www.reddit.com/r/LangChain/comments/1cqu5di/customer_support_rag_chatbot/" /><updated>2024-05-13T08:17:52+00:00</updated><published>2024-05-13T08:17:52+00:00</published><title>Customer Support RAG Chat-bot</title></entry><entry><author><name>/u/Agreeable_Fill_2868</name><uri>https://www.reddit.com/user/Agreeable_Fill_2868</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Is &lt;strong&gt;&lt;em&gt;GROK&lt;/em&gt;&lt;/strong&gt; API free of any cost?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Agreeable_Fill_2868&quot;&gt; /u/Agreeable_Fill_2868 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cqtmhq/grok/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cqtmhq/grok/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cqtmhq</id><link href="https://www.reddit.com/r/LangChain/comments/1cqtmhq/grok/" /><updated>2024-05-13T07:39:29+00:00</updated><published>2024-05-13T07:39:29+00:00</published><title>GROK</title></entry><entry><author><name>/u/Zenwills</name><uri>https://www.reddit.com/user/Zenwills</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi all newbie question here. I would like to summarize a large document using map_reduce but facing the 1,024 token limits. I understand this could be due to the model using GPT 2 tokenizer which has a max of 1,024. &lt;/p&gt; &lt;p&gt;Due to this limit, my summarization failed to summarize contents from the earlier chunks.&lt;/p&gt; &lt;p&gt;has anyone done a workaround on this? potentially if making the model to use another tokenizer instead of GPT 2 to able to take in way more tokens?&lt;/p&gt; &lt;p&gt;many thanks for your insights&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Zenwills&quot;&gt; /u/Zenwills &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cqqvtn/map_reduce_summarization_tackling_the_1024_token/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cqqvtn/map_reduce_summarization_tackling_the_1024_token/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cqqvtn</id><link href="https://www.reddit.com/r/LangChain/comments/1cqqvtn/map_reduce_summarization_tackling_the_1024_token/" /><updated>2024-05-13T04:37:24+00:00</updated><published>2024-05-13T04:37:24+00:00</published><title>map_reduce summarization - tackling the 1,024 token limit</title></entry><entry><author><name>/u/MadK92</name><uri>https://www.reddit.com/user/MadK92</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;m currently integrating a llm into our system for job classification. Users can submit requests via text or voice, for example, &amp;#39;I want to design a cake for my son’s birthday.&amp;#39; My goal is for the model to accurately classify these inputs into predefined job categories and types (e.g., {&amp;quot;job_type&amp;quot;:&amp;quot;Plumbing&amp;quot;, &amp;quot;job_category&amp;quot;:&amp;quot;Handyman&amp;quot;}) and subsequently trigger a specific function. However, I&amp;#39;m encountering issues with open-source models like Functionary, which sometimes incorrectly assign random categories or types not present in my predefined list. As I&amp;#39;m still very new to this, I would greatly appreciate any advice on how to ensure the model strictly adheres to my existing categories and types. Thanks for your help!&lt;/p&gt; &lt;p&gt;Edit: I got some interesting results with OpenAI API + pydantic for Hermes-2-Pro-Mistral-7B. Still testing it&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/MadK92&quot;&gt; /u/MadK92 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cqab15/function_calling/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cqab15/function_calling/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cqab15</id><link href="https://www.reddit.com/r/LangChain/comments/1cqab15/function_calling/" /><updated>2024-05-12T15:23:50+00:00</updated><published>2024-05-12T15:23:50+00:00</published><title>Function Calling</title></entry><entry><author><name>/u/magic_26</name><uri>https://www.reddit.com/user/magic_26</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hello. I&amp;#39;m trying to find the best and easiest approach for someone who knows some python to create a script etc to determine most popular topics within unstructured data ie. Product reviews and text responses in surveys. Also, is there a way to leverage LangChain to do this? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/magic_26&quot;&gt; /u/magic_26 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cqnmt2/easiest_method_for_topic_modelling_in_2024_and/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cqnmt2/easiest_method_for_topic_modelling_in_2024_and/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cqnmt2</id><link href="https://www.reddit.com/r/LangChain/comments/1cqnmt2/easiest_method_for_topic_modelling_in_2024_and/" /><updated>2024-05-13T01:37:33+00:00</updated><published>2024-05-13T01:37:33+00:00</published><title>Easiest method for topic modelling in 2024 and can LangChain help?</title></entry><entry><author><name>/u/Smooth-Loquat-4954</name><uri>https://www.reddit.com/user/Smooth-Loquat-4954</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cqdqhe/build_a_rag_pipeline_for_your_blog_with_langchain/&quot;&gt; &lt;img src=&quot;https://external-preview.redd.it/-O7gvFEBkVog1UZh_wSCyLF_DU9VUIxtgOUCFlF_ZCM.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=430e01c4f8de7857f3e4c7ca7f056cd9d5cb8a6d&quot; alt=&quot;Build a RAG pipeline for your blog with LangChain, OpenAI and Pinecone&quot; title=&quot;Build a RAG pipeline for your blog with LangChain, OpenAI and Pinecone&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Smooth-Loquat-4954&quot;&gt; /u/Smooth-Loquat-4954 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://zackproser.com/blog/langchain-pinecone-chat-with-my-blog&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cqdqhe/build_a_rag_pipeline_for_your_blog_with_langchain/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1cqdqhe</id><media:thumbnail url="https://external-preview.redd.it/-O7gvFEBkVog1UZh_wSCyLF_DU9VUIxtgOUCFlF_ZCM.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=430e01c4f8de7857f3e4c7ca7f056cd9d5cb8a6d" /><link href="https://www.reddit.com/r/LangChain/comments/1cqdqhe/build_a_rag_pipeline_for_your_blog_with_langchain/" /><updated>2024-05-12T17:58:04+00:00</updated><published>2024-05-12T17:58:04+00:00</published><title>Build a RAG pipeline for your blog with LangChain, OpenAI and Pinecone</title></entry><entry><author><name>/u/cholekulchhe</name><uri>https://www.reddit.com/user/cholekulchhe</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I am using GPT 3.5 turbo model in my program via below statement but still getting &amp;#39;not a chat model&amp;#39; error, can someone please help me fix it?&lt;/p&gt; &lt;pre&gt;&lt;code&gt;from langchain_openai import ChatOpenAI llm = ChatOpenAI(model=&amp;quot;gpt-3.5-turbo&amp;quot;) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Error:&lt;/p&gt; &lt;p&gt;&lt;code&gt;NotFoundError: Error code: 404 - {&amp;#39;error&amp;#39;: {&amp;#39;message&amp;#39;: &amp;#39;This is not a chat model and thus not supported in the v1/chat/completions endpoint. Did you mean to use v1/completions?&amp;#39;, &amp;#39;type&amp;#39;: &amp;#39;invalid_request_error&amp;#39;, &amp;#39;param&amp;#39;: &amp;#39;model&amp;#39;, &amp;#39;code&amp;#39;: None}}&lt;/code&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/cholekulchhe&quot;&gt; /u/cholekulchhe &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cqhs7x/langchain_calling_incorrect_openai_api_for_gpt_35/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cqhs7x/langchain_calling_incorrect_openai_api_for_gpt_35/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cqhs7x</id><link href="https://www.reddit.com/r/LangChain/comments/1cqhs7x/langchain_calling_incorrect_openai_api_for_gpt_35/" /><updated>2024-05-12T20:54:52+00:00</updated><published>2024-05-12T20:54:52+00:00</published><title>Langchain calling incorrect OpenAI API for GPT 3.5 turbo</title></entry><entry><author><name>/u/bubble_h13</name><uri>https://www.reddit.com/user/bubble_h13</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I want to use RetrievalQA with chromadb, but it seems to store something after I use the retriever each time.&lt;/p&gt; &lt;p&gt;And the action will make my next question have an effect.&lt;/p&gt; &lt;p&gt;I wonder, &amp;quot;Is there any way to use chains(after get_relevant_document) without storing retrieval data in DB?&amp;quot;&lt;/p&gt; &lt;p&gt;Here is my code:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;llm_chain = LLMChain(llm=llm, prompt=RAG_prompt, verbose=True) qa = RetrievalQA.from_chain_type( llm=llm, retriever=vectordb.as_retriever(), chain_type=&amp;quot;stuff&amp;quot;, # chain_type_kwargs={&amp;quot;prompt&amp;quot;: RAG_prompt}, verbose=True, ) &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/bubble_h13&quot;&gt; /u/bubble_h13 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cqajd7/can_i_use_retrieval_without_stored/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cqajd7/can_i_use_retrieval_without_stored/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cqajd7</id><link href="https://www.reddit.com/r/LangChain/comments/1cqajd7/can_i_use_retrieval_without_stored/" /><updated>2024-05-12T15:34:23+00:00</updated><published>2024-05-12T15:34:23+00:00</published><title>Can I use Retrieval without stored ?</title></entry><entry><author><name>/u/admiraltot22</name><uri>https://www.reddit.com/user/admiraltot22</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I am using the latest version of pinecone and langchain.I am encountering an error while trying to upsert vector &lt;/p&gt; &lt;p&gt;using the code &lt;/p&gt; &lt;pre&gt;&lt;code&gt;index = Pinecone.from_documents(docs, embeddings, index_name=index_name) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;i am encountering the error AttributeError: type object &amp;#39;Pinecone&amp;#39; has no attribute &amp;#39;from_documents&amp;#39;&lt;/p&gt; &lt;p&gt;how to resolve this &lt;/p&gt; &lt;p&gt;Any input greatly appreciated&lt;/p&gt; &lt;p&gt;Thanks&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/admiraltot22&quot;&gt; /u/admiraltot22 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cq3ius/problem_with_upserting_vectors_from_documents/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cq3ius/problem_with_upserting_vectors_from_documents/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cq3ius</id><link href="https://www.reddit.com/r/LangChain/comments/1cq3ius/problem_with_upserting_vectors_from_documents/" /><updated>2024-05-12T08:54:22+00:00</updated><published>2024-05-12T08:54:22+00:00</published><title>Problem with upserting vectors from documents</title></entry><entry><author><name>/u/Monkey_D_Uzumaki7</name><uri>https://www.reddit.com/user/Monkey_D_Uzumaki7</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hello, I am a beginner in LLM and LangChain, and I am developing a small application that allows me to query PDF documents with my OpenAI API. Everything works well so far with the PDFs. I am able to query the PDFs. If the question is out of context, it shows that the information is not in the PDF, otherwise, it displays the information. So, everything is going well at the moment, but the problem is that with documents that are a bit longer, 100 pages or more, I really have problems because I can&amp;#39;t even load them to query them. So, what should I do?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Monkey_D_Uzumaki7&quot;&gt; /u/Monkey_D_Uzumaki7 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cps3s5/problem_with_heavy_documents/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cps3s5/problem_with_heavy_documents/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cps3s5</id><link href="https://www.reddit.com/r/LangChain/comments/1cps3s5/problem_with_heavy_documents/" /><updated>2024-05-11T21:48:17+00:00</updated><published>2024-05-11T21:48:17+00:00</published><title>Problem with heavy documents</title></entry></feed>