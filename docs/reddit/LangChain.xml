<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/"><category term="LangChain" label="r/LangChain"/><updated>2024-06-18T18:15:27+00:00</updated><icon>https://www.redditstatic.com/icon.png/</icon><id>/r/LangChain.rss</id><link rel="self" href="https://www.reddit.com/r/LangChain.rss" type="application/atom+xml" /><link rel="alternate" href="https://www.reddit.com/r/LangChain" type="text/html" /><subtitle>LangChain is an open-source framework and developer toolkit that helps developers get LLM applications from prototype to production. It is available for Python and Javascript at https://www.langchain.com/.</subtitle><title>LangChain</title><entry><author><name>/u/zchaarm</name><uri>https://www.reddit.com/user/zchaarm</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;A place for members of &lt;a href=&quot;/r/LangChain&quot;&gt;r/LangChain&lt;/a&gt; to chat with each other&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/zchaarm&quot;&gt; /u/zchaarm &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_10ljho9</id><link href="https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/" /><updated>2023-01-26T04:33:05+00:00</updated><published>2023-01-26T04:33:05+00:00</published><title>r/LangChain Lounge</title></entry><entry><author><name>/u/jeffrey-0711</name><uri>https://www.reddit.com/user/jeffrey-0711</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dimeme/made_dashboard_that_compares_14_retrieval/&quot;&gt; &lt;img src=&quot;https://a.thumbs.redditmedia.com/E-Tt9dJSWO5LJR1e8B77O-f1YESGpubwD6fikDQilT4.jpg&quot; alt=&quot;Made dashboard that compares 14+ retrieval combinations.&quot; title=&quot;Made dashboard that compares 14+ retrieval combinations.&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/jeffrey-0711&quot;&gt; /u/jeffrey-0711 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/gallery/1dimeme&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dimeme/made_dashboard_that_compares_14_retrieval/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1dimeme</id><media:thumbnail url="https://a.thumbs.redditmedia.com/E-Tt9dJSWO5LJR1e8B77O-f1YESGpubwD6fikDQilT4.jpg" /><link href="https://www.reddit.com/r/LangChain/comments/1dimeme/made_dashboard_that_compares_14_retrieval/" /><updated>2024-06-18T09:23:00+00:00</updated><published>2024-06-18T09:23:00+00:00</published><title>Made dashboard that compares 14+ retrieval combinations.</title></entry><entry><author><name>/u/Minute_Yam_1053</name><uri>https://www.reddit.com/user/Minute_Yam_1053</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I love LangChain. Previously, I used LangChain v0.1.x for a simple invoice extraction app. Recently, I tried building a more complex app, an alternative to Perplexity AI using open-source LLMs, which proved challenging. Here’s my experience:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Dependencies&lt;/strong&gt;: Getting started with LangChain required the installation of multiple dependencies, such as core, hub, community, etc. Some of these dependencies rely on third-party libraries that needed manual installation. While not a major issue, it was somewhat inconvenient. Additionally, some dependencies were quite large, which goes against my preference for avoiding hefty installations.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Documentation&lt;/strong&gt;: The documentation isn&amp;#39;t always current, and occasionally, Google searches led me to outdated versions. It&amp;#39;s crucial to be mindful of the information I use. Using outdated docs without immediate issues might cause hidden problems later.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Debugging&lt;/strong&gt;: Debugging was tough due to LangChain&amp;#39;s complexity with many abstractions. I used LangSmith to trace requests and responses. &lt;/li&gt; &lt;li&gt;&lt;strong&gt;Agent and Tools&lt;/strong&gt;: LangChain’s unified interface for adding tools and building agents is great. It likely performs better with advanced commercial LLMs like GPT4o. However, the open-source LLMs I used and agents I built with LangChain wrapper didn’t produce consistent, production-ready results.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;LangGraph&lt;/strong&gt;: LangGraph looks interesting. I plan to explore it more in the future.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;In the end, I built an agent without LangChain, using the OpenAI client, Python coroutines for async flow, and FastAPI for the web server. The code is a few hundred lines and can be find here &lt;a href=&quot;https://github.com/jjleng/sensei/blob/main/backend/sensei_search/agents/samurai_agent.py&quot;&gt;Open Source Perplexity&lt;/a&gt;.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Minute_Yam_1053&quot;&gt; /u/Minute_Yam_1053 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1diers2/my_experience_of_building_a_rag_based_agent_with/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1diers2/my_experience_of_building_a_rag_based_agent_with/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1diers2</id><link href="https://www.reddit.com/r/LangChain/comments/1diers2/my_experience_of_building_a_rag_based_agent_with/" /><updated>2024-06-18T01:30:51+00:00</updated><published>2024-06-18T01:30:51+00:00</published><title>My experience of building a RAG based agent with LangChain</title></entry><entry><author><name>/u/CodingButStillAlive</name><uri>https://www.reddit.com/user/CodingButStillAlive</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;To me, langgraph appears to be the better backbone structure. And it can completely substitute langchain‘s concept of „a chain“. Thus, langchain seems to provide only all the integrations.&lt;/p&gt; &lt;p&gt;Will these integrations finally become a part of langgraph, instead of the other way around?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/CodingButStillAlive&quot;&gt; /u/CodingButStillAlive &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dikyp6/will_langgraph_absorb_langchain/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dikyp6/will_langgraph_absorb_langchain/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dikyp6</id><link href="https://www.reddit.com/r/LangChain/comments/1dikyp6/will_langgraph_absorb_langchain/" /><updated>2024-06-18T07:37:43+00:00</updated><published>2024-06-18T07:37:43+00:00</published><title>Will langgraph absorb langchain?</title></entry><entry><author><name>/u/Important-Motor-3383</name><uri>https://www.reddit.com/user/Important-Motor-3383</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I have a json file that contains data in below format. { &amp;quot;key1&amp;quot;: &amp;quot;{json1}&amp;quot;, key2&amp;quot;: &amp;quot;{json2}&amp;quot;,&lt;/p&gt; &lt;p&gt;key3&amp;quot;: &amp;quot;{json3}&amp;quot;, }&lt;/p&gt; &lt;p&gt;json1,json2, json3... has same Structure. I am trying to summarize above data. I have tried: 1. Summarizing using stuffchain, without splitting text. &amp;amp; with splitting. 2. Summarizing using refine,.with and without splitting. &lt;/p&gt; &lt;p&gt;The summarization is working but it is not taking the whole data in consideration. Sometimes it summarizes only key1,key2 data only. Sometimes on key1 json. &lt;/p&gt; &lt;p&gt;I am using mistral: text model for summarization via langchain. &lt;/p&gt; &lt;ol&gt; &lt;li&gt;How to tackle this problem? &lt;/li&gt; &lt;li&gt;How do we summarize json properly? &lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Important-Motor-3383&quot;&gt; /u/Important-Motor-3383 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1diug6j/how_to_summarize_really_large_json/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1diug6j/how_to_summarize_really_large_json/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1diug6j</id><link href="https://www.reddit.com/r/LangChain/comments/1diug6j/how_to_summarize_really_large_json/" /><updated>2024-06-18T16:10:51+00:00</updated><published>2024-06-18T16:10:51+00:00</published><title>How to summarize really large json</title></entry><entry><author><name>/u/Hour-Benefit-7389</name><uri>https://www.reddit.com/user/Hour-Benefit-7389</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey, there!&lt;/p&gt; &lt;p&gt;Running into an error trying to stuff a large amount of text into GPT 4o -- it&amp;#39;s small enough for the context window, but I&amp;#39;m exceeding the TPM limit (30k, according to OpenAI). Here&amp;#39;s the exact error:&lt;/p&gt; &lt;p&gt;RateLimitError: Error code: 429 - {&amp;#39;error&amp;#39;: {&amp;#39;message&amp;#39;: &amp;#39;Request too large for gpt-4o in organization org-Qvve8O3Iihgaa2kduvOVIemS on tokens per min (TPM): Limit 30000, Requested 45544. The input or output tokens must be reduced in order to run successfully. Visit &lt;a href=&quot;https://platform.openai.com/account/rate-limits&quot;&gt;https://platform.openai.com/account/rate-limits&lt;/a&gt; to learn more.&amp;#39;, &amp;#39;type&amp;#39;: &amp;#39;tokens&amp;#39;, &amp;#39;param&amp;#39;: None, &amp;#39;code&amp;#39;: &amp;#39;rate_limit_exceeded&amp;#39;}}&lt;/p&gt; &lt;p&gt;I&amp;#39;ve looked at a few solutions, but those all rely on feeding the prompt in chunks with delay in between, whereas I need GPT to spit out a single response to my prompt based on the text.&lt;/p&gt; &lt;p&gt;Any way you guys know of to slow down the speed at which I&amp;#39;m passing tokens to GPT? &lt;/p&gt; &lt;p&gt;Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Hour-Benefit-7389&quot;&gt; /u/Hour-Benefit-7389 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ditzwa/help_tpm_rate_limit_error/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ditzwa/help_tpm_rate_limit_error/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1ditzwa</id><link href="https://www.reddit.com/r/LangChain/comments/1ditzwa/help_tpm_rate_limit_error/" /><updated>2024-06-18T15:52:00+00:00</updated><published>2024-06-18T15:52:00+00:00</published><title>HELP: TPM Rate Limit Error</title></entry><entry><author><name>/u/Ukpersfidev</name><uri>https://www.reddit.com/user/Ukpersfidev</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;&lt;strong&gt;Context&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I&amp;#39;m making a sales pitch creator where users can upload a bunch of PDF files relating to their company, and it will output a sales pitch.&lt;/p&gt; &lt;p&gt;I&amp;#39;m handling file uploads separately and would to extract key information from the PDF file and save it in my database, the key here is that I don&amp;#39;t want to summarise the info but rather extract the info without the filler and redundancy.&lt;/p&gt; &lt;p&gt;I can see there are a bunch of pre-made chains, but I&amp;#39;m not sure which can be used in the case of extracting and not summarising, I can also see there is a general map reduce chain, however I can&amp;#39;t tell what this is doing exactly or how to use it with my prompts where I want to state specifically what information should be extracted.&lt;/p&gt; &lt;p&gt;I&amp;#39;ve taken a look through this:&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://js.langchain.com/v0.1/docs/modules/chains/document/map_reduce/#with-lcel&quot;&gt;https://js.langchain.com/v0.1/docs/modules/chains/document/map_reduce/#with-lcel&lt;/a&gt;&lt;/p&gt; &lt;p&gt;And find it extremely confusing, I can&amp;#39;t see what all the complexity is about as in my mind doing what I want is just a case of:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Breaking down the file into text chunks&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Extracting info from each using an await Promise.all(chunks.map(c =&amp;gt; chain.invoke(...)))&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Combining the extracted info &lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;Question&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Why the complexity in the above, what am I missing? Should I just do my simple version, or will it break in certain contexts?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Ukpersfidev&quot;&gt; /u/Ukpersfidev &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ditui4/should_you_use_the_premade_chains_for_map_reducing/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ditui4/should_you_use_the_premade_chains_for_map_reducing/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1ditui4</id><link href="https://www.reddit.com/r/LangChain/comments/1ditui4/should_you_use_the_premade_chains_for_map_reducing/" /><updated>2024-06-18T15:45:26+00:00</updated><published>2024-06-18T15:45:26+00:00</published><title>Should you use the pre-made chains for map reducing?</title></entry><entry><author><name>/u/ChallengeOk6437</name><uri>https://www.reddit.com/user/ChallengeOk6437</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Right now I’m using LlamaParse and it works really well. I want to know what is the best open source tool out there for parsing my PDFs before sending it to the other parts of my RAG. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/ChallengeOk6437&quot;&gt; /u/ChallengeOk6437 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dicr6p/best_open_source_document_parser/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dicr6p/best_open_source_document_parser/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dicr6p</id><link href="https://www.reddit.com/r/LangChain/comments/1dicr6p/best_open_source_document_parser/" /><updated>2024-06-17T23:51:23+00:00</updated><published>2024-06-17T23:51:23+00:00</published><title>Best open source document PARSER??!!</title></entry><entry><author><name>/u/_b_2_v_</name><uri>https://www.reddit.com/user/_b_2_v_</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Am curious how you articulate an actionable response for end-user&amp;#39;s intent, e.g. delete account option that actually deletes account and not a link to the how-to guide. How would you even go about abstracting it without some sort GUI on the frontend? Do you just implement your own or are there solutions out there? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/_b_2_v_&quot;&gt; /u/_b_2_v_ &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1disrgq/how_do_you_return_actionable_responses/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1disrgq/how_do_you_return_actionable_responses/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1disrgq</id><link href="https://www.reddit.com/r/LangChain/comments/1disrgq/how_do_you_return_actionable_responses/" /><updated>2024-06-18T14:59:47+00:00</updated><published>2024-06-18T14:59:47+00:00</published><title>How do you return actionable responses?</title></entry><entry><author><name>/u/ravediamond000</name><uri>https://www.reddit.com/user/ravediamond000</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hello everyone,&lt;/p&gt; &lt;p&gt;Here&amp;#39;s a nice article showing how to add LLM monitoring to your Langchain application that uses LCEL. I found out first hand how painful it can be to not have anything to monitor your token consumption so I hope it helps you avoid this.&lt;br/&gt; Here&amp;#39;s the link: &lt;a href=&quot;https://www.metadocs.co/2024/06/18/add-monitoring-easily-to-your-langchain-chains-with-langfuse/&quot;&gt;article&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Have a nice read :D&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/ravediamond000&quot;&gt; /u/ravediamond000 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1din8vv/llm_monitoring_with_langfuse_in_lcel_langchain/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1din8vv/llm_monitoring_with_langfuse_in_lcel_langchain/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1din8vv</id><link href="https://www.reddit.com/r/LangChain/comments/1din8vv/llm_monitoring_with_langfuse_in_lcel_langchain/" /><updated>2024-06-18T10:19:53+00:00</updated><published>2024-06-18T10:19:53+00:00</published><title>LLM monitoring with Langfuse in LCEL Langchain application</title></entry><entry><author><name>/u/i_am_innovative</name><uri>https://www.reddit.com/user/i_am_innovative</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/i_am_innovative&quot;&gt; /u/i_am_innovative &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dimixi/can_someone_explain_me_what_is_the_difference/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dimixi/can_someone_explain_me_what_is_the_difference/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dimixi</id><link href="https://www.reddit.com/r/LangChain/comments/1dimixi/can_someone_explain_me_what_is_the_difference/" /><updated>2024-06-18T09:31:32+00:00</updated><published>2024-06-18T09:31:32+00:00</published><title>Can someone explain me what is the difference between nvidia nemo and nvidia nim framework?</title></entry><entry><author><name>/u/liljuden</name><uri>https://www.reddit.com/user/liljuden</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi there,&lt;/p&gt; &lt;p&gt;I am currently thinking about making a chatbot which has a vector database containing knowledge from a website. As many others I have witnessed that the embedding search rarely performe well in more complex cases where further contextual understanding is needed. Therefore I want to add a KnowledgeGraph to the RAG. My knowledge is centered around tech products such as TV and Internet &lt;/p&gt; &lt;p&gt;I have played around with using an LLM to define entities and relationships between these. In this process I have these three questions:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Does the graph need multiple types of entities and multiple types of relations? Or could it for example be as simple as Entities (Products) and relationships (related products)?&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Is it best to use the LLM on the entire documents or could there be and advantage of using it on a chunk level? &lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;When I use the LLM on the sources over several runs (due to token limits) how do I ensure that the LLM uses the same entity names across the sources, instead of making slight variation in the name for the same entity? Could an accumulated list of possible entities created during the runs work as an input in the prompt? &lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Next, I need to now how to use the outcome of the KnowledgeGraph. Currently I am thinking about assigning the entities and their relationships as meta data, so lets say my vector search find a document/chunk the meta data should contain an ID of other relevant document/chunks (as they include the same entities). Is that a good approach?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/liljuden&quot;&gt; /u/liljuden &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dikb6p/building_a_knowledgegraph_and_combining_with/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dikb6p/building_a_knowledgegraph_and_combining_with/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dikb6p</id><link href="https://www.reddit.com/r/LangChain/comments/1dikb6p/building_a_knowledgegraph_and_combining_with/" /><updated>2024-06-18T06:51:44+00:00</updated><published>2024-06-18T06:51:44+00:00</published><title>Building a KnowledgeGraph and combining with Vector search</title></entry><entry><author><name>/u/sarthakai</name><uri>https://www.reddit.com/user/sarthakai</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Information on entities like people, institutions, etc. is often highly interconnected, and this might be the case for your data too.&lt;/p&gt; &lt;p&gt;If so, you can:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;Create a graph connecting documents which have common n-grams, using TF-IDF etc.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;During inference, search this graph to get neighbours containing common n-grams and use them in the LLM’s context.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Search results from Graph RAG are more likely to give you a comprehensive view of the entity being searched and the info connected to it.&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Eg, , if doc A is selected as highly relevant, the docs containing data closely linked to doc A must be included in the context to give a full picture.&lt;/p&gt; &lt;p&gt;I spent the weekend creating a Python library which automatically creates this graph for the documents in your vectordb. It also makes it easy for you to retrieve relevant documents connected to the best matches.&lt;/p&gt; &lt;p&gt;Here’s the repo for the library: &lt;a href=&quot;https://github.com/sarthakrastogi/graph-rag/tree/main&quot;&gt;https://github.com/sarthakrastogi/graph-rag/tree/main&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/sarthakai&quot;&gt; /u/sarthakai &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1di09bu/heres_how_to_use_graph_rag_to_get_better_accuracy/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1di09bu/heres_how_to_use_graph_rag_to_get_better_accuracy/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1di09bu</id><link href="https://www.reddit.com/r/LangChain/comments/1di09bu/heres_how_to_use_graph_rag_to_get_better_accuracy/" /><updated>2024-06-17T15:03:40+00:00</updated><published>2024-06-17T15:03:40+00:00</published><title>Here’s how to use Graph RAG to get better accuracy than std RAG</title></entry><entry><author><name>/u/Strange-Ant-4194</name><uri>https://www.reddit.com/user/Strange-Ant-4194</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Are there any models of similar size that are better than llama3 8b and that can be run on ollama?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Strange-Ant-4194&quot;&gt; /u/Strange-Ant-4194 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dimec1/better_models_than_llama3_8b/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dimec1/better_models_than_llama3_8b/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dimec1</id><link href="https://www.reddit.com/r/LangChain/comments/1dimec1/better_models_than_llama3_8b/" /><updated>2024-06-18T09:22:21+00:00</updated><published>2024-06-18T09:22:21+00:00</published><title>Better models than llama3 8b?</title></entry><entry><author><name>/u/JAYBORICHA07</name><uri>https://www.reddit.com/user/JAYBORICHA07</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I am creating custom gpt agents and i want output in fixed schema how can I do that ? &lt;/p&gt; &lt;p&gt;```const responseSchema = z.object({&lt;br/&gt; problem: z&lt;br/&gt; .string()&lt;br/&gt; .describe(&lt;br/&gt; &amp;quot;Here you will get the problem according to the questions provided in the prompt&amp;quot;&lt;br/&gt; ),&lt;br/&gt; problemScore: z&lt;br/&gt; .number()&lt;br/&gt; .describe(&lt;br/&gt; &amp;quot;Here you will get the score of the problem according to the questions provided in the prompt&amp;quot;&lt;br/&gt; ),&lt;br/&gt; solution: z&lt;br/&gt; .string()&lt;br/&gt; .describe(&lt;br/&gt; &amp;quot;Here you will get the solution according to the questions provided in the prompt&amp;quot;&lt;br/&gt; ),&lt;br/&gt; solutionScore: z&lt;br/&gt; .number()&lt;br/&gt; .describe(&lt;br/&gt; &amp;quot;Here you will get the score of the solution according to the questions provided in the prompt&amp;quot;&lt;br/&gt; ),&lt;br/&gt; targetAudience: z&lt;br/&gt; .string()&lt;br/&gt; .describe(&lt;br/&gt; &amp;quot;Here you will get the target audience according to the questions provided in the prompt&amp;quot;&lt;br/&gt; ),&lt;br/&gt; targetAudienceScore: z&lt;br/&gt; .number()&lt;br/&gt; .describe(&lt;br/&gt; &amp;quot;Here you will get the score of the target audience according to the questions provided in the prompt&amp;quot;&lt;br/&gt; ),&lt;br/&gt; objections: z&lt;br/&gt; .string()&lt;br/&gt; .describe(&lt;br/&gt; &amp;quot;Here you will get the objections according to the questions provided in the prompt&amp;quot;&lt;br/&gt; ),&lt;br/&gt; objectionsScore: z&lt;br/&gt; .number()&lt;br/&gt; .describe(&lt;br/&gt; &amp;quot;Here you will get the score of the objections according to the questions provided in the prompt&amp;quot;&lt;br/&gt; ),&lt;br/&gt; riskReversal: z&lt;br/&gt; .string()&lt;br/&gt; .describe(&lt;br/&gt; &amp;quot;Here you will get the risk reversal according to the questions provided in the prompt&amp;quot;&lt;br/&gt; ),&lt;br/&gt; riskReversalScore: z&lt;br/&gt; .number()&lt;br/&gt; .describe(&lt;br/&gt; &amp;quot;Here you will get the score of the risk reversal according to the questions provided in the prompt&amp;quot;&lt;br/&gt; ),&lt;br/&gt; uniqueness: z&lt;br/&gt; .string()&lt;br/&gt; .describe(&lt;br/&gt; &amp;quot;Here you will get the uniqueness according to the questions provided in the prompt&amp;quot;&lt;br/&gt; ),&lt;br/&gt; uniquenessScore: z&lt;br/&gt; .number()&lt;br/&gt; .describe(&lt;br/&gt; &amp;quot;Here you will get the score of the uniqueness according to the questions provided in the prompt&amp;quot;&lt;br/&gt; ),&lt;br/&gt; });&lt;/p&gt; &lt;p&gt;const agent = new OpenAIAssistantRunnable({&lt;br/&gt; clientOptions: {&lt;br/&gt; apiKey: process.env.OPENAIGPTKEY,&lt;br/&gt; },&lt;br/&gt; assistantId: &amp;quot;YOUR_GPT_ASSISTENE_ID&amp;quot;,&lt;br/&gt; asAgent: true,&lt;br/&gt; });&lt;/p&gt; &lt;p&gt;await agent&lt;br/&gt; .invoke({&lt;br/&gt; content: input.url,&lt;br/&gt; })&lt;br/&gt; .then((response) =&amp;gt; {&lt;br/&gt; console.info(response);&lt;br/&gt; });&lt;br/&gt; ```&lt;/p&gt; &lt;p&gt;I have defined the zod schema and I also tried to attach it with making a function, but I cannot bind it with the agent.&lt;/p&gt; &lt;p&gt;any help would be grateful&lt;/p&gt; &lt;p&gt;thank you.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/JAYBORICHA07&quot;&gt; /u/JAYBORICHA07 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dimbqa/how_can_i_get_custom_agentgenerated_output_in_the/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dimbqa/how_can_i_get_custom_agentgenerated_output_in_the/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dimbqa</id><link href="https://www.reddit.com/r/LangChain/comments/1dimbqa/how_can_i_get_custom_agentgenerated_output_in_the/" /><updated>2024-06-18T09:16:56+00:00</updated><published>2024-06-18T09:16:56+00:00</published><title>How can I get custom agent-generated output in the structured format?</title></entry><entry><author><name>/u/Mediocre-Card8046</name><uri>https://www.reddit.com/user/Mediocre-Card8046</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;I have a LCEL Chain but now want to adapt it. The idea is to use a RAG chain if the retriever finds something, but if the retriever does not find something, I want to use a different prompt. How can I do this?&lt;/p&gt; &lt;p&gt;This is my chain at the moment:&lt;/p&gt; &lt;pre&gt;&lt;code&gt; context = itemgetter(&amp;quot;question&amp;quot;) | retriever first_step = RunnablePassthrough.assign(context=context) chain = ( first_step | format_docs | chat_history_prompt | llm | StrOutputParser() ) &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Mediocre-Card8046&quot;&gt; /u/Mediocre-Card8046 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dikwmc/conditions_in_lcel_chain_different_chain_if/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dikwmc/conditions_in_lcel_chain_different_chain_if/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dikwmc</id><link href="https://www.reddit.com/r/LangChain/comments/1dikwmc/conditions_in_lcel_chain_different_chain_if/" /><updated>2024-06-18T07:33:32+00:00</updated><published>2024-06-18T07:33:32+00:00</published><title>Conditions in LCEL Chain: Different Chain if retriever does not find something</title></entry><entry><author><name>/u/HornetQuick5526</name><uri>https://www.reddit.com/user/HornetQuick5526</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I am trying to build a chatbot using Express. It is a simple setup with an LLM and tools. I followed the official documentation to define a shouldContinue function and added a condition edge from the agent node to the action node or END. However, when I run the graph through streamEvent, I can see in the shouldContinue function that the format of the tool_calls in the final AIMessage is not as expected.&lt;/p&gt; &lt;p&gt;The code is as follows:&lt;/p&gt; &lt;p&gt;```typescript&lt;/p&gt; &lt;p&gt;import type { StreamEvent } from &amp;#39;@langchain/core/tracers/log_stream&amp;#39;;&lt;/p&gt; &lt;p&gt;import type { BaseChatModel } from &amp;#39;@langchain/core/language_models/chat_models&amp;#39;;&lt;/p&gt; &lt;p&gt;import type { Embeddings } from &amp;#39;@langchain/core/embeddings&amp;#39;;&lt;/p&gt; &lt;p&gt;import type { AIMessageChunk } from &amp;#39;@langchain/core/messages&amp;#39;;&lt;/p&gt; &lt;p&gt;import type { ChatGenerationChunk } from &amp;quot;@langchain/core/outputs&amp;quot;;&lt;/p&gt; &lt;p&gt;import EventEmitter from &amp;#39;events&amp;#39;;&lt;/p&gt; &lt;p&gt;import logger from &amp;#39;../utils/logger&amp;#39;;&lt;/p&gt; &lt;p&gt;import feishuTools from &amp;#39;../toolTikets/feishu&amp;#39;;&lt;/p&gt; &lt;p&gt;import getImageUrls from &amp;#39;../utils/getImageUrls&amp;#39;;&lt;/p&gt; &lt;p&gt;import { AIMessage, HumanMessage, BaseMessage } from &amp;quot;@langchain/core/messages&amp;quot;;&lt;/p&gt; &lt;p&gt;import { START, END, MessageGraph } from &amp;quot;@langchain/langgraph&amp;quot;;&lt;/p&gt; &lt;p&gt;import { ToolNode } from &amp;#39;@langchain/langgraph/prebuilt&amp;#39;;&lt;/p&gt; &lt;p&gt;function shouldContinue(messages: BaseMessage[]): &amp;quot;action&amp;quot; | typeof END {&lt;/p&gt; &lt;p&gt;const lastMessage = messages[messages.length - 1] as AIMessage;&lt;/p&gt; &lt;p&gt;if (!lastMessage?.tool_calls?.length) {&lt;/p&gt; &lt;p&gt;console.log(`in shouldContinue: end`);&lt;/p&gt; &lt;p&gt;return END;&lt;/p&gt; &lt;p&gt;} else {&lt;/p&gt; &lt;p&gt;console.log(`in shouldContinue tool calls: ${JSON.stringify(lastMessage.tool_calls)}`);&lt;/p&gt; &lt;p&gt;return &amp;quot;action&amp;quot;;&lt;/p&gt; &lt;p&gt;}&lt;/p&gt; &lt;p&gt;}&lt;/p&gt; &lt;p&gt;function createGraph(llm: BaseChatModel) {&lt;/p&gt; &lt;p&gt;const model = llm.bindTools(feishuTools);&lt;/p&gt; &lt;p&gt;console.log(`in createGraph:`);&lt;/p&gt; &lt;p&gt;const workflow = new MessageGraph()&lt;/p&gt; &lt;p&gt;.addNode(&amp;quot;agent&amp;quot;, model)&lt;/p&gt; &lt;p&gt;.addNode(&amp;quot;action&amp;quot;, new ToolNode&amp;lt;BaseMessage\[\]&amp;gt;(feishuTools))&lt;/p&gt; &lt;p&gt;.addEdge(START, &amp;quot;agent&amp;quot;)&lt;/p&gt; &lt;p&gt;.addConditionalEdges(&amp;quot;agent&amp;quot;, shouldContinue)&lt;/p&gt; &lt;p&gt;.addEdge(&amp;quot;action&amp;quot;, &amp;quot;agent&amp;quot;);&lt;/p&gt; &lt;p&gt;return workflow.compile();&lt;/p&gt; &lt;p&gt;}&lt;/p&gt; &lt;p&gt;async function handleStream(&lt;/p&gt; &lt;p&gt;stream: AsyncGenerator&amp;lt;StreamEvent, any, unknown&amp;gt;,&lt;/p&gt; &lt;p&gt;emitter: EventEmitter,&lt;/p&gt; &lt;p&gt;) {&lt;/p&gt; &lt;p&gt;for await (const event of stream) {&lt;/p&gt; &lt;p&gt;console.log(`in handleStream:`);&lt;/p&gt; &lt;p&gt;if (event.event === &amp;quot;on_llm_stream&amp;quot;) {&lt;/p&gt; &lt;p&gt;const chunk: ChatGenerationChunk = &lt;a href=&quot;http://event.data?.chunk&quot;&gt;event.data?.chunk&lt;/a&gt;;&lt;/p&gt; &lt;p&gt;const msg = chunk.message as AIMessageChunk;&lt;/p&gt; &lt;p&gt;if (msg.tool_call_chunks &amp;amp;&amp;amp; msg.tool_call_chunks.length &amp;gt; 0) {&lt;/p&gt; &lt;p&gt;console.log(`msg.tool_call_chunks: ${JSON.stringify(msg.tool_call_chunks)}`);&lt;/p&gt; &lt;p&gt;} else {&lt;/p&gt; &lt;p&gt;emitter.emit(&lt;/p&gt; &lt;p&gt;&amp;#39;data&amp;#39;,&lt;/p&gt; &lt;p&gt;JSON.stringify({ type: &amp;#39;response&amp;#39;, data: msg.content }),&lt;/p&gt; &lt;p&gt;);&lt;/p&gt; &lt;p&gt;console.log(`msg.content: ${JSON.stringify(msg.content)}`);&lt;/p&gt; &lt;p&gt;}&lt;/p&gt; &lt;p&gt;}&lt;/p&gt; &lt;p&gt;}&lt;/p&gt; &lt;p&gt;emitter.emit(&amp;#39;end&amp;#39;);&lt;/p&gt; &lt;p&gt;}&lt;/p&gt; &lt;p&gt;function initializeGraph(&lt;/p&gt; &lt;p&gt;messages: BaseMessage[],&lt;/p&gt; &lt;p&gt;llm: BaseChatModel,&lt;/p&gt; &lt;p&gt;embeddings: Embeddings,&lt;/p&gt; &lt;p&gt;): EventEmitter {&lt;/p&gt; &lt;p&gt;const emitter = new EventEmitter();&lt;/p&gt; &lt;p&gt;try {&lt;/p&gt; &lt;p&gt;const basicGraph = createGraph(llm);&lt;/p&gt; &lt;p&gt;const stream = basicGraph.streamEvents(&lt;/p&gt; &lt;p&gt;messages,&lt;/p&gt; &lt;p&gt;{&lt;/p&gt; &lt;p&gt;streamMode: &amp;quot;values&amp;quot;,&lt;/p&gt; &lt;p&gt;version: &amp;quot;v1&amp;quot;,&lt;/p&gt; &lt;p&gt;}&lt;/p&gt; &lt;p&gt;);&lt;/p&gt; &lt;p&gt;handleStream(stream, emitter);&lt;/p&gt; &lt;p&gt;} catch (err) {&lt;/p&gt; &lt;p&gt;emitter.emit(&lt;/p&gt; &lt;p&gt;&amp;#39;error&amp;#39;,&lt;/p&gt; &lt;p&gt;JSON.stringify({ data: &amp;#39;An error has occurred please try again later&amp;#39; }),&lt;/p&gt; &lt;p&gt;);&lt;/p&gt; &lt;p&gt;logger.error(`Error in websearch: ${err}`);&lt;/p&gt; &lt;p&gt;}&lt;/p&gt; &lt;p&gt;return emitter;&lt;/p&gt; &lt;p&gt;}&lt;/p&gt; &lt;p&gt;function handleChat(&lt;/p&gt; &lt;p&gt;message: string,&lt;/p&gt; &lt;p&gt;history: BaseMessage[],&lt;/p&gt; &lt;p&gt;llm: BaseChatModel,&lt;/p&gt; &lt;p&gt;embeddings: Embeddings,&lt;/p&gt; &lt;p&gt;files: { id: string; name: string }[],&lt;/p&gt; &lt;p&gt;): EventEmitter {&lt;/p&gt; &lt;p&gt;const messages = history.concat([&lt;/p&gt; &lt;p&gt;new HumanMessage({&lt;/p&gt; &lt;p&gt;content: [&lt;/p&gt; &lt;p&gt;{ type: &amp;quot;text&amp;quot;, text: message },&lt;/p&gt; &lt;p&gt;...(files.length &amp;gt; 0 ? getImageUrls(files) : []),&lt;/p&gt; &lt;p&gt;],&lt;/p&gt; &lt;p&gt;})&lt;/p&gt; &lt;p&gt;]);&lt;/p&gt; &lt;p&gt;return initializeGraph(messages, llm, embeddings);&lt;/p&gt; &lt;p&gt;}&lt;/p&gt; &lt;p&gt;export default handleChat;&lt;/p&gt; &lt;p&gt;```&lt;/p&gt; &lt;p&gt;The console output is as follows:&lt;br/&gt; ```&lt;br/&gt; in createGraph:&lt;/p&gt; &lt;p&gt;in handleStream:&lt;/p&gt; &lt;p&gt;in handleStream:&lt;/p&gt; &lt;p&gt;in handleStream:&lt;/p&gt; &lt;p&gt;in handleStream:&lt;/p&gt; &lt;p&gt;in handleStream:&lt;/p&gt; &lt;p&gt;in handleStream:&lt;/p&gt; &lt;p&gt;in handleStream:&lt;/p&gt; &lt;p&gt;msg.tool_call_chunks: [{&amp;quot;name&amp;quot;:&amp;quot;getWeather&amp;quot;,&amp;quot;args&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;id&amp;quot;:&amp;quot;call_4qu6Nl2rosohAaHxAigzG9Fd&amp;quot;}]&lt;/p&gt; &lt;p&gt;in handleStream:&lt;/p&gt; &lt;p&gt;msg.tool_call_chunks: [{&amp;quot;args&amp;quot;:&amp;quot;{\&amp;quot;&amp;quot;}]&lt;/p&gt; &lt;p&gt;in handleStream:&lt;/p&gt; &lt;p&gt;msg.tool_call_chunks: [{&amp;quot;args&amp;quot;:&amp;quot;location&amp;quot;}]&lt;/p&gt; &lt;p&gt;in handleStream:&lt;/p&gt; &lt;p&gt;msg.tool_call_chunks: [{&amp;quot;args&amp;quot;:&amp;quot;\&amp;quot;:\&amp;quot;&amp;quot;}]&lt;/p&gt; &lt;p&gt;in handleStream:&lt;/p&gt; &lt;p&gt;msg.tool_call_chunks: [{&amp;quot;args&amp;quot;:&amp;quot;Seattle&amp;quot;}]&lt;/p&gt; &lt;p&gt;in handleStream:&lt;/p&gt; &lt;p&gt;msg.tool_call_chunks: [{&amp;quot;args&amp;quot;:&amp;quot;\&amp;quot;}&amp;quot;}]&lt;/p&gt; &lt;p&gt;in handleStream:&lt;/p&gt; &lt;p&gt;msg.content: &amp;quot;&amp;quot;&lt;/p&gt; &lt;p&gt;in handleStream:&lt;/p&gt; &lt;p&gt;in handleStream:&lt;/p&gt; &lt;p&gt;in handleStream:&lt;/p&gt; &lt;p&gt;in handleStream:&lt;/p&gt; &lt;p&gt;in shouldContinue tool calls: [{&amp;quot;name&amp;quot;:&amp;quot;getWeather&amp;quot;,&amp;quot;args&amp;quot;:{},&amp;quot;id&amp;quot;:&amp;quot;call_4qu6Nl2rosohAaHxAigzG9Fd&amp;quot;},{&amp;quot;name&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;args&amp;quot;:{}},{&amp;quot;name&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;args&amp;quot;:{}},{&amp;quot;name&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;args&amp;quot;:{}}]&lt;/p&gt; &lt;p&gt;in handleStream:&lt;/p&gt; &lt;p&gt;in handleStream:&lt;/p&gt; &lt;p&gt;in handleStream:&lt;/p&gt; &lt;p&gt;in handleStream:&lt;/p&gt; &lt;p&gt;in handleStream:&lt;/p&gt; &lt;p&gt;in handleStream:&lt;/p&gt; &lt;p&gt;in handleStream:&lt;/p&gt; &lt;p&gt;in handleStream:&lt;/p&gt; &lt;p&gt;in handleStream:&lt;/p&gt; &lt;p&gt;/Users/mac/WorkSpace/project/Perplexica-backend/node_modules/@langchain/langgraph/dist/prebuilt/tool_node.cjs:31&lt;/p&gt; &lt;p&gt;const outputs = await Promise.all(message.tool_calls?.map(async (call) =&amp;gt; {&lt;/p&gt; &lt;p&gt;^&lt;/p&gt; &lt;p&gt;Error: Tool not found.&lt;/p&gt; &lt;p&gt;at /Users/mac/WorkSpace/project/Perplexica-backend/node_modules/@langchain/langgraph/dist/prebuilt/tool_node.cjs:34:23&lt;/p&gt; &lt;p&gt;at &lt;a href=&quot;http://Array.map&quot;&gt;Array.map&lt;/a&gt; (&amp;lt;anonymous&amp;gt;)&lt;/p&gt; &lt;p&gt;at &lt;a href=&quot;http://ToolNode.run&quot;&gt;ToolNode.run&lt;/a&gt; (/Users/mac/WorkSpace/project/Perplexica-backend/node_modules/@langchain/langgraph/dist/prebuilt/tool_node.cjs:31:63)&lt;/p&gt; &lt;p&gt;at ToolNode.func (/Users/mac/WorkSpace/project/Perplexica-backend/node_modules/@langchain/langgraph/dist/prebuilt/tool_node.cjs:9:59)&lt;/p&gt; &lt;p&gt;at ToolNode._callWithConfig (/Users/mac/WorkSpace/project/Perplexica-backend/node_modules/@langchain/core/dist/runnables/base.cjs:217:33)&lt;/p&gt; &lt;p&gt;at processTicksAndRejections (node:internal/process/task_queues:95:5)&lt;/p&gt; &lt;p&gt;at async ToolNode.invoke (/Users/mac/WorkSpace/project/Perplexica-backend/node_modules/@langchain/langgraph/dist/utils.cjs:62:27)&lt;/p&gt; &lt;p&gt;at async RunnableSequence.invoke (/Users/mac/WorkSpace/project/Perplexica-backend/node_modules/@langchain/core/dist/runnables/base.cjs:1131:33)&lt;/p&gt; &lt;p&gt;at async Promise.allSettled (index 0)&lt;/p&gt; &lt;p&gt;at async executeTasks (/Users/mac/WorkSpace/project/Perplexica-backend/node_modules/@langchain/langgraph/dist/pregel/index.cjs:546:21)&lt;br/&gt; ```&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/HornetQuick5526&quot;&gt; /u/HornetQuick5526 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1diiqwe/the_tools_cannot_be_invoked_when_starting/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1diiqwe/the_tools_cannot_be_invoked_when_starting/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1diiqwe</id><link href="https://www.reddit.com/r/LangChain/comments/1diiqwe/the_tools_cannot_be_invoked_when_starting/" /><updated>2024-06-18T05:07:36+00:00</updated><published>2024-06-18T05:07:36+00:00</published><title>The tools cannot be invoked when starting langgraphjs through streamEvent.</title></entry><entry><author><name>/u/Ukpersfidev</name><uri>https://www.reddit.com/user/Ukpersfidev</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Let&amp;#39;s say I&amp;#39;m building an app where someone can upload up to 6 documents describing their company to generate a sales pitch, where the prompt is something like &amp;quot;Take these documents and create a 2-minute sales pitch targetted towards my target market&amp;quot;&lt;/p&gt; &lt;p&gt;Does it make sense to use RAG here? My concern is that the prompt needs the full context of the document, and we wouldn&amp;#39;t be able to retrieve the information we need.&lt;/p&gt; &lt;p&gt;I&amp;#39;m considering an alternative method of using AI to summarise each document and then just embed the entire summary into the prompt&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Ukpersfidev&quot;&gt; /u/Ukpersfidev &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1difzaj/should_i_use_rag_in_this_scenario/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1difzaj/should_i_use_rag_in_this_scenario/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1difzaj</id><link href="https://www.reddit.com/r/LangChain/comments/1difzaj/should_i_use_rag_in_this_scenario/" /><updated>2024-06-18T02:32:13+00:00</updated><published>2024-06-18T02:32:13+00:00</published><title>Should I use RAG in this scenario?</title></entry><entry><author><name>/u/maniac_runner</name><uri>https://www.reddit.com/user/maniac_runner</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1di19ey/comparing_approaches_for_using_llms_for/&quot;&gt; &lt;img src=&quot;https://external-preview.redd.it/aF6AJsUDANteUHYF9N5cw_xPHzLYcy-pY8wM5iGPATI.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d3309587d649ed30160c31f656e0d9d8e74573b0&quot; alt=&quot;Comparing approaches for using LLMs for Structured Data Extraction from PDFs &quot; title=&quot;Comparing approaches for using LLMs for Structured Data Extraction from PDFs &quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/maniac_runner&quot;&gt; /u/maniac_runner &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://unstract.com/blog/comparing-approaches-for-using-llms-for-structured-data-extraction-from-pdfs/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1di19ey/comparing_approaches_for_using_llms_for/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1di19ey</id><media:thumbnail url="https://external-preview.redd.it/aF6AJsUDANteUHYF9N5cw_xPHzLYcy-pY8wM5iGPATI.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=d3309587d649ed30160c31f656e0d9d8e74573b0" /><link href="https://www.reddit.com/r/LangChain/comments/1di19ey/comparing_approaches_for_using_llms_for/" /><updated>2024-06-17T15:45:21+00:00</updated><published>2024-06-17T15:45:21+00:00</published><title>Comparing approaches for using LLMs for Structured Data Extraction from PDFs</title></entry><entry><author><name>/u/New-Contribution6302</name><uri>https://www.reddit.com/user/New-Contribution6302</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I am working on a semi-structured document. I want help finding the best way to perform RAG using open-source only. Please guide me on the same. Have worked in RAG, but this is tedious when comes to optimization and accuracy. Thanks in advance:)..&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/New-Contribution6302&quot;&gt; /u/New-Contribution6302 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1die1tn/help_required_on_rag/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1die1tn/help_required_on_rag/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1die1tn</id><link href="https://www.reddit.com/r/LangChain/comments/1die1tn/help_required_on_rag/" /><updated>2024-06-18T00:54:22+00:00</updated><published>2024-06-18T00:54:22+00:00</published><title>Help required on RAG</title></entry><entry><author><name>/u/gabbom_XCII</name><uri>https://www.reddit.com/user/gabbom_XCII</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;So, I’m using a Neo4jVector.as_retriever() as a retriever for a chain.&lt;/p&gt; &lt;p&gt;The retriever outputs and array of Document type objects that gets used as a {context} in my prompt template.&lt;/p&gt; &lt;p&gt;What’s the appropriate way to format this output into a sequence of string objects to be used in my prompt_template?&lt;/p&gt; &lt;p&gt;I’ve been looking into Runnables and just found out about RunnableLambda and been thinking into turning a function into a Runnable just to format this retriever output, is this really the way to go here? am I missing something?&lt;/p&gt; &lt;p&gt;Thanks in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/gabbom_XCII&quot;&gt; /u/gabbom_XCII &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1diabgh/formatting_retriever_outputs/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1diabgh/formatting_retriever_outputs/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1diabgh</id><link href="https://www.reddit.com/r/LangChain/comments/1diabgh/formatting_retriever_outputs/" /><updated>2024-06-17T22:01:27+00:00</updated><published>2024-06-17T22:01:27+00:00</published><title>Formatting Retriever Outputs</title></entry><entry><author><name>/u/bubble_h13</name><uri>https://www.reddit.com/user/bubble_h13</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I using llm locally when i use &lt;code&gt;create_openai_tools_agent&lt;/code&gt; &lt;/p&gt; &lt;p&gt;, then i will get &lt;code&gt;TypeError: __call__() got an unexpected keyword argument &amp;#39;tools&amp;#39;&lt;/code&gt;&lt;/p&gt; &lt;p&gt;how do i fix it, my code is below:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;from langchain.tools.retriever import create_retriever_tool retriever_tool = create_retriever_tool( retriever = retriever, name = &amp;quot;retriever_tool&amp;quot;, description = &amp;quot;Retrieve the most relevant document from the database&amp;quot;, ) tools = [retriever_tool] from langchain import hub prompt = hub.pull(&amp;quot;hwchase17/openai-tools-agent&amp;quot;) from langchain.agents import AgentExecutor, create_openai_tools_agent agent = create_openai_tools_agent(llm, tools, prompt) agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True) agent_executor.invoke({&amp;quot;input&amp;quot;:&amp;quot;What can i do in this system?&amp;quot;}) &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/bubble_h13&quot;&gt; /u/bubble_h13 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1di3cfa/using_rag_agent_got_error_with_tools_parameter/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1di3cfa/using_rag_agent_got_error_with_tools_parameter/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1di3cfa</id><link href="https://www.reddit.com/r/LangChain/comments/1di3cfa/using_rag_agent_got_error_with_tools_parameter/" /><updated>2024-06-17T17:11:05+00:00</updated><published>2024-06-17T17:11:05+00:00</published><title>Using RAG Agent got error with tools parameter</title></entry><entry><author><name>/u/Hemidt</name><uri>https://www.reddit.com/user/Hemidt</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I know this might be a really stupid question, but i thought I could gain some insight, by asking it here.&lt;br/&gt; I am currently working on a simple RAG application, where I load in documents from a larger local library.&lt;br/&gt; This library consists of multiple different document types, although mostly .docx and .pdf files.&lt;br/&gt; When loading the PDF files I am currently using the PyMuPDFLoader. However it chunks the pdf into pages from the start.&lt;br/&gt; My questoin is, whether there is a way to circumvent loading a PDF file, and already chunking it into a document for each individual page. Some information is lost between the documents, as it might traverse at a page break. &lt;/p&gt; &lt;p&gt;I would like to implement the chunking strategy later on, as we are looking into implementing a costum chunking strategy. &lt;/p&gt; &lt;p&gt;Is there a smart strategy for this? &lt;/p&gt; &lt;p&gt;Thanks in advance&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Hemidt&quot;&gt; /u/Hemidt &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dhv3a5/how_to_circumvent_one_document_per_page_when/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dhv3a5/how_to_circumvent_one_document_per_page_when/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dhv3a5</id><link href="https://www.reddit.com/r/LangChain/comments/1dhv3a5/how_to_circumvent_one_document_per_page_when/" /><updated>2024-06-17T10:44:45+00:00</updated><published>2024-06-17T10:44:45+00:00</published><title>How to circumvent one document per page when loading PDF files?</title></entry><entry><author><name>/u/copp</name><uri>https://www.reddit.com/user/copp</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I am trying to use the helper method create_sql_agent to create an agent and add it in a simple graph to create a database team. I tried to see if &lt;a href=&quot;https://python.langchain.com/v0.2/docs/integrations/toolkits/sql_database/&quot;&gt;SQL tutorial&lt;/a&gt; can be repurposed.&lt;/p&gt; &lt;p&gt;The graph structure that I wanted to design is very simple. A supervisor that delegates to a SQL Team.&lt;/p&gt; &lt;p&gt;I tried following &lt;a href=&quot;https://python.langchain.com/v0.2/docs/tutorials/sql_qa/#agents&quot;&gt;https://python.langchain.com/v0.2/docs/tutorials/sql_qa/#agents&lt;/a&gt; as suggested by &lt;a href=&quot;/u/hwchase17&quot;&gt;u/hwchase17&lt;/a&gt; for one of the previous &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d4lwt0/am_i_the_only_one_who_feels_langgraph/&quot;&gt;question&lt;/a&gt;. However, it uses the AgentExecutor instead of using langgraph as Agent Executor.&lt;/p&gt; &lt;p&gt;There are examples of connecting to SQL database in the &lt;a href=&quot;https://langchain-ai.github.io/langgraph/tutorials/customer-support/customer-support/&quot;&gt;customer support chat bot tutorial&lt;/a&gt;. How we are not able to leverage the advantages that we get by using an SQL Agent namely:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Recovering from errors and regenerating query correctly.&lt;/li&gt; &lt;li&gt;Querying database multiple times.&lt;/li&gt; &lt;li&gt;Save Tokens by looking into schema etc.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;If there are any notebooks or code that you could point to that would be great.&lt;/p&gt; &lt;p&gt;I started learning LangGraph for the promise of composing and using agents. If this is not doable with critical agents like SQL Database, then that defeats the purpose. Also that AgentExecutor is deprecated, it would be great if some of these tutorials are updated to use LangGraph as AgentExecutor / ChatExecutor.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/copp&quot;&gt; /u/copp &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dhy51m/how_to_add_an_agent_in_a_langgraph_as_a_node/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dhy51m/how_to_add_an_agent_in_a_langgraph_as_a_node/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dhy51m</id><link href="https://www.reddit.com/r/LangChain/comments/1dhy51m/how_to_add_an_agent_in_a_langgraph_as_a_node/" /><updated>2024-06-17T13:31:16+00:00</updated><published>2024-06-17T13:31:16+00:00</published><title>How to add an Agent in a LangGraph as a node? Specifically agent created out of create_sql_agent</title></entry><entry><author><name>/u/mmedinajp</name><uri>https://www.reddit.com/user/mmedinajp</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Using Langchain v0.2, model is limited to gpt-3.5-turbo with Azure (environment provided by the client)&lt;/p&gt; &lt;p&gt;Hello!&lt;/p&gt; &lt;p&gt;Looking for some directions on where to read or how to proceed to solve the following situation:&lt;/p&gt; &lt;p&gt;I have very big texts that I want to summarize. In the past I had read about MapReduce with Langchain so I tried with it. I am creating chunks of texts and processing them. It works, but when something in a chunk triggers the OpenAI content filter, the summarization for the whole chunk fails. I was wondering if there is a way to keep processing the chunk in order to get a result for the parts of the chunk that didn&amp;#39;t have issues with the content filter. Otherwise, I&amp;#39;d have to implement the MapReduce myself.&lt;/p&gt; &lt;p&gt;I&amp;#39;d appreciate if you could point me to some documents, or if you could suggest how I may approach this problem.&lt;/p&gt; &lt;p&gt;Thank you very much in advance.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/mmedinajp&quot;&gt; /u/mmedinajp &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dhl76g/openai_content_filter_when_using_mapreduce/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dhl76g/openai_content_filter_when_using_mapreduce/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dhl76g</id><link href="https://www.reddit.com/r/LangChain/comments/1dhl76g/openai_content_filter_when_using_mapreduce/" /><updated>2024-06-17T00:14:11+00:00</updated><published>2024-06-17T00:14:11+00:00</published><title>OpenAI Content Filter when using MapReduce</title></entry><entry><author><name>/u/sarthakai</name><uri>https://www.reddit.com/user/sarthakai</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Most RAG apps use Dense Passage Retrieval to find relevant docs. But there are better methods:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;RAG-Token:&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;It generates each token by considering different docs and chooses the most probable token at each step. So that every part of the answer is influenced by the best possible context.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;RAG-Sequence:&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;It calculates the probability of each answer and selects the one with the highest combined probability, getting you the best possible answer based on multiple sources. It’s a lot like RAG-token but less granular.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Fusion-in-Decoder (FiD):&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;It encodes all pairs of questions and chunks in parallel and then combines these encodings before feeding them into the decoder, which generates the answer step-by-step.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Graph RAG:&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;In case your documents are highly interconnected, the links between them are probably important to generate a relevant response.&lt;/p&gt; &lt;p&gt;Search results from Graph RAG are more likely to give you a comprehensive view of the entity being searched and the info connected to it.&lt;/p&gt; &lt;p&gt;I spent the weekend creating a Python library which automatically creates this graph for the documents present in your vectordb. It also makes it easy for you to retrieve relevant documents connected to the best matches.&lt;/p&gt; &lt;p&gt;Currently testing the library on medical documents to gauge its performance.&lt;/p&gt; &lt;p&gt;Sharing version 0.1 tomorrow! You can follow my social media to stay tuned: &lt;a href=&quot;https://linktr.ee/sarthakrastogi&quot;&gt;https://linktr.ee/sarthakrastogi&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/sarthakai&quot;&gt; /u/sarthakai &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dh79xx/suggesting_which_rag_method_will_work_best_for/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dh79xx/suggesting_which_rag_method_will_work_best_for/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dh79xx</id><link href="https://www.reddit.com/r/LangChain/comments/1dh79xx/suggesting_which_rag_method_will_work_best_for/" /><updated>2024-06-16T13:10:38+00:00</updated><published>2024-06-16T13:10:38+00:00</published><title>Suggesting which RAG method will work best for you, based on your use case 🔎📑</title></entry></feed>