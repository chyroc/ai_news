<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/"><category term="LangChain" label="r/LangChain"/><updated>2024-05-08T12:42:22+00:00</updated><icon>https://www.redditstatic.com/icon.png/</icon><id>/r/LangChain.rss</id><link rel="self" href="https://www.reddit.com/r/LangChain.rss" type="application/atom+xml" /><link rel="alternate" href="https://www.reddit.com/r/LangChain" type="text/html" /><subtitle>LangChain is an open-source framework and developer toolkit that helps developers get LLM applications from prototype to production. It is available for Python and Javascript at https://www.langchain.com/.</subtitle><title>LangChain</title><entry><author><name>/u/zchaarm</name><uri>https://www.reddit.com/user/zchaarm</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;A place for members of &lt;a href=&quot;/r/LangChain&quot;&gt;r/LangChain&lt;/a&gt; to chat with each other&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/zchaarm&quot;&gt; /u/zchaarm &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_10ljho9</id><link href="https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/" /><updated>2023-01-26T04:33:05+00:00</updated><published>2023-01-26T04:33:05+00:00</published><title>r/LangChain Lounge</title></entry><entry><author><name>/u/pikaLuffy</name><uri>https://www.reddit.com/user/pikaLuffy</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;To my fellow experts, I am having trouble to extract tables from PDF. I know there are some packages out there that claim to do the job, but I canâ€™t seem to get good results from it. Moreover, my work laptop kinda restrict on installation of softwares and the most I can do is download open source library package. Wondering if there are any straightforward ways on how to do that ? Or I have to a rite the code from scratch to process the tables but there seem to be many types of tables I need to consider. &lt;/p&gt; &lt;p&gt;Here are the packages I tried and the reasons why they didnâ€™t work. &lt;/p&gt; &lt;ol&gt; &lt;li&gt;Pymupdf- messy table formatting, can misinterpret title of the page as column headers&lt;/li&gt; &lt;li&gt;Tabula/pdfminer- same performance as Pymupdf &lt;/li&gt; &lt;li&gt;Camelot- I canâ€™t seem to get it to work given that it needs to download Ghostscript and tkinter, which require admin privilege which is blocked in my work laptop. &lt;/li&gt; &lt;li&gt;Unstructured- complicated setup as require a lot of dependencies and they are hard to set up &lt;/li&gt; &lt;li&gt;Llamaparse from llama: need cloud api key which is blocked &lt;/li&gt; &lt;/ol&gt; &lt;p&gt;I tried converting pdf to html but canâ€™t seem to identify the tables very well. &lt;/p&gt; &lt;p&gt;Please help a beginner ðŸ¥º&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/pikaLuffy&quot;&gt; /u/pikaLuffy &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cn0z11/extract_tables_from_pdf_for_rag/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cn0z11/extract_tables_from_pdf_for_rag/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cn0z11</id><link href="https://www.reddit.com/r/LangChain/comments/1cn0z11/extract_tables_from_pdf_for_rag/" /><updated>2024-05-08T10:10:38+00:00</updated><published>2024-05-08T10:10:38+00:00</published><title>Extract tables from PDF for RAG</title></entry><entry><author><name>/u/Flaky_Assistant8371</name><uri>https://www.reddit.com/user/Flaky_Assistant8371</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;0&lt;/p&gt; &lt;p&gt;I used the Python LangChain UnstructuredURLLoader to retrieve all our products on the company website for RAG purposes. The products were on different pages in the company website.&lt;/p&gt; &lt;p&gt;UnstructuredURLLoader was able to retrieve the products in multiple Document objects before they were chunked, embedded and stored in the vector database.&lt;/p&gt; &lt;p&gt;With the OpenAI LLM and RAG module, I asked the AI, &lt;strong&gt;&amp;quot;How many products in the company A?&amp;quot; AI replied &amp;quot;There are 11 products. You should check the company A website for more info...&amp;quot;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;If I asked &amp;quot;Please list all the products in the company A&amp;quot;, AI replied the list of the 11 products only.&lt;/p&gt; &lt;p&gt;The problem is, there are more than 11 products. Why can&amp;#39;t LLM read and aggregate the products in the Documents to count and to return all of the products?&lt;/p&gt; &lt;p&gt;Is there any context hint or prompt to tell LLM to read and return all products? Is it because of the chunking process?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Flaky_Assistant8371&quot;&gt; /u/Flaky_Assistant8371 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cmzazp/langchain_with_openai_not_return_full_products_in/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cmzazp/langchain_with_openai_not_return_full_products_in/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cmzazp</id><link href="https://www.reddit.com/r/LangChain/comments/1cmzazp/langchain_with_openai_not_return_full_products_in/" /><updated>2024-05-08T08:10:57+00:00</updated><published>2024-05-08T08:10:57+00:00</published><title>LangChain with OpenAI not return full products in RAG QnA</title></entry><entry><author><name>/u/theferalmonkey</name><uri>https://www.reddit.com/user/theferalmonkey</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cmyi9k/discussion_declaratively_orchestrate_your_code/&quot;&gt; &lt;img src=&quot;https://external-preview.redd.it/sAUbNyH098cBBqicB9rc8ofLEQDpkmgZKE1tSibcy5k.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6631c24e2bdd21f4aa327a412b8faec0969e28a2&quot; alt=&quot;Discussion: Declaratively orchestrate your code instead of using LCEL &quot; title=&quot;Discussion: Declaratively orchestrate your code instead of using LCEL &quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi All,&lt;/p&gt; &lt;p&gt;I&amp;#39;d be curious to discuss what peoples&amp;#39; thoughts would be on the following API to express their LLM workflows in place of LCEL. LangChain has the kitchen sink of things, so useful for that, but I haven&amp;#39;t been fond of LCEL...&lt;/p&gt; &lt;p&gt;&lt;strong&gt;LCEL&lt;/strong&gt; - it&amp;#39;s terse, but it pains me to come back to the code each time to figure out what it&amp;#39;s going on. Then if I want to do anything complex it gets worse. Simple example from the docs:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;from langchain_core.output_parsers import StrOutputParser from langchain_core.prompts import ChatPromptTemplate from langchain_core.runnables import RunnablePassthrough from langchain_openai import ChatOpenAI prompt = ChatPromptTemplate.from_template( &amp;quot;Tell me a short joke about {topic}&amp;quot;) output_parser = StrOutputParser() model = ChatOpenAI(model=&amp;quot;gpt-3.5-turbo&amp;quot;) chain = ( {&amp;quot;topic&amp;quot;: RunnablePassthrough()} | prompt | model | output_parser ) if __name__ == &amp;quot;__main__&amp;quot;: print(chain.invoke(&amp;quot;ice cream&amp;quot;)) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;What about this &lt;strong&gt;declarative API&lt;/strong&gt;, using a framework called &lt;a href=&quot;https://github.com/dagworks-inc/hamilton/&quot;&gt;Hamilton&lt;/a&gt; (note: I&amp;#39;m one of the authors)- it&amp;#39;s more verbose, but I can always clearly see how things connect and make modifications -- Hamilton knows which function to call when stitching things together based on the function name and function input arguments -- as you write functions you &amp;quot;declare&amp;quot; what they are and what they require.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;# hamilton_invoke.py from typing import List import openai def llm_client() -&amp;gt; openai.OpenAI: return openai.OpenAI() def joke_prompt(topic: str) -&amp;gt; str: return f&amp;quot;Tell me a short joke about {topic}&amp;quot; def joke_messages(joke_prompt: str) -&amp;gt; List[dict]: return [{&amp;quot;role&amp;quot;: &amp;quot;user&amp;quot;, &amp;quot;content&amp;quot;: joke_prompt}] def joke_response(llm_client: openai.OpenAI, joke_messages: List[dict]) -&amp;gt; str: response = llm_client.chat.completions.create( model=&amp;quot;gpt-3.5-turbo&amp;quot;, messages=joke_messages, ) return response.choices[0].message.content if __name__ == &amp;quot;__main__&amp;quot;: import hamilton_invoke from hamilton import driver dr = ( driver.Builder() .with_modules(hamilton_invoke) .build() ) dr.display_all_functions(&amp;quot;hamilton-invoke.png&amp;quot;) # see image below print(dr.execute([&amp;quot;joke_response&amp;quot;], inputs={&amp;quot;topic&amp;quot;: &amp;quot;ice cream&amp;quot;})) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This image (generated by Hamilton) represents how Hamilton stitches together the code to then run it&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/tq5ms3ltj5zc1.png?width=702&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=048f54f953ab50996e459b93d034771d9a943c7c&quot;&gt;Result of dr.display_all_functions(\&amp;quot;hamilton-invoke.png\&amp;quot;)&lt;/a&gt;&lt;/p&gt; &lt;p&gt;To see more comparisons (e.g. conditionally swapping anthropic for openai) &lt;a href=&quot;https://hamilton.dagworks.io/en/latest/code-comparisons/langchain/&quot;&gt;click here&lt;/a&gt;. For code that is both Hamilton &amp;amp; LangChain &lt;a href=&quot;https://hub.dagworks.io/docs/DAGWorks/conversational_rag/&quot;&gt;see this example&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Now I wouldn&amp;#39;t use Hamilton for a simple function call -- much like I wouldn&amp;#39;t use LangChain for that either.&lt;/p&gt; &lt;p&gt;I&amp;#39;m interested in discussing thoughts and opinions to see if there&amp;#39;s (a) appetite for this style of API, and (b) therefore should we integrate more closely with LangChain. Cheers!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/theferalmonkey&quot;&gt; /u/theferalmonkey &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cmyi9k/discussion_declaratively_orchestrate_your_code/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cmyi9k/discussion_declaratively_orchestrate_your_code/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1cmyi9k</id><media:thumbnail url="https://external-preview.redd.it/sAUbNyH098cBBqicB9rc8ofLEQDpkmgZKE1tSibcy5k.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=6631c24e2bdd21f4aa327a412b8faec0969e28a2" /><link href="https://www.reddit.com/r/LangChain/comments/1cmyi9k/discussion_declaratively_orchestrate_your_code/" /><updated>2024-05-08T07:12:45+00:00</updated><published>2024-05-08T07:12:45+00:00</published><title>Discussion: Declaratively orchestrate your code instead of using LCEL</title></entry><entry><author><name>/u/ramirez_tn</name><uri>https://www.reddit.com/user/ramirez_tn</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I am using Langchain to load PDF files and ask questions using RetrievalQA but when I ask to generate a solution or be creative it does not .It looks like it is limited to the content of the provided files only. Is there a limitation for RertievalQA or just an issue with my prompts ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/ramirez_tn&quot;&gt; /u/ramirez_tn &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cn3d2x/how_to_make_llm_answers_more_creative_and_find/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cn3d2x/how_to_make_llm_answers_more_creative_and_find/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cn3d2x</id><link href="https://www.reddit.com/r/LangChain/comments/1cn3d2x/how_to_make_llm_answers_more_creative_and_find/" /><updated>2024-05-08T12:29:23+00:00</updated><published>2024-05-08T12:29:23+00:00</published><title>How to make LLM answers more creative and find answers from the internet</title></entry><entry><author><name>/u/VRoid</name><uri>https://www.reddit.com/user/VRoid</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Current LangGraph is just libraries for multiple Agents functionality built on Langchain but it can be more useful to have GUI within LangFlow. Any attempt to expand LangFlow with LangGraph? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/VRoid&quot;&gt; /u/VRoid &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cn21xp/any_langflow_update_planned_for_langgraph/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cn21xp/any_langflow_update_planned_for_langgraph/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cn21xp</id><link href="https://www.reddit.com/r/LangChain/comments/1cn21xp/any_langflow_update_planned_for_langgraph/" /><updated>2024-05-08T11:19:02+00:00</updated><published>2024-05-08T11:19:02+00:00</published><title>Any LangFlow update planned for LangGraph?</title></entry><entry><author><name>/u/RoboCoachTech</name><uri>https://www.reddit.com/user/RoboCoachTech</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cmquwv/using_langchain_agents_to_create_a_multiagent/&quot;&gt; &lt;img src=&quot;https://external-preview.redd.it/GklbHOLRONUSqoLxfRvIdxg-bpVWervSnhvxAvD8mnc.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c6e26ad3b1025c14d13502efeae32c2669bf11c4&quot; alt=&quot;Using LangChain agents to create a multi-agent platform that creates robot softwares&quot; title=&quot;Using LangChain agents to create a multi-agent platform that creates robot softwares&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;When using LLMs for your generative AI needs, it&amp;#39;s best to think of the LLM as a person rather than as a traditional AI engine. You can train and tune an LLM and give it memory to create an agent. The LLM-agent can act like a domain-expert for whatever domain you&amp;#39;ve trained and equipped it for. Using one agent to solve a complex problem is not the optimum solution. Much like how a project manager breaks a complex project into different tasks and assigns different individuals with different skills and trainings to manage each task, a multi-agent solution, where each agent has different capabilities and trainings, can be applied to a complex problem. &lt;/p&gt; &lt;p&gt;In our case, we want to automatically generate the entire robot software (for any given robot description) in ROS (Robot Operating System); In order to do so, first, we need to understand the overall design of the robot (a.k.a the ROS graph) and then for each ROS node we need to know if the LLM should generate the code, or if the LLM can fetch a suitable code from online open-source repositories (a.k.a. RAG: Retrieval Augmented Generation). Each of these steps can be handled by different agents which have different sets of tools at their disposal. The following figure shows how we are doing this:&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/qcvb8y98c3zc1.png?width=1570&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5f4072288e470fd9e2d946e471f35e4c2dff1f94&quot;&gt;Robot software generation using four collaborating agents each responsible for a different part of the problem, each equipped with different toolsets.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This is a free and open-source tool that we have released. We named it &lt;a href=&quot;https://github.com/RoboCoachTechnologies/ROScribe&quot;&gt;ROScribe&lt;/a&gt;. Please checkout our &lt;a href=&quot;https://github.com/RoboCoachTechnologies/ROScribe&quot;&gt;repository&lt;/a&gt; for more information and give us a star if you like what you see. :) &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/RoboCoachTech&quot;&gt; /u/RoboCoachTech &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cmquwv/using_langchain_agents_to_create_a_multiagent/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cmquwv/using_langchain_agents_to_create_a_multiagent/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1cmquwv</id><media:thumbnail url="https://external-preview.redd.it/GklbHOLRONUSqoLxfRvIdxg-bpVWervSnhvxAvD8mnc.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=c6e26ad3b1025c14d13502efeae32c2669bf11c4" /><link href="https://www.reddit.com/r/LangChain/comments/1cmquwv/using_langchain_agents_to_create_a_multiagent/" /><updated>2024-05-08T00:05:42+00:00</updated><published>2024-05-08T00:05:42+00:00</published><title>Using LangChain agents to create a multi-agent platform that creates robot softwares</title></entry><entry><author><name>/u/Top_Raccoon_1493</name><uri>https://www.reddit.com/user/Top_Raccoon_1493</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;If we are using a GPU for running the LLama2/LLama3 model, which library should I use? LLama CPP or Ctransformers? I&amp;#39;m a bit confused about both of these libraries. Can anyone please clear my doubt?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Top_Raccoon_1493&quot;&gt; /u/Top_Raccoon_1493 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cn08m1/choosing_between_llama_cpp_and_ctransformers_for/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cn08m1/choosing_between_llama_cpp_and_ctransformers_for/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cn08m1</id><link href="https://www.reddit.com/r/LangChain/comments/1cn08m1/choosing_between_llama_cpp_and_ctransformers_for/" /><updated>2024-05-08T09:18:24+00:00</updated><published>2024-05-08T09:18:24+00:00</published><title>Choosing Between LLama CPP and Ctransformers for GPU-based LLama2/LLama3 Model Execution</title></entry><entry><author><name>/u/Guizkane</name><uri>https://www.reddit.com/user/Guizkane</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey everyone! I recently wrote a blog post about &lt;a href=&quot;http://affiliate.ai/&quot;&gt;Affiliate.ai&lt;/a&gt;, a chat-based affiliate marketing analytics tool we&amp;#39;ve been working on. It simplifies the analytics process, letting you ask natural language questions and get insights, reports, and even spreadsheets delivered right within Microsoft Teams or Slack.&lt;/p&gt; &lt;p&gt;But the interesting part (for this audience, at least) is how it works under the hood. Here&amp;#39;s a breakdown of some key elements:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Intent Discernment with Function Calling:&lt;/strong&gt; We use simple function calling to quickly determine whether a user wants data or is just chatting, ensuring the bot stays focused.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;LLM-Powered Named Entity Recognition:&lt;/strong&gt; Instead of complex pipelines, we feed the LLM a list of advertisers and let it figure out the matchesâ€“ surprisingly effective!&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Query Reconstruction for Context:&lt;/strong&gt; Understanding context is tricky. We use a dedicated module to rewrite queries based on chat history.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Parallelization for Speed:&lt;/strong&gt; We run multiple potential routes simultaneously, speeding up response times dramatically.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Interested in the specifics? The full blog post has more details (link below). If you&amp;#39;re building similar GenAI apps, I&amp;#39;d love to hear about your approaches and techniques!&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://www.affiliate.ai/post/a-technical-deepdive-into-affiliate-ai&quot;&gt;https://www.affiliate.ai/post/a-technical-deepdive-into-affiliate-ai&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Guizkane&quot;&gt; /u/Guizkane &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cn04dj/deep_dive_building_affiliateai_a_genaipowered/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cn04dj/deep_dive_building_affiliateai_a_genaipowered/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cn04dj</id><link href="https://www.reddit.com/r/LangChain/comments/1cn04dj/deep_dive_building_affiliateai_a_genaipowered/" /><updated>2024-05-08T09:09:59+00:00</updated><published>2024-05-08T09:09:59+00:00</published><title>Deep Dive: Building Affiliate.ai, a GenAI-Powered Affiliate Marketing Analytics Tool</title></entry><entry><author><name>/u/Basil2BulgarSlayer</name><uri>https://www.reddit.com/user/Basil2BulgarSlayer</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I am working on a Nextjs demo app that needs to use inference on a custom LLM I will train. When I deploy it, Iâ€™m planning on using Baseten but for local development I am now considering using Lanchain in Node (as opposed to setting up a Flask server to handle inference and stream the responses back). Has anyone used it before? Is it a total disaster? know itâ€™s not going to be as good as the Python version but maybe itâ€™s good enough for my situation. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Basil2BulgarSlayer&quot;&gt; /u/Basil2BulgarSlayer &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cmz6uh/node_js_support/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cmz6uh/node_js_support/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cmz6uh</id><link href="https://www.reddit.com/r/LangChain/comments/1cmz6uh/node_js_support/" /><updated>2024-05-08T08:02:19+00:00</updated><published>2024-05-08T08:02:19+00:00</published><title>Node JS Support</title></entry><entry><author><name>/u/easy_breeze5634</name><uri>https://www.reddit.com/user/easy_breeze5634</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I want to ingest hundreds of csv files, all the column data is&lt;br/&gt; different except for them sharing a similar column related to state. So I&lt;br/&gt; am able to capture the location of the data observations and relate&lt;br/&gt; them to other data. The data is mostly pertaining to demographics like&lt;br/&gt; economics, age, race, income, education, and health related outcomes. I&lt;br/&gt; need a general way to ingest all these csv files and load them into a&lt;br/&gt; knowledge graph, then use OpenAI to send a cypher query to the knowledge&lt;br/&gt; graph to gain context of the user&amp;#39;s question and then return an answer.&lt;br/&gt; A question might be &amp;quot;What is the highest mortality rate in the country&lt;br/&gt; and what might be causing this?&amp;quot; or &amp;quot;Tell me counties with the lowest&lt;br/&gt; morbidity rates and why they might be lower than average&amp;quot;. I was&lt;br/&gt; thinking I could use vector embeddings as well for matching columns&lt;br/&gt; together and clustering the data. Im just wondering what the best way to&lt;br/&gt; construct the graph will be so that the LLM can easily traverse it and&lt;br/&gt; get the correct information back to the user. What is the best way to&lt;br/&gt; set all this up? Does it make sense to construct a knowledge graph here&lt;br/&gt; so that LLM has context. &lt;/p&gt; &lt;p&gt;Could use advice on how to set something up like this. &lt;/p&gt; &lt;p&gt;Thanks &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/easy_breeze5634&quot;&gt; /u/easy_breeze5634 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cmpkyl/ingesting_hundreds_of_csv_files_loading_them_into/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cmpkyl/ingesting_hundreds_of_csv_files_loading_them_into/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cmpkyl</id><link href="https://www.reddit.com/r/LangChain/comments/1cmpkyl/ingesting_hundreds_of_csv_files_loading_them_into/" /><updated>2024-05-07T23:06:04+00:00</updated><published>2024-05-07T23:06:04+00:00</published><title>Ingesting hundreds of csv files, loading them into a knowledge graph (RAG) then use LLM chatbot to query</title></entry><entry><author><name>/u/moonbunR</name><uri>https://www.reddit.com/user/moonbunR</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Here is a simple code snippet on how to use the Cycls chatbot library&lt;/p&gt; &lt;pre&gt;&lt;code&gt;main.py from cycls import App app = App(secret=&amp;quot;sk-secret&amp;quot;, handler=&amp;quot;@handler-name&amp;quot;) @app def entry_point(context): # Capture the received message received_message = context.message.content.text # Reply back with a simple message context.send.text(f&amp;quot;Received message: {received_message}&amp;quot;) app.publish() &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This is a simplified example but when you run &lt;a href=&quot;http://main.py&quot;&gt;main.py&lt;/a&gt;, the chatbot immediately gets deployed with a public url and a chat interface. This has helped me a huge deal with testing while developing chatbots.&lt;br/&gt; Here are the docs: &lt;a href=&quot;https://docs.cycls.com/getting-started&quot;&gt;https://docs.cycls.com/getting-started&lt;/a&gt; &lt;a href=&quot;https://docs.cycls.com/getting-started&quot;&gt; &lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/moonbunR&quot;&gt; /u/moonbunR &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cmdxyi/python_library_to_deploy_llm_chat_bots_fast/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cmdxyi/python_library_to_deploy_llm_chat_bots_fast/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cmdxyi</id><link href="https://www.reddit.com/r/LangChain/comments/1cmdxyi/python_library_to_deploy_llm_chat_bots_fast/" /><updated>2024-05-07T14:59:16+00:00</updated><published>2024-05-07T14:59:16+00:00</published><title>Python library to deploy LLM chat bots fast?</title></entry><entry><author><name>/u/Organic_Manner359</name><uri>https://www.reddit.com/user/Organic_Manner359</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I just saw that Langchain is now a legacy provider. How can i still use Langchain with the Vercel AI SDK for my NextJS apps in a futureproof way. On the website it says, that the legacy providers are not recommended for new projects.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Organic_Manner359&quot;&gt; /u/Organic_Manner359 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cmkcsb/langchain_is_legacy_in_vercel_ai_sdk_how_to_still/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cmkcsb/langchain_is_legacy_in_vercel_ai_sdk_how_to_still/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cmkcsb</id><link href="https://www.reddit.com/r/LangChain/comments/1cmkcsb/langchain_is_legacy_in_vercel_ai_sdk_how_to_still/" /><updated>2024-05-07T19:29:22+00:00</updated><published>2024-05-07T19:29:22+00:00</published><title>Langchain is legacy in Vercel AI SDK, how to still use Langchain in a stable and futureproof way?</title></entry><entry><author><name>/u/cryptokaykay</name><uri>https://www.reddit.com/user/cryptokaykay</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cmk0dn/langtrace_added_support_for_prompt_playground/&quot;&gt; &lt;img src=&quot;https://external-preview.redd.it/NC1sW6cwQYTGxuUvSZWkfQnn_WLopaQp781No9M4rkg.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1f8d9f7c872f91e520fbcd2f3139df9bf6c76dca&quot; alt=&quot;Langtrace - Added support for Prompt Playground&quot; title=&quot;Langtrace - Added support for Prompt Playground&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey all,&lt;/p&gt; &lt;p&gt;We just added support for prompt playground. The goal of this feature is to help you test and iterate on your prompts from a single view across different combinations of models and model settings. &lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;Support for OpenAI, Anthropic, Cohere and Groq&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Side by side comparison view.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Comprehensive API settings tab to tweak and iterate on your prompts with different combinations of settings and models. &lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Please check it out and let me know if you have any feedback.&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://langtrace.ai/&quot;&gt;https://langtrace.ai/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/Scale3-Labs/langtrace&quot;&gt;https://github.com/Scale3-Labs/langtrace&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://reddit.com/link/1cmk0dn/video/y0tve9hb02zc1/player&quot;&gt;https://reddit.com/link/1cmk0dn/video/y0tve9hb02zc1/player&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/cryptokaykay&quot;&gt; /u/cryptokaykay &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cmk0dn/langtrace_added_support_for_prompt_playground/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cmk0dn/langtrace_added_support_for_prompt_playground/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1cmk0dn</id><media:thumbnail url="https://external-preview.redd.it/NC1sW6cwQYTGxuUvSZWkfQnn_WLopaQp781No9M4rkg.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=1f8d9f7c872f91e520fbcd2f3139df9bf6c76dca" /><link href="https://www.reddit.com/r/LangChain/comments/1cmk0dn/langtrace_added_support_for_prompt_playground/" /><updated>2024-05-07T19:14:36+00:00</updated><published>2024-05-07T19:14:36+00:00</published><title>Langtrace - Added support for Prompt Playground</title></entry><entry><author><name>/u/Calm_Pea_2428</name><uri>https://www.reddit.com/user/Calm_Pea_2428</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;m thinking about writing a blog on this topic &amp;quot;Why specialized vector databases are not the future?&amp;quot;&lt;/p&gt; &lt;p&gt;In this blog, I&amp;#39;ll try to explain why you need Integrated vector databases rather than a specialised vector database. &lt;/p&gt; &lt;p&gt;Do you have any arguments that support or refute this narrative?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Calm_Pea_2428&quot;&gt; /u/Calm_Pea_2428 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cmqx7z/why_specialized_vector_databases_are_not_the/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cmqx7z/why_specialized_vector_databases_are_not_the/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cmqx7z</id><link href="https://www.reddit.com/r/LangChain/comments/1cmqx7z/why_specialized_vector_databases_are_not_the/" /><updated>2024-05-08T00:08:50+00:00</updated><published>2024-05-08T00:08:50+00:00</published><title>Why specialized vector databases are not the future?</title></entry><entry><author><name>/u/ravediamond000</name><uri>https://www.reddit.com/user/ravediamond000</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hello everyone,&lt;/p&gt; &lt;p&gt;Just finished writing a post on how to create real world application using Langchain.&lt;br/&gt; I talk about :&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Langchain LCEL&lt;/li&gt; &lt;li&gt;How to create composition of multiple chains.&lt;/li&gt; &lt;li&gt;How to integrate user parameters like output type or specified vector store in chains.&lt;/li&gt; &lt;li&gt;How to use configuration to change the prompt and the retriever at run time.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Check it out: &lt;a href=&quot;https://www.metadocs.co/2024/05/07/create-a-complex-rag-chat-app-with-langchain-lcel/&quot;&gt;Link&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/ravediamond000&quot;&gt; /u/ravediamond000 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cm5ktx/create_a_real_world_rag_chat_app_with_langchain/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cm5ktx/create_a_real_world_rag_chat_app_with_langchain/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cm5ktx</id><link href="https://www.reddit.com/r/LangChain/comments/1cm5ktx/create_a_real_world_rag_chat_app_with_langchain/" /><updated>2024-05-07T06:50:45+00:00</updated><published>2024-05-07T06:50:45+00:00</published><title>Create a real world RAG chat app with Langchain LCEL</title></entry><entry><author><name>/u/mahadevbhakti</name><uri>https://www.reddit.com/user/mahadevbhakti</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;HI &lt;/p&gt; &lt;p&gt;I have a RunnableWithMessageHistory and agent_executor to create a chatbot agent with tools to fetch data from an API and then Redis to story chat history. &lt;/p&gt; &lt;p&gt;I see that the RunnableWithMessageHistory is not recording the raw response of the function calling to the chat history. How do I solve for this?&lt;/p&gt; &lt;p&gt;I have been reading the API docs but couldn&amp;#39;t find anything. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/mahadevbhakti&quot;&gt; /u/mahadevbhakti &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cmdvls/passing_the_output_of_function_calling_to/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cmdvls/passing_the_output_of_function_calling_to/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cmdvls</id><link href="https://www.reddit.com/r/LangChain/comments/1cmdvls/passing_the_output_of_function_calling_to/" /><updated>2024-05-07T14:56:13+00:00</updated><published>2024-05-07T14:56:13+00:00</published><title>Passing the output of function calling to RedisChatHistory in LCEL</title></entry><entry><author><name>/u/Fresh_Skin130</name><uri>https://www.reddit.com/user/Fresh_Skin130</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I am developing my own Rag. Using the text_splitter there is a performance tradeoff for the chunk size and chunk overlap parameters. Short chunks may not include all desired context, long chunks may hallucinate or cause info loss. Has anyone tried to embed twice the same document with multiple and different splitters? Are there noticeable advantages / disadvantages?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Fresh_Skin130&quot;&gt; /u/Fresh_Skin130 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cm8oek/combined_embeddings/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cm8oek/combined_embeddings/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cm8oek</id><link href="https://www.reddit.com/r/LangChain/comments/1cm8oek/combined_embeddings/" /><updated>2024-05-07T10:31:26+00:00</updated><published>2024-05-07T10:31:26+00:00</published><title>Combined Embeddings</title></entry><entry><author><name>/u/ZuckyFox</name><uri>https://www.reddit.com/user/ZuckyFox</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;How can we pass images or base64 to llama2 and ask certain questions like describe this image??&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/ZuckyFox&quot;&gt; /u/ZuckyFox &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cm9e3l/is_llama2_multimodal/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cm9e3l/is_llama2_multimodal/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cm9e3l</id><link href="https://www.reddit.com/r/LangChain/comments/1cm9e3l/is_llama2_multimodal/" /><updated>2024-05-07T11:15:31+00:00</updated><published>2024-05-07T11:15:31+00:00</published><title>Is llama2 multimodal?</title></entry><entry><author><name>/u/Koltchak</name><uri>https://www.reddit.com/user/Koltchak</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi&lt;/p&gt; &lt;p&gt;I&amp;#39;m currently using a code that processes a series of reports (PDF files), posing the same set of questions to every report. The output is a dataframe containing the responses to each question for every report. For instance, the question &amp;quot;Does the report explains why the amount of X decreased?&amp;quot; will be answered in a column of my output dataframe. So every line of this column will consist of the answer for a specific report. My setup includes the use of &lt;a href=&quot;https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2&quot;&gt;https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2&lt;/a&gt; and a locally downloaded LLM from Hugging Face. Although the code is functional, the reports are highly technical and complex, and the local LLM lacks the necessary understanding of the content (I notice that when I read the answers).&lt;/p&gt; &lt;p&gt;I&amp;#39;m looking to enhance the LLM&amp;#39;s comprehension by training it on additional technical documents of the same nature, hoping this will improve its ability to accurately answer queries from my reports. If I&amp;#39;ve understood correctly, one approach might be to utilize a RAG on the technical documents, but I&amp;#39;m unsure of the exact steps to implement this effectively. I&amp;#39;ve attempted to merge the embeddings from the downloaded &amp;#39;all-MiniLM-L6-v2&amp;#39; model with those I generated from the technical documents, as described here: &lt;a href=&quot;https://python.langchain.com/docs/integrations/retrievers/merger_retriever/&quot;&gt;https://python.langchain.com/docs/integrations/retrievers/merger_retriever/&lt;/a&gt;, but without success.&lt;/p&gt; &lt;p&gt;Could you suggest a viable strategy for this? Should I discard the &amp;#39;all-MiniLM-L6-v2&amp;#39; and focus solely on embeddings derived from my technical documents? This approach seems to require an extensive collection of documents, which I currently don&amp;#39;t have.&lt;/p&gt; &lt;p&gt;I&amp;#39;ve tried various other local LLMs (Mistral, Phi, Llama, Orca), but I encounter the same issue each time. &amp;quot;Large&amp;quot; LLMs (Mistral e.g.) tend to hallucinate, while &amp;quot;smaller&amp;quot; (Orca e.g.) LLMs often respond that they do not know.&lt;/p&gt; &lt;p&gt;Thanks&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Koltchak&quot;&gt; /u/Koltchak &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1clui48/enhancing_local_llms_understanding_of_technical/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1clui48/enhancing_local_llms_understanding_of_technical/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1clui48</id><link href="https://www.reddit.com/r/LangChain/comments/1clui48/enhancing_local_llms_understanding_of_technical/" /><updated>2024-05-06T21:21:11+00:00</updated><published>2024-05-06T21:21:11+00:00</published><title>Enhancing Local LLM's Understanding of Technical Documents</title></entry><entry><author><name>/u/Adventurous-Fox698</name><uri>https://www.reddit.com/user/Adventurous-Fox698</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hello, could you assist me? I&amp;#39;m encountering difficulties with Langsmith&amp;#39;s evaluation process. I&amp;#39;m able to evaluate my context using &amp;#39;cont_qa&amp;#39;, but when I attempt to incorporate additional evaluators such as &amp;#39;accuracy&amp;#39;, my kernel crashes if there&amp;#39;s more than one element in my dataset. Interestingly, when there&amp;#39;s only one element, it works perfectly. I&amp;#39;m seeking guidance on how to address this issue. Below is the code I&amp;#39;m using&lt;/p&gt; &lt;pre&gt;&lt;code&gt;import langsmith from langchain import chat_models, prompts, smith from langchain.schema import output_parser # Define your runnable or chain below. prompt = prompts.ChatPromptTemplate.from_messages( [ (&amp;quot;system&amp;quot;, &amp;quot;You are a helpful AI assistant.&amp;quot;), (&amp;quot;human&amp;quot;, &amp;quot;{question}&amp;quot;) ] ) # Define the evaluators to apply eval_config = smith.RunEvalConfig( evaluators=[ &amp;quot;cot_qa&amp;quot;, smith.RunEvalConfig.LabeledCriteria(&amp;quot;harmfulness&amp;quot;), smith.RunEvalConfig.LabeledCriteria(&amp;quot;relevance&amp;quot;), smith.RunEvalConfig.LabeledCriteria(&amp;quot;helpfulness&amp;quot;) ], custom_evaluators=[], eval_llm=chat_models.ChatOpenAI(model=&amp;quot;gpt-4&amp;quot;, temperature=0) ) client = langsmith.Client() chain_results = client.run_on_dataset( dataset_name=dataset_name, llm_or_chain_factory=open_ai_rag, evaluation=eval_config, concurrency_level=5, verbose=True, ) &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Adventurous-Fox698&quot;&gt; /u/Adventurous-Fox698 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cm7lbr/kernel_crashing_when_using_langsmith_evaluation/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cm7lbr/kernel_crashing_when_using_langsmith_evaluation/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cm7lbr</id><link href="https://www.reddit.com/r/LangChain/comments/1cm7lbr/kernel_crashing_when_using_langsmith_evaluation/" /><updated>2024-05-07T09:17:32+00:00</updated><published>2024-05-07T09:17:32+00:00</published><title>Kernel crashing when using Langsmith Evaluation</title></entry><entry><author><name>/u/AI_technologies</name><uri>https://www.reddit.com/user/AI_technologies</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Just to clarify, if you want to be the first to know about trends in the startup world, you have to keep an eye on what&amp;#39;s going on inside Y Combinator. This incubator has extensive experience with startups. You may know Stripe, Airbnb, Dropbox, Reddit, and Twitch among its alumni. And I&amp;#39;m sure YC won&amp;#39;t slow down.&lt;/p&gt; &lt;p&gt;According to Garry Tan (President and CEO of YC), &lt;strong&gt;the 260 companies&lt;/strong&gt; in the W24 cohort were selected from over &lt;strong&gt;27,000&lt;/strong&gt; applications. The acceptance rate for startups was less than &lt;strong&gt;1%&lt;/strong&gt;! And AI was the biggest topic, with over &lt;strong&gt;85 companies&lt;/strong&gt; calling themselves &amp;quot;AI Startups&amp;quot; and over &lt;strong&gt;180 mentioning&lt;/strong&gt; machine learning in their demos.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/AI_technologies&quot;&gt; /u/AI_technologies &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cmiclo/why_should_we_keep_an_eye_out_for_yc_batches/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cmiclo/why_should_we_keep_an_eye_out_for_yc_batches/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cmiclo</id><link href="https://www.reddit.com/r/LangChain/comments/1cmiclo/why_should_we_keep_an_eye_out_for_yc_batches/" /><updated>2024-05-07T18:05:50+00:00</updated><published>2024-05-07T18:05:50+00:00</published><title>Why Should We Keep an Eye Out for YC Batches?</title></entry><entry><author><name>/u/emersounds</name><uri>https://www.reddit.com/user/emersounds</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hello everyone, I&amp;#39;m trying to follow this example I found in the Vertex AI documentation.&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://cloud.google.com/vertex-ai/generative-ai/docs/reasoning-engine/develop&quot;&gt;https://cloud.google.com/vertex-ai/generative-ai/docs/reasoning-engine/develop&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I&amp;#39;m following the steps carefully, but when I try:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;response = agent.query( &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;input=&amp;quot;What is the exchange rate from US dollars to Swedish currency?&amp;quot; )&lt;/p&gt; &lt;p&gt;I receive the following error:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;{ &amp;quot;name&amp;quot;: &amp;quot;KeyError&amp;quot;, &amp;quot;message&amp;quot;: &amp;quot;&amp;#39;agent&amp;#39;&amp;quot;, &amp;quot;stack&amp;quot;: &amp;quot;--------------------------------------------------------------------------- KeyError Traceback (most recent call last) Cell In[92], line 1 ----&amp;gt; 1 response = agent.query( 2 input=\&amp;quot;What is the exchange rate from US dollars to Swedish currency?\&amp;quot; 3 ) File /opt/homebrew/lib/python3.11/site-packages/vertexai/preview/reasoning_engines/templates/langchain.py:439, in LangchainAgent.query(self, input, config, **kwargs) 437 input = {\&amp;quot;input\&amp;quot;: input} 438 if not self._runnable: --&amp;gt; 439 self.set_up() 440 return langchain_load_dump.dumpd( 441 self._runnable.invoke(input=input, config=config, **kwargs) 442 ) File /opt/homebrew/lib/python3.11/site-packages/vertexai/preview/reasoning_engines/templates/langchain.py:400, in LangchainAgent.set_up(self) 398 self._llm = self._llm.bind(functions=self._tools) 399 self._agent = self._prompt | self._llm | self._output_parser --&amp;gt; 400 self._agent_executor = AgentExecutor( 401 agent=self._agent, 402 tools=self._tools, 403 **self._agent_executor_kwargs, 404 ) 405 runnable = self._agent_executor 406 if has_history: File /opt/homebrew/lib/python3.11/site-packages/langchain/load/serializable.py:97, in Serializable.__init__(self, **kwargs) 96 def __init__(self, **kwargs: Any) -&amp;gt; None: ---&amp;gt; 97 super().__init__(**kwargs) 98 self._lc_kwargs = kwargs File /opt/homebrew/lib/python3.11/site-packages/pydantic/v1/main.py:339, in BaseModel.__init__(__pydantic_self__, **data) 333 \&amp;quot;\&amp;quot;\&amp;quot; 334 Create a new model by parsing and validating input data from keyword arguments. 335 336 Raises ValidationError if the input data cannot be parsed to form a valid model. 337 \&amp;quot;\&amp;quot;\&amp;quot; 338 # Uses something other than `self` the first arg to allow \&amp;quot;self\&amp;quot; as a settable attribute --&amp;gt; 339 values, fields_set, validation_error = validate_model(__pydantic_self__.__class__, data) 340 if validation_error: 341 raise validation_error File /opt/homebrew/lib/python3.11/site-packages/pydantic/v1/main.py:1102, in validate_model(model, input_data, cls) 1100 continue 1101 try: -&amp;gt; 1102 values = validator(cls_, values) 1103 except (ValueError, TypeError, AssertionError) as exc: 1104 errors.append(ErrorWrapper(exc, loc=ROOT_KEY)) File /opt/homebrew/lib/python3.11/site-packages/langchain/agents/agent.py:881, in AgentExecutor.validate_tools(cls, values) 878 () 879 def validate_tools(cls, values: Dict) -&amp;gt; Dict: 880 \&amp;quot;\&amp;quot;\&amp;quot;Validate that tools are compatible with agent.\&amp;quot;\&amp;quot;\&amp;quot; --&amp;gt; 881 agent = values[\&amp;quot;agent\&amp;quot;] 882 tools = values[\&amp;quot;tools\&amp;quot;] 883 allowed_tools = agent.get_allowed_tools() KeyError: &amp;#39;agent&amp;#39;&amp;quot; } &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Has anyone else encountered this error while working with Vertex AI? If so, how did you resolve it? Any tips or tricks would be greatly appreciated!&lt;/p&gt; &lt;p&gt;UPDATE:&lt;/p&gt; &lt;p&gt;I fix it downgrading the langchain-core version from 0.1.51 to 0.1.50!!!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/emersounds&quot;&gt; /u/emersounds &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cm2lvb/error_following_langchainvertex_ai_reasoning/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cm2lvb/error_following_langchainvertex_ai_reasoning/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cm2lvb</id><link href="https://www.reddit.com/r/LangChain/comments/1cm2lvb/error_following_langchainvertex_ai_reasoning/" /><updated>2024-05-07T03:45:43+00:00</updated><published>2024-05-07T03:45:43+00:00</published><title>Error Following Langchain/Vertex AI Reasoning Engine Docs</title></entry><entry><author><name>/u/MountainBlock</name><uri>https://www.reddit.com/user/MountainBlock</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;I&amp;#39;m relatively novice when it comes to LLMs, so my understanding is very limited. However, I came across &lt;a href=&quot;https://youtu.be/9ccl1_Wu24Q?si=gHdxkoCE0gulV3Px&quot;&gt;this YouTube Video &lt;/a&gt;and was impressed with how simple the whole project was set up using Langchain and MySQL. I&amp;#39;ve been experimenting with it using a local version of our company&amp;#39;s database, and I have this vision of developing a chatbot that can talk to our database and answer questions related to the information we have in our database.&lt;/p&gt; &lt;p&gt;I have a few questions:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;I&amp;#39;ve read a few comments on this subreddit indicating that Langchain is not good for SQL. What does this mean, and why is it so?&lt;/li&gt; &lt;li&gt;One of the comments on the video critiques the use of Langchain, stating that Langchain packages are not secure, due to passing database schema across third party providers. Is this true? My idea would be to create a chatbot for internal use, and implement it into our intranet.&lt;/li&gt; &lt;li&gt;In general, how beginner friendly is Langchain? I&amp;#39;m under the impression it&amp;#39;s moreso compared to other models. &lt;/li&gt; &lt;li&gt;Is Langchain the right tool for my task?&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;If I can provide any more context, I&amp;#39;d be happy to do so. Thanks in advance. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/MountainBlock&quot;&gt; /u/MountainBlock &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1clj3k0/starting_out_with_langchain_sql_some_questions/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1clj3k0/starting_out_with_langchain_sql_some_questions/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1clj3k0</id><link href="https://www.reddit.com/r/LangChain/comments/1clj3k0/starting_out_with_langchain_sql_some_questions/" /><updated>2024-05-06T13:30:08+00:00</updated><published>2024-05-06T13:30:08+00:00</published><title>Starting out with Langchain SQL: some questions</title></entry><entry><author><name>/u/Beginning_Land_5775</name><uri>https://www.reddit.com/user/Beginning_Land_5775</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;hi, if i run my code for rag made with python and langchain,the firtst time everything goes well and in the second question it spits out this error:&lt;/p&gt; &lt;p&gt;raise ValueError(f&amp;quot;Missing some input keys: {missing_keys}&amp;quot;)&lt;/p&gt; &lt;p&gt;ValueError: Missing some input keys: {&amp;#39;context&amp;#39;}&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Beginning_Land_5775&quot;&gt; /u/Beginning_Land_5775 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1clmt84/problem_with_multiple_rag_questions/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1clmt84/problem_with_multiple_rag_questions/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1clmt84</id><link href="https://www.reddit.com/r/LangChain/comments/1clmt84/problem_with_multiple_rag_questions/" /><updated>2024-05-06T16:07:11+00:00</updated><published>2024-05-06T16:07:11+00:00</published><title>problem with multiple rag questions</title></entry><entry><author><name>/u/_Mahin_</name><uri>https://www.reddit.com/user/_Mahin_</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi, recently while I was building a RAG Chatbot I came through an issue i.e below is the code I executed on 4 different machines(laptops) &lt;/p&gt; &lt;pre&gt;&lt;code&gt;from langchain_community.embeddings import OllamaEmbeddings embedding = embeddings.OllamaEmbeddings(model=&amp;#39;nomic-embed-text&amp;#39;) db = FAISS.load_local(&amp;quot;MS_VDB&amp;quot;, embeddings=embedding,allow_dangerous_deserialization=True) query = &amp;quot;Recently I was diagnosed with Relapsing-onset MS and the indicated MS phenotype is Highly active. List the name of the drugs which can be used to treat my condition&amp;quot; docs = db.similarity_search(query) print(docs[0].page_content) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;All the 4 machines use the same vector db file but in 3 machines the similarity search result was exactly the same but on the other one it was different even though the code was executed within a virtual environment. Can anyone tell me what&amp;#39;s the issue and how I could resolve it so that all the 4 machines give the same output. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/_Mahin_&quot;&gt; /u/_Mahin_ &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1clfwla/is_it_possible_to_get_different_similarity_search/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1clfwla/is_it_possible_to_get_different_similarity_search/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1clfwla</id><link href="https://www.reddit.com/r/LangChain/comments/1clfwla/is_it_possible_to_get_different_similarity_search/" /><updated>2024-05-06T10:40:44+00:00</updated><published>2024-05-06T10:40:44+00:00</published><title>Is it possible to get different similarity search results with same vector database on different machines for the same similarity search query?</title></entry></feed>