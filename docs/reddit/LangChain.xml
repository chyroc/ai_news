<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/"><category term="LangChain" label="r/LangChain"/><updated>2024-03-20T23:25:03+00:00</updated><icon>https://www.redditstatic.com/icon.png/</icon><id>/r/LangChain.rss</id><link rel="self" href="https://www.reddit.com/r/LangChain.rss" type="application/atom+xml" /><link rel="alternate" href="https://www.reddit.com/r/LangChain" type="text/html" /><subtitle>LangChain is an open-source framework and developer toolkit that helps developers get LLM applications from prototype to production. It is available for Python and Javascript at https://www.langchain.com/.</subtitle><title>LangChain</title><entry><author><name>/u/zchaarm</name><uri>https://www.reddit.com/user/zchaarm</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;A place for members of &lt;a href=&quot;/r/LangChain&quot;&gt;r/LangChain&lt;/a&gt; to chat with each other&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/zchaarm&quot;&gt; /u/zchaarm &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_10ljho9</id><link href="https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/" /><updated>2023-01-26T04:33:05+00:00</updated><published>2023-01-26T04:33:05+00:00</published><title>r/LangChain Lounge</title></entry><entry><author><name>/u/redditforgets</name><uri>https://www.reddit.com/user/redditforgets</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bjlldg/got_the_accuracy_of_gpt4_function_calling_from_35/&quot;&gt; &lt;img src=&quot;https://b.thumbs.redditmedia.com/EVm_c1rJmIBCr6_4BoHQygB8rtJ_kZYrD-LWEoWIYPQ.jpg&quot; alt=&quot;Got the accuracy of GPT4 Function Calling from 35% to 75% by tweaking function definitions.&quot; title=&quot;Got the accuracy of GPT4 Function Calling from 35% to 75% by tweaking function definitions.&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;ul&gt; &lt;li&gt;Adding function definitions in the system prompt of functions (Clickup&amp;#39;s API calls).&lt;/li&gt; &lt;li&gt;Flattening the Schema of the function&lt;/li&gt; &lt;li&gt;Adding system prompts&lt;/li&gt; &lt;li&gt;Adding function definitions in system prompt&lt;/li&gt; &lt;li&gt;Adding individual parameter examples&lt;/li&gt; &lt;li&gt;Adding function examples&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Wrote a nice blog with an &lt;a href=&quot;https://blog.composio.dev/improving-function-calling-accuracy-for-agentic-integrations/&quot;&gt;Indepth explanation&lt;/a&gt; here.&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/rmxgt35zfjpc1.png?width=816&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=934eddf839e17f2324c590157943a92ebbdedffa&quot;&gt;https://preview.redd.it/rmxgt35zfjpc1.png?width=816&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=934eddf839e17f2324c590157943a92ebbdedffa&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/redditforgets&quot;&gt; /u/redditforgets &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bjlldg/got_the_accuracy_of_gpt4_function_calling_from_35/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bjlldg/got_the_accuracy_of_gpt4_function_calling_from_35/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1bjlldg</id><media:thumbnail url="https://b.thumbs.redditmedia.com/EVm_c1rJmIBCr6_4BoHQygB8rtJ_kZYrD-LWEoWIYPQ.jpg" /><link href="https://www.reddit.com/r/LangChain/comments/1bjlldg/got_the_accuracy_of_gpt4_function_calling_from_35/" /><updated>2024-03-20T19:10:44+00:00</updated><published>2024-03-20T19:10:44+00:00</published><title>Got the accuracy of GPT4 Function Calling from 35% to 75% by tweaking function definitions.</title></entry><entry><author><name>/u/Bhaag_Jaa</name><uri>https://www.reddit.com/user/Bhaag_Jaa</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;m working on a project to generate insights from the data.&lt;/p&gt; &lt;p&gt;Extracting data from DB and passing it as context to LLM&lt;/p&gt; &lt;p&gt;I am using AWS bedrock service , antropic claude v2 as LLM(coz 100k tokken limit.)&lt;/p&gt; &lt;p&gt;Data comprises audiences and one base Audience with multiple attributes related to demographics,geography and employment.&lt;br/&gt; Each attribute have multiple attribute values&lt;/p&gt; &lt;p&gt;Issue I&amp;#39;m facing is some attributes have 20-35k rows of unique attribute values, i want to generate the insghits from them.&lt;/p&gt; &lt;p&gt;I tried passing 400-500 rows at a time with a loop to cover 20k rows and store the output in a list and passing that list again through LLM to generate summary of all the 400 rows loop output.. something like Mapdreduce.&lt;/p&gt; &lt;p&gt;but it is taking a lot of time (40-min to generate insights from 20k rows)and there is loss of information, LLM ignores some of the important rows as every row is unique and important.&lt;/p&gt; &lt;p&gt;Pls suggest some better way to solve this problem.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Bhaag_Jaa&quot;&gt; /u/Bhaag_Jaa &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bjmv5j/how_to_pass_30k_rows_as_context_to_llm/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bjmv5j/how_to_pass_30k_rows_as_context_to_llm/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bjmv5j</id><link href="https://www.reddit.com/r/LangChain/comments/1bjmv5j/how_to_pass_30k_rows_as_context_to_llm/" /><updated>2024-03-20T20:03:07+00:00</updated><published>2024-03-20T20:03:07+00:00</published><title>How to pass 30k rows as context to LLM.</title></entry><entry><author><name>/u/Zealousideal-Fall705</name><uri>https://www.reddit.com/user/Zealousideal-Fall705</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I’m a Ph.D. student who recently try to switch from hugging face to langchain. It feels like huggingface organize their libraries the research way (or the PyTorch way? It just feel like I can use them the same way I use research papers’ code), but langchain is more like something developed by JavaScript engineers and designed with no research user cases. &lt;/p&gt; &lt;p&gt;For example, all the “batch inference “ requirements on GitHub are ignored. The interface for customized functions (e.g., chat history post processing) are ill-designed. &lt;/p&gt; &lt;p&gt;I chose langchain in the beginning because the LLMs hosted by langchain responds faster than my local ones. But it seems that it’s really hard to customize the functionalities for research purposes.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Zealousideal-Fall705&quot;&gt; /u/Zealousideal-Fall705 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bjks1c/do_researchers_like_langchain/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bjks1c/do_researchers_like_langchain/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bjks1c</id><link href="https://www.reddit.com/r/LangChain/comments/1bjks1c/do_researchers_like_langchain/" /><updated>2024-03-20T18:37:14+00:00</updated><published>2024-03-20T18:37:14+00:00</published><title>Do researchers like langchain?</title></entry><entry><author><name>/u/tisi3000</name><uri>https://www.reddit.com/user/tisi3000</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;When building LLM workflows with LangChain/LangGraph what&amp;#39;s the best way to build a node in the workflow &lt;strong&gt;where a human can validate/approve/reject&lt;/strong&gt; a flow? I know there is a Human-in-the-loop component in LangGraph that will prompt the user for input. But what if I&amp;#39;m not creating a user-initiated chat conversation, but a flow that reacts to e.g. incoming emails?&lt;/p&gt; &lt;p&gt;I guess I&amp;#39;d have to design my UI so that it&amp;#39;s not only a simple single-threaded chat interface, but some sort of inbox, right? Or is there any standard way that comes to mind?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/tisi3000&quot;&gt; /u/tisi3000 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bjnmu4/human_intervention_in_agent_workflows/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bjnmu4/human_intervention_in_agent_workflows/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bjnmu4</id><link href="https://www.reddit.com/r/LangChain/comments/1bjnmu4/human_intervention_in_agent_workflows/" /><updated>2024-03-20T20:34:23+00:00</updated><published>2024-03-20T20:34:23+00:00</published><title>Human intervention in agent workflows</title></entry><entry><author><name>/u/redfuel2</name><uri>https://www.reddit.com/user/redfuel2</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hello everyone,&lt;/p&gt; &lt;p&gt;I&amp;#39;m embarking on a project that requires a fresh start, and I find myself at a crossroads trying to decide on the optimal technology stack. The core objective is to enable conversations with a database using natural language, aiming for precise outcomes. This involves working with tabular data, applying filters, and conducting semantic searches.&lt;/p&gt; &lt;p&gt;Given the plethora of options out there, from graph databases and SQLCoder models to Retrieval-Augmented Generation (RAG) techniques, making a choice feels overwhelming. Each of these technologies brings something unique to the table, but I&amp;#39;m looking for a solution that balances ease of integration, scalability, and, most importantly, the ability to understand and process natural language queries effectively.&lt;/p&gt; &lt;p&gt;I would greatly appreciate your insights, experiences, or any advice you could share on this matter. Which stack or combination of technologies have you found to be the most effective for interacting with databases through natural language? Any pitfalls or success stories you could share would also be incredibly helpful as I navigate through these options.&lt;/p&gt; &lt;p&gt;Thank you in advance for your time and help!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/redfuel2&quot;&gt; /u/redfuel2 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bjf4xd/seeking_the_ideal_stack_for_natural_language/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bjf4xd/seeking_the_ideal_stack_for_natural_language/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bjf4xd</id><link href="https://www.reddit.com/r/LangChain/comments/1bjf4xd/seeking_the_ideal_stack_for_natural_language/" /><updated>2024-03-20T14:43:52+00:00</updated><published>2024-03-20T14:43:52+00:00</published><title>Seeking the Ideal Stack for Natural Language Database Interactions</title></entry><entry><author><name>/u/HappyDataGuy</name><uri>https://www.reddit.com/user/HappyDataGuy</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/HappyDataGuy&quot;&gt; /u/HappyDataGuy &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;/r/LLMDevs/comments/1bjctuz/has_anyone_used_dspy_for_rag_how_does_it_compare/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bjcun4/has_anyone_used_dspy_for_rag_how_does_it_compare/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bjcun4</id><link href="https://www.reddit.com/r/LangChain/comments/1bjcun4/has_anyone_used_dspy_for_rag_how_does_it_compare/" /><updated>2024-03-20T13:00:30+00:00</updated><published>2024-03-20T13:00:30+00:00</published><title>Has anyone used dspy for RAG? how does it compare to langchain/llama-index? and how does it &quot;train&quot; an LLM?</title></entry><entry><author><name>/u/fish2079</name><uri>https://www.reddit.com/user/fish2079</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I have built a simple RAG chain with message history using Mistral-7b model with 4bit quantization. &lt;/p&gt; &lt;p&gt;Whenever I build this chain using a model from the dockerized Ollama, everything works fine and I can have a long conversation with the chain. &lt;/p&gt; &lt;p&gt;However, as soon as I switch to HF model, only the first message goes through, everything else gets the OOM memory. In fact, the memory usage seems to increase with each subsequent invoke. &lt;/p&gt; &lt;p&gt;In both cases, I am using the Mistral-7b model with quantization. So I am confused as to where the memory issue comes from. &lt;/p&gt; &lt;p&gt;Here are the code snippets: &lt;/p&gt; &lt;p&gt;Using HF model:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;model_name = &amp;quot;mistralai/Mistral-7B-Instruct-v0.2&amp;quot; bnb_config = BitsAndBytesConfig( load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=&amp;quot;nf4&amp;quot;, bnb_4bit_compute_dtype=torch.bfloat16 ) model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=bnb_config) tokenizer = AutoTokenizer.from_pretrained(model_name) text_generation_pipeline = pipeline( model=model, tokenizer=tokenizer, task=&amp;quot;text-generation&amp;quot;, temperature=0.2, do_sample=True, repetition_penalty=1.1, max_new_tokens=400, ) chat_llm = HuggingFacePipeline(pipeline=text_generation_pipeline) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Using Ollama model:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;chat_llm = ChatOllama(model=&amp;quot;mistral:7b&amp;quot;) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Overall chain setup&lt;/p&gt; &lt;pre&gt;&lt;code&gt;chatbot_conversation_with_context_chain = &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;RunnablePassthrough.assign(standalone_message=standalone_message_chain).assign(context= itemgetter(&amp;#39;standalone_message&amp;#39;) | retriever).assign(output= question_answering_prompt | chat_llm | StrOutputParser())&lt;/p&gt; &lt;pre&gt;&lt;code&gt;chatbot = RunnableWithMessageHistory( chatbot_conversation_with_context_chain, get_session_history=get_session_history, input_messages_key=&amp;quot;messages&amp;quot;, history_messages_key=&amp;quot;chat_history&amp;quot;, history_factory_config=[ ConfigurableFieldSpec( id=&amp;quot;user_id&amp;quot;, annotation=str, name=&amp;quot;User ID&amp;quot;, description=&amp;quot;Unique identifier for the user.&amp;quot;, default=&amp;quot;&amp;quot;, is_shared=True, ), ConfigurableFieldSpec( id=&amp;quot;conversation_id&amp;quot;, annotation=str, name=&amp;quot;Conversation ID&amp;quot;, description=&amp;quot;Unique identifier for the conversation.&amp;quot;, default=&amp;quot;&amp;quot;, is_shared=True, ), ], &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;) response = chatbot.invoke( {&amp;#39;messages&amp;#39;: &amp;quot;Can you give me the basic Java code for reading a CSV file?&amp;quot;}, config={ &amp;quot;configurable&amp;quot;: {&amp;quot;user_id&amp;quot;: &amp;quot;test&amp;quot;, &amp;quot;conversation_id&amp;quot;: &amp;quot;dummy&amp;quot;} }, )&lt;/p&gt; &lt;p&gt;print(response.keys()) for key in response.keys(): print(key+&amp;quot;: &amp;quot;, end=&amp;quot;&amp;quot;) print(response[key])&lt;/p&gt; &lt;p&gt;response = chatbot.invoke( {&amp;#39;messages&amp;#39;: &amp;quot;Can you elaborate on the first function?&amp;quot;}, config={&amp;quot;configurable&amp;quot;: {&amp;quot;user_id&amp;quot;: &amp;quot;test&amp;quot;, &amp;quot;conversation_id&amp;quot;: &amp;quot;dummy&amp;quot;} }, )&lt;/p&gt; &lt;p&gt;print(response.keys()) for key in response.keys(): print(key+&amp;quot;: &amp;quot;, end=&amp;quot;&amp;quot;) print(response[key])&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/fish2079&quot;&gt; /u/fish2079 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bjm5rp/rag_chain_with_hf_model_works_fine_for_first/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bjm5rp/rag_chain_with_hf_model_works_fine_for_first/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bjm5rp</id><link href="https://www.reddit.com/r/LangChain/comments/1bjm5rp/rag_chain_with_hf_model_works_fine_for_first/" /><updated>2024-03-20T19:33:59+00:00</updated><published>2024-03-20T19:33:59+00:00</published><title>RAG chain with HF model works fine for first quest, then OOM for subsequent chain. No OOM issue when using Ollama model instead</title></entry><entry><author><name>/u/VegetableAddendum888</name><uri>https://www.reddit.com/user/VegetableAddendum888</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;If any Colab notebook or github repo available then it will be helpful&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/VegetableAddendum888&quot;&gt; /u/VegetableAddendum888 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bjiabv/can_anyone_suggest_a_idea_to_implement_rag_with/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bjiabv/can_anyone_suggest_a_idea_to_implement_rag_with/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bjiabv</id><link href="https://www.reddit.com/r/LangChain/comments/1bjiabv/can_anyone_suggest_a_idea_to_implement_rag_with/" /><updated>2024-03-20T16:55:26+00:00</updated><published>2024-03-20T16:55:26+00:00</published><title>Can anyone suggest a idea to implement RAG with LLm.Like if the searched query not in RAG data then LLm responses to the query</title></entry><entry><author><name>/u/Mediocre-Card8046</name><uri>https://www.reddit.com/user/Mediocre-Card8046</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;i have built a Langchain RAG app with a local model and now want to be able to run it on a Laptop. I am using a quantized Mixtral Model (Q5_0) and for this I want to conntect 2 GeoForce RTX 4090 to my laptop. As I am a newby (and nooby) in the Hardware topic, is it even possible to connect 2 RTX 4090 to a more or less &amp;quot;normal&amp;quot; Laptop?&lt;/p&gt; &lt;p&gt;The use case would be that the customer tries the (local) application on a standalone device and if he is happy with it he buys more Hardware to host it for production.&lt;/p&gt; &lt;p&gt;At the moment I am running everything on my Macbook with 64GB RAM but I need a solution for a customer with a Windows PC.&lt;/p&gt; &lt;p&gt;One other option would be that the customer just buys a Macbook, but the 2 GeForece RTX 4090 would be a better investment I think because these could further be used for a prodcution setting.&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;Thanks for you suggestions!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Mediocre-Card8046&quot;&gt; /u/Mediocre-Card8046 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bji0np/is_it_possible_to_connect_2_geforce_rtx_4090_to_a/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bji0np/is_it_possible_to_connect_2_geforce_rtx_4090_to_a/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bji0np</id><link href="https://www.reddit.com/r/LangChain/comments/1bji0np/is_it_possible_to_connect_2_geforce_rtx_4090_to_a/" /><updated>2024-03-20T16:44:18+00:00</updated><published>2024-03-20T16:44:18+00:00</published><title>is it possible to connect 2 GeForce RTX 4090 to a Laptop?</title></entry><entry><author><name>/u/stargazer1Q84</name><uri>https://www.reddit.com/user/stargazer1Q84</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi everyone, I hope it is fine to post questions here. &lt;/p&gt; &lt;p&gt;I am just getting started with output-parsers and I&amp;#39;m impressed with their usefulness when they work properly. I have, however, run into a case where every now and then, a chain returns an error that seems to be related to the JsonOutputParser that I use, as indicated by the following (condensed) error message:&lt;/p&gt; &lt;p&gt;&lt;code&gt;JSONDecodeError&lt;/code&gt;&lt;br/&gt; &lt;code&gt;JsonOutputParser.parse_result(self, result, partial)&lt;/code&gt;&lt;br/&gt; &lt;code&gt;156 # Parse the JSON string into a Python dictionary&lt;/code&gt;&lt;br/&gt; &lt;code&gt;--&amp;gt; 157 parsed = parser(json_str)&lt;/code&gt;&lt;br/&gt; &lt;code&gt;159 return parsed&lt;/code&gt;&lt;br/&gt; &lt;code&gt;122 # If we got here, we ran out of characters to remove&lt;/code&gt;&lt;br/&gt; &lt;code&gt;123 # and still couldn&amp;#39;t parse the string as JSON, so return the parse error&lt;/code&gt;&lt;br/&gt; &lt;code&gt;124 # for the original string.&lt;/code&gt;&lt;br/&gt; &lt;code&gt;--&amp;gt; 125 return json.loads(s, strict=strict)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;According to &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/17hep0o/comment/k6na6nd/?utm_source=share&amp;amp;utm_medium=web3x&amp;amp;utm_name=web3xcss&amp;amp;utm_term=1&amp;amp;utm_content=share_button&quot;&gt;this post here&lt;/a&gt; this could be related to there not being &amp;quot;enough tokens left to fully generate my output&amp;quot;, which seems to be in line with the error message above:&lt;/p&gt; &lt;p&gt;&amp;gt;&lt;code&gt;122 # If we got here, we ran out of characters to remove&lt;/code&gt; &lt;/p&gt; &lt;p&gt;although I am not fully sure what that means or how it can be fixed. &lt;/p&gt; &lt;p&gt;Has anybody encountered this problem before and could offer some guidance? I must admit that I&amp;#39;m feeling kind of stumped, especially since the error can&amp;#39;t be reproduced reliably and only occurs every other time I run my script. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/stargazer1Q84&quot;&gt; /u/stargazer1Q84 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bjdjk0/understanding_jsondecodeerror_when_using/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bjdjk0/understanding_jsondecodeerror_when_using/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bjdjk0</id><link href="https://www.reddit.com/r/LangChain/comments/1bjdjk0/understanding_jsondecodeerror_when_using/" /><updated>2024-03-20T13:33:06+00:00</updated><published>2024-03-20T13:33:06+00:00</published><title>Understanding JSONDecodeError when using JsonOutputParser</title></entry><entry><author><name>/u/mehul_gupta1997</name><uri>https://www.reddit.com/user/mehul_gupta1997</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/mehul_gupta1997&quot;&gt; /u/mehul_gupta1997 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;/r/ArtificialInteligence/comments/1bjbd36/multiagent_conversation_using_crewai_genai/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bjbe0j/multiagent_conversation_using_crewai_genai/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bjbe0j</id><link href="https://www.reddit.com/r/LangChain/comments/1bjbe0j/multiagent_conversation_using_crewai_genai/" /><updated>2024-03-20T11:40:44+00:00</updated><published>2024-03-20T11:40:44+00:00</published><title>Multi-Agent Conversation using CrewAI (GenAI)</title></entry><entry><author><name>/u/Thegunsmith98</name><uri>https://www.reddit.com/user/Thegunsmith98</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I am trying to build an application that takes templates of things like a cover letter , resume , medical research document. Now based on this template I will upload another document containing information to be used to fill the template. However after the model generates a new document following the template and information , the whole alignment of the document is wrong and it doesnt bold the necessary parts. Is there any way to ensure that a model can follow the format for a template like center allignment , bolding the headers , etc. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Thegunsmith98&quot;&gt; /u/Thegunsmith98 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bj6xl3/langchain_usage_doubt_for_document_generation/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bj6xl3/langchain_usage_doubt_for_document_generation/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bj6xl3</id><link href="https://www.reddit.com/r/LangChain/comments/1bj6xl3/langchain_usage_doubt_for_document_generation/" /><updated>2024-03-20T06:25:51+00:00</updated><published>2024-03-20T06:25:51+00:00</published><title>Langchain Usage doubt for document generation</title></entry><entry><author><name>/u/Mediocre-Card8046</name><uri>https://www.reddit.com/user/Mediocre-Card8046</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;when thinking about RAG evaluation, everybody talks about RAGAS. It is generally nice to have a framework where you can evaluate your RAG workflows. However I tried it with an own local LLM as well as with the gpt-4-turbo model and the results really are not reliable. &lt;/p&gt; &lt;p&gt;I adapted prompts to my language (german) and with my test dataset, the answer_correctness, answer_relevancy scores are often times very low, zero or NaN, even if the answer is completely correct. &lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;Does anyone have similar experiences? &lt;/p&gt; &lt;p&gt;With my experience, I am not feeling comfortable using ragas as results differ heavenly from run to run, so all the evaluation doesn&amp;#39;t really help me. &lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Mediocre-Card8046&quot;&gt; /u/Mediocre-Card8046 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bijg75/why_is_everyone_using_ragas_for_rag_evaluation/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bijg75/why_is_everyone_using_ragas_for_rag_evaluation/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bijg75</id><link href="https://www.reddit.com/r/LangChain/comments/1bijg75/why_is_everyone_using_ragas_for_rag_evaluation/" /><updated>2024-03-19T12:49:43+00:00</updated><published>2024-03-19T12:49:43+00:00</published><title>Why is everyone using RAGAS for RAG evaluation? For me it looks very unreliable</title></entry><entry><author><name>/u/PreparationSad1717</name><uri>https://www.reddit.com/user/PreparationSad1717</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi folks, If you are building with LangChain and want to turn your work into sharable chat app in minutes and in pure python, then join the waitlist &lt;a href=&quot;https://cycls.typeform.com/waitlist&quot;&gt;https://cycls.typeform.com/waitlist&lt;/a&gt; .&lt;br/&gt; We&amp;#39;re gearing up for a release in just a few weeks. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/PreparationSad1717&quot;&gt; /u/PreparationSad1717 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bj3qjq/want_to_turn_your_work_into_sharable_chat_app_in/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bj3qjq/want_to_turn_your_work_into_sharable_chat_app_in/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bj3qjq</id><link href="https://www.reddit.com/r/LangChain/comments/1bj3qjq/want_to_turn_your_work_into_sharable_chat_app_in/" /><updated>2024-03-20T03:13:12+00:00</updated><published>2024-03-20T03:13:12+00:00</published><title>Want to turn your work into sharable chat app in minutes ?</title></entry><entry><author><name>/u/TheBroWhoLifts</name><uri>https://www.reddit.com/user/TheBroWhoLifts</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I work in education and envision a way of using RAG to help my students develop their writing. &lt;/p&gt; &lt;p&gt;My vision is to have students keep a digital portfolio of all their writing over the course of the semester or the year. I&amp;#39;d like to then use a RAG/LLM setup to provide students with feedback regarding their writing development over the course of the year. Ultimately, I&amp;#39;d like to load all of their writing into a RAG for my own analysis. I would be running this on an local LM Studio LLM for student privacy. &lt;/p&gt; &lt;p&gt;Is Langchain an appropriate tool to achieve this? Would I be able to set it up to analyze thousands of pages of student work? And can I design it so that it is a conversational interaction with a history window? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/TheBroWhoLifts&quot;&gt; /u/TheBroWhoLifts &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bimg0l/high_school_teacher_here_with_a_use_case_question/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bimg0l/high_school_teacher_here_with_a_use_case_question/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bimg0l</id><link href="https://www.reddit.com/r/LangChain/comments/1bimg0l/high_school_teacher_here_with_a_use_case_question/" /><updated>2024-03-19T15:06:18+00:00</updated><published>2024-03-19T15:06:18+00:00</published><title>High school teacher here with a use case question for the educational setting.</title></entry><entry><author><name>/u/logandarknight</name><uri>https://www.reddit.com/user/logandarknight</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hello! I have a question I haven’t been able to find online, and was hoping someone could explain it to me. &lt;/p&gt; &lt;p&gt;I need to build a “chatbot” where the user asks questions about history, and the agent must reply with the correct answer. Here’s the thing: The context needs to be fed from certain books, and some pdfs.&lt;/p&gt; &lt;p&gt;Why’s the best way to do this? The objective is to outperform in replying correctly to OpenAI models. Or, which model would be the best (or combination of model + RAG) to get the best result?&lt;/p&gt; &lt;p&gt;Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/logandarknight&quot;&gt; /u/logandarknight &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1biwtil/use_case_doubt/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1biwtil/use_case_doubt/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1biwtil</id><link href="https://www.reddit.com/r/LangChain/comments/1biwtil/use_case_doubt/" /><updated>2024-03-19T22:05:22+00:00</updated><published>2024-03-19T22:05:22+00:00</published><title>Use case doubt</title></entry><entry><author><name>/u/DXVA</name><uri>https://www.reddit.com/user/DXVA</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Is there a good way to integrate LangChain with a personal LLM RESTAPI yet?&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/17v1rhv/integrating_llm_rest_api_into_a_langchain/&quot;&gt;https://www.reddit.com/r/LangChain/comments/17v1rhv/integrating_llm_rest_api_into_a_langchain/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I saw this post, but it doesn&amp;#39;t explain any of the integration with basic chains like LLMChain. There&amp;#39;re so many integrations, but nothing I see so far for interacting with your own tooling?&lt;/p&gt; &lt;p&gt;The API just has the basic response structure for a LLM, but that should just be one piece of the connection right? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/DXVA&quot;&gt; /u/DXVA &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1billcu/integration_with_restapis/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1billcu/integration_with_restapis/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1billcu</id><link href="https://www.reddit.com/r/LangChain/comments/1billcu/integration_with_restapis/" /><updated>2024-03-19T14:28:38+00:00</updated><published>2024-03-19T14:28:38+00:00</published><title>Integration with RESTAPIs?</title></entry><entry><author><name>/u/heybigeyes123</name><uri>https://www.reddit.com/user/heybigeyes123</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Where are you guys finding customers to sell your RAG products? What do thesr customers look like?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/heybigeyes123&quot;&gt; /u/heybigeyes123 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bitjgv/rag_customers/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bitjgv/rag_customers/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bitjgv</id><link href="https://www.reddit.com/r/LangChain/comments/1bitjgv/rag_customers/" /><updated>2024-03-19T19:54:58+00:00</updated><published>2024-03-19T19:54:58+00:00</published><title>RAG customers</title></entry><entry><author><name>/u/Not-That-rpg</name><uri>https://www.reddit.com/user/Not-That-rpg</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I was just changing an existing langchain workflow from using an OpenAI model to using one from Replicate.&lt;/p&gt; &lt;p&gt;This showed the value of using &lt;code&gt;langchain&lt;/code&gt; because it pretty much Just Worked to change the LLM model constructor I had used originally, and then rerun all my code (in a Jupyter Notebook).&lt;/p&gt; &lt;p&gt;But it only &amp;quot;pretty much&amp;quot; worked: in particular, when I invoked the OpenAI models, I would get an &lt;code&gt;AIMessage&lt;/code&gt; object out of the chain. When I invoke a Replicate model, I am just getting a string.&lt;/p&gt; &lt;p&gt;I imagine that this could cause issues if trying to extend a chain past the Replicate LLM to something like an Output Parser couldn&amp;#39;t it?&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Not-That-rpg&quot;&gt; /u/Not-That-rpg &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bish94/should_i_report_this_as_a_bug/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bish94/should_i_report_this_as_a_bug/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bish94</id><link href="https://www.reddit.com/r/LangChain/comments/1bish94/should_i_report_this_as_a_bug/" /><updated>2024-03-19T19:12:17+00:00</updated><published>2024-03-19T19:12:17+00:00</published><title>Should I report this as a bug?</title></entry><entry><author><name>/u/danipudani</name><uri>https://www.reddit.com/user/danipudani</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1biwioh/intro_to_langchain_full_documentation_overview/&quot;&gt; &lt;img src=&quot;https://external-preview.redd.it/sVTHLyvfg970cr9MD_72wQqkiADi53dPj4mMz7rqK4w.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=362f231282720dbda6fe4de5188bd10440805d1e&quot; alt=&quot;Intro to LangChain - Full Documentation Overview&quot; title=&quot;Intro to LangChain - Full Documentation Overview&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/danipudani&quot;&gt; /u/danipudani &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://youtu.be/dXP841pBcJw?si=w8NWHE6uv-5vzSTq&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1biwioh/intro_to_langchain_full_documentation_overview/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1biwioh</id><media:thumbnail url="https://external-preview.redd.it/sVTHLyvfg970cr9MD_72wQqkiADi53dPj4mMz7rqK4w.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=362f231282720dbda6fe4de5188bd10440805d1e" /><link href="https://www.reddit.com/r/LangChain/comments/1biwioh/intro_to_langchain_full_documentation_overview/" /><updated>2024-03-19T21:53:38+00:00</updated><published>2024-03-19T21:53:38+00:00</published><title>Intro to LangChain - Full Documentation Overview</title></entry><entry><author><name>/u/Supersam6341</name><uri>https://www.reddit.com/user/Supersam6341</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;So I am running langchain on a doc that sometimes has parts of the same answer split into 2 different sections, therefore not in the same chunk. What RAG technique can I use to handle this? I read somewhere that there is a technique where you can include the summaries of the previous chunks into the next chunk, but was not able to find it. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Supersam6341&quot;&gt; /u/Supersam6341 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bi9w3p/chunking_doesnt_have_the_full_answer/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bi9w3p/chunking_doesnt_have_the_full_answer/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bi9w3p</id><link href="https://www.reddit.com/r/LangChain/comments/1bi9w3p/chunking_doesnt_have_the_full_answer/" /><updated>2024-03-19T02:48:48+00:00</updated><published>2024-03-19T02:48:48+00:00</published><title>Chunking doesn’t have the full answer</title></entry><entry><author><name>/u/Comprehensive-Pay530</name><uri>https://www.reddit.com/user/Comprehensive-Pay530</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;So I am a lead software engineer in a SaaS startups we are exploring many use cases for implement GenAI solutions and are building most of them inhouse so we are writing a lot of prompts across various teams in product and engineering.&lt;/p&gt; &lt;p&gt;I was trying to explore some best tools for managing and testing prompts for different use cases things i am looking for : &lt;/p&gt; &lt;p&gt;Must have :&lt;br/&gt; 1. UI where PM&amp;#39;s can go and test prompts - here they should be able to test same prompt on different model and a high level overview of cost incurred across these model for the result.&lt;br/&gt; 2. SDK/api to fetch these prompts in code with versing and all for different use-cases.&lt;br/&gt; 3. Dynamic rules for A/B testing of prompts.&lt;/p&gt; &lt;p&gt;Good to have :&lt;br/&gt; Maybe if the tool helps in crafting the prompts, create nested prompts workflows (chain of prompts) , etc.&lt;/p&gt; &lt;p&gt;Basically looking for Launchdarkly type solution for prompts where you can also create dynamic rules to load different prompt feature flag them based on user persona and teams.&lt;/p&gt; &lt;p&gt;Also interested in hearing how teams are managing or doing this is there a better way or something that I am missing?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Comprehensive-Pay530&quot;&gt; /u/Comprehensive-Pay530 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bigg4l/best_prompt_testing_and_management_tools/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bigg4l/best_prompt_testing_and_management_tools/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bigg4l</id><link href="https://www.reddit.com/r/LangChain/comments/1bigg4l/best_prompt_testing_and_management_tools/" /><updated>2024-03-19T09:49:31+00:00</updated><published>2024-03-19T09:49:31+00:00</published><title>Best prompt testing and management tools</title></entry><entry><author><name>/u/UsamaHussain99</name><uri>https://www.reddit.com/user/UsamaHussain99</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I am working on a chat application in Langchain, Python. The idea is that user submits some pdf files that the chat model is trained on and then asks questions from the model regarding those documents. The embeddings are stored in Chromadb vector database. So effectively a RAG-based solution. &lt;/p&gt; &lt;p&gt;Now, both the creation and storage of embeddings are working fine and also chat is working good. However, I am storing my custom metadata to the embeddings and some ids. The code for that is given as under: &lt;/p&gt; &lt;pre&gt;&lt;code&gt;def read_docs(pdf_file): &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;pdf_loader = PyPDFLoader(pdf_file) pdf_documents = pdf_loader.load()&lt;/p&gt; &lt;p&gt;text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200) documents = text_splitter.split_documents(pdf_documents)&lt;/p&gt; &lt;p&gt;return documents&lt;/p&gt; &lt;pre&gt;&lt;code&gt;def generate_and_store_embeddings(documents, pdf_file, user_id): client = chromadb.PersistentClient(path=&amp;quot;./trained_db&amp;quot;) collection = client.get_or_create_collection(&amp;quot;PDF_Embeddings&amp;quot;, embedding_function=embedding_functions.OpenAIEmbeddingFunction(api_key=config[&amp;quot;OPENAI_API_KEY&amp;quot;], model_name=configs.EMBEDDINGS_MODEL)) now = datetime.now() #custom metadata and ids I want to store along with the embeddings for each pdf metadata = {&amp;quot;source&amp;quot;: pdf_file.filename, &amp;quot;user&amp;quot;: str(user_id), &amp;#39;created_at&amp;#39;: now.strftime(&amp;quot;%d/%m/%Y %H:%M:%S&amp;quot;)} ids = [str(uuid.uuid4()) for _ in range(len(documents))] try: vectordb = Chroma.from_documents( documents, embedding=OpenAIEmbeddings(openai_api_key=config[&amp;quot;OPENAI_API_KEY&amp;quot;], model=configs.EMBEDDINGS_MODEL), persist_directory=&amp;#39;./trained_db&amp;#39;, collection_name = collection.name, client = client, ids = ids, collection_metadata = {item: value for (item, value) in metadata.items()} ) vectordb.persist() except Exception as err: print(f&amp;quot;An error occured: {err=}, {type(err)=}&amp;quot;) return {&amp;quot;answer&amp;quot;: &amp;quot;An error occured while generating embeddings. Please check terminal for more details.&amp;quot;} return vectordb &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Now, what I want is to retrieve those ids and metadata associated with the pdf file rather than all the ids/metadata in the collection. This is so that when a user enters the pdf file to delete the embeddings of, I can retrieve the metadata and the ids of &lt;em&gt;that pdf file only&lt;/em&gt; and then delete those embeddings from the collection.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/UsamaHussain99&quot;&gt; /u/UsamaHussain99 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bifpp0/how_to_retrieve_ids_and_metadata_associated_with/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bifpp0/how_to_retrieve_ids_and_metadata_associated_with/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bifpp0</id><link href="https://www.reddit.com/r/LangChain/comments/1bifpp0/how_to_retrieve_ids_and_metadata_associated_with/" /><updated>2024-03-19T08:55:23+00:00</updated><published>2024-03-19T08:55:23+00:00</published><title>How to retrieve ids and metadata associated with embeddings of a particular file and not just for the entire collection?</title></entry><entry><author><name>/u/major_grooves</name><uri>https://www.reddit.com/user/major_grooves</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Would a data store that is capable of doing entity resolution (ER; link and deduplicate all structured data regarding an entity, each in its own graph) be useful for RAG and LLMs?&lt;/p&gt; &lt;p&gt;We recently had a bunch of people contact us asking if they could use our ER solution as a &amp;quot;source of truth&amp;quot; for LLM RAG. We don&amp;#39;t know much about LLM or RAG so have been trying to get up to speed quickly, so wanted to ask the question here - if you work on RAG do you see a use case for a fuzzy search engine for structured data (which is effectively what our solution is), where the underlying data is considered a &amp;quot;source of truth&amp;quot;?&lt;/p&gt; &lt;p&gt;Probably should mention the underlying data is deduplicated and linked (and searched) using rules based on various phonetic, similarity and distance algorithms (including Cosine). We don&amp;#39;t use vectors or embeddings in our matching, although we plan to later.&lt;/p&gt; &lt;p&gt;We are just now trying to evaluate whether we should double down on the LLM/RAG space and build a LangChain connector for our solution.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/major_grooves&quot;&gt; /u/major_grooves &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bih5kc/is_there_a_need_for_entitybased_rag/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bih5kc/is_there_a_need_for_entitybased_rag/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bih5kc</id><link href="https://www.reddit.com/r/LangChain/comments/1bih5kc/is_there_a_need_for_entitybased_rag/" /><updated>2024-03-19T10:36:17+00:00</updated><published>2024-03-19T10:36:17+00:00</published><title>Is there a need for entity-based RAG?</title></entry><entry><author><name>/u/o3omoomin</name><uri>https://www.reddit.com/user/o3omoomin</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hello, I want to implement a chatbot. This will implement a QA chatbot. There are quite a few retrievers in the docs below. What I want is a retriever that gives very accurate answers. When I need to implement a QA chatbot, what is the most popular retriever? &lt;a href=&quot;https://docs.llamaindex.ai/en/stable/module_guides/querying/retriever/retrievers.html&quot;&gt;https://docs.llamaindex.ai/en/stable/module_guides/querying/retriever/retrievers.html&lt;/a&gt; (수정됨)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/o3omoomin&quot;&gt; /u/o3omoomin &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bi8n3b/llama_index_which_retriever_has_the_most_accurate/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bi8n3b/llama_index_which_retriever_has_the_most_accurate/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bi8n3b</id><link href="https://www.reddit.com/r/LangChain/comments/1bi8n3b/llama_index_which_retriever_has_the_most_accurate/" /><updated>2024-03-19T01:48:32+00:00</updated><published>2024-03-19T01:48:32+00:00</published><title>llama index - Which retriever has the most accurate answers and best performance?</title></entry></feed>