<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/"><category term="LangChain" label="r/LangChain"/><updated>2024-07-16T19:50:03+00:00</updated><icon>https://www.redditstatic.com/icon.png/</icon><id>/r/LangChain.rss</id><link rel="self" href="https://www.reddit.com/r/LangChain.rss" type="application/atom+xml" /><link rel="alternate" href="https://www.reddit.com/r/LangChain" type="text/html" /><subtitle>LangChain is an open-source framework and developer toolkit that helps developers get LLM applications from prototype to production. It is available for Python and Javascript at https://www.langchain.com/.</subtitle><title>LangChain</title><entry><author><name>/u/zchaarm</name><uri>https://www.reddit.com/user/zchaarm</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;A place for members of &lt;a href=&quot;/r/LangChain&quot;&gt;r/LangChain&lt;/a&gt; to chat with each other&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/zchaarm&quot;&gt; /u/zchaarm &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_10ljho9</id><link href="https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/" /><updated>2023-01-26T04:33:05+00:00</updated><published>2023-01-26T04:33:05+00:00</published><title>r/LangChain Lounge</title></entry><entry><author><name>/u/mehul_gupta1997</name><uri>https://www.reddit.com/user/mehul_gupta1997</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;GraphRAG is an advanced RAG system that uses Knowledge Graphs instead of Vector DBs improving retrieval. Check out the implementation using GraphQAChain in this video : &lt;a href=&quot;https://youtu.be/wZHkeon42Aw&quot;&gt;https://youtu.be/wZHkeon42Aw&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/mehul_gupta1997&quot;&gt; /u/mehul_gupta1997 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e4rkrd/graphrag_using_langchain/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e4rkrd/graphrag_using_langchain/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1e4rkrd</id><link href="https://www.reddit.com/r/LangChain/comments/1e4rkrd/graphrag_using_langchain/" /><updated>2024-07-16T15:29:50+00:00</updated><published>2024-07-16T15:29:50+00:00</published><title>GraphRAG using LangChain</title></entry><entry><author><name>/u/Uiqueblhats</name><uri>https://www.reddit.com/user/Uiqueblhats</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e4k0is/gptinstagram_instagram_viral_posts_with_user_own/&quot;&gt; &lt;img src=&quot;https://external-preview.redd.it/QUer_6VdQ0MXHShyaUUxe_WIEaN1O-pzO2bByFl5EbM.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0c678ab30f1b0eeabc82c953f3d8b8f8d5ef1d78&quot; alt=&quot;GPT-Instagram : Instagram Viral Posts with user own personality&quot; title=&quot;GPT-Instagram : Instagram Viral Posts with user own personality&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;As a weekend project created a Multi Agent AI app in Next.js, LangChain.js &amp;amp; LangGraph.js to simulate a Marketing department to recommend Instagram Viral Posts with user own personality.&lt;/p&gt; &lt;p&gt;If anyone interested to try or look at code: &lt;a href=&quot;https://github.com/MODSetter/gpt-instagram&quot;&gt;https://github.com/MODSetter/gpt-instagram&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://reddit.com/link/1e4k0is/video/ig7wjanujucd1/player&quot;&gt;https://reddit.com/link/1e4k0is/video/ig7wjanujucd1/player&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Uiqueblhats&quot;&gt; /u/Uiqueblhats &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e4k0is/gptinstagram_instagram_viral_posts_with_user_own/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e4k0is/gptinstagram_instagram_viral_posts_with_user_own/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1e4k0is</id><media:thumbnail url="https://external-preview.redd.it/QUer_6VdQ0MXHShyaUUxe_WIEaN1O-pzO2bByFl5EbM.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=0c678ab30f1b0eeabc82c953f3d8b8f8d5ef1d78" /><link href="https://www.reddit.com/r/LangChain/comments/1e4k0is/gptinstagram_instagram_viral_posts_with_user_own/" /><updated>2024-07-16T09:07:19+00:00</updated><published>2024-07-16T09:07:19+00:00</published><title>GPT-Instagram : Instagram Viral Posts with user own personality</title></entry><entry><author><name>/u/TableauforViz</name><uri>https://www.reddit.com/user/TableauforViz</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;In my use case, the most important thing is accuracy, of retrieved documents from them&lt;/p&gt; &lt;p&gt;I&amp;#39;m going to create vectorstore of my codebase, so when codes get updated, I have to update those in my vectorstore periodically (not all codes will get updated)&lt;/p&gt; &lt;p&gt;Keeping these two things in mind, which one should I go with?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/TableauforViz&quot;&gt; /u/TableauforViz &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e4r8an/which_vectorstore_should_i_choose/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e4r8an/which_vectorstore_should_i_choose/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1e4r8an</id><link href="https://www.reddit.com/r/LangChain/comments/1e4r8an/which_vectorstore_should_i_choose/" /><updated>2024-07-16T15:15:54+00:00</updated><published>2024-07-16T15:15:54+00:00</published><title>Which vectorstore should I choose?</title></entry><entry><author><name>/u/bferreira85</name><uri>https://www.reddit.com/user/bferreira85</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi! As mentioned before, I have built a langgraph with a supervisor agent and two specialised agents. My question is: what are the best practices for having some of these agents working in multiple steps? For example: one of the agents will receive the user input and first present to the user what the agent is about to do and after the user confirms it then the agent will finish the work.&lt;br/&gt; My question is: how do I get the same agent to do both of these steps? Should I add system prompt to the agent on each step? (For example: adding something like &amp;quot;In this step now you must do X and display Y information to the user&amp;quot;). The problem here is that if there are multiple back and forth interactions between agent and user then the system prompt will become long and confusing.&lt;br/&gt; Other way could be saying to agent &amp;quot;when you are on step 1 you should focus on this, when you are on step 2 you should focus on that&amp;quot;.&lt;/p&gt; &lt;p&gt;Do any of you have experience with this kind of scenario?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/bferreira85&quot;&gt; /u/bferreira85 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e4l74f/langgraph_best_practices_for_multiple_steps_graph/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e4l74f/langgraph_best_practices_for_multiple_steps_graph/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1e4l74f</id><link href="https://www.reddit.com/r/LangChain/comments/1e4l74f/langgraph_best_practices_for_multiple_steps_graph/" /><updated>2024-07-16T10:25:57+00:00</updated><published>2024-07-16T10:25:57+00:00</published><title>Langgraph: best practices for multiple steps graph</title></entry><entry><author><name>/u/Disneyskidney</name><uri>https://www.reddit.com/user/Disneyskidney</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Was wondering what tools exist for generating automatic insights from data. For example you feed in a large data set and based on the context of the data set a genAI tool is able to tell you insights positive or negative that are useful. Things like &amp;quot;Revenue has grown by 10% since last month&amp;quot; or &amp;quot;Customer X usage has dropped since __&amp;quot;. I&amp;#39;ve found some generative BI tools online but my use case requires something that&amp;#39;s more of a dev tool. Also, open to hearing about ideas of how to do something like this from scratch.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Disneyskidney&quot;&gt; /u/Disneyskidney &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e4xjhu/genai_for_automatic_insights_from_data/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e4xjhu/genai_for_automatic_insights_from_data/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1e4xjhu</id><link href="https://www.reddit.com/r/LangChain/comments/1e4xjhu/genai_for_automatic_insights_from_data/" /><updated>2024-07-16T19:30:15+00:00</updated><published>2024-07-16T19:30:15+00:00</published><title>GenAI for automatic insights from data</title></entry><entry><author><name>/u/not_bsb7838</name><uri>https://www.reddit.com/user/not_bsb7838</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;Is there a reliable agentic tool on the market that can operate independently and effectively? So far, I&amp;#39;ve tried &lt;strong&gt;CrewAI&lt;/strong&gt; and have explored &lt;strong&gt;LangGraph&lt;/strong&gt;, though I haven&amp;#39;t tested it yet. Despite adjusting the max iterations, these tools often take a lot of time and work best with OpenAI. However, I want to use them with AWS Bedrock models, specifically Mistral or Claude 3.&lt;/p&gt; &lt;p&gt;My primary use case is an internal application where the agent needs to automatically decide whether to use a specific agent, perform RAG, or search the web for information. This must be done without compromising our confidential company data by sending it to external Search APIs (like Google or DuckDuckGo) and without taking an excessive amount of time to provide answers.&lt;/p&gt; &lt;p&gt;I&amp;#39;d really appreciate any recommendations or advice you can offer.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/not_bsb7838&quot;&gt; /u/not_bsb7838 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e4s6xt/is_there_any_reliable_agentic_tool/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e4s6xt/is_there_any_reliable_agentic_tool/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1e4s6xt</id><link href="https://www.reddit.com/r/LangChain/comments/1e4s6xt/is_there_any_reliable_agentic_tool/" /><updated>2024-07-16T15:55:08+00:00</updated><published>2024-07-16T15:55:08+00:00</published><title>Is There Any Reliable Agentic Tool?</title></entry><entry><author><name>/u/PhotoAcceptable3563</name><uri>https://www.reddit.com/user/PhotoAcceptable3563</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;I need some clarifications. In my use case the first step requires an LLM to identify and split different segment of the input text / document. Then, for each of the segments I have a linear flow to follow (extract info, call agents, ...). Finally I have to collect all the outputs.&lt;/p&gt; &lt;p&gt;I am unsure how to achieve the &amp;quot;for loop&amp;quot; (if possible). Instead of an &lt;code&gt;add_edge&lt;/code&gt;, I&amp;#39;d need an add edges&lt;/p&gt; &lt;pre&gt;&lt;code&gt;workflow.add_node(&amp;quot;split&amp;quot;, split) workflow.add_node(&amp;quot;extract&amp;quot;, extract) workflow.add_node(&amp;quot;collect&amp;quot;, collect) workflow.set_entry_point(&amp;quot;split&amp;quot;) # after split I get an array of chunks workflow.add_edges(&amp;quot;split&amp;quot;, &amp;quot;extract&amp;quot;) # for each chunk do some custom logic workflow.collect_edges(&amp;quot;extract&amp;quot;, &amp;quot;collect&amp;quot;) # collect everything &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/PhotoAcceptable3563&quot;&gt; /u/PhotoAcceptable3563 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e4rcha/graph_with_a_for_loop/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e4rcha/graph_with_a_for_loop/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1e4rcha</id><link href="https://www.reddit.com/r/LangChain/comments/1e4rcha/graph_with_a_for_loop/" /><updated>2024-07-16T15:20:32+00:00</updated><published>2024-07-16T15:20:32+00:00</published><title>Graph with a for loop</title></entry><entry><author><name>/u/Comfortable_Dog5217</name><uri>https://www.reddit.com/user/Comfortable_Dog5217</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Are there anyone who has successfully done langgraph with human in the loop in production? How does that work? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Comfortable_Dog5217&quot;&gt; /u/Comfortable_Dog5217 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e4hays/langgraph_human_in_the_loop_in_production/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e4hays/langgraph_human_in_the_loop_in_production/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1e4hays</id><link href="https://www.reddit.com/r/LangChain/comments/1e4hays/langgraph_human_in_the_loop_in_production/" /><updated>2024-07-16T06:05:52+00:00</updated><published>2024-07-16T06:05:52+00:00</published><title>Langgraph Human in the loop in Production</title></entry><entry><author><name>/u/andyeverything</name><uri>https://www.reddit.com/user/andyeverything</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hello dear community,&lt;/p&gt; &lt;p&gt;I am currently familiarising myself with LLM, especially RAG. I have an idea for a Q&amp;amp;A ChatBot and would like to learn how to build one.&lt;/p&gt; &lt;p&gt;I would like to use an Excel spreadsheet with three columns as a database. The first column is a typical question. The second column contains tags that describe the question. The third column contains an answer to the question.&lt;/p&gt; &lt;p&gt;When the user asks a question, a semantic search should first be performed using the first and second columns. This will find the top of the most appropriate questions in a cell. The answers to these questions in the third column are provided to the LLM as context.&lt;/p&gt; &lt;p&gt;It is important to me that the structured properties of the table are fully exploited. Conventional RAGs return a fixed chunk size, even if it spans cells. However, I only want to return the cell that is relevant to the question.&lt;/p&gt; &lt;p&gt;My question is, what is the best way to implement this in LangChain? Does it make sense to store the questions and tags as metadata? Can I do a semantic search on the metadata and give the cell with the corresponding answer as context to the LLM? With the ulterior motive of returning only the relevant context? I have also thought about a self-query retriever. However, this tends to use keywords for filtering, not quite what I am looking for. Or am I on the wrong track?&lt;/p&gt; &lt;p&gt;Kind regards&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/andyeverything&quot;&gt; /u/andyeverything &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e4pqgn/qa_rag_over_tabular_data/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e4pqgn/qa_rag_over_tabular_data/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1e4pqgn</id><link href="https://www.reddit.com/r/LangChain/comments/1e4pqgn/qa_rag_over_tabular_data/" /><updated>2024-07-16T14:14:43+00:00</updated><published>2024-07-16T14:14:43+00:00</published><title>Q&amp;A RAG over tabular data</title></entry><entry><author><name>/u/thedabking123</name><uri>https://www.reddit.com/user/thedabking123</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;ve set up a dict of main and sub questions(the answer for each feeds into the next) that build on an initial user request. &lt;/p&gt; &lt;p&gt;I set up a RAG agent workflow on langgraph and need to debug the overall conversational thread by reading through what is generated for each question.&lt;/p&gt; &lt;p&gt;I know I can extract the current state from agent_executor.stream() but how do I get every iteration for every question asked? Is the only way to set up a for loop and print/ save the streamed outputs each time? What does the llm get at each turn in the conversation? Shouldn&amp;#39;t it be fed the full history?&lt;/p&gt; &lt;p&gt;Also any way to summarize and store a thread for downstream agents to absorb? If so any way to surface that summary for user inspection?&lt;/p&gt; &lt;p&gt;Sorry if this is basic- still learning.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/thedabking123&quot;&gt; /u/thedabking123 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e4masy/new_here_how_do_i_surface_and_save_historical/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e4masy/new_here_how_do_i_surface_and_save_historical/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1e4masy</id><link href="https://www.reddit.com/r/LangChain/comments/1e4masy/new_here_how_do_i_surface_and_save_historical/" /><updated>2024-07-16T11:31:52+00:00</updated><published>2024-07-16T11:31:52+00:00</published><title>New here - how do I surface and save historical threads of agent actions and conversations in langgraoh</title></entry><entry><author><name>/u/neilkatz</name><uri>https://www.reddit.com/user/neilkatz</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey Everyone,&lt;/p&gt; &lt;p&gt;I&amp;#39;m curious what the group thinks are the biggest pain points for devs getting started with RAG? My list would be: &lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;hallucination&lt;/strong&gt;: especially with complex docs&lt;/li&gt; &lt;li&gt;&lt;strong&gt;eval&lt;/strong&gt;: there are tools to score completions vs retrievals, but what about the rest of the RAG pipeline where the problems actually occur. &lt;/li&gt; &lt;li&gt;&lt;strong&gt;complexity:&lt;/strong&gt; many pieces of the pipeline to master (parse, extract, convert to LLM friendly data, chunk, embed, create metadata for context, search, rerank, etc) and lots of theories on best approach to each one. &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;What&amp;#39;s everyone else dealing with?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/neilkatz&quot;&gt; /u/neilkatz &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e3ygh6/biggest_rag_hurdles_for_beginners/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e3ygh6/biggest_rag_hurdles_for_beginners/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1e3ygh6</id><link href="https://www.reddit.com/r/LangChain/comments/1e3ygh6/biggest_rag_hurdles_for_beginners/" /><updated>2024-07-15T16:01:08+00:00</updated><published>2024-07-15T16:01:08+00:00</published><title>Biggest RAG Hurdles for Beginners?</title></entry><entry><author><name>/u/Big_Barracuda_6753</name><uri>https://www.reddit.com/user/Big_Barracuda_6753</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;pre&gt;&lt;code&gt; if prompt := st.chat_input(&amp;quot;Hey, What&amp;#39;s up?&amp;quot;): if len(pc.list_indexes()) == 0: st.error(&amp;quot;Please upload some files first!&amp;quot;) else: st.session_state.messages.append({&amp;quot;role&amp;quot;: &amp;quot;user&amp;quot;, &amp;quot;content&amp;quot;: prompt}) contextualize_q_system_prompt = ( &amp;quot;Given a chat history and the latest user question &amp;quot; &amp;quot;which might reference context in the chat history, &amp;quot; &amp;quot;formulate a standalone question which can be understood &amp;quot; &amp;quot;without the chat history. Do NOT answer the question, &amp;quot; &amp;quot;just reformulate it if needed and otherwise return it as is.&amp;quot; ) contextualize_q_prompt = ChatPromptTemplate.from_messages( [ (&amp;quot;system&amp;quot;, contextualize_q_system_prompt), MessagesPlaceholder(&amp;quot;chat_history&amp;quot;), (&amp;quot;human&amp;quot;, &amp;quot;{input}&amp;quot;), ] ) # chunks ( 5 ) compression_retriever = reRanker() history_aware_retriever = create_history_aware_retriever( llm, compression_retriever, contextualize_q_prompt ) system_prompt = ( &amp;quot;You are an assistant for question-answering tasks. &amp;quot; &amp;quot;Use the following pieces of retrieved context to answer &amp;quot; &amp;quot;the question. If you don&amp;#39;t know the answer, say that you &amp;quot; &amp;quot;don&amp;#39;t know.&amp;quot; &amp;quot;\n\n&amp;quot; &amp;quot;{context}&amp;quot; ) chatPrompt = ChatPromptTemplate.from_messages( [ (&amp;quot;system&amp;quot;, system_prompt), MessagesPlaceholder(&amp;quot;chat_history&amp;quot;), (&amp;quot;human&amp;quot;, &amp;quot;{input}&amp;quot;), ] ) question_answer_chain = create_stuff_documents_chain(llm, chatPrompt) rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain) conversational_rag_chain = RunnableWithMessageHistory( rag_chain, get_session_history, input_messages_key=&amp;quot;input&amp;quot;, history_messages_key=&amp;quot;chat_history&amp;quot;, output_messages_key=&amp;quot;answer&amp;quot;, ) response = conversational_rag_chain.invoke( input={&amp;quot;input&amp;quot;: prompt}, config={&amp;#39;configurable&amp;#39;: {&amp;#39;session_id&amp;#39;: &amp;#39;hdf23me23edewDFSDMS&amp;#39;}} ) print(&amp;quot;response:&amp;quot;, response[&amp;quot;answer&amp;quot;]) st.session_state.messages.append({&amp;quot;role&amp;quot;: &amp;quot;assistant&amp;quot;, &amp;quot;content&amp;quot;: response[&amp;quot;answer&amp;quot;]}) # Display updated messages for message in st.session_state.messages: with st.chat_message(message[&amp;quot;role&amp;quot;]): st.markdown(message[&amp;quot;content&amp;quot;]) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;How do I output my response as a stream using Langchain&amp;#39;s create_retrieval_chain ?&lt;br/&gt; Currently , I get my response when all of it is generated ( at once ) , but I want streaming feature instead .&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Big_Barracuda_6753&quot;&gt; /u/Big_Barracuda_6753 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e4lnd5/how_do_i_stream_my_output_using/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e4lnd5/how_do_i_stream_my_output_using/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1e4lnd5</id><link href="https://www.reddit.com/r/LangChain/comments/1e4lnd5/how_do_i_stream_my_output_using/" /><updated>2024-07-16T10:54:22+00:00</updated><published>2024-07-16T10:54:22+00:00</published><title>How do I stream my output using ?</title></entry><entry><author><name>/u/MagentaSpark</name><uri>https://www.reddit.com/user/MagentaSpark</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;m using supervisor graph and various agent graphs. How do pass state within nodes of child graph and parent graph? I couldn&amp;#39;t get what &lt;a href=&quot;https://langchain-ai.github.io/langgraph/how-tos/subgraph/&quot;&gt;documentation&lt;/a&gt; was trying to say.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/MagentaSpark&quot;&gt; /u/MagentaSpark &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e4l92y/how_to_deal_with_multiple_states/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e4l92y/how_to_deal_with_multiple_states/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1e4l92y</id><link href="https://www.reddit.com/r/LangChain/comments/1e4l92y/how_to_deal_with_multiple_states/" /><updated>2024-07-16T10:29:28+00:00</updated><published>2024-07-16T10:29:28+00:00</published><title>How to deal with multiple states?</title></entry><entry><author><name>/u/Babe_My_Name_Is_Hung</name><uri>https://www.reddit.com/user/Babe_My_Name_Is_Hung</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi everyone, I have been reading around about Graph RAG lately. I don‚Äôt quite understand how the retriever searches for ‚Äúrelevant entities‚Äù? I have thought of exact full text search, but they would not be quite effective when the entities can be (and will be!) ambiguous ( e.g. two people with the same name, or same people with different names, etc.). Or maybe semantic search is utilized here? If so, I don‚Äôt think it would be efficient for a large graph with many entities. Really appreciate your help! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Babe_My_Name_Is_Hung&quot;&gt; /u/Babe_My_Name_Is_Hung &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e4c1j5/how_do_graph_rag_search_for_relevant_nodes/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e4c1j5/how_do_graph_rag_search_for_relevant_nodes/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1e4c1j5</id><link href="https://www.reddit.com/r/LangChain/comments/1e4c1j5/how_do_graph_rag_search_for_relevant_nodes/" /><updated>2024-07-16T01:21:32+00:00</updated><published>2024-07-16T01:21:32+00:00</published><title>How do Graph RAG search for relevant nodes?</title></entry><entry><author><name>/u/fpgsgamer</name><uri>https://www.reddit.com/user/fpgsgamer</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;As of now I have a chatbot (OpenAI embeddings &amp;amp; chat model) that can take in text/pdf files and answer questions about them using embeddings and a vectorDB. I was wondering if it would be plausible to embed an image into the vectorDB, and use a multi modal LLM to be able to answer questions about said picture. I&amp;#39;ve been looking at other approaches (generating text summaries of image and embedding that/sending the image straight to a multimodal LLM with the prompt) but I like the idea of embedding images into my vectorDB, and I don&amp;#39;t really know the pros/cons between these methods. Any help is appreciated !&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/fpgsgamer&quot;&gt; /u/fpgsgamer &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e4gvl1/how_should_i_set_up_my_multimodal_chatbot/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e4gvl1/how_should_i_set_up_my_multimodal_chatbot/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1e4gvl1</id><link href="https://www.reddit.com/r/LangChain/comments/1e4gvl1/how_should_i_set_up_my_multimodal_chatbot/" /><updated>2024-07-16T05:39:01+00:00</updated><published>2024-07-16T05:39:01+00:00</published><title>How should I set up my multimodal chatbot?</title></entry><entry><author><name>/u/jeffrey-0711</name><uri>https://www.reddit.com/user/jeffrey-0711</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e3q2lb/experiment_good_chunking_will_lead_you_to_the/&quot;&gt; &lt;img src=&quot;https://b.thumbs.redditmedia.com/i3T3Wy5JfGyG-uY9Ri9xzDXuU_jQxYvBmFZVHH3aGkU.jpg&quot; alt=&quot;[Experiment] Good chunking will lead you to the better RAG performance&quot; title=&quot;[Experiment] Good chunking will lead you to the better RAG performance&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Yes, chunking document in RAG is important. But, how much? I got curious and ran a experiment to see how chunking method will effect to the RAG performance.&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/krnc1j0fancd1.png?width=2894&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a045b1d5ed51e6d721adc1112805e2bea5968dae&quot;&gt;The diagram of the experiment&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I selected total five chunking method. I ran exact same RAG pipeline with different chunking method and measure the answer quality. It was Korean document with 40 questions.&lt;/p&gt; &lt;h1&gt;The five chunking method&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;No Chunk : PDF page itself is the chunk.&lt;br/&gt;&lt;/li&gt; &lt;li&gt;Token Splitter&lt;br/&gt;&lt;/li&gt; &lt;li&gt;Recursive Splitter&lt;br/&gt;&lt;/li&gt; &lt;li&gt;Window Splitter : comes from &lt;a href=&quot;https://docs.llamaindex.ai/en/stable/examples/node_postprocessor/MetadataReplacementDemo/&quot;&gt;LlamaIndex&lt;/a&gt; : passage merge after retrieval&lt;br/&gt;&lt;/li&gt; &lt;li&gt;Semantic Splitter : used openai embedding&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I used G-eval as a LLM-eval metric. And I used &lt;a href=&quot;https://github.com/Marker-Inc-Korea/AutoRAG&quot;&gt;AutoRAG&lt;/a&gt; to quickly run the RAG experiment. (The G-eval range is 1 to 5)&lt;/p&gt; &lt;p&gt;Plus, since the G-eval is not perfect metric, I checked all question and answers manually and measure the factualness of the RAG answers. &lt;/p&gt; &lt;p&gt;If the answer was perfect, I gave 1. If the RAG system said &amp;quot;don&amp;#39;t know&amp;#39;, I gave 0. If there was hallucination, I gave -1. And the answer might be vague, then I gave 0.5.&lt;/p&gt; &lt;h1&gt;Result&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align=&quot;left&quot;&gt;Metric Result&lt;/th&gt; &lt;th align=&quot;left&quot;&gt;No Chunk&lt;/th&gt; &lt;th align=&quot;left&quot;&gt;Token Splitter&lt;/th&gt; &lt;th align=&quot;left&quot;&gt;Recursive Splitter&lt;/th&gt; &lt;th align=&quot;left&quot;&gt;Window Splitter&lt;/th&gt; &lt;th align=&quot;left&quot;&gt;Semantic Splitter&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align=&quot;left&quot;&gt;G-eval&lt;/td&gt; &lt;td align=&quot;left&quot;&gt;2.4706&lt;/td&gt; &lt;td align=&quot;left&quot;&gt;3.3529&lt;/td&gt; &lt;td align=&quot;left&quot;&gt;3.1176&lt;/td&gt; &lt;td align=&quot;left&quot;&gt;3.3529&lt;/td&gt; &lt;td align=&quot;left&quot;&gt;&lt;strong&gt;3.7647&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align=&quot;left&quot;&gt;Human Eval&lt;/td&gt; &lt;td align=&quot;left&quot;&gt;0.3529&lt;/td&gt; &lt;td align=&quot;left&quot;&gt;0.6471&lt;/td&gt; &lt;td align=&quot;left&quot;&gt;0.4706&lt;/td&gt; &lt;td align=&quot;left&quot;&gt;0.5882&lt;/td&gt; &lt;td align=&quot;left&quot;&gt;&lt;strong&gt;0.7941&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Both G-eval and human eval, the semantic splitter performs best. The score is quite different, and its impact is quite big. Find the right chunking is essential to build great RAG system.&lt;/p&gt; &lt;h1&gt;Limitation&lt;/h1&gt; &lt;p&gt;Since I used only 40 questions, and human eval was performed only on the 17 questions, the statistic needs to be larger for more precise result. &lt;/p&gt; &lt;hr/&gt; &lt;p&gt;This experiment took only a few hours, thanks to the &lt;a href=&quot;https://github.com/Marker-Inc-Korea/AutoRAG/&quot;&gt;AutoRAG&lt;/a&gt;. You can optimize and run the experiment easily using it. It is open-source project so feel free to use it on your RAG project.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/jeffrey-0711&quot;&gt; /u/jeffrey-0711 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e3q2lb/experiment_good_chunking_will_lead_you_to_the/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e3q2lb/experiment_good_chunking_will_lead_you_to_the/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1e3q2lb</id><media:thumbnail url="https://b.thumbs.redditmedia.com/i3T3Wy5JfGyG-uY9Ri9xzDXuU_jQxYvBmFZVHH3aGkU.jpg" /><link href="https://www.reddit.com/r/LangChain/comments/1e3q2lb/experiment_good_chunking_will_lead_you_to_the/" /><updated>2024-07-15T08:56:25+00:00</updated><published>2024-07-15T08:56:25+00:00</published><title>[Experiment] Good chunking will lead you to the better RAG performance</title></entry><entry><author><name>/u/angularlicious</name><uri>https://www.reddit.com/user/angularlicious</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;This LangChain tool is a little naive- it is just a text splitter with charLength and overlap size. Does anyone know of a compatible splitter that can breakdown a Markdown based on header sections and other elements (eg, lists, code, tables)??&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/angularlicious&quot;&gt; /u/angularlicious &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e47vwu/markdowntextsplitter/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e47vwu/markdowntextsplitter/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1e47vwu</id><link href="https://www.reddit.com/r/LangChain/comments/1e47vwu/markdowntextsplitter/" /><updated>2024-07-15T22:13:14+00:00</updated><published>2024-07-15T22:13:14+00:00</published><title>MarkdownTextSplitter</title></entry><entry><author><name>/u/FunInformation2332</name><uri>https://www.reddit.com/user/FunInformation2332</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I am creating a RAG application based on legal texts. I have documents approx. 80 million tokens long. As you know for RAG application I need to embed documents first.&lt;/p&gt; &lt;p&gt;Formerly I was using openai embeddings &lt;em&gt;text-embedding-3-large&lt;/em&gt; model but it will cost 10k$. For better outputs and lower cost I want to switch &lt;em&gt;Nvidia nim nv-embed-v1&lt;/em&gt; but I have never hosted an AI model before so I cannot predict approx. cost.&lt;/p&gt; &lt;p&gt;I will be glad if you guys can share any source for hosting AI models, calculating Host Cost and maybe tell me which one is cheaper &lt;em&gt;text-embedding-3-large&lt;/em&gt; or &lt;em&gt;Nvidia nim nv-embed-v1&lt;/em&gt;?&lt;/p&gt; &lt;p&gt;Thank you.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/FunInformation2332&quot;&gt; /u/FunInformation2332 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e3w5lq/cost_prediction_of_nvidia_nim_nvembedv1/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e3w5lq/cost_prediction_of_nvidia_nim_nvembedv1/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1e3w5lq</id><link href="https://www.reddit.com/r/LangChain/comments/1e3w5lq/cost_prediction_of_nvidia_nim_nvembedv1/" /><updated>2024-07-15T14:27:54+00:00</updated><published>2024-07-15T14:27:54+00:00</published><title>Cost Prediction of nvidia nim nv-embed-v1</title></entry><entry><author><name>/u/vns1311</name><uri>https://www.reddit.com/user/vns1311</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey everyone, I am looking to integrate ragas scoring for every api call made to a langserve endpoint. Does anyone have any references or thoughts on how one can do this. I am planning to further log these metrics into langsmith to track the performance over time&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/vns1311&quot;&gt; /u/vns1311 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e3u619/integration_of_ragas_evaluation_with_langserve/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e3u619/integration_of_ragas_evaluation_with_langserve/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1e3u619</id><link href="https://www.reddit.com/r/LangChain/comments/1e3u619/integration_of_ragas_evaluation_with_langserve/" /><updated>2024-07-15T12:58:55+00:00</updated><published>2024-07-15T12:58:55+00:00</published><title>Integration of RAGAS Evaluation with Langserve</title></entry><entry><author><name>/u/xandie985</name><uri>https://www.reddit.com/user/xandie985</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;What is the default search metric while using:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;similarity_search() &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Also, can I explicitly give my own metrics, what are the other metrics available for ChromaDB?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/xandie985&quot;&gt; /u/xandie985 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e4059d/default_search_method_in_chromdb_while_performing/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e4059d/default_search_method_in_chromdb_while_performing/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1e4059d</id><link href="https://www.reddit.com/r/LangChain/comments/1e4059d/default_search_method_in_chromdb_while_performing/" /><updated>2024-07-15T17:07:38+00:00</updated><published>2024-07-15T17:07:38+00:00</published><title>Default search method in ChromDB while performing similarity_search()</title></entry><entry><author><name>/u/Cultural-Educator-43</name><uri>https://www.reddit.com/user/Cultural-Educator-43</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;So I have a llama 2 7b chat model on my machine running locally, what I want to do is that the model takes a CSV file as input the file can contain anything ranging from text, numbers, shareholder info, etc., process the file and gives out text in a particular paragraph form that makes sense, I tried asking multiple GPT&amp;#39;s but I couldn&amp;#39;t get anywhere &lt;/p&gt; &lt;p&gt;So if any of you esteemed AI engineers could help a frustrated college student it would mean a lot &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Cultural-Educator-43&quot;&gt; /u/Cultural-Educator-43 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e3yv1o/help_ideas_needed_i_have_a_llama_2_chat_model/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e3yv1o/help_ideas_needed_i_have_a_llama_2_chat_model/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1e3yv1o</id><link href="https://www.reddit.com/r/LangChain/comments/1e3yv1o/help_ideas_needed_i_have_a_llama_2_chat_model/" /><updated>2024-07-15T16:17:03+00:00</updated><published>2024-07-15T16:17:03+00:00</published><title>[HELP/ IDEAS NEEDED] I have a llama 2 chat model working locally i want the model to take a CSV file as an input and give an output in text form like paragraphs</title></entry><entry><author><name>/u/Interesting_Net_9628</name><uri>https://www.reddit.com/user/Interesting_Net_9628</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I am trying to get `AzureChatOpenAI` to work, but keep getting `openai.NotFoundError: Error code: 404 - {&amp;#39;error&amp;#39;: {&amp;#39;code&amp;#39;: &amp;#39;404&amp;#39;, &amp;#39;message&amp;#39;: &amp;#39;Resource not found&amp;#39;}}&lt;/p&gt; &lt;p&gt;` &lt;/p&gt; &lt;p&gt;It works if I use `AzureOpenAI`. However, the issue with this is I got the following error&lt;/p&gt; &lt;p&gt;```&lt;br/&gt; raise ValueError(&lt;/p&gt; &lt;p&gt;ValueError: OpenAIChat currently only supports single prompt, got&lt;br/&gt; ```&lt;/p&gt; &lt;p&gt;I am trying a simple tutorial to use `load_summarize_chain`&lt;/p&gt; &lt;p&gt;Does anyone have any idea?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Interesting_Net_9628&quot;&gt; /u/Interesting_Net_9628 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e3w7l9/azurechatopenai_not_working/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e3w7l9/azurechatopenai_not_working/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1e3w7l9</id><link href="https://www.reddit.com/r/LangChain/comments/1e3w7l9/azurechatopenai_not_working/" /><updated>2024-07-15T14:30:12+00:00</updated><published>2024-07-15T14:30:12+00:00</published><title>AzureChatOpenAI not working</title></entry><entry><author><name>/u/jscraft</name><uri>https://www.reddit.com/user/jscraft</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e3oap0/chatgpt_vision_api_with_langchain_and_javascript/&quot;&gt; &lt;img src=&quot;https://external-preview.redd.it/2347JJ6JbmR5WGmdxSM3iu0OnGohPD3MltcLei8lmx8.jpg?width=108&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7a4559964a97155995aa6f0c96a36eebba24728a&quot; alt=&quot;ChatGPT Vision API with LangChain and JavaScript&quot; title=&quot;ChatGPT Vision API with LangChain and JavaScript&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi folks! &lt;/p&gt; &lt;p&gt;Made this short example with ChatGPT Vision API, LangChain and Node: &lt;a href=&quot;https://www.js-craft.io/blog/vision-api-langchain-javascript/&quot;&gt;https://www.js-craft.io/blog/vision-api-langchain-javascript/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/wxpxg0erqmcd1.jpg?width=1582&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=82969758c98e21ef28bd13caccdcc04fc9aa651e&quot;&gt;https://preview.redd.it/wxpxg0erqmcd1.jpg?width=1582&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=82969758c98e21ef28bd13caccdcc04fc9aa651e&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hope it&amp;#39;s helpful for someone as a getting started example üôÇ&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/jscraft&quot;&gt; /u/jscraft &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e3oap0/chatgpt_vision_api_with_langchain_and_javascript/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e3oap0/chatgpt_vision_api_with_langchain_and_javascript/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1e3oap0</id><media:thumbnail url="https://external-preview.redd.it/2347JJ6JbmR5WGmdxSM3iu0OnGohPD3MltcLei8lmx8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=7a4559964a97155995aa6f0c96a36eebba24728a" /><link href="https://www.reddit.com/r/LangChain/comments/1e3oap0/chatgpt_vision_api_with_langchain_and_javascript/" /><updated>2024-07-15T06:54:40+00:00</updated><published>2024-07-15T06:54:40+00:00</published><title>ChatGPT Vision API with LangChain and JavaScript</title></entry><entry><author><name>/u/gtxktm</name><uri>https://www.reddit.com/user/gtxktm</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hello. Has anybody tried finetuning ColBERT models? Did it help you? How much data do you need to finetune it well? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/gtxktm&quot;&gt; /u/gtxktm &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e3plel/finetuning_colbert_reranker/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e3plel/finetuning_colbert_reranker/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1e3plel</id><link href="https://www.reddit.com/r/LangChain/comments/1e3plel/finetuning_colbert_reranker/" /><updated>2024-07-15T08:22:55+00:00</updated><published>2024-07-15T08:22:55+00:00</published><title>Finetuning ColBERT reranker</title></entry><entry><author><name>/u/sks8100</name><uri>https://www.reddit.com/user/sks8100</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey guys so I had a question. Have a simple api that in an input, calls an LLM for processing (I use llama3 with grok at the moment) and returns an output. When I run it on my computer it runs fine. Takes a few seconds but runs. Most of this is fancy prompting so I haven‚Äôt fine tuned the model or anything at the moment. It‚Äôs just chained prompts and LLMs&lt;/p&gt; &lt;p&gt;I tried to deploy it on render to test out but I keep getting a memory error given it‚Äôs taken more than 512MB given that is renders free tier. &lt;/p&gt; &lt;p&gt;Does anybody know where else I can try this out? Another server or even a real cheap one? &lt;/p&gt; &lt;p&gt;As for long term I need to move passed grok and deploy my own LLM using olama or just move to open AI. I haven‚Äôt moved yet because I find the output to be better with llama3 vs OpenAI 3.5‚Ä¶.4o is too cost prohibitive at the moment &lt;/p&gt; &lt;p&gt;Any advice? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/sks8100&quot;&gt; /u/sks8100 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e3lu2v/deploy_api_with_llm_backend/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e3lu2v/deploy_api_with_llm_backend/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1e3lu2v</id><link href="https://www.reddit.com/r/LangChain/comments/1e3lu2v/deploy_api_with_llm_backend/" /><updated>2024-07-15T04:20:37+00:00</updated><published>2024-07-15T04:20:37+00:00</published><title>Deploy Api with LLM backend</title></entry></feed>