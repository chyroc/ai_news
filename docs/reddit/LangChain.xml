<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/"><category term="LangChain" label="r/LangChain"/><updated>2023-12-28T03:55:39+00:00</updated><icon>https://www.redditstatic.com/icon.png/</icon><id>/r/LangChain.rss</id><link rel="self" href="https://www.reddit.com/r/LangChain.rss" type="application/atom+xml" /><link rel="alternate" href="https://www.reddit.com/r/LangChain" type="text/html" /><subtitle>LangChain is an open-source framework and developer toolkit that helps developers get LLM applications from prototype to production. It is available for Python and Javascript at https://www.langchain.com/.</subtitle><title>LangChain</title><entry><author><name>/u/zchaarm</name><uri>https://www.reddit.com/user/zchaarm</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;A place for members of &lt;a href=&quot;/r/LangChain&quot;&gt;r/LangChain&lt;/a&gt; to chat with each other&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/zchaarm&quot;&gt; /u/zchaarm &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_10ljho9</id><link href="https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/" /><updated>2023-01-26T04:33:05+00:00</updated><published>2023-01-26T04:33:05+00:00</published><title>r/LangChain Lounge</title></entry><entry><author><name>/u/Material_Policy6327</name><uri>https://www.reddit.com/user/Material_Policy6327</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Been using langchain for a bit but these docs just annoy the hell out of me now. Is there any possible refactor that could help maybe or it’s just too bloated now? Seems like 5 ways to do 1 thing at times.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Material_Policy6327&quot;&gt; /u/Material_Policy6327 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18sdap4/why_do_the_langchain_docs_feel_so_all_over_the/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18sdap4/why_do_the_langchain_docs_feel_so_all_over_the/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18sdap4</id><link href="https://www.reddit.com/r/LangChain/comments/18sdap4/why_do_the_langchain_docs_feel_so_all_over_the/" /><updated>2023-12-27T22:34:50+00:00</updated><published>2023-12-27T22:34:50+00:00</published><title>Why do the langchain docs feel so all over the place?</title></entry><entry><author><name>/u/IlEstLaPapi</name><uri>https://www.reddit.com/user/IlEstLaPapi</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I have a project that requires to extract data from complex word documents.&lt;/p&gt; &lt;p&gt;Each document is composed of a few tables (10 to 30). In each tables I might have :&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Text&lt;/li&gt; &lt;li&gt;Mathematical equations&lt;/li&gt; &lt;li&gt;Images (mostly math graphs).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;My initial goal is to be able to process the text and equations, I&amp;#39;ll leave the images for latter.&lt;/p&gt; &lt;p&gt;So far I haven&amp;#39;t been able to retrieve the equations in the table. I have tried the following methods:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;langchain + docx2txt : 1 huge blob of text, not exploitable at all.&lt;/li&gt; &lt;li&gt;lagnchain + unstructured in classical mode : same.&lt;/li&gt; &lt;li&gt;langchain + unstructured + elements : I got a list of tables, which is nice, however in both the tables[1][&amp;#39;kwargs&amp;#39;][&amp;#39;metadata&amp;#39;][&amp;#39;text_as_html&amp;#39;] and tables[1][&amp;#39;kwargs&amp;#39;][&amp;#39;page_content&amp;#39;] the equations aren&amp;#39;t shown.&lt;/li&gt; &lt;li&gt;pandoc docs -&amp;gt; html : both the equations and images are represented as html &lt;strong&gt;{=html}&lt;/strong&gt; and no content is shown.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I suppose that my next best solution is to use OCR like logic but I don&amp;#39;t really like the idea. Do you have any suggestion ?&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/IlEstLaPapi&quot;&gt; /u/IlEstLaPapi &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18s4fxg/processing_complex_word_documents_with_equations/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18s4fxg/processing_complex_word_documents_with_equations/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18s4fxg</id><link href="https://www.reddit.com/r/LangChain/comments/18s4fxg/processing_complex_word_documents_with_equations/" /><updated>2023-12-27T16:22:09+00:00</updated><published>2023-12-27T16:22:09+00:00</published><title>Processing complex word documents with equations.</title></entry><entry><author><name>/u/Karl_Pilkingt0n</name><uri>https://www.reddit.com/user/Karl_Pilkingt0n</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I ingested a bunch of documents into a postgres table, where 1 column is a vector embedding (pgvector extension) and the rest of them are a bunch of metadata and 1 column of the original text. &lt;/p&gt; &lt;p&gt;I want to use this as a retrieval source in a RAG application. I&amp;#39;m finding it hard to use Langchain&amp;#39;s &lt;a href=&quot;https://python.langchain.com/docs/integrations/vectorstores/pgvector&quot;&gt;pgvector&lt;/a&gt; feature - since I didn&amp;#39;t create the table itself using langchain. &lt;/p&gt; &lt;p&gt;Does anyone have examples or pointers to using a separately generated vector table with langchain&amp;#39;s pgvector feature? Or is using a custom retriever my only path forward?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Karl_Pilkingt0n&quot;&gt; /u/Karl_Pilkingt0n &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18sivbd/langchain_pgvector_need_help/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18sivbd/langchain_pgvector_need_help/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18sivbd</id><link href="https://www.reddit.com/r/LangChain/comments/18sivbd/langchain_pgvector_need_help/" /><updated>2023-12-28T02:40:32+00:00</updated><published>2023-12-28T02:40:32+00:00</published><title>Langchain + pgvector, need help.</title></entry><entry><author><name>/u/DevelopmentNo409</name><uri>https://www.reddit.com/user/DevelopmentNo409</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18sh4zw/can_anyone_help_me_to_solve_this_error/&quot;&gt; &lt;img src=&quot;https://b.thumbs.redditmedia.com/Tz5G8mH0FqbgBdwlE_imp6mMnTht8KSSQ-jbc4B0FNU.jpg&quot; alt=&quot;Can anyone help me to solve this error ValidationError: 1 validation error for LLMChain prompt Can't instantiate abstract class BasePromptTemplate with abstract methods format, format_prompt (type=type_error)&quot; title=&quot;Can anyone help me to solve this error ValidationError: 1 validation error for LLMChain prompt Can't instantiate abstract class BasePromptTemplate with abstract methods format, format_prompt (type=type_error)&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;pre&gt;&lt;code&gt;from langchain.output_parsers import ResponseSchema, StructuredOutputParser from langchain_core.prompts import PromptTemplate from langchain.memory import ConversationSummaryBufferMemory checker_tmpl = &amp;quot;&amp;quot;&amp;quot; # Task Description: As a customer support representative, your role is to provide accurate and helpful responses to customer inquiries. Use the provided context to understand the customer&amp;#39;s issue and answer their questions directly. Avoid any extraneous information or explanations that are not directly relevant to the customer&amp;#39;s query. {format_instructions} # Given Context: {context} #Chat History:\n\n{chat_history} \n\n # Customer&amp;#39;s Question: {question} # Your Response: [Please type your answer here, ensuring it is concise, relevant, and directly addresses the customer&amp;#39;s question based on the given context.] &amp;quot;&amp;quot;&amp;quot; checker_response_schemas = [ ResponseSchema( name=&amp;quot;requires_customer_support_contact&amp;quot;, description=&amp;quot;Indicates whether the user needs to contact customer support. Set to True if the context does not sufficiently address the user&amp;#39;s issue and further assistance is needed. Set to False if the provided context is adequate to answer the user&amp;#39;s query.&amp;quot;, type=&amp;quot;boolean&amp;quot; ), ResponseSchema( name=&amp;quot;contextual_answer&amp;quot;, description=&amp;quot;Provides a direct answer to the user&amp;#39;s question, utilizing the given context. The response should be concise, accurate, and specifically tailored to address the query based on the context provided.&amp;quot;, ), ] check_output_parser = StructuredOutputParser.from_response_schemas(checker_response_schemas) resume_checker_prompt = PromptTemplate( template=checker_tmpl, input_variables=[&amp;quot;chat_history&amp;quot;,&amp;quot;context&amp;quot;, &amp;quot;question&amp;quot;], partial_variables={ &amp;quot;format_instructions&amp;quot;: check_output_parser.get_format_instructions() } ) openai = ChatOpenAI(model_name=&amp;quot;gpt-4-0613&amp;quot;) memory = ConversationSummaryBufferMemory(llm = openai, memory_key=&amp;quot;chat_history&amp;quot;) # check_prompt_str = resume_checker_prompt.format(chat_history = memory.get_memory(), context=context, question = &amp;quot;I am 80 yeard old guy suggest some best plans&amp;quot; ) chat_chain = LLMChain( llm=openai, prompt=resume_checker_prompt, verbose=True, memory=memory, ) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/lfitazcosx8c1.png?width=1615&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5a4e30848189e74d03baa4fd4790a58d81a86333&quot;&gt;https://preview.redd.it/lfitazcosx8c1.png?width=1615&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5a4e30848189e74d03baa4fd4790a58d81a86333&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/DevelopmentNo409&quot;&gt; /u/DevelopmentNo409 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18sh4zw/can_anyone_help_me_to_solve_this_error/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18sh4zw/can_anyone_help_me_to_solve_this_error/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_18sh4zw</id><media:thumbnail url="https://b.thumbs.redditmedia.com/Tz5G8mH0FqbgBdwlE_imp6mMnTht8KSSQ-jbc4B0FNU.jpg" /><link href="https://www.reddit.com/r/LangChain/comments/18sh4zw/can_anyone_help_me_to_solve_this_error/" /><updated>2023-12-28T01:18:29+00:00</updated><published>2023-12-28T01:18:29+00:00</published><title>Can anyone help me to solve this error ValidationError: 1 validation error for LLMChain prompt Can't instantiate abstract class BasePromptTemplate with abstract methods format, format_prompt (type=type_error)</title></entry><entry><author><name>/u/BankHottas</name><uri>https://www.reddit.com/user/BankHottas</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;m pretty new to Langchain, but I&amp;#39;m comfortable building RAG applications with &amp;quot;static&amp;quot; data, such as PDFs, webpages, etc. I&amp;#39;m now trying out the SQL toolkit, but I have some questions.&lt;/p&gt; &lt;p&gt;Since you&amp;#39;re still limited by the model&amp;#39;s context window size, it seems that only certain queries make sense. But what if you wanted to find trends or insights from a larger amount of data, like store orders, or analytics data?&lt;/p&gt; &lt;p&gt;Would you do this in batches and then combine them? Would you vectorize the data first? Is it even feasible to do this currently?&lt;/p&gt; &lt;p&gt;I&amp;#39;m very interested to hear how you would approach this problem&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/BankHottas&quot;&gt; /u/BankHottas &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18rw5rm/what_is_your_strategy_for_doing_inference_over_a/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18rw5rm/what_is_your_strategy_for_doing_inference_over_a/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18rw5rm</id><link href="https://www.reddit.com/r/LangChain/comments/18rw5rm/what_is_your_strategy_for_doing_inference_over_a/" /><updated>2023-12-27T08:43:26+00:00</updated><published>2023-12-27T08:43:26+00:00</published><title>What is your strategy for doing inference over a large SQL dataset?</title></entry><entry><author><name>/u/Objective-Cobbler22</name><uri>https://www.reddit.com/user/Objective-Cobbler22</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I am trying to implement a chatbot where a user can chat about a document. I am trying to implement it where I read the document through Langcahin -&amp;gt; create embedding and store them (use a huggingface model for embedding) in Chroma -&amp;gt; answer question where I use a huggingface model for LLM. I tried to use a QuestionAnswering Huggingface model, but I got an error. Then I looked at the github code of Langchain and it says they only support few model types which are text-generation, text2text-generation and summarization. Has anyone been able to implement this?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Objective-Cobbler22&quot;&gt; /u/Objective-Cobbler22 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18sbixy/rag_with_langchain_huggingface/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18sbixy/rag_with_langchain_huggingface/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18sbixy</id><link href="https://www.reddit.com/r/LangChain/comments/18sbixy/rag_with_langchain_huggingface/" /><updated>2023-12-27T21:20:06+00:00</updated><published>2023-12-27T21:20:06+00:00</published><title>RAG with Langchain Huggingface</title></entry><entry><author><name>/u/Parking_Skin9598</name><uri>https://www.reddit.com/user/Parking_Skin9598</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;guys i have problem with my chatgpt4 acc i never use the api before but when i call for useing it he gives me this error msg &amp;quot; You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: &lt;a href=&quot;https://platform.openai.com/docs/guides/error-codes/api-errors&quot;&gt;https://platform.openai.com/docs/guides/error-codes/api-errors&lt;/a&gt;. &amp;quot; even if i have already 18 dollas as credit and the graph said i never use it how can i solve this problem and can someone give me apt key i really need it cuz i need to use it in univ project &lt;/p&gt; &lt;p&gt;thanks to all of you .&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Parking_Skin9598&quot;&gt; /u/Parking_Skin9598 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18s9pai/chatgpt4_api_dont_work_for_me/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18s9pai/chatgpt4_api_dont_work_for_me/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18s9pai</id><link href="https://www.reddit.com/r/LangChain/comments/18s9pai/chatgpt4_api_dont_work_for_me/" /><updated>2023-12-27T20:02:50+00:00</updated><published>2023-12-27T20:02:50+00:00</published><title>chatgpt4 api dont work for me</title></entry><entry><author><name>/u/Calm_Pea_2428</name><uri>https://www.reddit.com/user/Calm_Pea_2428</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I just received a call from Turing for a job, but it mentions a very broad domain, including:&lt;/p&gt; &lt;p&gt;Communication Skills&lt;/p&gt; &lt;p&gt;Machine Learning and Deep Learning Models&lt;/p&gt; &lt;p&gt;Python and large Language Models (LLMs)&lt;/p&gt; &lt;p&gt;AI Problem Solving&lt;/p&gt; &lt;p&gt;My call is in one day, and I&amp;#39;m confused about which concepts I should revise and the people in the CC are mostly Non-technical.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Calm_Pea_2428&quot;&gt; /u/Calm_Pea_2428 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18s795c/turing_technical_call/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18s795c/turing_technical_call/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18s795c</id><link href="https://www.reddit.com/r/LangChain/comments/18s795c/turing_technical_call/" /><updated>2023-12-27T18:20:19+00:00</updated><published>2023-12-27T18:20:19+00:00</published><title>Turing Technical call</title></entry><entry><author><name>/u/duddai</name><uri>https://www.reddit.com/user/duddai</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey guys,&lt;/p&gt; &lt;p&gt;im new to langchain and I&amp;#39;m wondering how the mapreduce function works. &lt;/p&gt; &lt;p&gt;If I have a chunksize of 10.000 and run the mapreduce function, how can it summarize the chunks with a size of 10.000? I thought the token limit is under 10.000. &lt;/p&gt; &lt;p&gt;And when i change the verbose bool to true, it just shows me that it summarize the content of the chunk (which is 10.000, so I thought it doenst work). &lt;/p&gt; &lt;p&gt;Im trying to understand the logic but im so confused. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/duddai&quot;&gt; /u/duddai &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18s064m/how_does_mapreduce_work/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18s064m/how_does_mapreduce_work/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18s064m</id><link href="https://www.reddit.com/r/LangChain/comments/18s064m/how_does_mapreduce_work/" /><updated>2023-12-27T13:00:20+00:00</updated><published>2023-12-27T13:00:20+00:00</published><title>How does mapReduce work</title></entry><entry><author><name>/u/Intelligent-Drink631</name><uri>https://www.reddit.com/user/Intelligent-Drink631</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi everyone, I am building a python app that creates a new agent for each user. For security reasons, I need to delete the agent completely after use. I have searched everywhere but didn&amp;#39;t find a way to do it...&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Intelligent-Drink631&quot;&gt; /u/Intelligent-Drink631 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18s3qre/how_to_delete_an_agent/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18s3qre/how_to_delete_an_agent/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18s3qre</id><link href="https://www.reddit.com/r/LangChain/comments/18s3qre/how_to_delete_an_agent/" /><updated>2023-12-27T15:51:01+00:00</updated><published>2023-12-27T15:51:01+00:00</published><title>How to delete an agent?</title></entry><entry><author><name>/u/East-Bug6675</name><uri>https://www.reddit.com/user/East-Bug6675</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;The below code is taking more than 5 min to answer the question, any solution to this? &lt;/p&gt; &lt;p&gt;llm = CTransformers(model=&amp;quot;TheBloke/CodeLlama-7B-Instruct-GGUF&amp;quot;&lt;strong&gt;,&lt;/strong&gt;&lt;br/&gt; model_file=&amp;quot;codellama-7b-instruct.Q5_K_M.gguf&amp;quot;&lt;strong&gt;,&lt;/strong&gt;&lt;br/&gt; # callbacks=[StreamingStdOutCallbackHandler()],&lt;br/&gt; config={&amp;#39;max_new_tokens&amp;#39;: &lt;strong&gt;4096,&lt;/strong&gt;&lt;br/&gt; &amp;#39;context_length&amp;#39;: &lt;strong&gt;4000,&lt;/strong&gt;&lt;br/&gt; &amp;#39;temperature&amp;#39;: &lt;strong&gt;0.01&lt;/strong&gt;})&lt;br/&gt; agent = create_csv_agent(llm&lt;strong&gt;,&lt;/strong&gt;&lt;br/&gt; &amp;#39;August2023.csv&amp;#39;&lt;strong&gt;,&lt;/strong&gt;&lt;br/&gt; verbose=True&lt;strong&gt;,&lt;/strong&gt;&lt;br/&gt; agent_type=AgentType.ZERO_SHOT_REACT_DESCRIPTION&lt;strong&gt;,&lt;/strong&gt;)&lt;/p&gt; &lt;p&gt;agent.run(&amp;quot;how many rows are there?&amp;quot;)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/East-Bug6675&quot;&gt; /u/East-Bug6675 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18ry05l/llm_taking_too_long_time_to_respond/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18ry05l/llm_taking_too_long_time_to_respond/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18ry05l</id><link href="https://www.reddit.com/r/LangChain/comments/18ry05l/llm_taking_too_long_time_to_respond/" /><updated>2023-12-27T10:46:41+00:00</updated><published>2023-12-27T10:46:41+00:00</published><title>LLM taking too long time to respond</title></entry><entry><author><name>/u/East-Bug6675</name><uri>https://www.reddit.com/user/East-Bug6675</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;The below code is taking more than 5 min to answer the question, any solution to this? &lt;/p&gt; &lt;p&gt;llm = CTransformers(model=&amp;quot;TheBloke/CodeLlama-7B-Instruct-GGUF&amp;quot;&lt;strong&gt;,&lt;/strong&gt;&lt;br/&gt; model_file=&amp;quot;codellama-7b-instruct.Q5_K_M.gguf&amp;quot;&lt;strong&gt;,&lt;/strong&gt;&lt;br/&gt; # callbacks=[StreamingStdOutCallbackHandler()],&lt;br/&gt; config={&amp;#39;max_new_tokens&amp;#39;: &lt;strong&gt;4096,&lt;/strong&gt;&lt;br/&gt; &amp;#39;context_length&amp;#39;: &lt;strong&gt;4000,&lt;/strong&gt;&lt;br/&gt; &amp;#39;temperature&amp;#39;: &lt;strong&gt;0.01&lt;/strong&gt;})&lt;br/&gt; agent = create_csv_agent(llm&lt;strong&gt;,&lt;/strong&gt;&lt;br/&gt; &amp;#39;August2023.csv&amp;#39;&lt;strong&gt;,&lt;/strong&gt;&lt;br/&gt; verbose=True&lt;strong&gt;,&lt;/strong&gt;&lt;br/&gt; agent_type=AgentType.ZERO_SHOT_REACT_DESCRIPTION&lt;strong&gt;,&lt;/strong&gt;)&lt;/p&gt; &lt;p&gt;agent.run(&amp;quot;how many rows are there?&amp;quot;)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/East-Bug6675&quot;&gt; /u/East-Bug6675 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18ry05g/llm_taking_too_long_time_to_respond/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18ry05g/llm_taking_too_long_time_to_respond/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18ry05g</id><link href="https://www.reddit.com/r/LangChain/comments/18ry05g/llm_taking_too_long_time_to_respond/" /><updated>2023-12-27T10:46:40+00:00</updated><published>2023-12-27T10:46:40+00:00</published><title>LLM taking too long time to respond</title></entry><entry><author><name>/u/topdownAC</name><uri>https://www.reddit.com/user/topdownAC</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;m looking for an advice on how to create REST service that configures agents on runtime.&lt;/p&gt; &lt;p&gt;I have a service that currently runs on fastapi, but if there&amp;#39;s any other tool that might be useful for this, I&amp;#39;m open to suggestion. This app is very simple, it has a route &lt;code&gt;POST /ask&lt;/code&gt; that simply prompts a pre-configured agent. In addition, I want to have another &lt;code&gt;POST /refresh-agent&lt;/code&gt; that in the body gets &amp;quot;uri&amp;quot; argument that will point to a storage path that is the serialized object of the new agent.&lt;/p&gt; &lt;p&gt;For example, I have an agent that I&amp;#39;ve pickled:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;from langchain.agents import AgentType, initialize_agent from langchain.agents import Tool from langchain.chains import LLMMathChain from langchain.llms import OpenAI llm = OpenAI(openai_api_key=&amp;quot;...&amp;quot;) llm_math = LLMMathChain(llm=llm) math_tool = Tool( name=&amp;quot;Calculator&amp;quot;, func=llm_math.run, description=&amp;#39;Useful for when you need to answer questions about math.&amp;#39; ) tools = [math_tool] agent = initialize_agent( tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, ) with open(&amp;quot;fresh_agent.pkl&amp;quot;, &amp;quot;wb&amp;quot;) as f: pickle.dump(agent, handle, protocol=pickle.HIGHEST_PROTOCOL) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;And the &lt;code&gt;/refresh-agent&lt;/code&gt; will look like - &lt;/p&gt; &lt;pre&gt;&lt;code&gt;@router.post(&amp;quot;/refresh-agent&amp;quot;) def refresh_agent(uri: str): with open(uri, &amp;quot;rb&amp;quot;) as f: agent = pickle.load(f) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Is this a flow that exists in applications? is it common? Is there any tool or file format (that might be better than pickle) or serving framework that can somehow help me in this? Any help would be appreciated.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/topdownAC&quot;&gt; /u/topdownAC &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18rglvu/serialize_agentllm_objects_into_files_for_serving/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18rglvu/serialize_agentllm_objects_into_files_for_serving/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18rglvu</id><link href="https://www.reddit.com/r/LangChain/comments/18rglvu/serialize_agentllm_objects_into_files_for_serving/" /><updated>2023-12-26T19:53:48+00:00</updated><published>2023-12-26T19:53:48+00:00</published><title>Serialize agent/llm objects into files for serving</title></entry><entry><author><name>/u/LongjumpingPop3419</name><uri>https://www.reddit.com/user/LongjumpingPop3419</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;There are tools out there like PromptHub, or PromptKnit, that let you manage prompts, compare versions, and easily test them.&lt;/p&gt; &lt;p&gt;But that&amp;#39;s &lt;strong&gt;all they do&lt;/strong&gt;, they only focus on prompts.&lt;/p&gt; &lt;p&gt;On the other hand you have tools like Flowise and Langflow which are robust and great for LLM pipelines, and fast prototyping. But they are &lt;strong&gt;not good&lt;/strong&gt; for versioning, and collaborating with non-technical people on prompt design.&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;I couldn&amp;#39;t find a tool where I enjoy &lt;strong&gt;both worlds&lt;/strong&gt;, but it would be enough to keep the tools separate, and integrate. For example manage the prompts &amp;amp; their versions in Service A, and use them in Service B (e.g. Flowise).&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;Our team is building LLM apps, and is trying to find a good way to prototype and collaborate, where someone like the product manager can come in and play with different versions of one of the prompts in the chain.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/LongjumpingPop3419&quot;&gt; /u/LongjumpingPop3419 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18rb334/any_good_prompt_management_versioning_tools_out/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18rb334/any_good_prompt_management_versioning_tools_out/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18rb334</id><link href="https://www.reddit.com/r/LangChain/comments/18rb334/any_good_prompt_management_versioning_tools_out/" /><updated>2023-12-26T15:54:14+00:00</updated><published>2023-12-26T15:54:14+00:00</published><title>Any good prompt management &amp; versioning tools out there, that integrate nicely?</title></entry><entry><author><name>/u/duddai</name><uri>https://www.reddit.com/user/duddai</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey guys,&lt;/p&gt; &lt;p&gt;maybe someone can help me.&lt;/p&gt; &lt;p&gt;I have this code and an input text which is 15.737 characters long.&lt;/p&gt; &lt;p&gt;And my endresult is cut off at around 1400 characters but idk why&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;pre&gt;&lt;code&gt; try { // Create language model const model = new OpenAI({ openAIApiKey: &amp;quot;key&amp;quot;, temperature: 0, }); // Splitting text const textSplitter = new RecursiveCharacterTextSplitter({ chunkSize: 1000, // 1000 chunkOverlap: 200, // 200 }); const docs = await textSplitter.createDocuments([formData.text]); console.log(docs); const combineMapPromptTemplate = `You will be given a part of a scientific paper. This part will be enclosed in triple hashtags (###). Extract the key ideas and concepts in 3 bullet points. ###&amp;quot;{text}&amp;quot;###`; const combinePromptTemplate = `As a professional summarizer, create a concise and comprehensive summary of the provided text - The text will be enclosed in triple hashtags (###) - while adhering to these guidelines: 1. Craft a summary that is concise and to the point with a well-organized structure. 2. Write in a natural and conversational language with an engaging and informative tone. 3. Incorporate main ideas and essential information, eliminating extraneous language and focusing on critical aspects. 4. Rely strictly on the provided text, without including external information. 5. Your response should be at least three paragraphs and fully encompass what was said in the text. ###&amp;quot;{text}&amp;quot;###`; const combineMapPrompt = new PromptTemplate({ template: combineMapPromptTemplate, inputVariables: [&amp;quot;text&amp;quot;], }); const combinePrompt = new PromptTemplate({ template: combinePromptTemplate, inputVariables: [&amp;quot;text&amp;quot;], }); // This convenience function creates a document chain prompted to summarize a set of documents. const chain = loadSummarizationChain(model, { type: &amp;quot;map_reduce&amp;quot;, returnIntermediateSteps: true, combineMapPrompt: combineMapPrompt, combinePrompt: combinePrompt, }); const res = await chain.call({ input_documents: docs, }); console.log(res); await insertData(formData, res); setResponse([res]); } catch (e) { console.error(e); throw new Error(&amp;quot;Something failed&amp;quot;); } &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/duddai&quot;&gt; /u/duddai &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18repd6/output_get_cuts_off_at_around_1400_characters/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18repd6/output_get_cuts_off_at_around_1400_characters/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18repd6</id><link href="https://www.reddit.com/r/LangChain/comments/18repd6/output_get_cuts_off_at_around_1400_characters/" /><updated>2023-12-26T18:31:32+00:00</updated><published>2023-12-26T18:31:32+00:00</published><title>Output get cuts off at around 1400 characters</title></entry><entry><author><name>/u/devinbost</name><uri>https://www.reddit.com/user/devinbost</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;ve noticed that LCEL is picky about the curly braces used within prompts, which is somewhat problematic when the prompt contains code, like for few-shot code gen use cases. Has anyone found a graceful way to handle the curly braces so LangChain doesn&amp;#39;t think they&amp;#39;re parameters for string substitution? So far, I&amp;#39;ve been replacing them with double curly braces, but it&amp;#39;s not very elegant.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/devinbost&quot;&gt; /u/devinbost &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18rhe6i/lcel_with_prompts_containing_code/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18rhe6i/lcel_with_prompts_containing_code/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18rhe6i</id><link href="https://www.reddit.com/r/LangChain/comments/18rhe6i/lcel_with_prompts_containing_code/" /><updated>2023-12-26T20:27:07+00:00</updated><published>2023-12-26T20:27:07+00:00</published><title>LCEL with prompts containing code</title></entry><entry><author><name>/u/Achiev0r</name><uri>https://www.reddit.com/user/Achiev0r</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hello y&amp;#39;all! I have a working demo for a tool that reads in ebook files, splits chapters into document objects, then feeds it into a VectorstoreIndex. I am able to query against it&amp;#39;s data. Once it&amp;#39;s finished I want to share it with everyone as I see people have trouble remembering what they read in books.&lt;/p&gt; &lt;p&gt;My question is, if I want to add more books, or just use another one to question, should I be creating separate VectorstoreIndexes or just feed everything into one and filter it based on metadata (title, author, ISBN, chapter)? If I want a summary for example, would langchain be able to select all the chapters and not just what it sees as relevant?&lt;/p&gt; &lt;p&gt;And also I have some problems with prompting, I use GPT 3.5 turbo, and when I ask it to create rehearsal questions based on the book it gives quite mediocre and hyper specific ones that are almost irrelevant. The prompt I used: &lt;code&gt;Write 10 test questions from this book in the following format, while escaping special characters from the source: {&amp;quot;questions&amp;quot;:[{&amp;quot;source&amp;quot;:&amp;quot;exact sentences for the information source only&amp;quot;,&amp;quot;question&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;answer_1&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;answer_2&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;answer_3&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;answer_4&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;correct_answer_key&amp;quot;:&amp;quot;answer_*&amp;quot;}]}&lt;/code&gt;. I want to feed the results into some UI to see what I was able to remember from a book on my phone as well.&lt;/p&gt; &lt;p&gt;Thanks for reading, hopefully if you could help me I could help many others.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Achiev0r&quot;&gt; /u/Achiev0r &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18r7ls5/creating_a_scalable_book_questioner/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18r7ls5/creating_a_scalable_book_questioner/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18r7ls5</id><link href="https://www.reddit.com/r/LangChain/comments/18r7ls5/creating_a_scalable_book_questioner/" /><updated>2023-12-26T12:58:10+00:00</updated><published>2023-12-26T12:58:10+00:00</published><title>Creating a scalable book questioner</title></entry><entry><author><name>/u/phicreative1997</name><uri>https://www.reddit.com/user/phicreative1997</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Pretty straight forward question, apart from langchain documentation. Do you have a resource?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/phicreative1997&quot;&gt; /u/phicreative1997 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18r3awq/where_can_i_learn_about_schemas/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18r3awq/where_can_i_learn_about_schemas/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18r3awq</id><link href="https://www.reddit.com/r/LangChain/comments/18r3awq/where_can_i_learn_about_schemas/" /><updated>2023-12-26T08:09:00+00:00</updated><published>2023-12-26T08:09:00+00:00</published><title>Where can I learn about schemas?</title></entry><entry><author><name>/u/DOKim_98</name><uri>https://www.reddit.com/user/DOKim_98</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I have a json file less than &amp;lt; 50mb, that has this format:&lt;/p&gt; &lt;p&gt;{&lt;br/&gt; &amp;quot;title&amp;quot;: &amp;quot;string&amp;quot;,&lt;/p&gt; &lt;p&gt;&amp;quot;url&amp;quot;: &amp;quot;&lt;a href=&quot;https://www.uvu.edu/cet/blog/posts/cgmt_fundraiser_2022.html&quot;&gt;u&lt;/a&gt;rl.html&amp;quot;,&lt;/p&gt; &lt;p&gt;&amp;quot;html&amp;quot;: &amp;quot;html content...&amp;quot;&lt;br/&gt; },&lt;/p&gt; &lt;p&gt;...&lt;/p&gt; &lt;p&gt;and I tried to look for langchain doc that can let openai api like gpt3.5 read json file and give an answer from those data, but it was really hard to find out the doc I wanted. &lt;/p&gt; &lt;p&gt;So, I wonder if anyone knows how to connect json data, and llm to make a chatbot like llm. &lt;/p&gt; &lt;p&gt;#Langchain #Python #json &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/DOKim_98&quot;&gt; /u/DOKim_98 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18qs349/how_do_i_let_llm_read_my_json_file_and_give_an/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18qs349/how_do_i_let_llm_read_my_json_file_and_give_an/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18qs349</id><link href="https://www.reddit.com/r/LangChain/comments/18qs349/how_do_i_let_llm_read_my_json_file_and_give_an/" /><updated>2023-12-25T21:56:35+00:00</updated><published>2023-12-25T21:56:35+00:00</published><title>How do I let llm read my json file and give an answer to a question? Python</title></entry><entry><author><name>/u/iTzPhiil</name><uri>https://www.reddit.com/user/iTzPhiil</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;m interested in creating an AI model that takes in a user&amp;#39;s input, which comes in the form of a JSON dictionary of the music the user likes, and provides recommendations. For example: {name: Mike, genre: hip-hop, song: Gangsta&amp;#39;s Paradise}. I want to create a prompt where I explain what different genres mean, such as rock, pop, and R&amp;amp;B music. The AI would then look at the prompt to understand the genre and provide me with recommendations for similar music.&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;I&amp;#39;ve been playing around, but I&amp;#39;m unsure of how to accomplish this initial idea. The initial idea was to use Structured Output Parser where I define the genre and a Prompt Template where I specify how it should answer the user&amp;#39;s input. Am I on the right track? Thanks in advance.&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/iTzPhiil&quot;&gt; /u/iTzPhiil &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18qtquw/is_langchain_the_right_choice_or_can_i_rely_on/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18qtquw/is_langchain_the_right_choice_or_can_i_rely_on/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18qtquw</id><link href="https://www.reddit.com/r/LangChain/comments/18qtquw/is_langchain_the_right_choice_or_can_i_rely_on/" /><updated>2023-12-25T23:20:03+00:00</updated><published>2023-12-25T23:20:03+00:00</published><title>Is Langchain the right choice, or can I rely on Chat GPT for this?</title></entry><entry><author><name>/u/overflow74</name><uri>https://www.reddit.com/user/overflow74</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18qjcxc/mongodb_agent/&quot;&gt; &lt;img src=&quot;https://b.thumbs.redditmedia.com/d4NwBHYnXAEg1Maiy-OkSp4LrQYORBdaLDEobDyMWgQ.jpg&quot; alt=&quot;MongoDB Agent&quot; title=&quot;MongoDB Agent&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi everyone,I created a basic tools for interacting with mongodb using React agent.&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/q04t7n0p6g8c1.png?width=1286&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0f02e9226b77e9a67fc956718ffbc92826972860&quot;&gt;generated aggregation pipeline suitable for pymongo&lt;/a&gt;&lt;/p&gt; &lt;p&gt;the agent currently has two tools : detect aggregation, execute aggregationthe goal is to convert a natural language query to an aggregation pipeline when executed it would get the desired answer.currently the first tool works perfectly, however the agent fails to execute the aggregation using pymongo.this is the output when it calls the execution tool:&lt;strong&gt;ValueError: An output parsing error occurred. In order to pass this error back to the agent and have it try again, pass `handle_parsing_errors=True` to the AgentExecutor. This is the error: Could not parse LLM output: {&amp;quot;action&amp;quot;: &amp;quot;Execute mongodb aggregation pipeline tool&amp;quot;,&amp;quot;action_input&amp;quot;: [{&amp;#39;$match&amp;#39;: {&amp;#39;overall_rating&amp;#39;: {&amp;#39;$gte&amp;#39;: 4}}}, {&amp;#39;$count&amp;#39;: &amp;#39;satisfied_customers&amp;#39;}]}&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;what could be the issue?&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/overflow74&quot;&gt; /u/overflow74 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18qjcxc/mongodb_agent/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18qjcxc/mongodb_agent/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_18qjcxc</id><media:thumbnail url="https://b.thumbs.redditmedia.com/d4NwBHYnXAEg1Maiy-OkSp4LrQYORBdaLDEobDyMWgQ.jpg" /><link href="https://www.reddit.com/r/LangChain/comments/18qjcxc/mongodb_agent/" /><updated>2023-12-25T14:09:10+00:00</updated><published>2023-12-25T14:09:10+00:00</published><title>MongoDB Agent</title></entry><entry><author><name>/u/Mobile-Hospital-1025</name><uri>https://www.reddit.com/user/Mobile-Hospital-1025</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Mobile-Hospital-1025&quot;&gt; /u/Mobile-Hospital-1025 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;/r/LocalLLaMA/comments/18q3y1p/looking_for_project_ideas/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18q40sf/looking_for_project_ideas/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18q40sf</id><link href="https://www.reddit.com/r/LangChain/comments/18q40sf/looking_for_project_ideas/" /><updated>2023-12-24T21:13:46+00:00</updated><published>2023-12-24T21:13:46+00:00</published><title>Looking for Project Ideas</title></entry><entry><author><name>/u/52637</name><uri>https://www.reddit.com/user/52637</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I’m currently working on an Agent implementation that uses tools to update a pre-defined pydantic model which needs values to be added/updated based on context retrieved by the agent. My problem, however, is that I cannot seem to figure out how to keep an instantiation of this pydantic model linked to the agent’s execution. &lt;/p&gt; &lt;p&gt;Is it possible to access and update metadata in the AgentExecutor? &lt;/p&gt; &lt;p&gt;Wondering if anyone has experience doing something similar to this?&lt;/p&gt; &lt;p&gt;My current implementation is stateless, building the pydantic models during the output parsing step, however, this often results in output parsing errors. My thinking is also that this new implementation method will make evaluation simpler, since the quality of the model would be tied to correct tool execution as opposed to completeness of the pydantic model. &lt;/p&gt; &lt;p&gt;All thoughts/questions/comments appreciated!&lt;/p&gt; &lt;p&gt;Happy Holidays.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/52637&quot;&gt; /u/52637 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18pzqkk/best_way_to_populate_a_pydantic_model_during_an/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18pzqkk/best_way_to_populate_a_pydantic_model_during_an/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18pzqkk</id><link href="https://www.reddit.com/r/LangChain/comments/18pzqkk/best_way_to_populate_a_pydantic_model_during_an/" /><updated>2023-12-24T17:34:00+00:00</updated><published>2023-12-24T17:34:00+00:00</published><title>Best way to populate a pydantic model during an agent run</title></entry><entry><author><name>/u/WishboneReal534</name><uri>https://www.reddit.com/user/WishboneReal534</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey! I am trying to create a vector store using langchain and faiss for RAG(Retrieval-augmented generation) with about 6 millions abstracts. is there a strategy to create this vector store efficiently? currently it takes very long time to create it (can take up to 5 days)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/WishboneReal534&quot;&gt; /u/WishboneReal534 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18ph140/creating_a_vectordb_from_millions_of_documents/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18ph140/creating_a_vectordb_from_millions_of_documents/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18ph140</id><link href="https://www.reddit.com/r/LangChain/comments/18ph140/creating_a_vectordb_from_millions_of_documents/" /><updated>2023-12-23T22:43:13+00:00</updated><published>2023-12-23T22:43:13+00:00</published><title>creating a vectordb from millions of documents</title></entry><entry><author><name>/u/enspiralart</name><uri>https://www.reddit.com/user/enspiralart</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;With the ability of agents to search the web and use the data it finds in RAG, it seems that one could effectively make a research agent who&amp;#39;s sole purpose is to find datasets for the LLM to consume:&lt;/p&gt; &lt;p&gt;Obstacles&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Navigating and choosing the data to make datasets from (research path if you will), maybe using ToT&lt;/li&gt; &lt;li&gt;Data preparation, coming up with a standard to store the data chosen and creating an entry for the yaml&lt;/li&gt; &lt;li&gt;Some way to test if the data is within the existing dataset for the model in order to skip it or treat it with less weight. Could possibly be done using a carefully crafted completion prompt and check the completion tokens against ground truth in the data being scrutinized&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Do you think this would be helpful as a way to automatically generate non-synthetic datasets?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/enspiralart&quot;&gt; /u/enspiralart &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18prq9i/has_anyone_used_llms_to_compile_training_data_for/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18prq9i/has_anyone_used_llms_to_compile_training_data_for/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18prq9i</id><link href="https://www.reddit.com/r/LangChain/comments/18prq9i/has_anyone_used_llms_to_compile_training_data_for/" /><updated>2023-12-24T09:15:10+00:00</updated><published>2023-12-24T09:15:10+00:00</published><title>Has anyone used LLMs to compile training data for LLMs?</title></entry></feed>