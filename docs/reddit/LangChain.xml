<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/"><category term="LangChain" label="r/LangChain"/><updated>2023-12-27T10:15:04+00:00</updated><icon>https://www.redditstatic.com/icon.png/</icon><id>/r/LangChain.rss</id><link rel="self" href="https://www.reddit.com/r/LangChain.rss" type="application/atom+xml" /><link rel="alternate" href="https://www.reddit.com/r/LangChain" type="text/html" /><subtitle>LangChain is an open-source framework and developer toolkit that helps developers get LLM applications from prototype to production. It is available for Python and Javascript at https://www.langchain.com/.</subtitle><title>LangChain</title><entry><author><name>/u/zchaarm</name><uri>https://www.reddit.com/user/zchaarm</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;A place for members of &lt;a href=&quot;/r/LangChain&quot;&gt;r/LangChain&lt;/a&gt; to chat with each other&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/zchaarm&quot;&gt; /u/zchaarm &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_10ljho9</id><link href="https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/" /><updated>2023-01-26T04:33:05+00:00</updated><published>2023-01-26T04:33:05+00:00</published><title>r/LangChain Lounge</title></entry><entry><author><name>/u/BankHottas</name><uri>https://www.reddit.com/user/BankHottas</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;m pretty new to Langchain, but I&amp;#39;m comfortable building RAG applications with &amp;quot;static&amp;quot; data, such as PDFs, webpages, etc. I&amp;#39;m now trying out the SQL toolkit, but I have some questions.&lt;/p&gt; &lt;p&gt;Since you&amp;#39;re still limited by the model&amp;#39;s context window size, it seems that only certain queries make sense. But what if you wanted to find trends or insights from a larger amount of data, like store orders, or analytics data?&lt;/p&gt; &lt;p&gt;Would you do this in batches and then combine them? Would you vectorize the data first? Is it even feasible to do this currently?&lt;/p&gt; &lt;p&gt;I&amp;#39;m very interested to hear how you would approach this problem&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/BankHottas&quot;&gt; /u/BankHottas &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18rw5rm/what_is_your_strategy_for_doing_inference_over_a/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18rw5rm/what_is_your_strategy_for_doing_inference_over_a/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18rw5rm</id><link href="https://www.reddit.com/r/LangChain/comments/18rw5rm/what_is_your_strategy_for_doing_inference_over_a/" /><updated>2023-12-27T08:43:26+00:00</updated><published>2023-12-27T08:43:26+00:00</published><title>What is your strategy for doing inference over a large SQL dataset?</title></entry><entry><author><name>/u/topdownAC</name><uri>https://www.reddit.com/user/topdownAC</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;m looking for an advice on how to create REST service that configures agents on runtime.&lt;/p&gt; &lt;p&gt;I have a service that currently runs on fastapi, but if there&amp;#39;s any other tool that might be useful for this, I&amp;#39;m open to suggestion. This app is very simple, it has a route &lt;code&gt;POST /ask&lt;/code&gt; that simply prompts a pre-configured agent. In addition, I want to have another &lt;code&gt;POST /refresh-agent&lt;/code&gt; that in the body gets &amp;quot;uri&amp;quot; argument that will point to a storage path that is the serialized object of the new agent.&lt;/p&gt; &lt;p&gt;For example, I have an agent that I&amp;#39;ve pickled:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;from langchain.agents import AgentType, initialize_agent from langchain.agents import Tool from langchain.chains import LLMMathChain from langchain.llms import OpenAI llm = OpenAI(openai_api_key=&amp;quot;...&amp;quot;) llm_math = LLMMathChain(llm=llm) math_tool = Tool( name=&amp;quot;Calculator&amp;quot;, func=llm_math.run, description=&amp;#39;Useful for when you need to answer questions about math.&amp;#39; ) tools = [math_tool] agent = initialize_agent( tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, ) with open(&amp;quot;fresh_agent.pkl&amp;quot;, &amp;quot;wb&amp;quot;) as f: pickle.dump(agent, handle, protocol=pickle.HIGHEST_PROTOCOL) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;And the &lt;code&gt;/refresh-agent&lt;/code&gt; will look like - &lt;/p&gt; &lt;pre&gt;&lt;code&gt;@router.post(&amp;quot;/refresh-agent&amp;quot;) def refresh_agent(uri: str): with open(uri, &amp;quot;rb&amp;quot;) as f: agent = pickle.load(f) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Is this a flow that exists in applications? is it common? Is there any tool or file format (that might be better than pickle) or serving framework that can somehow help me in this? Any help would be appreciated.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/topdownAC&quot;&gt; /u/topdownAC &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18rglvu/serialize_agentllm_objects_into_files_for_serving/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18rglvu/serialize_agentllm_objects_into_files_for_serving/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18rglvu</id><link href="https://www.reddit.com/r/LangChain/comments/18rglvu/serialize_agentllm_objects_into_files_for_serving/" /><updated>2023-12-26T19:53:48+00:00</updated><published>2023-12-26T19:53:48+00:00</published><title>Serialize agent/llm objects into files for serving</title></entry><entry><author><name>/u/LongjumpingPop3419</name><uri>https://www.reddit.com/user/LongjumpingPop3419</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;There are tools out there like PromptHub, or PromptKnit, that let you manage prompts, compare versions, and easily test them.&lt;/p&gt; &lt;p&gt;But that&amp;#39;s &lt;strong&gt;all they do&lt;/strong&gt;, they only focus on prompts.&lt;/p&gt; &lt;p&gt;On the other hand you have tools like Flowise and Langflow which are robust and great for LLM pipelines, and fast prototyping. But they are &lt;strong&gt;not good&lt;/strong&gt; for versioning, and collaborating with non-technical people on prompt design.&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;I couldn&amp;#39;t find a tool where I enjoy &lt;strong&gt;both worlds&lt;/strong&gt;, but it would be enough to keep the tools separate, and integrate. For example manage the prompts &amp;amp; their versions in Service A, and use them in Service B (e.g. Flowise).&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;Our team is building LLM apps, and is trying to find a good way to prototype and collaborate, where someone like the product manager can come in and play with different versions of one of the prompts in the chain.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/LongjumpingPop3419&quot;&gt; /u/LongjumpingPop3419 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18rb334/any_good_prompt_management_versioning_tools_out/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18rb334/any_good_prompt_management_versioning_tools_out/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18rb334</id><link href="https://www.reddit.com/r/LangChain/comments/18rb334/any_good_prompt_management_versioning_tools_out/" /><updated>2023-12-26T15:54:14+00:00</updated><published>2023-12-26T15:54:14+00:00</published><title>Any good prompt management &amp; versioning tools out there, that integrate nicely?</title></entry><entry><author><name>/u/duddai</name><uri>https://www.reddit.com/user/duddai</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey guys,&lt;/p&gt; &lt;p&gt;maybe someone can help me.&lt;/p&gt; &lt;p&gt;I have this code and an input text which is 15.737 characters long.&lt;/p&gt; &lt;p&gt;And my endresult is cut off at around 1400 characters but idk why&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;pre&gt;&lt;code&gt; try { // Create language model const model = new OpenAI({ openAIApiKey: &amp;quot;key&amp;quot;, temperature: 0, }); // Splitting text const textSplitter = new RecursiveCharacterTextSplitter({ chunkSize: 1000, // 1000 chunkOverlap: 200, // 200 }); const docs = await textSplitter.createDocuments([formData.text]); console.log(docs); const combineMapPromptTemplate = `You will be given a part of a scientific paper. This part will be enclosed in triple hashtags (###). Extract the key ideas and concepts in 3 bullet points. ###&amp;quot;{text}&amp;quot;###`; const combinePromptTemplate = `As a professional summarizer, create a concise and comprehensive summary of the provided text - The text will be enclosed in triple hashtags (###) - while adhering to these guidelines: 1. Craft a summary that is concise and to the point with a well-organized structure. 2. Write in a natural and conversational language with an engaging and informative tone. 3. Incorporate main ideas and essential information, eliminating extraneous language and focusing on critical aspects. 4. Rely strictly on the provided text, without including external information. 5. Your response should be at least three paragraphs and fully encompass what was said in the text. ###&amp;quot;{text}&amp;quot;###`; const combineMapPrompt = new PromptTemplate({ template: combineMapPromptTemplate, inputVariables: [&amp;quot;text&amp;quot;], }); const combinePrompt = new PromptTemplate({ template: combinePromptTemplate, inputVariables: [&amp;quot;text&amp;quot;], }); // This convenience function creates a document chain prompted to summarize a set of documents. const chain = loadSummarizationChain(model, { type: &amp;quot;map_reduce&amp;quot;, returnIntermediateSteps: true, combineMapPrompt: combineMapPrompt, combinePrompt: combinePrompt, }); const res = await chain.call({ input_documents: docs, }); console.log(res); await insertData(formData, res); setResponse([res]); } catch (e) { console.error(e); throw new Error(&amp;quot;Something failed&amp;quot;); } &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/duddai&quot;&gt; /u/duddai &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18repd6/output_get_cuts_off_at_around_1400_characters/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18repd6/output_get_cuts_off_at_around_1400_characters/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18repd6</id><link href="https://www.reddit.com/r/LangChain/comments/18repd6/output_get_cuts_off_at_around_1400_characters/" /><updated>2023-12-26T18:31:32+00:00</updated><published>2023-12-26T18:31:32+00:00</published><title>Output get cuts off at around 1400 characters</title></entry><entry><author><name>/u/devinbost</name><uri>https://www.reddit.com/user/devinbost</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;ve noticed that LCEL is picky about the curly braces used within prompts, which is somewhat problematic when the prompt contains code, like for few-shot code gen use cases. Has anyone found a graceful way to handle the curly braces so LangChain doesn&amp;#39;t think they&amp;#39;re parameters for string substitution? So far, I&amp;#39;ve been replacing them with double curly braces, but it&amp;#39;s not very elegant.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/devinbost&quot;&gt; /u/devinbost &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18rhe6i/lcel_with_prompts_containing_code/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18rhe6i/lcel_with_prompts_containing_code/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18rhe6i</id><link href="https://www.reddit.com/r/LangChain/comments/18rhe6i/lcel_with_prompts_containing_code/" /><updated>2023-12-26T20:27:07+00:00</updated><published>2023-12-26T20:27:07+00:00</published><title>LCEL with prompts containing code</title></entry><entry><author><name>/u/Achiev0r</name><uri>https://www.reddit.com/user/Achiev0r</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hello y&amp;#39;all! I have a working demo for a tool that reads in ebook files, splits chapters into document objects, then feeds it into a VectorstoreIndex. I am able to query against it&amp;#39;s data. Once it&amp;#39;s finished I want to share it with everyone as I see people have trouble remembering what they read in books.&lt;/p&gt; &lt;p&gt;My question is, if I want to add more books, or just use another one to question, should I be creating separate VectorstoreIndexes or just feed everything into one and filter it based on metadata (title, author, ISBN, chapter)? If I want a summary for example, would langchain be able to select all the chapters and not just what it sees as relevant?&lt;/p&gt; &lt;p&gt;And also I have some problems with prompting, I use GPT 3.5 turbo, and when I ask it to create rehearsal questions based on the book it gives quite mediocre and hyper specific ones that are almost irrelevant. The prompt I used: &lt;code&gt;Write 10 test questions from this book in the following format, while escaping special characters from the source: {&amp;quot;questions&amp;quot;:[{&amp;quot;source&amp;quot;:&amp;quot;exact sentences for the information source only&amp;quot;,&amp;quot;question&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;answer_1&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;answer_2&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;answer_3&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;answer_4&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;correct_answer_key&amp;quot;:&amp;quot;answer_*&amp;quot;}]}&lt;/code&gt;. I want to feed the results into some UI to see what I was able to remember from a book on my phone as well.&lt;/p&gt; &lt;p&gt;Thanks for reading, hopefully if you could help me I could help many others.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Achiev0r&quot;&gt; /u/Achiev0r &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18r7ls5/creating_a_scalable_book_questioner/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18r7ls5/creating_a_scalable_book_questioner/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18r7ls5</id><link href="https://www.reddit.com/r/LangChain/comments/18r7ls5/creating_a_scalable_book_questioner/" /><updated>2023-12-26T12:58:10+00:00</updated><published>2023-12-26T12:58:10+00:00</published><title>Creating a scalable book questioner</title></entry><entry><author><name>/u/phicreative1997</name><uri>https://www.reddit.com/user/phicreative1997</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Pretty straight forward question, apart from langchain documentation. Do you have a resource?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/phicreative1997&quot;&gt; /u/phicreative1997 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18r3awq/where_can_i_learn_about_schemas/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18r3awq/where_can_i_learn_about_schemas/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18r3awq</id><link href="https://www.reddit.com/r/LangChain/comments/18r3awq/where_can_i_learn_about_schemas/" /><updated>2023-12-26T08:09:00+00:00</updated><published>2023-12-26T08:09:00+00:00</published><title>Where can I learn about schemas?</title></entry><entry><author><name>/u/DOKim_98</name><uri>https://www.reddit.com/user/DOKim_98</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I have a json file less than &amp;lt; 50mb, that has this format:&lt;/p&gt; &lt;p&gt;{&lt;br/&gt; &amp;quot;title&amp;quot;: &amp;quot;string&amp;quot;,&lt;/p&gt; &lt;p&gt;&amp;quot;url&amp;quot;: &amp;quot;&lt;a href=&quot;https://www.uvu.edu/cet/blog/posts/cgmt_fundraiser_2022.html&quot;&gt;u&lt;/a&gt;rl.html&amp;quot;,&lt;/p&gt; &lt;p&gt;&amp;quot;html&amp;quot;: &amp;quot;html content...&amp;quot;&lt;br/&gt; },&lt;/p&gt; &lt;p&gt;...&lt;/p&gt; &lt;p&gt;and I tried to look for langchain doc that can let openai api like gpt3.5 read json file and give an answer from those data, but it was really hard to find out the doc I wanted. &lt;/p&gt; &lt;p&gt;So, I wonder if anyone knows how to connect json data, and llm to make a chatbot like llm. &lt;/p&gt; &lt;p&gt;#Langchain #Python #json &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/DOKim_98&quot;&gt; /u/DOKim_98 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18qs349/how_do_i_let_llm_read_my_json_file_and_give_an/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18qs349/how_do_i_let_llm_read_my_json_file_and_give_an/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18qs349</id><link href="https://www.reddit.com/r/LangChain/comments/18qs349/how_do_i_let_llm_read_my_json_file_and_give_an/" /><updated>2023-12-25T21:56:35+00:00</updated><published>2023-12-25T21:56:35+00:00</published><title>How do I let llm read my json file and give an answer to a question? Python</title></entry><entry><author><name>/u/iTzPhiil</name><uri>https://www.reddit.com/user/iTzPhiil</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;m interested in creating an AI model that takes in a user&amp;#39;s input, which comes in the form of a JSON dictionary of the music the user likes, and provides recommendations. For example: {name: Mike, genre: hip-hop, song: Gangsta&amp;#39;s Paradise}. I want to create a prompt where I explain what different genres mean, such as rock, pop, and R&amp;amp;B music. The AI would then look at the prompt to understand the genre and provide me with recommendations for similar music.&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;I&amp;#39;ve been playing around, but I&amp;#39;m unsure of how to accomplish this initial idea. The initial idea was to use Structured Output Parser where I define the genre and a Prompt Template where I specify how it should answer the user&amp;#39;s input. Am I on the right track? Thanks in advance.&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/iTzPhiil&quot;&gt; /u/iTzPhiil &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18qtquw/is_langchain_the_right_choice_or_can_i_rely_on/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18qtquw/is_langchain_the_right_choice_or_can_i_rely_on/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18qtquw</id><link href="https://www.reddit.com/r/LangChain/comments/18qtquw/is_langchain_the_right_choice_or_can_i_rely_on/" /><updated>2023-12-25T23:20:03+00:00</updated><published>2023-12-25T23:20:03+00:00</published><title>Is Langchain the right choice, or can I rely on Chat GPT for this?</title></entry><entry><author><name>/u/overflow74</name><uri>https://www.reddit.com/user/overflow74</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18qjcxc/mongodb_agent/&quot;&gt; &lt;img src=&quot;https://b.thumbs.redditmedia.com/d4NwBHYnXAEg1Maiy-OkSp4LrQYORBdaLDEobDyMWgQ.jpg&quot; alt=&quot;MongoDB Agent&quot; title=&quot;MongoDB Agent&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi everyone,I created a basic tools for interacting with mongodb using React agent.&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/q04t7n0p6g8c1.png?width=1286&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0f02e9226b77e9a67fc956718ffbc92826972860&quot;&gt;generated aggregation pipeline suitable for pymongo&lt;/a&gt;&lt;/p&gt; &lt;p&gt;the agent currently has two tools : detect aggregation, execute aggregationthe goal is to convert a natural language query to an aggregation pipeline when executed it would get the desired answer.currently the first tool works perfectly, however the agent fails to execute the aggregation using pymongo.this is the output when it calls the execution tool:&lt;strong&gt;ValueError: An output parsing error occurred. In order to pass this error back to the agent and have it try again, pass `handle_parsing_errors=True` to the AgentExecutor. This is the error: Could not parse LLM output: {&amp;quot;action&amp;quot;: &amp;quot;Execute mongodb aggregation pipeline tool&amp;quot;,&amp;quot;action_input&amp;quot;: [{&amp;#39;$match&amp;#39;: {&amp;#39;overall_rating&amp;#39;: {&amp;#39;$gte&amp;#39;: 4}}}, {&amp;#39;$count&amp;#39;: &amp;#39;satisfied_customers&amp;#39;}]}&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;what could be the issue?&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/overflow74&quot;&gt; /u/overflow74 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18qjcxc/mongodb_agent/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18qjcxc/mongodb_agent/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_18qjcxc</id><media:thumbnail url="https://b.thumbs.redditmedia.com/d4NwBHYnXAEg1Maiy-OkSp4LrQYORBdaLDEobDyMWgQ.jpg" /><link href="https://www.reddit.com/r/LangChain/comments/18qjcxc/mongodb_agent/" /><updated>2023-12-25T14:09:10+00:00</updated><published>2023-12-25T14:09:10+00:00</published><title>MongoDB Agent</title></entry><entry><author><name>/u/Mobile-Hospital-1025</name><uri>https://www.reddit.com/user/Mobile-Hospital-1025</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Mobile-Hospital-1025&quot;&gt; /u/Mobile-Hospital-1025 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;/r/LocalLLaMA/comments/18q3y1p/looking_for_project_ideas/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18q40sf/looking_for_project_ideas/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18q40sf</id><link href="https://www.reddit.com/r/LangChain/comments/18q40sf/looking_for_project_ideas/" /><updated>2023-12-24T21:13:46+00:00</updated><published>2023-12-24T21:13:46+00:00</published><title>Looking for Project Ideas</title></entry><entry><author><name>/u/52637</name><uri>https://www.reddit.com/user/52637</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I’m currently working on an Agent implementation that uses tools to update a pre-defined pydantic model which needs values to be added/updated based on context retrieved by the agent. My problem, however, is that I cannot seem to figure out how to keep an instantiation of this pydantic model linked to the agent’s execution. &lt;/p&gt; &lt;p&gt;Is it possible to access and update metadata in the AgentExecutor? &lt;/p&gt; &lt;p&gt;Wondering if anyone has experience doing something similar to this?&lt;/p&gt; &lt;p&gt;My current implementation is stateless, building the pydantic models during the output parsing step, however, this often results in output parsing errors. My thinking is also that this new implementation method will make evaluation simpler, since the quality of the model would be tied to correct tool execution as opposed to completeness of the pydantic model. &lt;/p&gt; &lt;p&gt;All thoughts/questions/comments appreciated!&lt;/p&gt; &lt;p&gt;Happy Holidays.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/52637&quot;&gt; /u/52637 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18pzqkk/best_way_to_populate_a_pydantic_model_during_an/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18pzqkk/best_way_to_populate_a_pydantic_model_during_an/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18pzqkk</id><link href="https://www.reddit.com/r/LangChain/comments/18pzqkk/best_way_to_populate_a_pydantic_model_during_an/" /><updated>2023-12-24T17:34:00+00:00</updated><published>2023-12-24T17:34:00+00:00</published><title>Best way to populate a pydantic model during an agent run</title></entry><entry><author><name>/u/WishboneReal534</name><uri>https://www.reddit.com/user/WishboneReal534</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey! I am trying to create a vector store using langchain and faiss for RAG(Retrieval-augmented generation) with about 6 millions abstracts. is there a strategy to create this vector store efficiently? currently it takes very long time to create it (can take up to 5 days)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/WishboneReal534&quot;&gt; /u/WishboneReal534 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18ph140/creating_a_vectordb_from_millions_of_documents/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18ph140/creating_a_vectordb_from_millions_of_documents/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18ph140</id><link href="https://www.reddit.com/r/LangChain/comments/18ph140/creating_a_vectordb_from_millions_of_documents/" /><updated>2023-12-23T22:43:13+00:00</updated><published>2023-12-23T22:43:13+00:00</published><title>creating a vectordb from millions of documents</title></entry><entry><author><name>/u/enspiralart</name><uri>https://www.reddit.com/user/enspiralart</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;With the ability of agents to search the web and use the data it finds in RAG, it seems that one could effectively make a research agent who&amp;#39;s sole purpose is to find datasets for the LLM to consume:&lt;/p&gt; &lt;p&gt;Obstacles&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Navigating and choosing the data to make datasets from (research path if you will), maybe using ToT&lt;/li&gt; &lt;li&gt;Data preparation, coming up with a standard to store the data chosen and creating an entry for the yaml&lt;/li&gt; &lt;li&gt;Some way to test if the data is within the existing dataset for the model in order to skip it or treat it with less weight. Could possibly be done using a carefully crafted completion prompt and check the completion tokens against ground truth in the data being scrutinized&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Do you think this would be helpful as a way to automatically generate non-synthetic datasets?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/enspiralart&quot;&gt; /u/enspiralart &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18prq9i/has_anyone_used_llms_to_compile_training_data_for/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18prq9i/has_anyone_used_llms_to_compile_training_data_for/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18prq9i</id><link href="https://www.reddit.com/r/LangChain/comments/18prq9i/has_anyone_used_llms_to_compile_training_data_for/" /><updated>2023-12-24T09:15:10+00:00</updated><published>2023-12-24T09:15:10+00:00</published><title>Has anyone used LLMs to compile training data for LLMs?</title></entry><entry><author><name>/u/phicreative1997</name><uri>https://www.reddit.com/user/phicreative1997</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi, I am from Pakistan, don&amp;#39;t have access to a online payment system. &lt;/p&gt; &lt;p&gt;Really want to learn langchain, was wondering if someone has a digital copy of this book. Can you share the copy, I promise never to put it up for download, will just keep it and learn&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/phicreative1997&quot;&gt; /u/phicreative1997 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18pt9fd/anyone_can_lend_me_a_digital_copy_of_generative/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18pt9fd/anyone_can_lend_me_a_digital_copy_of_generative/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18pt9fd</id><link href="https://www.reddit.com/r/LangChain/comments/18pt9fd/anyone_can_lend_me_a_digital_copy_of_generative/" /><updated>2023-12-24T11:11:37+00:00</updated><published>2023-12-24T11:11:37+00:00</published><title>Anyone can lend me a digital copy of Generative AI with LangChain</title></entry><entry><author><name>/u/FistfulOfHaws</name><uri>https://www.reddit.com/user/FistfulOfHaws</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I am working on a project that uses Langchain in multiple places, I am getting inconsistent behavior, hoping someone can tell me what I am doing wrong here. (I am using a public bq dataset for this, so nothing proprietary in what I am posting). I first use agent_executor to generate a description of a table. I pass the table in as a variable. But the agent attempts to run the query return an error that the table is not found. In the same project I use db_chain to allow natural language to SQL querying of the table. In this case the table is found and a result is return. I have checked the SQL and results returned against the source data to confirm it is indeed querying the table. I am not sure why the table is found in one case but not the other&lt;/p&gt; &lt;pre&gt;&lt;code&gt;# Working agent system_prompt = f&amp;#39; in {table_to_query}.You are a BigQuery expert. You are able quickly review the tables in a dataset and understand the contents of each table along with their relation. You will be asked a question for which you need to generate and execute a query. The table in the question is the main focus of the question, but you may also need to join to other tables, so keep them in mind as your create your plan. The other tables are {dataset_table_names}. The column names may not match 1:1 in the prompt, use your best reasoning to select a column (for instance a user may ask for an account but in the table the column is account_name).Ensure that the columns you use in the query exist in the table. As you answer the users question, consider what other columns may be additive to their question and include those in your response&amp;#39; full_prompt = user_prompt + system_prompt if run_prompt: from langchain.utilities import SQLDatabase from langchain.llms import OpenAI from langchain_experimental.sql import SQLDatabaseChain db = SQLDatabase(engine) #, include_tables=prompt_tables) llm = OpenAI(temperature=.5, verbose=True) db_chain = SQLDatabaseChain.from_llm(llm, verbose=True,db=db, use_query_checker=True, top_k=10) db_chain.run(full_prompt) else: display(&amp;quot;Waiting on you to run the query&amp;quot;) &amp;gt; Entering new SQLDatabaseChain chain... What was the most popular name in Utah in 2010 in bigquery-public-data.usa_names.usa_1910_2013. SQLQuery:SELECT name, SUM(number) AS total FROM `bigquery-public-data`.usa_names.usa_1910_2013 WHERE state = &amp;#39;UT&amp;#39; AND year = 2010 GROUP BY name ORDER BY total DESC LIMIT 10 SQLResult: [(&amp;#39;Olivia&amp;#39;, 269), (&amp;#39;William&amp;#39;, 264), (&amp;#39;Mason&amp;#39;, 243), (&amp;#39;Jacob&amp;#39;, 235), (&amp;#39;Ethan&amp;#39;, 235), (&amp;#39;James&amp;#39;, 231), (&amp;#39;Samuel&amp;#39;, 227), (&amp;#39;Isaac&amp;#39;, 215), (&amp;#39;Abigail&amp;#39;, 210), (&amp;#39;Logan&amp;#39;, 206)] Answer:Olivia &amp;gt; Finished chain. &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;--&lt;/p&gt; &lt;pre&gt;&lt;code&gt;# Non Working from langchain.agents import create_sql_agent from langchain.agents.agent_toolkits import SQLDatabaseToolkit from langchain.utilities import SQLDatabase from langchain.llms import OpenAI # from langchain.agents import AgentExecutor from langchain.agents.agent_types import AgentType db = SQLDatabase(engine) agent_executor = create_sql_agent( llm=OpenAI(temperature=0), toolkit=SQLDatabaseToolkit(db=db, llm=OpenAI(temperature=0)), verbose=True, agent_type=AgentType.ZERO_SHOT_REACT_DESCRIPTION, ) agent_executor.run(f&amp;quot;Describe the {table_to_query} table&amp;quot;) &amp;gt; Entering new AgentExecutor chain... Action: sql_db_list_tables Action Input: Observation: Thought: I should query the schema of the usa_names table. Action: sql_db_schema Action Input: bigquery-public-data.usa_names.usa_1910_2013 Observation: Error: table_names {&amp;#39;bigquery-public-data.usa_names.usa_1910_2013&amp;#39;} not found in database Thought: I should check the spelling of the table name. Action: sql_db_schema Action Input: bigquery-public-data.usa_names.usa_1910_2013 Observation: Error: table_names {&amp;#39;bigquery-public-data.usa_names.usa_1910_2013&amp;#39;} not found in database &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/FistfulOfHaws&quot;&gt; /u/FistfulOfHaws &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18pfbu9/inconsistent_table_querying/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18pfbu9/inconsistent_table_querying/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18pfbu9</id><link href="https://www.reddit.com/r/LangChain/comments/18pfbu9/inconsistent_table_querying/" /><updated>2023-12-23T21:19:02+00:00</updated><published>2023-12-23T21:19:02+00:00</published><title>Inconsistent Table Querying</title></entry><entry><author><name>/u/coderinlaw</name><uri>https://www.reddit.com/user/coderinlaw</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;m working on a project involving where I want to analyze conversations between two individuals, let&amp;#39;s call them Person A and Person B. The primary function of this system is to enable a bot to answer questions about Person A&amp;#39;s interests based on past conversations between A and other individuals.&lt;/p&gt; &lt;p&gt;I am contemplating the best approach to use embedding models in this context. The challenge lies in conversational data, which is inherently different from structured documents. Conversations typically lack distinct paragraphs or sections, making traditional chunking and embedding techniques less straightforward.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Sentence-Level Embeddings&lt;/strong&gt;: Embedding each sentence individually to capture specific details. However, this might limit the response to only the information contained in that particular sentence.&lt;br/&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Conversation-Level Embeddings&lt;/strong&gt;: Creating embeddings for entire conversations. While this could capture the overall context, it might not be precise for detailed queries.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Summarization Before Embedding&lt;/strong&gt;: Generating a summarized version of the conversations and then embedding these summaries. I&amp;#39;m curious about the effectiveness and potential loss of detail with this method.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;My questions for the community are:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;What are the recommended practices for embedding models in this kind of RAG system, especially considering the conversational nature of the data?&lt;/li&gt; &lt;li&gt;Are there any specific techniques or methodologies that you would suggest for this type of application, possibly something that has worked well in your experience?&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/coderinlaw&quot;&gt; /u/coderinlaw &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18pa4gz/how_to_create_a_rag_for_all_the_chats/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18pa4gz/how_to_create_a_rag_for_all_the_chats/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18pa4gz</id><link href="https://www.reddit.com/r/LangChain/comments/18pa4gz/how_to_create_a_rag_for_all_the_chats/" /><updated>2023-12-23T17:10:14+00:00</updated><published>2023-12-23T17:10:14+00:00</published><title>how to create a rag for all the chats/ conversations between A and everyone else wherein a bot can answer a question about anyone A had a conversation with?</title></entry><entry><author><name>/u/phicreative1997</name><uri>https://www.reddit.com/user/phicreative1997</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi, I have been trying to use LangChain Selenium and other url loaders, but can&amp;#39;t find good documentation for now. Any information source is welcome.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/phicreative1997&quot;&gt; /u/phicreative1997 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18p25me/any_good_documentationtutorialebook_on_url_tools/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18p25me/any_good_documentationtutorialebook_on_url_tools/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18p25me</id><link href="https://www.reddit.com/r/LangChain/comments/18p25me/any_good_documentationtutorialebook_on_url_tools/" /><updated>2023-12-23T09:31:09+00:00</updated><published>2023-12-23T09:31:09+00:00</published><title>Any good documentation/tutorial/e-book on url tools in langchain?</title></entry><entry><author><name>/u/pmz</name><uri>https://www.reddit.com/user/pmz</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/pmz&quot;&gt; /u/pmz &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://adilmoujahid.com/posts/2023/10/kanji-gpt4/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18p2fii/building_a_japanese_kanji_flashcard_app_using/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18p2fii</id><link href="https://www.reddit.com/r/LangChain/comments/18p2fii/building_a_japanese_kanji_flashcard_app_using/" /><updated>2023-12-23T09:52:21+00:00</updated><published>2023-12-23T09:52:21+00:00</published><title>Building a Japanese Kanji Flashcard App using GPT-4, Python and Langchain</title></entry><entry><author><name>/u/mrripo</name><uri>https://www.reddit.com/user/mrripo</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;The application features integrations with various tools, including databases, Retrieval-Augmented Generation (RAG), and custom prompts, as well as custom tools within LangChain. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/mrripo&quot;&gt; /u/mrripo &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18p1ghh/what_frameworks_or_coding_structures_are/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18p1ghh/what_frameworks_or_coding_structures_are/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18p1ghh</id><link href="https://www.reddit.com/r/LangChain/comments/18p1ghh/what_frameworks_or_coding_structures_are/" /><updated>2023-12-23T08:39:44+00:00</updated><published>2023-12-23T08:39:44+00:00</published><title>What frameworks or coding structures are recommended for building applications powered by LangChain and large language models (LLMs)?</title></entry><entry><author><name>/u/peculiaroptimist</name><uri>https://www.reddit.com/user/peculiaroptimist</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18opwbl/ashamed_to_asked/&quot;&gt; &lt;img src=&quot;https://b.thumbs.redditmedia.com/1Q0_I_p3-jEZwHDBWF312WNb4Tlo44zFSQq5j8VT0dA.jpg&quot; alt=&quot;Ashamed to asked&quot; title=&quot;Ashamed to asked&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;kind of ashamed to ask but what am i missing here ?. the cash_flow_data method returns a list of cash flow statements in the form of dataframes, then i try to map each iteration to the prompt template but thats not working. instead i get this error. Any ideas why? &lt;a href=&quot;/u/hwchase17&quot;&gt;u/hwchase17&lt;/a&gt; . &lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/6o8c00uy2x7c1.png?width=2336&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=62d403052114c9de56573f1a85b4306497c23599&quot;&gt;https://preview.redd.it/6o8c00uy2x7c1.png?width=2336&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=62d403052114c9de56573f1a85b4306497c23599&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/r36pe0uy2x7c1.png?width=2336&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=89767aa7dbb3a4a2406229bc9611078ef0f4915d&quot;&gt;https://preview.redd.it/r36pe0uy2x7c1.png?width=2336&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=89767aa7dbb3a4a2406229bc9611078ef0f4915d&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/peculiaroptimist&quot;&gt; /u/peculiaroptimist &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18opwbl/ashamed_to_asked/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18opwbl/ashamed_to_asked/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_18opwbl</id><media:thumbnail url="https://b.thumbs.redditmedia.com/1Q0_I_p3-jEZwHDBWF312WNb4Tlo44zFSQq5j8VT0dA.jpg" /><link href="https://www.reddit.com/r/LangChain/comments/18opwbl/ashamed_to_asked/" /><updated>2023-12-22T21:51:15+00:00</updated><published>2023-12-22T21:51:15+00:00</published><title>Ashamed to asked</title></entry><entry><author><name>/u/Purity1212</name><uri>https://www.reddit.com/user/Purity1212</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;I&amp;#39;m currently trying to implement my RAG application in Streamlit without the use of LangChain for various reasons, however i have a problem to get the streaming response right for my LLM (AWS SageMaker endpoint).&lt;/p&gt; &lt;p&gt;The previous approach that works is passing a custom Streamhandler (that takes the streamlit container) to the Chain that overrides the on_llm_new_token method (writes to the container with every call of the method) and modifying the sagemaker_endpoint.py that it calls the method for every Token i get from the Event Stream. &lt;/p&gt; &lt;p&gt;As seen here: &lt;a href=&quot;https://github.com/langchain-ai/chat-langchain/issues/39&quot;&gt;https://github.com/langchain-ai/chat-langchain/issues/39&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;Now when trying to get this to work without LangChain i only get empty responses.&lt;/p&gt; &lt;p&gt;I call the endpoint via boto3 client and successfully get a response stream. However when iterating with the TokenIterator nothing happens. This approach would work in a normal python script with writing to the console but not within my Streamlit application.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;def call_llm(prompt, container): response = boto3_client.invoke_endpoint_with_response_stream( Arguments... (No errors here) ) print(response) # Shows that i get a valid EventStream current_completion = &amp;quot;&amp;quot; for token in TokenIterator(response[&amp;quot;Body&amp;quot;]): current_completion += token print(token) # Nothing happens here container.markdown(current_completion) # Nothing happens here either &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Same problem when i create a stream_handler class (not inherited from the LangChain BaseCallbackHandler) with the corresponding method. I don&amp;#39;t really understand how the Callbacks work within LangChain. It seems like i can&amp;#39;t get the same behaviour if i code it myself.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;def call_llm(prompt, stream_handler): # Give a streamhandler with corresponding container instead response = boto3_client.invoke_endpoint_with_response_stream( Arguments... (No errors here) ) print(response) # Shows that i get a valid EventStream current_completion = &amp;quot;&amp;quot; for token in TokenIterator(response[&amp;quot;Body&amp;quot;]): current_completion += token print(token) # Nothing happens here stream_handler.on_llm_new_token(current_completion) # Nothing happens here either &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;I&amp;#39;d be very thankful for a workaround or an explanation how the Callbacks work in LangChain.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Purity1212&quot;&gt; /u/Purity1212 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18ogw3p/how_do_callbacks_for_streaming_response_exactly/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18ogw3p/how_do_callbacks_for_streaming_response_exactly/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18ogw3p</id><link href="https://www.reddit.com/r/LangChain/comments/18ogw3p/how_do_callbacks_for_streaming_response_exactly/" /><updated>2023-12-22T15:02:10+00:00</updated><published>2023-12-22T15:02:10+00:00</published><title>How do Callbacks for streaming response exactly work? (In Streamlit application)</title></entry><entry><author><name>/u/debordian</name><uri>https://www.reddit.com/user/debordian</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18og9cn/langchain_state_of_ai_2023/&quot;&gt; &lt;img src=&quot;https://external-preview.redd.it/OwfKIOpfNaeizL9ZAEuyFGT4JpnvnQpYPxb-u2xBfTE.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=69887b5da6ec0a8e7469f24296fb78e3937f2406&quot; alt=&quot;LangChain State of AI 2023&quot; title=&quot;LangChain State of AI 2023&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/debordian&quot;&gt; /u/debordian &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://blog.langchain.dev/langchain-state-of-ai-2023/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18og9cn/langchain_state_of_ai_2023/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_18og9cn</id><media:thumbnail url="https://external-preview.redd.it/OwfKIOpfNaeizL9ZAEuyFGT4JpnvnQpYPxb-u2xBfTE.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=69887b5da6ec0a8e7469f24296fb78e3937f2406" /><link href="https://www.reddit.com/r/LangChain/comments/18og9cn/langchain_state_of_ai_2023/" /><updated>2023-12-22T14:33:17+00:00</updated><published>2023-12-22T14:33:17+00:00</published><title>LangChain State of AI 2023</title></entry><entry><author><name>/u/chrmcstingTom</name><uri>https://www.reddit.com/user/chrmcstingTom</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=D34PyNx71vk&quot;&gt;https://www.youtube.com/watch?v=D34PyNx71vk&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The YouTube video shares an architectural difference between LangChain and Microsofts orchestrator. Is there really no way to do the same in LangChain?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/chrmcstingTom&quot;&gt; /u/chrmcstingTom &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18om6xq/is_there_a_way_in_lc_to_centralize_event/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18om6xq/is_there_a_way_in_lc_to_centralize_event/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18om6xq</id><link href="https://www.reddit.com/r/LangChain/comments/18om6xq/is_there_a_way_in_lc_to_centralize_event/" /><updated>2023-12-22T18:57:36+00:00</updated><published>2023-12-22T18:57:36+00:00</published><title>Is there a way in LC to centralize event notifications and configurations?</title></entry><entry><author><name>/u/Pristine-Hawk-7841</name><uri>https://www.reddit.com/user/Pristine-Hawk-7841</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Anybody have any idea what I might be doing wrong here?&lt;/p&gt; &lt;pre&gt;&lt;code&gt;const claudeBedrock = new Bedrock({model: &amp;quot;anthropic.claude-v2:1&amp;quot;,region: aws_region,modelKwargs: {temperature: 0.0,top_k: 250,top_p: 0.999,stop_sequences: [&amp;#39;Human:&amp;#39;],max_tokens_to_sample: maxTokens}} const chain = = new LLMChain({llm: claudeBedrock, prompt: prompt}) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;I have tried .&lt;code&gt;call&lt;/code&gt;, .&lt;code&gt;_call&lt;/code&gt;, .&lt;code&gt;invoke&lt;/code&gt;, .&lt;code&gt;predict&lt;/code&gt; and they all produce worse results than through the AWS console.&lt;/p&gt; &lt;p&gt;I have verbose ON and literally copy and pasting the logged prompt produced by Langchain into the Bedrock playground and it produces better results.&lt;/p&gt; &lt;p&gt;I tried both Chat &amp;amp; Text in playgrounds using the EXACT same settings as above.&lt;/p&gt; &lt;p&gt;The prompt is asking Claude to classify a list of something. I have tried multiple times on both sides and the output through Langchain is WORSE by including results that is wrong. The console produces correct results everytime.&lt;/p&gt; &lt;p&gt;It&amp;#39;s driving me nuts that I&amp;#39;m about to tear out Langchain and use AWS&amp;#39;s SDK instead.&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Pristine-Hawk-7841&quot;&gt; /u/Pristine-Hawk-7841 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18opvla/bedrock_claude_performance_issue/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18opvla/bedrock_claude_performance_issue/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18opvla</id><link href="https://www.reddit.com/r/LangChain/comments/18opvla/bedrock_claude_performance_issue/" /><updated>2023-12-22T21:50:17+00:00</updated><published>2023-12-22T21:50:17+00:00</published><title>Bedrock Claude Performance Issue</title></entry></feed>