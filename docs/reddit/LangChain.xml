<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/"><category term="LangChain" label="r/LangChain"/><updated>2024-05-13T08:43:47+00:00</updated><icon>https://www.redditstatic.com/icon.png/</icon><id>/r/LangChain.rss</id><link rel="self" href="https://www.reddit.com/r/LangChain.rss" type="application/atom+xml" /><link rel="alternate" href="https://www.reddit.com/r/LangChain" type="text/html" /><subtitle>LangChain is an open-source framework and developer toolkit that helps developers get LLM applications from prototype to production. It is available for Python and Javascript at https://www.langchain.com/.</subtitle><title>LangChain</title><entry><author><name>/u/zchaarm</name><uri>https://www.reddit.com/user/zchaarm</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;A place for members of &lt;a href=&quot;/r/LangChain&quot;&gt;r/LangChain&lt;/a&gt; to chat with each other&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/zchaarm&quot;&gt; /u/zchaarm &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_10ljho9</id><link href="https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/" /><updated>2023-01-26T04:33:05+00:00</updated><published>2023-01-26T04:33:05+00:00</published><title>r/LangChain Lounge</title></entry><entry><author><name>/u/cryptokaykay</name><uri>https://www.reddit.com/user/cryptokaykay</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I have been tinkering with DSPy and thought I will share my 2 cents here for anyone who is planning to explore it:&lt;/p&gt; &lt;p&gt;The core idea behind DSPy are two things:&lt;/p&gt; &lt;ol&gt; &lt;li&gt; ⁠Separate programming from prompting&lt;/li&gt; &lt;li&gt; ⁠incorporate some of the best practice prompting techniques under the hood and expose it as a “signature”&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Imagine working on a RAG. Today, the typical approach is to write some retrieval and pass the results to a language model for natural language generation. But, after the first pass, you realize it’s not perfect and you need to iterate and improve it. Typically, there are 2 levers to pull:&lt;/p&gt; &lt;ol&gt; &lt;li&gt; ⁠Document Chunking, insertion and Retrieval strategy&lt;/li&gt; &lt;li&gt; ⁠Language model settings and prompt engineering&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Now, you try a few things, maybe document the performance in a google sheet, iterate and arrive at an ideal set of variables that gives max accuracy.&lt;/p&gt; &lt;p&gt;Now, let’s say after a month, model upgrades, and all of a sudden the accuracy of your RAG regresses. Again you are back to square one, cos you don’t know what to optimize now - retrieval or model? You see what the problem is with this approach? This is a very open ended, monolithic, brittle and unstructured way to optimize and build language model based applications.&lt;/p&gt; &lt;p&gt;This is precisely the problem DSPy is trying to solve. Whatever you can achieve with DSPy can be achieved with native prompt engineering and program composition techniques but it is purely dependent on the programmers skill. But DSPy provides native constructs which anyone can learn and use for trying different techniques in a systematic manner.&lt;/p&gt; &lt;p&gt;DSPy the concept:&lt;/p&gt; &lt;p&gt;Separate prompting from programming and signatures&lt;/p&gt; &lt;p&gt;DSPy does not do any magic with the language model. It just uses a bunch of prompt templates behind the scenes and exposes them as signatures. Ex: when you write a signature like ‘context, question -&amp;gt; answer’, DSPy adds a typical RAG prompt before it makes the call to the LLM. But DSPy also gives you nice features like module settings, assertion based backtracking and automatic prompt optimization.&lt;/p&gt; &lt;p&gt;Basically, you can do something like this with DSPy,&lt;/p&gt; &lt;p&gt;“Given a context and question, answer the following question. Make sure the answer is only “yes” or “no””. If the language model responds with anything else, traditionally we prompt engineer our way to fix it. In DSPy, you can assert the answer for “yes” or “no” and if the assertion fails, DSPy will backtrack automatically, update the prompt to say something like, “this is not a correct answer- {previous_answer} and always only respond with a “yes” or “no”” and makes another language model call which improves the LLMs response because of this newly optimized prompt. In addition, you can also incorporate things like multi hops in your retrieval where you can do something like “retrieve -&amp;gt; generate queries and then retrieve again using the generated queries” for n times and build up a larger context to answer the original question.&lt;/p&gt; &lt;p&gt;Obviously, this can also be done using usual prompt engineering and programming techniques, but the framework exposes native easy to use settings and constructs to do these things more naturally. DSPy as a concept really shines when you are composing a pipeline of language model calls where prompt engineering the entire pipeline or even module wise can lead to a brittle Pipeline.&lt;/p&gt; &lt;p&gt;DSPy the Framework:&lt;/p&gt; &lt;p&gt;Now coming to the framework which is built in python, I think the framework as it stands today is&lt;/p&gt; &lt;ol&gt; &lt;li&gt; ⁠Not production ready&lt;/li&gt; &lt;li&gt; ⁠Buggy and poorly implemented&lt;/li&gt; &lt;li&gt; ⁠Lacks proper documentation&lt;/li&gt; &lt;li&gt; ⁠Poorly designed&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;To me it felt like a rushed implementation with little thought for design thinking, testing and programming principles. The framework code is very hard to understand with a lot of meta programming and data structure parsing and construction going behind the scenes that are scary to run in production.&lt;/p&gt; &lt;p&gt;This is a huge deterrent for anyone trying to learn and use this framework. But, I am sure the creators are thinking about all this and are working to reengineer the framework. There’s also a typescript implementation of this framework that is fairly less popular but has a much better and cleaner design and codebase:&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/dosco/llm-client/&quot;&gt;https://github.com/dosco/llm-client/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;My final thought about this framework is, it’s a promising concept, but it does not change anything about what we already know about LLMs. Also, hiding prompts as templates does not mean prompt engineering is going away, someone still needs to “engineer” the prompts the framework uses and imo the framework should expose these templates and give control back to the developers that way, the vision of separate programming and prompting co exists with giving control not only to program but also to prompt.&lt;/p&gt; &lt;p&gt;Finally, I was able to understand all this by running DSPy programs and visualizing the LLM calls and what prompts it’s adding using my open source tool - &lt;a href=&quot;https://github.com/Scale3-Labs/langtrace&quot;&gt;https://github.com/Scale3-Labs/langtrace&lt;/a&gt; . Do check it out and let me know if you have any feedback.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/cryptokaykay&quot;&gt; /u/cryptokaykay &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cqexk6/thoughts_on_dspy/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cqexk6/thoughts_on_dspy/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cqexk6</id><link href="https://www.reddit.com/r/LangChain/comments/1cqexk6/thoughts_on_dspy/" /><updated>2024-05-12T18:50:23+00:00</updated><published>2024-05-12T18:50:23+00:00</published><title>Thoughts on DSPy</title></entry><entry><author><name>/u/EngineeringFree313</name><uri>https://www.reddit.com/user/EngineeringFree313</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hello,&lt;/p&gt; &lt;p&gt;I am trying to build customer support chat-bot with the goal of answering questions related to some training center courses. I have pdf document per each course containing all the information like course content, tools, price etc..&lt;br/&gt; If I load all documents into one index on a vector db, the returned chunks from db using similarity search given user&amp;#39;s query will probably related to other courses. for example, if the customer asks for a price of some course, All price chunks of all courses will be similar and a price of different course will probably be the answer which is wrong.&lt;br/&gt; So What is the standard way to solve this problem and target the right document only relevant to a specific course regardless all other courses documents.&lt;br/&gt; Also, I might in some cases need to retrieve data from multiple documents or all documents in case of a general question from the user like &amp;quot;List all courses&amp;quot; for example. In this case, I need all courses documents to answer this question.&lt;/p&gt; &lt;p&gt;I am building my project using langchain and gpt-3.5.&lt;/p&gt; &lt;p&gt;Thanks in advance.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/EngineeringFree313&quot;&gt; /u/EngineeringFree313 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cqu5di/customer_support_rag_chatbot/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cqu5di/customer_support_rag_chatbot/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cqu5di</id><link href="https://www.reddit.com/r/LangChain/comments/1cqu5di/customer_support_rag_chatbot/" /><updated>2024-05-13T08:17:52+00:00</updated><published>2024-05-13T08:17:52+00:00</published><title>Customer Support RAG Chat-bot</title></entry><entry><author><name>/u/Agreeable_Fill_2868</name><uri>https://www.reddit.com/user/Agreeable_Fill_2868</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Is &lt;strong&gt;&lt;em&gt;GROK&lt;/em&gt;&lt;/strong&gt; API free of any cost?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Agreeable_Fill_2868&quot;&gt; /u/Agreeable_Fill_2868 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cqtmhq/grok/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cqtmhq/grok/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cqtmhq</id><link href="https://www.reddit.com/r/LangChain/comments/1cqtmhq/grok/" /><updated>2024-05-13T07:39:29+00:00</updated><published>2024-05-13T07:39:29+00:00</published><title>GROK</title></entry><entry><author><name>/u/giobirkelund</name><uri>https://www.reddit.com/user/giobirkelund</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;When sending confidential, and highly sensitive data in rag search, I believe everything needs to be encrypted, so that even me, as the database operator, doesn&amp;#39;t have access to the data.&lt;/p&gt; &lt;p&gt;This must be a common use-case, as any company doing rag search on sensitive data has this problem. So I wonder, does anyone know how to do RAG search for sensitive data? &lt;/p&gt; &lt;p&gt;I would imagine you need to encrypt the embeddings, but how do you do the cosine similarity search on encrypted data? Seems like a tricky problem. I&amp;#39;m currently using mongodb atlas vector store, but they don&amp;#39;t offer search on encrypted data.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/giobirkelund&quot;&gt; /u/giobirkelund &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cqr72f/rag_search_on_sensitive_data/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cqr72f/rag_search_on_sensitive_data/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cqr72f</id><link href="https://www.reddit.com/r/LangChain/comments/1cqr72f/rag_search_on_sensitive_data/" /><updated>2024-05-13T04:56:26+00:00</updated><published>2024-05-13T04:56:26+00:00</published><title>RAG Search on sensitive data?</title></entry><entry><author><name>/u/Zenwills</name><uri>https://www.reddit.com/user/Zenwills</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi all newbie question here. I would like to summarize a large document using map_reduce but facing the 1,024 token limits. I understand this could be due to the model using GPT 2 tokenizer which has a max of 1,024. &lt;/p&gt; &lt;p&gt;Due to this limit, my summarization failed to summarize contents from the earlier chunks.&lt;/p&gt; &lt;p&gt;has anyone done a workaround on this? potentially if making the model to use another tokenizer instead of GPT 2 to able to take in way more tokens?&lt;/p&gt; &lt;p&gt;many thanks for your insights&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Zenwills&quot;&gt; /u/Zenwills &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cqqvtn/map_reduce_summarization_tackling_the_1024_token/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cqqvtn/map_reduce_summarization_tackling_the_1024_token/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cqqvtn</id><link href="https://www.reddit.com/r/LangChain/comments/1cqqvtn/map_reduce_summarization_tackling_the_1024_token/" /><updated>2024-05-13T04:37:24+00:00</updated><published>2024-05-13T04:37:24+00:00</published><title>map_reduce summarization - tackling the 1,024 token limit</title></entry><entry><author><name>/u/MadK92</name><uri>https://www.reddit.com/user/MadK92</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;m currently integrating a llm into our system for job classification. Users can submit requests via text or voice, for example, &amp;#39;I want to design a cake for my son’s birthday.&amp;#39; My goal is for the model to accurately classify these inputs into predefined job categories and types (e.g., {&amp;quot;job_type&amp;quot;:&amp;quot;Plumbing&amp;quot;, &amp;quot;job_category&amp;quot;:&amp;quot;Handyman&amp;quot;}) and subsequently trigger a specific function. However, I&amp;#39;m encountering issues with open-source models like Functionary, which sometimes incorrectly assign random categories or types not present in my predefined list. As I&amp;#39;m still very new to this, I would greatly appreciate any advice on how to ensure the model strictly adheres to my existing categories and types. Thanks for your help!&lt;/p&gt; &lt;p&gt;Edit: I got some interesting results with OpenAI API + pydantic for Hermes-2-Pro-Mistral-7B. Still testing it&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/MadK92&quot;&gt; /u/MadK92 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cqab15/function_calling/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cqab15/function_calling/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cqab15</id><link href="https://www.reddit.com/r/LangChain/comments/1cqab15/function_calling/" /><updated>2024-05-12T15:23:50+00:00</updated><published>2024-05-12T15:23:50+00:00</published><title>Function Calling</title></entry><entry><author><name>/u/magic_26</name><uri>https://www.reddit.com/user/magic_26</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hello. I&amp;#39;m trying to find the best and easiest approach for someone who knows some python to create a script etc to determine most popular topics within unstructured data ie. Product reviews and text responses in surveys. Also, is there a way to leverage LangChain to do this? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/magic_26&quot;&gt; /u/magic_26 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cqnmt2/easiest_method_for_topic_modelling_in_2024_and/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cqnmt2/easiest_method_for_topic_modelling_in_2024_and/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cqnmt2</id><link href="https://www.reddit.com/r/LangChain/comments/1cqnmt2/easiest_method_for_topic_modelling_in_2024_and/" /><updated>2024-05-13T01:37:33+00:00</updated><published>2024-05-13T01:37:33+00:00</published><title>Easiest method for topic modelling in 2024 and can LangChain help?</title></entry><entry><author><name>/u/Smooth-Loquat-4954</name><uri>https://www.reddit.com/user/Smooth-Loquat-4954</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cqdqhe/build_a_rag_pipeline_for_your_blog_with_langchain/&quot;&gt; &lt;img src=&quot;https://external-preview.redd.it/-O7gvFEBkVog1UZh_wSCyLF_DU9VUIxtgOUCFlF_ZCM.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=430e01c4f8de7857f3e4c7ca7f056cd9d5cb8a6d&quot; alt=&quot;Build a RAG pipeline for your blog with LangChain, OpenAI and Pinecone&quot; title=&quot;Build a RAG pipeline for your blog with LangChain, OpenAI and Pinecone&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Smooth-Loquat-4954&quot;&gt; /u/Smooth-Loquat-4954 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://zackproser.com/blog/langchain-pinecone-chat-with-my-blog&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cqdqhe/build_a_rag_pipeline_for_your_blog_with_langchain/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1cqdqhe</id><media:thumbnail url="https://external-preview.redd.it/-O7gvFEBkVog1UZh_wSCyLF_DU9VUIxtgOUCFlF_ZCM.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=430e01c4f8de7857f3e4c7ca7f056cd9d5cb8a6d" /><link href="https://www.reddit.com/r/LangChain/comments/1cqdqhe/build_a_rag_pipeline_for_your_blog_with_langchain/" /><updated>2024-05-12T17:58:04+00:00</updated><published>2024-05-12T17:58:04+00:00</published><title>Build a RAG pipeline for your blog with LangChain, OpenAI and Pinecone</title></entry><entry><author><name>/u/cholekulchhe</name><uri>https://www.reddit.com/user/cholekulchhe</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I am using GPT 3.5 turbo model in my program via below statement but still getting &amp;#39;not a chat model&amp;#39; error, can someone please help me fix it?&lt;/p&gt; &lt;pre&gt;&lt;code&gt;from langchain_openai import ChatOpenAI llm = ChatOpenAI(model=&amp;quot;gpt-3.5-turbo&amp;quot;) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Error:&lt;/p&gt; &lt;p&gt;&lt;code&gt;NotFoundError: Error code: 404 - {&amp;#39;error&amp;#39;: {&amp;#39;message&amp;#39;: &amp;#39;This is not a chat model and thus not supported in the v1/chat/completions endpoint. Did you mean to use v1/completions?&amp;#39;, &amp;#39;type&amp;#39;: &amp;#39;invalid_request_error&amp;#39;, &amp;#39;param&amp;#39;: &amp;#39;model&amp;#39;, &amp;#39;code&amp;#39;: None}}&lt;/code&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/cholekulchhe&quot;&gt; /u/cholekulchhe &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cqhs7x/langchain_calling_incorrect_openai_api_for_gpt_35/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cqhs7x/langchain_calling_incorrect_openai_api_for_gpt_35/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cqhs7x</id><link href="https://www.reddit.com/r/LangChain/comments/1cqhs7x/langchain_calling_incorrect_openai_api_for_gpt_35/" /><updated>2024-05-12T20:54:52+00:00</updated><published>2024-05-12T20:54:52+00:00</published><title>Langchain calling incorrect OpenAI API for GPT 3.5 turbo</title></entry><entry><author><name>/u/bubble_h13</name><uri>https://www.reddit.com/user/bubble_h13</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I want to use RetrievalQA with chromadb, but it seems to store something after I use the retriever each time.&lt;/p&gt; &lt;p&gt;And the action will make my next question have an effect.&lt;/p&gt; &lt;p&gt;I wonder, &amp;quot;Is there any way to use chains(after get_relevant_document) without storing retrieval data in DB?&amp;quot;&lt;/p&gt; &lt;p&gt;Here is my code:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;llm_chain = LLMChain(llm=llm, prompt=RAG_prompt, verbose=True) qa = RetrievalQA.from_chain_type( llm=llm, retriever=vectordb.as_retriever(), chain_type=&amp;quot;stuff&amp;quot;, # chain_type_kwargs={&amp;quot;prompt&amp;quot;: RAG_prompt}, verbose=True, ) &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/bubble_h13&quot;&gt; /u/bubble_h13 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cqajd7/can_i_use_retrieval_without_stored/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cqajd7/can_i_use_retrieval_without_stored/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cqajd7</id><link href="https://www.reddit.com/r/LangChain/comments/1cqajd7/can_i_use_retrieval_without_stored/" /><updated>2024-05-12T15:34:23+00:00</updated><published>2024-05-12T15:34:23+00:00</published><title>Can I use Retrieval without stored ?</title></entry><entry><author><name>/u/admiraltot22</name><uri>https://www.reddit.com/user/admiraltot22</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I am using the latest version of pinecone and langchain.I am encountering an error while trying to upsert vector &lt;/p&gt; &lt;p&gt;using the code &lt;/p&gt; &lt;pre&gt;&lt;code&gt;index = Pinecone.from_documents(docs, embeddings, index_name=index_name) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;i am encountering the error AttributeError: type object &amp;#39;Pinecone&amp;#39; has no attribute &amp;#39;from_documents&amp;#39;&lt;/p&gt; &lt;p&gt;how to resolve this &lt;/p&gt; &lt;p&gt;Any input greatly appreciated&lt;/p&gt; &lt;p&gt;Thanks&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/admiraltot22&quot;&gt; /u/admiraltot22 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cq3ius/problem_with_upserting_vectors_from_documents/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cq3ius/problem_with_upserting_vectors_from_documents/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cq3ius</id><link href="https://www.reddit.com/r/LangChain/comments/1cq3ius/problem_with_upserting_vectors_from_documents/" /><updated>2024-05-12T08:54:22+00:00</updated><published>2024-05-12T08:54:22+00:00</published><title>Problem with upserting vectors from documents</title></entry><entry><author><name>/u/Monkey_D_Uzumaki7</name><uri>https://www.reddit.com/user/Monkey_D_Uzumaki7</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hello, I am a beginner in LLM and LangChain, and I am developing a small application that allows me to query PDF documents with my OpenAI API. Everything works well so far with the PDFs. I am able to query the PDFs. If the question is out of context, it shows that the information is not in the PDF, otherwise, it displays the information. So, everything is going well at the moment, but the problem is that with documents that are a bit longer, 100 pages or more, I really have problems because I can&amp;#39;t even load them to query them. So, what should I do?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Monkey_D_Uzumaki7&quot;&gt; /u/Monkey_D_Uzumaki7 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cps3s5/problem_with_heavy_documents/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cps3s5/problem_with_heavy_documents/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cps3s5</id><link href="https://www.reddit.com/r/LangChain/comments/1cps3s5/problem_with_heavy_documents/" /><updated>2024-05-11T21:48:17+00:00</updated><published>2024-05-11T21:48:17+00:00</published><title>Problem with heavy documents</title></entry><entry><author><name>/u/Honest-Worth3677</name><uri>https://www.reddit.com/user/Honest-Worth3677</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cq0lim/hugging_face_langchain_upwork_how_to_solve_real/&quot;&gt; &lt;img src=&quot;https://external-preview.redd.it/KB4ehp_t6BKwbj7Wv4cBOf_o6gTw9Y8Qo_eyjFZ_EIU.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d487ddc85293aceb7a5da849fae6075c0fd444bc&quot; alt=&quot;Hugging Face + Langchain+ Upwork | How to Solve Real World AI Job.&quot; title=&quot;Hugging Face + Langchain+ Upwork | How to Solve Real World AI Job.&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Honest-Worth3677&quot;&gt; /u/Honest-Worth3677 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=0G3rqJhcMwA&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cq0lim/hugging_face_langchain_upwork_how_to_solve_real/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1cq0lim</id><media:thumbnail url="https://external-preview.redd.it/KB4ehp_t6BKwbj7Wv4cBOf_o6gTw9Y8Qo_eyjFZ_EIU.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=d487ddc85293aceb7a5da849fae6075c0fd444bc" /><link href="https://www.reddit.com/r/LangChain/comments/1cq0lim/hugging_face_langchain_upwork_how_to_solve_real/" /><updated>2024-05-12T05:30:06+00:00</updated><published>2024-05-12T05:30:06+00:00</published><title>Hugging Face + Langchain+ Upwork | How to Solve Real World AI Job.</title></entry><entry><author><name>/u/dipta10</name><uri>https://www.reddit.com/user/dipta10</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I was following the &lt;a href=&quot;https://python.langchain.com/v0.1/docs/use_cases/sql/agents/&quot;&gt;LangChain SQL Documentation&lt;/a&gt;. So far I&amp;#39;m using Few shot prompt template as the prompt for the agent.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;agent = create_sql_agent( llm=self.llm, db=self.db, prompt=self.few_shot_prompt, verbose=True, agent_type=&amp;quot;openai-tools&amp;quot;, top_k=10, ) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Now what I want to do is&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Create a chain that creates a custom prompt based on user question&lt;/li&gt; &lt;li&gt;Then pass that custom prompt created by the chain to the agent to use when I invoke it&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;I&amp;#39;ve created the chain like this&lt;/p&gt; &lt;pre&gt;&lt;code&gt;rg = RunnableParallel( { &amp;quot;input&amp;quot;: lambda x: x[&amp;quot;input&amp;quot;], &amp;quot;dialect&amp;quot;: lambda x: x[&amp;quot;dialect&amp;quot;], &amp;quot;top_k&amp;quot;: lambda x: x[&amp;quot;top_k&amp;quot;], &amp;quot;tables&amp;quot;: self.table_extraction_chain, } ).assign(prompt=self.few_shot_prompt) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;can I pipe in the Agent here somehow to use the custom prompt?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/dipta10&quot;&gt; /u/dipta10 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cpsg8f/is_it_possible_to_change_agent_prompt_based_on/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cpsg8f/is_it_possible_to_change_agent_prompt_based_on/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cpsg8f</id><link href="https://www.reddit.com/r/LangChain/comments/1cpsg8f/is_it_possible_to_change_agent_prompt_based_on/" /><updated>2024-05-11T22:04:46+00:00</updated><published>2024-05-11T22:04:46+00:00</published><title>Is it possible to change agent prompt based on user question after the agent is already created?</title></entry><entry><author><name>/u/urfath3r</name><uri>https://www.reddit.com/user/urfath3r</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Using the same prompt, GPT 3.5 seems more likely to use tools correctly all the time whereas Claude haiku needs better prompting to achieve tool calling? Even so, it misses out a good number of times. &lt;/p&gt; &lt;p&gt;For eg. I don’t even have to mention any tools in the gpt prompt but I have to mention in for haiku to even work. Putting the tool description and query works enough for gpt.&lt;/p&gt; &lt;p&gt;E.g a tool parameter for “rag vector query” correctly abstracts the query to keywords but haiku dumps the entire question in. GPT is also able to infer from history and use tools (eg, tell me more) while haiku just plain responds saying it has no more data. &lt;/p&gt; &lt;p&gt;Am I using anything wrong? Everyone is saying haiku is smarter and all.. main reason I am trying it out is that it is cheaper, potentially allowing for more documents to stuff in. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/urfath3r&quot;&gt; /u/urfath3r &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cpjgn1/gpt35_tool_calling_more_accurately_than_claude/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cpjgn1/gpt35_tool_calling_more_accurately_than_claude/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cpjgn1</id><link href="https://www.reddit.com/r/LangChain/comments/1cpjgn1/gpt35_tool_calling_more_accurately_than_claude/" /><updated>2024-05-11T15:03:08+00:00</updated><published>2024-05-11T15:03:08+00:00</published><title>GPT3.5 tool calling more accurately than Claude haiku.</title></entry><entry><author><name>/u/thanghaimeow</name><uri>https://www.reddit.com/user/thanghaimeow</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Good to be back making content for y’all.&lt;/p&gt; &lt;p&gt;Video: &lt;a href=&quot;https://www.youtube.com/watch?v=RkWor1BZOn0&quot;&gt;https://www.youtube.com/watch?v=RkWor1BZOn0&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Code: &lt;a href=&quot;https://github.com/trancethehuman/ai-workshop-code/blob/main/Long_term_memory_%26_personalized_LLM_responses.ipynb&quot;&gt;https://github.com/trancethehuman/ai-workshop-code/blob/main/Long_term_memory_%26_personalized_LLM_responses.ipynb&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Let me know in the comments what you liked / didn’t like about this and I’ll make it better next time.&lt;/p&gt; &lt;p&gt;or if you need clarifications, I‘m happy to answer to the best of my ability.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/thanghaimeow&quot;&gt; /u/thanghaimeow &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cp5q9p/code_included_i_did_a_workshop_on_longterm/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cp5q9p/code_included_i_did_a_workshop_on_longterm/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cp5q9p</id><link href="https://www.reddit.com/r/LangChain/comments/1cp5q9p/code_included_i_did_a_workshop_on_longterm/" /><updated>2024-05-11T01:22:37+00:00</updated><published>2024-05-11T01:22:37+00:00</published><title>(Code included) I did a workshop on long-term selective memory for LLM applications recently</title></entry><entry><author><name>/u/hwchase17</name><uri>https://www.reddit.com/user/hwchase17</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;hi all! we&amp;#39;re gearing up for a release of langchain 0.2. The main change is no longer depending on langchain-community (this will increase modularity, decrease size of package, make more secure). We&amp;#39;re also adding in a new docs structure and highlighting a bunch of the changes we made as part of 0.1&lt;/p&gt; &lt;p&gt;We posted more about this on GitHub (&lt;a href=&quot;https://github.com/langchain-ai/langchain/discussions/21437&quot;&gt;https://github.com/langchain-ai/langchain/discussions/21437&lt;/a&gt;) but happy to answer any questions here! Would obviously love and really appreciate any feedback :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/hwchase17&quot;&gt; /u/hwchase17 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1coyy48/langchain_02_prerelease/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1coyy48/langchain_02_prerelease/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1coyy48</id><link href="https://www.reddit.com/r/LangChain/comments/1coyy48/langchain_02_prerelease/" /><updated>2024-05-10T20:13:19+00:00</updated><published>2024-05-10T20:13:19+00:00</published><title>LangChain 0.2 prerelease</title></entry><entry><author><name>/u/Exciting_Maximum_335</name><uri>https://www.reddit.com/user/Exciting_Maximum_335</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey there, Is there any way to generate a voice?&lt;br/&gt; I found some impressive tools out there, but wish they were integrated with langchain like this one: &lt;a href=&quot;https://github.com/jasonppy/VoiceCraft&quot;&gt;https://github.com/jasonppy/VoiceCraft&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Exciting_Maximum_335&quot;&gt; /u/Exciting_Maximum_335 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cpcwcg/voice_generation/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cpcwcg/voice_generation/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cpcwcg</id><link href="https://www.reddit.com/r/LangChain/comments/1cpcwcg/voice_generation/" /><updated>2024-05-11T08:37:38+00:00</updated><published>2024-05-11T08:37:38+00:00</published><title>Voice generation</title></entry><entry><author><name>/u/mmkostov</name><uri>https://www.reddit.com/user/mmkostov</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I have a Next.js application that requires RAG (Specifically web search) with multiple open source models. Is there a way to get them to work with function/tool calling?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/mmkostov&quot;&gt; /u/mmkostov &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cpagwd/functiontool_calling_on_nonopenai_models/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cpagwd/functiontool_calling_on_nonopenai_models/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cpagwd</id><link href="https://www.reddit.com/r/LangChain/comments/1cpagwd/functiontool_calling_on_nonopenai_models/" /><updated>2024-05-11T05:56:25+00:00</updated><published>2024-05-11T05:56:25+00:00</published><title>Function/tool calling on non-OpenAI models?</title></entry><entry><author><name>/u/moonbunR</name><uri>https://www.reddit.com/user/moonbunR</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;&lt;a href=&quot;https://docs.cycls.com/getting-started&quot;&gt;Cycls&lt;/a&gt; Python library significantly simplifies the development of AI chatbots for developers, particularly those who specialize in backend development and may find creating user interfaces tiring. This library eliminates the need to develop a UI from scratch by providing a prebuilt, ChatGPTlike user interface that developers can use immediately. By simply importing Cycls into your project, you can integrate it with major LLM Python libraries and APIs, enabling the use of various LLM models easily.&lt;/p&gt; &lt;p&gt;Upon importing and running your application, it is automatically deployed with the user interface and a public URL, enhancing the efficiency of both development and testing processes. This feature is especially valuable for speeding up deployment and reducing the workload on developers focusing on backend functionalities.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/moonbunR&quot;&gt; /u/moonbunR &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cot1we/library_that_automatically_creates_a_ui_for_your/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cot1we/library_that_automatically_creates_a_ui_for_your/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cot1we</id><link href="https://www.reddit.com/r/LangChain/comments/1cot1we/library_that_automatically_creates_a_ui_for_your/" /><updated>2024-05-10T16:01:39+00:00</updated><published>2024-05-10T16:01:39+00:00</published><title>Library that automatically creates a UI for your chatbot?</title></entry><entry><author><name>/u/Minute_Scientist8107</name><uri>https://www.reddit.com/user/Minute_Scientist8107</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cp9gqj/agent_to_generate_charts_based_on_the_users_prompt/&quot;&gt; &lt;img src=&quot;https://b.thumbs.redditmedia.com/w6YCT1Y6VWdm7hpM7h3n1m598hhj19Pr3FJPvQJXIqU.jpg&quot; alt=&quot;Agent to generate Charts based on the user's prompt&quot; title=&quot;Agent to generate Charts based on the user's prompt&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;&lt;a href=&quot;https://preview.redd.it/4738d3i5aqzc1.png?width=723&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=89b493a6fc2271d489b0df2e6faddd3bf4a19f43&quot;&gt;https://preview.redd.it/4738d3i5aqzc1.png?width=723&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=89b493a6fc2271d489b0df2e6faddd3bf4a19f43&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hi Guys, I have a RAG for csv file, the data is about temperature sensors and apps usage percentage in a phone. I was able to use azure gpt 4 and generate a textual response. Since we have a csv file, I want the llm to also generate a graph along with the textual response. Please tell me how to code this up. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Minute_Scientist8107&quot;&gt; /u/Minute_Scientist8107 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cp9gqj/agent_to_generate_charts_based_on_the_users_prompt/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cp9gqj/agent_to_generate_charts_based_on_the_users_prompt/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1cp9gqj</id><media:thumbnail url="https://b.thumbs.redditmedia.com/w6YCT1Y6VWdm7hpM7h3n1m598hhj19Pr3FJPvQJXIqU.jpg" /><link href="https://www.reddit.com/r/LangChain/comments/1cp9gqj/agent_to_generate_charts_based_on_the_users_prompt/" /><updated>2024-05-11T04:51:54+00:00</updated><published>2024-05-11T04:51:54+00:00</published><title>Agent to generate Charts based on the user's prompt</title></entry><entry><author><name>/u/Flying_Doge123</name><uri>https://www.reddit.com/user/Flying_Doge123</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Flying_Doge123&quot;&gt; /u/Flying_Doge123 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;/r/OpenAI/comments/1cp0tzk/getting_answers_from_gpt_based_on_document/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cp4l08/getting_answers_from_gpt_based_on_document/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cp4l08</id><link href="https://www.reddit.com/r/LangChain/comments/1cp4l08/getting_answers_from_gpt_based_on_document/" /><updated>2024-05-11T00:23:23+00:00</updated><published>2024-05-11T00:23:23+00:00</published><title>Getting Answers from GPT based on document</title></entry><entry><author><name>/u/theferalmonkey</name><uri>https://www.reddit.com/user/theferalmonkey</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cp3y1l/whats_the_scoop_on_serialization_its_all_over_the/&quot;&gt; &lt;img src=&quot;https://a.thumbs.redditmedia.com/SK3wRBz6PKwgSOdCQ0nsJhIrSkiavJY9Ee44pLez-64.jpg&quot; alt=&quot;What's the scoop on serialization? It's all over the show with LangChain&quot; title=&quot;What's the scoop on serialization? It's all over the show with LangChain&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi am I doing something wrong?&lt;/p&gt; &lt;p&gt;I&amp;#39;m using Documents and I want to save/load them. I&amp;#39;ve tried, `.json()` the new `load` module&amp;#39;s `.dumpd()` and `.load()` -- but some documents can&amp;#39;t be deserialized.&lt;/p&gt; &lt;p&gt;e.g. I am trying to build something that uses LangChain documents. But not all of them are deserializable -- instead you get goop like this when using the output of `ContextualCompressionRetriever` (which returns some documents) and use one of the above functions to serialize:&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/74vlfp1wsozc1.png?width=417&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0d3d238376da4f9305eed45ee212ce873602526e&quot;&gt;https://preview.redd.it/74vlfp1wsozc1.png?width=417&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0d3d238376da4f9305eed45ee212ce873602526e&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Basically I have to parse the repr to get the document? Any pointers? Or write my own custom serialization/deserialization?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/theferalmonkey&quot;&gt; /u/theferalmonkey &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cp3y1l/whats_the_scoop_on_serialization_its_all_over_the/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cp3y1l/whats_the_scoop_on_serialization_its_all_over_the/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1cp3y1l</id><media:thumbnail url="https://a.thumbs.redditmedia.com/SK3wRBz6PKwgSOdCQ0nsJhIrSkiavJY9Ee44pLez-64.jpg" /><link href="https://www.reddit.com/r/LangChain/comments/1cp3y1l/whats_the_scoop_on_serialization_its_all_over_the/" /><updated>2024-05-10T23:52:13+00:00</updated><published>2024-05-10T23:52:13+00:00</published><title>What's the scoop on serialization? It's all over the show with LangChain</title></entry><entry><author><name>/u/Choice_Engineering73</name><uri>https://www.reddit.com/user/Choice_Engineering73</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1coz2io/how_to_get_llm_to_infer_dates_from_a_json_file_or/&quot;&gt; &lt;img src=&quot;https://b.thumbs.redditmedia.com/N_ewQPz50EtO5bC2hySBwspZZfVxNl3hUQqU_JHhggk.jpg&quot; alt=&quot;how to get LLM to infer dates from a json file or text file?&quot; title=&quot;how to get LLM to infer dates from a json file or text file?&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;im currently using mistral-small in my project, i want to create a chatbot that can give answers about the weather in the future, so i have a huge json file that i want to feed to the model as a vector embedding and then let the user ask the chatbot questions about future weather such as what should i wear tomorrow etc.&lt;/p&gt; &lt;p&gt;the problem is that the LLM seems to not read the files, i even tried to programatically rebuild the file as a text file.&lt;/p&gt; &lt;p&gt;this is my code:&lt;/p&gt; &lt;p&gt;```&lt;/p&gt; &lt;pre&gt;&lt;code&gt;json_loader = JSONLoader(&amp;quot;C:\\xxxx\\data\\all_weather_by_date.json&amp;quot;) # Split text into chunks text_splitter = RecursiveCharacterTextSplitter() documents = text_splitter.split_documents(docs) # Define the embedding model embeddings = MistralAIEmbeddings(model=&amp;quot;mistral-embed&amp;quot;, mistral_api_key=&amp;quot;XXXXXXXXXXXXXXXXXXXXXXX&amp;quot;, ) # Create the vector store vector = FAISS.from_documents(documents, embeddings) vector.save_local(&amp;#39;cache&amp;#39;) # Define a retriever interface retriever = vector.as_retriever() # Define LLM model = ChatMistralAI(mistral_api_key=&amp;quot;XXXXXXXXXXXXXXXXXXXXX&amp;quot;, model=&amp;#39;mistral-small&amp;#39;, temperature=1) # Define prompt template prompt = ChatPromptTemplate.from_template(&amp;quot;&amp;quot;&amp;quot; &amp;lt;context&amp;gt; {context} &amp;lt;/context&amp;gt; Question: {input}&amp;quot;&amp;quot;&amp;quot;) # Create a retrieval chain to answer questions document_chain = create_stuff_documents_chain(model, prompt) retrieval_chain = create_retrieval_chain(retriever, document_chain) response = retrieval_chain.invoke({&amp;#39;context&amp;#39;: instructions,&amp;quot;input&amp;quot;: f&amp;quot;answer max 60 words, end with follow-up question, no clauses: what should i wear tomorrow? today is 10.5.2024&amp;quot;}) print(response[&amp;quot;answer&amp;quot;]) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;```&lt;/p&gt; &lt;p&gt;my json is built like this:&lt;/p&gt; &lt;p&gt;{&lt;/p&gt; &lt;p&gt;&amp;quot;current_year_dates&amp;quot; : {&lt;/p&gt; &lt;p&gt;&amp;quot;2024.05.11&amp;quot; : {&lt;/p&gt; &lt;p&gt;&amp;quot;humidity&amp;quot;: &amp;quot;50%&amp;quot;,&lt;/p&gt; &lt;p&gt;&amp;quot;degrees&amp;quot;: &amp;quot;34&amp;quot;&lt;/p&gt; &lt;p&gt;}&lt;/p&gt; &lt;p&gt;.... all the other dates throught the year&lt;/p&gt; &lt;p&gt;}&lt;/p&gt; &lt;p&gt;when i attempted to change the json file to text file, i changed it to this format:&lt;/p&gt; &lt;p&gt;&amp;quot;on 2024.10.5 there will be 50% humidity and 30 degress.&amp;quot;&lt;/p&gt; &lt;p&gt;and still it didnt work&lt;/p&gt; &lt;p&gt;also i have always get a warning when running my code about HF_TOKEN not being set and using len splitter.&lt;/p&gt; &lt;p&gt;please help me!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Choice_Engineering73&quot;&gt; /u/Choice_Engineering73 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1coz2io/how_to_get_llm_to_infer_dates_from_a_json_file_or/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1coz2io/how_to_get_llm_to_infer_dates_from_a_json_file_or/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1coz2io</id><media:thumbnail url="https://b.thumbs.redditmedia.com/N_ewQPz50EtO5bC2hySBwspZZfVxNl3hUQqU_JHhggk.jpg" /><link href="https://www.reddit.com/r/LangChain/comments/1coz2io/how_to_get_llm_to_infer_dates_from_a_json_file_or/" /><updated>2024-05-10T20:18:45+00:00</updated><published>2024-05-10T20:18:45+00:00</published><title>how to get LLM to infer dates from a json file or text file?</title></entry><entry><author><name>/u/echopurpose</name><uri>https://www.reddit.com/user/echopurpose</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Would LC be a good &amp;lt;framework&amp;gt; for building a LLM-based chat which builds user profiles?&lt;/p&gt; &lt;p&gt;For example, over the course of the conversation it could fill slots such as name, location, etc. &lt;/p&gt; &lt;p&gt;or there an off the shelf tool which is already doing this? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/echopurpose&quot;&gt; /u/echopurpose &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cooqdx/would_lc_be_a_good_platform_for_building_a/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cooqdx/would_lc_be_a_good_platform_for_building_a/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cooqdx</id><link href="https://www.reddit.com/r/LangChain/comments/1cooqdx/would_lc_be_a_good_platform_for_building_a/" /><updated>2024-05-10T12:47:06+00:00</updated><published>2024-05-10T12:47:06+00:00</published><title>Would LC be a good platform for building a LLM-based chat which builds user profiles?</title></entry></feed>