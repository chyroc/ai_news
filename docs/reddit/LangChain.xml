<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/"><category term="LangChain" label="r/LangChain"/><updated>2024-07-31T14:51:07+00:00</updated><icon>https://www.redditstatic.com/icon.png/</icon><id>/r/LangChain.rss</id><link rel="self" href="https://www.reddit.com/r/LangChain.rss" type="application/atom+xml" /><link rel="alternate" href="https://www.reddit.com/r/LangChain" type="text/html" /><subtitle>LangChain is an open-source framework and developer toolkit that helps developers get LLM applications from prototype to production. It is available for Python and Javascript at https://www.langchain.com/.</subtitle><title>LangChain</title><entry><author><name>/u/zchaarm</name><uri>https://www.reddit.com/user/zchaarm</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;A place for members of &lt;a href=&quot;/r/LangChain&quot;&gt;r/LangChain&lt;/a&gt; to chat with each other&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/zchaarm&quot;&gt; /u/zchaarm &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_10ljho9</id><link href="https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/" /><updated>2023-01-26T04:33:05+00:00</updated><published>2023-01-26T04:33:05+00:00</published><title>r/LangChain Lounge</title></entry><entry><author><name>/u/bferreira85</name><uri>https://www.reddit.com/user/bferreira85</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi, right now my graph is not displaying to the user most of the messages that AI generate as it goes through the graph. This is specially bad in steps when I am getting the user confirmation for something, but it would be nice to display some messages as the llm moves from one node to another.&lt;br/&gt; What is the best practice for that? I&amp;#39;ve been doing console.print for displaying the last message in the message array in certain parts of the code, but I guess that&amp;#39;s not the best way to solve it. How do you usually do it? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/bferreira85&quot;&gt; /u/bferreira85 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1egjtsg/langgraph_what_is_the_best_practice_for/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1egjtsg/langgraph_what_is_the_best_practice_for/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1egjtsg</id><link href="https://www.reddit.com/r/LangChain/comments/1egjtsg/langgraph_what_is_the_best_practice_for/" /><updated>2024-07-31T10:32:36+00:00</updated><published>2024-07-31T10:32:36+00:00</published><title>Langgraph: What is the best practice for displaying messages to the user as the we move through the graph?</title></entry><entry><author><name>/u/jscraft</name><uri>https://www.reddit.com/user/jscraft</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi folks,&lt;/p&gt; &lt;p&gt;Exciting news! Two weeks ago, I had the pleasure of recording a podcast interview with Jacob Lee, the lead maintainer of LangChain.js. &lt;/p&gt; &lt;p&gt;Check it out here: &lt;a href=&quot;https://www.js-craft.io/blog/interview-jacob-lee-langchain-js/&quot;&gt;https://www.js-craft.io/blog/interview-jacob-lee-langchain-js/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;During this short talk, we go through topics such as:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The advantages of using LangChain&lt;/li&gt; &lt;li&gt;A good roadmap for learning LangChain&lt;/li&gt; &lt;li&gt;How all the Langs work together (LangGraph, LangSmith, LangServe, LangChain itself)&lt;/li&gt; &lt;li&gt;Using LangChain.js with or without TypeScript &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;... and much more.&lt;/p&gt; &lt;p&gt;Listen now:: &lt;a href=&quot;https://www.js-craft.io/blog/interview-jacob-lee-langchain-js/&quot;&gt;https://www.js-craft.io/blog/interview-jacob-lee-langchain-js/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hope you will like it, and happy to hear your opinions! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/jscraft&quot;&gt; /u/jscraft &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1egmw06/interview_with_jacob_lee_lead_maintainer_of/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1egmw06/interview_with_jacob_lee_lead_maintainer_of/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1egmw06</id><link href="https://www.reddit.com/r/LangChain/comments/1egmw06/interview_with_jacob_lee_lead_maintainer_of/" /><updated>2024-07-31T13:14:34+00:00</updated><published>2024-07-31T13:14:34+00:00</published><title>Interview with Jacob Lee, lead maintainer of LangChain.js</title></entry><entry><author><name>/u/dhj9817</name><uri>https://www.reddit.com/user/dhj9817</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/dhj9817&quot;&gt; /u/dhj9817 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;/r/documentAutomation/comments/1egjm4g/a_call_to_individuals_who_want_document/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1egobx3/a_call_to_individuals_who_want_document/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1egobx3</id><link href="https://www.reddit.com/r/LangChain/comments/1egobx3/a_call_to_individuals_who_want_document/" /><updated>2024-07-31T14:17:40+00:00</updated><published>2024-07-31T14:17:40+00:00</published><title>A call to individuals who want Document Automation as the future</title></entry><entry><author><name>/u/ravediamond000</name><uri>https://www.reddit.com/user/ravediamond000</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hello everyone,&lt;/p&gt; &lt;p&gt;Just wrote an article on how to use LangServe to create an API over LangChain chains.&lt;br/&gt; Here&amp;#39;s the &lt;a href=&quot;https://www.metadocs.co/2024/07/31/easily-create-production-ready-apis-over-your-langchain-chains-using-langserve/&quot;&gt;link&lt;/a&gt;.&lt;br/&gt; This is actually something that I use in production in my company :D.&lt;/p&gt; &lt;p&gt;Enjoy!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/ravediamond000&quot;&gt; /u/ravediamond000 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1egn31o/create_robust_api_over_langchain_chains_using/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1egn31o/create_robust_api_over_langchain_chains_using/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1egn31o</id><link href="https://www.reddit.com/r/LangChain/comments/1egn31o/create_robust_api_over_langchain_chains_using/" /><updated>2024-07-31T13:23:29+00:00</updated><published>2024-07-31T13:23:29+00:00</published><title>Create robust API over Langchain chains using Langserve</title></entry><entry><author><name>/u/Ignorance998</name><uri>https://www.reddit.com/user/Ignorance998</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;ps: This is a repost (2 days ago). Reddit decided to shadow-ban my previous new account simply because i have posted this. They mark it as &amp;quot;scam&amp;quot;. I hope they will not do so again this time, like this is using a open source license and i didn&amp;#39;t get any commercial benefit from it.&lt;/p&gt; &lt;h1&gt;Introduction (skip this if you like)&lt;/h1&gt; &lt;p&gt;I am an intermediate self-taught python coder with no formal CS experience. I have spent 5 months for this and learnt a lot when writing this project. I have never written anything this complicated before, and I have rewrite this project from scratch at least several times. There are many smaller-scale rewrite when i am not satisfied with the structure of anything. I hope it is useful for somebody. (Also warning, this might not be the most professional piece of code) Any feedback is appreciated!&lt;/p&gt; &lt;h1&gt;What My Project Does&lt;/h1&gt; &lt;p&gt;GPT Graph is a pipeline for llm data transfer. When I first studied LangChain, I don&amp;#39;t understand why we need a server(langsmith) to do debug, and things get so complicated. Therefore, i have spent time in order to write a pipeline structure targeting being flexible and easy to debug. While it&amp;#39;s still in early development and far less sophisticated as Langchain, I think my idea is better at least in some way in turns of how to abstract things (maybe i am wrong).&lt;/p&gt; &lt;p&gt;This library allows you to create more complex pipelines with features like dynamic caching, conditional execution, and easy debugging.&lt;/p&gt; &lt;p&gt;The main features of GPT Graph include:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Component-based pipelines&lt;/li&gt; &lt;li&gt;Allowing nested Pipeline&lt;/li&gt; &lt;li&gt;Dynamic caching according to defined keys&lt;/li&gt; &lt;li&gt;Conditional execution of components using bindings or linkings&lt;/li&gt; &lt;li&gt;Debugging and analysis methods&lt;/li&gt; &lt;li&gt;Priority Queue to run Steps in the Pipeline&lt;/li&gt; &lt;li&gt;Parameters can be updated with priority score. (e.g. if a Pipeline contains 4 Components, you can write config files for each of the Component and Pipeline, as Pipeline has higher priority than each component, if there are any conflict in parameters, the parent Pipeline&amp;#39;s parameters will be used)&lt;/li&gt; &lt;li&gt;One of the key advantages of GPT Graph is its debuggability. Every output is stored in a node (a dict with structure {&amp;quot;content&amp;quot;:xxx, ‚Äúextra‚Äù:xxx})&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;The following features are lacking (They are all TODO in the future)&lt;/p&gt; &lt;ol&gt; &lt;li&gt;currently all are using sync mode&lt;/li&gt; &lt;li&gt;No database is used at this moment. All data stored in networkx graph&amp;#39;s wrapper.&lt;/li&gt; &lt;li&gt;No RAG at this moment. Although I have already written some prototype for it, basically calculate the vector and store in the nodes. They are not submitted yet.&lt;/li&gt; &lt;/ol&gt; &lt;h1&gt;Example&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;from gpt_graph.core.pipeline import Pipeline from gpt_graph.core.decorators.component import component @component() def greet(x): return x + &amp;quot; world!&amp;quot; pipeline = Pipeline() pipeline | greet() result = pipeline.run(input_data=&amp;quot;Hello&amp;quot;) print(result) # Output: [&amp;#39;Hello world!&amp;#39;] &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Comparison&lt;/h1&gt; &lt;p&gt;As for as I know and my understanding(which may be wrong)(e.g. Langgraph or Langchain), there is no framework that can do nested pipeline, or using priority queue.&lt;/p&gt; &lt;h1&gt;Target Audience&lt;/h1&gt; &lt;p&gt;Fast prototyping and small project related to llm data pipelines. It is because currently everything is stored as a wrapper of networkx graph (including outputs of each Step and step structure). Later I may write implementation for graph database, although I don&amp;#39;t have the skill now.&lt;/p&gt; &lt;h1&gt;Welcome Feedback and Contributions&lt;/h1&gt; &lt;p&gt;I welcome any comments, recommendations, or contributions from the community.&lt;br/&gt; I know that as someone that releases his first complicated project (at least for me), there may be a lot of things that i am not doing correctly, including documentations/ writing style/ testing or others. So any recommendation is encouraged! Your feedback will be invaluable for me.&lt;br/&gt; If you have any questions about the project, feel free to ask me as well. My documentation may not be the easiest to understand. I will soon take a long holiday for several months, and when I come back I will try to enhance this project to a better and usable level.&lt;br/&gt; The license now is GPL v3, if more people feel interested in or contribute to the project, i will consider change it to more permissive license.&lt;/p&gt; &lt;h1&gt;Link to Github&lt;/h1&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/Ignorance999/gpt_graph&quot;&gt;https://github.com/Ignorance999/gpt_graph&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Link to Documentation&lt;/h1&gt; &lt;p&gt;&lt;a href=&quot;https://gpt-graph.readthedocs.io/en/latest/hello_world.html&quot;&gt;https://gpt-graph.readthedocs.io/en/latest/hello_world.html&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;More Advanced Example (you can check documentation tutorial 1 Basics):&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;class z: def __init__(self): self.z = 0 def run(self): self.z += 1 return self.z @component( step_type=&amp;quot;node_to_list&amp;quot;, cache_schema={ &amp;quot;z&amp;quot;: { &amp;quot;key&amp;quot;: &amp;quot;[cp_or_pp.name]&amp;quot;, &amp;quot;initializer&amp;quot;: lambda: z(), } }, ) def f4(x, z, y=1): return x + y + z.run(), x - y + z.run() @component(step_type=&amp;quot;list_to_node&amp;quot;) def f5(x): return np.sum(x) @component( step_type=&amp;quot;node_to_list&amp;quot;, cache_schema={&amp;quot;z&amp;quot;: {&amp;quot;key&amp;quot;: &amp;quot;[base_name]&amp;quot;, &amp;quot;initializer&amp;quot;: lambda: z()}}, ) def f6(x, z): return [x, x - z.run(), x - z.run()] s = Session() s.f4 = f4() s.f6 = f6() s.f5 = f5() s.p6 = s.f4 | s.f6 | s.f5 result = s.p6.run(input_data=10) # output: 59 &amp;quot;&amp;quot;&amp;quot; output: Step: p6;InputInitializer:sp0 text = 10 (2 characters) Step: p6;f4.0:sp0 text = 12 (2 characters) text = 11 (2 characters) Step: p6;f6.0:sp0 text = 12 (2 characters) text = 11 (2 characters) text = 10 (2 characters) text = 11 (2 characters) text = 8 (1 characters) text = 7 (1 characters) Step: p6;f5.0:sp0 text = 59 (2 characters) &amp;quot;&amp;quot;&amp;quot; &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Ignorance998&quot;&gt; /u/Ignorance998 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1egh70y/gpt_graph_a_flexible_pipeline_library/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1egh70y/gpt_graph_a_flexible_pipeline_library/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1egh70y</id><link href="https://www.reddit.com/r/LangChain/comments/1egh70y/gpt_graph_a_flexible_pipeline_library/" /><updated>2024-07-31T07:32:46+00:00</updated><published>2024-07-31T07:32:46+00:00</published><title>GPT Graph: A Flexible Pipeline Library</title></entry><entry><author><name>/u/mehul_gupta1997</name><uri>https://www.reddit.com/user/mehul_gupta1997</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/mehul_gupta1997&quot;&gt; /u/mehul_gupta1997 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;/r/learnmachinelearning/comments/1egiiw2/llama_31_fine_tuning_codes_explained/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1egikdw/llama_31_fine_tuning_codes_explained_using_unsloth/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1egikdw</id><link href="https://www.reddit.com/r/LangChain/comments/1egikdw/llama_31_fine_tuning_codes_explained_using_unsloth/" /><updated>2024-07-31T09:09:39+00:00</updated><published>2024-07-31T09:09:39+00:00</published><title>Llama 3.1 Fine Tuning codes explained using unsloth</title></entry><entry><author><name>/u/HopeAway7784</name><uri>https://www.reddit.com/user/HopeAway7784</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I need it to process documents for government agency. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/HopeAway7784&quot;&gt; /u/HopeAway7784 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1egcvsy/is_anyone_aware_of_a_good_ocr_model_that_can_be/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1egcvsy/is_anyone_aware_of_a_good_ocr_model_that_can_be/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1egcvsy</id><link href="https://www.reddit.com/r/LangChain/comments/1egcvsy/is_anyone_aware_of_a_good_ocr_model_that_can_be/" /><updated>2024-07-31T03:12:11+00:00</updated><published>2024-07-31T03:12:11+00:00</published><title>Is anyone aware of a good OCR model that can be used for document processing (Multi-language support with Hindi/ Indian languages)?</title></entry><entry><author><name>/u/kingai404</name><uri>https://www.reddit.com/user/kingai404</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey everyone! I‚Äôm excited to share a new project: SWEKit, a powerful framework for building software engineering agents using the Composio tooling ecosystem.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Objectives&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;SWEKit allows you to:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Scaffold agents that work out-of-the-box with frameworks like CrewAI and LlamaIndex.&lt;/li&gt; &lt;li&gt;Add or optimize your agent&amp;#39;s abilities.&lt;/li&gt; &lt;li&gt;Benchmark your agents against SWE-Bench.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Implementation Details&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Tools Used&lt;/strong&gt;: Composio, CrewAI, Python&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Setup&lt;/strong&gt;:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Install agentic framework of your choice and the Composio plugin&lt;/li&gt; &lt;li&gt;The agent requires a github access token to work with your repositories&lt;/li&gt; &lt;li&gt;You also need to setup API key for the LLM provider you&amp;#39;re planning to use &lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;Scaffold and Run Your Agent&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Workspace Environment:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;SWEKit supports different workspace environments:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Host&lt;/strong&gt;: Run on the host machine.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Docker&lt;/strong&gt;: Run inside a Docker container.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;E2B&lt;/strong&gt;: Run inside an E2B Sandbox.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;FlyIO&lt;/strong&gt;: Run inside a FlyIO machine.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Running the Benchmark:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;SWE-Bench&lt;/strong&gt; evaluates the performance of software engineering agents using real-world issues from popular Python open-source projects.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href=&quot;https://git.new/SWE&quot;&gt;GitHub&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Feel free to explore the project, give it a star if you find it useful, and let me know your thoughts or suggestions for improvements! üåü&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/kingai404&quot;&gt; /u/kingai404 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ege590/i_was_working_on_this_for_a_long_time_a_swe_kit/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ege590/i_was_working_on_this_for_a_long_time_a_swe_kit/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1ege590</id><link href="https://www.reddit.com/r/LangChain/comments/1ege590/i_was_working_on_this_for_a_long_time_a_swe_kit/" /><updated>2024-07-31T04:20:02+00:00</updated><published>2024-07-31T04:20:02+00:00</published><title>I was working on this for a long time - a SWE Kit that simplifies SWE Agent Creation</title></entry><entry><author><name>/u/achsha02</name><uri>https://www.reddit.com/user/achsha02</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Did anyone try to create a custom tool that can compile your generated code to check if the code is working or not?&lt;br/&gt; Can I get some help creating this type of custom tool?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/achsha02&quot;&gt; /u/achsha02 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1egirrf/using_code_compiler_as_a_tool/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1egirrf/using_code_compiler_as_a_tool/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1egirrf</id><link href="https://www.reddit.com/r/LangChain/comments/1egirrf/using_code_compiler_as_a_tool/" /><updated>2024-07-31T09:24:27+00:00</updated><published>2024-07-31T09:24:27+00:00</published><title>Using Code Compiler as a Tool?</title></entry><entry><author><name>/u/CantaloupeLeading646</name><uri>https://www.reddit.com/user/CantaloupeLeading646</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I‚Äôve been working with LangChain to create an efficient document retriever using embeddings and caching mechanisms. I ran into a problem where the PyPDF loading process takes a substantial amount of time, and I believe this can be optimized by leveraging caching because i can only access the specifc location in the pdf where the retrieved context is - but the code right now doesn&amp;#39;t achieve this.&lt;/p&gt; &lt;p&gt;my question is then - how to achieve this more efficient functinoality. &lt;/p&gt; &lt;p&gt;Here&amp;#39;s my original setup:&lt;/p&gt; &lt;p&gt;&lt;code&gt;from langchain_community.vectorstores import Chroma&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;from langchain_openai import OpenAIEmbeddings&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;from langchain.storage import LocalFileStore&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;from langchain.embeddings import CacheBackedEmbeddings&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;from langchain_community.document_loaders import PyPDFLoader&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;from langchain.text_splitter import RecursiveCharacterTextSplitter&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;def load_jodrag_retriever(use_cache=True, chunk_size=2500, chunk_overlap=300, num_docs_retrieve=2):&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;embed = OpenAIEmbeddings()&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;docs = load_jodrag(chunk_size=chunk_size, chunk_overlap=chunk_overlap)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;retriever = retriever_setup(docs, use_cache=use_cache, embed=embed, num_docs_retrieve=num_docs_retrieve)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;return retriever&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;def load_rag(path = &amp;quot;paper1.pdf&amp;quot;, chunk_size = 500, chunk_overlap = 20):&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;loader = PyPDFLoader(path)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;pages = loader.load()&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;pages = pages[9:360]&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;text = &amp;quot;&amp;quot;&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;for page in pages:&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;text += page.page_content&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;text = text.replace(&amp;#39;\t&amp;#39;, &amp;#39; &amp;#39;)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;text_splitter = RecursiveCharacterTextSplitter(separators=[&amp;quot;\n\n&amp;quot;, &amp;quot;\n&amp;quot;, &amp;quot;\t&amp;quot;], chunk_size=chunk_size, chunk_overlap=chunk_overlap)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;docs = text_splitter.create_documents([text])&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;return docs&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;def retriever_setup(docs, use_cache=True, cache_dir= &amp;quot;./cache/&amp;quot;, embed=OpenAIEmbeddings(), num_docs_retrieve=2):&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;if use_cache:&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;store = LocalFileStore(cache_dir)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;cached_embedder = CacheBackedEmbeddings.from_bytes_store(embed, store, namespace=embed.model)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;vectorstore = Chroma.from_documents(docs, cached_embedder)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;else:&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;vectorstore = Chroma.from_documents(documents=docs, collection_name=&amp;quot;rag-chroma&amp;quot;, embedding=embed)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;retriever = vectorstore.as_retriever(search_kwargs={&amp;quot;k&amp;quot;: num_docs_retrieve})&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;return retriever&lt;/code&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/CantaloupeLeading646&quot;&gt; /u/CantaloupeLeading646 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1eghtid/question_about_retrievers_slow_pdf_loading_times/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1eghtid/question_about_retrievers_slow_pdf_loading_times/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1eghtid</id><link href="https://www.reddit.com/r/LangChain/comments/1eghtid/question_about_retrievers_slow_pdf_loading_times/" /><updated>2024-07-31T08:16:41+00:00</updated><published>2024-07-31T08:16:41+00:00</published><title>question about retrievers - slow pdf loading times also when using cache</title></entry><entry><author><name>/u/yjgoh</name><uri>https://www.reddit.com/user/yjgoh</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I come from an AI developer background but i want to build an AI web app myself. &lt;/p&gt; &lt;p&gt;I have 2 options:&lt;br/&gt; A. Build every components ( calling AI models, parsing , injestion ) in TS/NextJS which im not familiar with at all, but if this will help long term im willing to put in the work.&lt;/p&gt; &lt;p&gt;B. Deploy my AI components using FastAPI. Im much more familiar with python, but im trying not to overcomplicate the architecture of my first webapp ( need to host frontend and backend separately )&lt;/p&gt; &lt;p&gt;Has anyone deploy any AI webapps/ SAAS here? Would like to have some suggestions&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/yjgoh&quot;&gt; /u/yjgoh &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1efz85n/ai_web_app_ts_vs_python_fastapi/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1efz85n/ai_web_app_ts_vs_python_fastapi/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1efz85n</id><link href="https://www.reddit.com/r/LangChain/comments/1efz85n/ai_web_app_ts_vs_python_fastapi/" /><updated>2024-07-30T17:26:14+00:00</updated><published>2024-07-30T17:26:14+00:00</published><title>AI web app TS vs Python + FastAPI?</title></entry><entry><author><name>/u/jiraiya1729</name><uri>https://www.reddit.com/user/jiraiya1729</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I want to build a chatbot which gives recommendations say books based on the conversation it had before with the user and I also want it to have good language skills like fundamental models so what are resource for learning this and what are the techstack to be used for this?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/jiraiya1729&quot;&gt; /u/jiraiya1729 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1eg9g86/rag_based_recommendation_system/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1eg9g86/rag_based_recommendation_system/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1eg9g86</id><link href="https://www.reddit.com/r/LangChain/comments/1eg9g86/rag_based_recommendation_system/" /><updated>2024-07-31T00:26:43+00:00</updated><published>2024-07-31T00:26:43+00:00</published><title>RAG based recommendation system</title></entry><entry><author><name>/u/Silver_Equivalent_58</name><uri>https://www.reddit.com/user/Silver_Equivalent_58</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Is there a tool out there that can find the optimal pipeline for a RAG for a given data? &lt;/p&gt; &lt;p&gt;Im planning to build one and was wondering how helpful something like this would be?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Silver_Equivalent_58&quot;&gt; /u/Silver_Equivalent_58 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1efz9nj/is_there_a_tool_for_finds_optimal_pipeline_for_a/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1efz9nj/is_there_a_tool_for_finds_optimal_pipeline_for_a/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1efz9nj</id><link href="https://www.reddit.com/r/LangChain/comments/1efz9nj/is_there_a_tool_for_finds_optimal_pipeline_for_a/" /><updated>2024-07-30T17:27:52+00:00</updated><published>2024-07-30T17:27:52+00:00</published><title>Is there a tool for finds optimal pipeline for a RAG?</title></entry><entry><author><name>/u/emersoftware</name><uri>https://www.reddit.com/user/emersoftware</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Does anyone know how to dynamically modify the description of a Tool?&lt;/p&gt; &lt;p&gt;I am using ToolNode in Langgraph with tools defined with the decorator, and to define the args, I am using a Pydantic BaseModel, something like:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;class ToolInput(BaseModel): arg_1: str = Field(description=&amp;quot;...&amp;quot;, type=&amp;quot;string&amp;quot;) ... u/tool(&amp;quot;get_data&amp;quot;, args_schema=ToolInput) def get_data( arg_1: str, ... ): &amp;quot;&amp;quot;&amp;quot;Get the data, the accepted values of the arg_1 are: - val_1, val_2, val_3 ... val_n &amp;quot;&amp;quot;&amp;quot; ... return data &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The point is, I want to dynamically pass data from the graph&amp;#39;s state to construct the prompt, something like:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;class ToolInput(BaseModel): arg_1: int = Field(description=&amp;quot;...&amp;quot;, type=&amp;quot;string&amp;quot;) ... @tool(&amp;quot;get_data&amp;quot;, args_schema=ToolInput) def get_data( arg_1: str, ... ): &amp;quot;&amp;quot;&amp;quot;Get the data, the accepted values of the arg_1 are: - {val_1}, {val_2}, {val_3}, ... , {val_n} &amp;quot;&amp;quot;&amp;quot; # Where the {val_x} come from the State, for example state[&amp;quot;available_values&amp;quot;] ... return data &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Does anyone have an idea of how I can do this?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/emersoftware&quot;&gt; /u/emersoftware &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1eg12qg/discussion_how_to_dynamically_modify_tool/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1eg12qg/discussion_how_to_dynamically_modify_tool/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1eg12qg</id><link href="https://www.reddit.com/r/LangChain/comments/1eg12qg/discussion_how_to_dynamically_modify_tool/" /><updated>2024-07-30T18:40:26+00:00</updated><published>2024-07-30T18:40:26+00:00</published><title>Discussion: How to dynamically modify tool descriptions in Langgraph?</title></entry><entry><author><name>/u/Exciting-Rest-395</name><uri>https://www.reddit.com/user/Exciting-Rest-395</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I have been trying to build a RAG over a database that has mulitple tables. Often times, for a user query, the data has to be searched by joining multiple tables. I followed this approach as mentioned in Langchain documents.&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://python.langchain.com/v0.1/docs/use_cases/sql/quickstart/&quot;&gt;https://python.langchain.com/v0.1/docs/use_cases/sql/quickstart/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;What I am observing is that many times the query generated by LLM is not correct and the data that user wants is incorrect. We have provided almost 60 queries in Fewshot prompts example and send 3 as example that are closes semantic match. The accuracy still seems far from expected one. Are we missing something. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Exciting-Rest-395&quot;&gt; /u/Exciting-Rest-395 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1efnx5u/rag_over_database/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1efnx5u/rag_over_database/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1efnx5u</id><link href="https://www.reddit.com/r/LangChain/comments/1efnx5u/rag_over_database/" /><updated>2024-07-30T08:12:27+00:00</updated><published>2024-07-30T08:12:27+00:00</published><title>RAG over Database</title></entry><entry><author><name>/u/Gullible-Being-8595</name><uri>https://www.reddit.com/user/Gullible-Being-8595</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I am working on a RAG chatbot application where we will first to have if either the given question is having enough knowledge to be answered or we need some further information before answering. Or if a question is a general question which doesn&amp;#39;t even need RAG search. For example, if a question is &amp;quot;What are the main components of a car?&amp;quot; then we don&amp;#39;t need RAG search but if a question is &amp;quot;What type of suspensions do you have for a car?&amp;quot; then we will do RAG search.&lt;/p&gt; &lt;p&gt;Till now, I created a simple ReACT agent in langchain to ask the followup questions with a tool and now I need to integrate if the given query is something that can be answered without any tool or not and for this, I am thinking about first having an agent which qualifies the given query and if its qualified for RAG search then second agent will do either RAG search or follow-up questions.&lt;/p&gt; &lt;p&gt;In the past of couple of days, I have been exploring langgraph and I feel like simple langchain is enough for my solution like a chain of agents. So please make me understand, why one should use Langgraph?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Gullible-Being-8595&quot;&gt; /u/Gullible-Being-8595 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1efnpgv/langchain_agents_or_langgraph_agents/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1efnpgv/langchain_agents_or_langgraph_agents/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1efnpgv</id><link href="https://www.reddit.com/r/LangChain/comments/1efnpgv/langchain_agents_or_langgraph_agents/" /><updated>2024-07-30T07:57:23+00:00</updated><published>2024-07-30T07:57:23+00:00</published><title>Langchain Agents or Langgraph Agents</title></entry><entry><author><name>/u/agile_crossover</name><uri>https://www.reddit.com/user/agile_crossover</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hello -- I am trying to incrementally create a chatbot that will do three things (depending on user input)&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Summarize a JSON specification for the product (thinking some simple prompt engineering here should be able to do this)&lt;/li&gt; &lt;li&gt;Answer questions about some ontologies/hierarchies we maintain (thinking RAG)&lt;/li&gt; &lt;li&gt;Generate / Modify a JSON specification for the product (thinking a fine-tuned model for this specific structured output we use - internally before JSON we use pydantic models)&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;My question is what is the best way to use LangChains building blocks to properly route a user&amp;#39;s request to the appropriate model within the chat?&lt;/p&gt; &lt;p&gt;I was reading the docs and I wasn&amp;#39;t sure if I needed to create a custom agent (and somehow let it decide which of the three to use?) or if I should do a &amp;quot;dumber&amp;quot; rule-based function to then determine which of the three to use and just have that integrate with the basic chatbot.&lt;/p&gt; &lt;p&gt;Any help / guidance would be greatly appreciated! Am supposed to look into this for work and a little out of my depth right now.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/agile_crossover&quot;&gt; /u/agile_crossover &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1eg41kn/confused_on_what_to_use_for_chatbot/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1eg41kn/confused_on_what_to_use_for_chatbot/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1eg41kn</id><link href="https://www.reddit.com/r/LangChain/comments/1eg41kn/confused_on_what_to_use_for_chatbot/" /><updated>2024-07-30T20:38:22+00:00</updated><published>2024-07-30T20:38:22+00:00</published><title>Confused on What to Use for Chatbot?</title></entry><entry><author><name>/u/neilkatz</name><uri>https://www.reddit.com/user/neilkatz</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ef12q6/the_rag_engineers_guide_to_document_parsing/&quot;&gt; &lt;img src=&quot;https://external-preview.redd.it/dMnqh7zskubdEjd4AqGra79-CfzcacpPLEPtlfKOTak.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b61c1baeab07db5c51a79199868d4f269f7650ce&quot; alt=&quot;The RAG Engineer's Guide to Document Parsing&quot; title=&quot;The RAG Engineer's Guide to Document Parsing&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi Group,&lt;/p&gt; &lt;p&gt;I made a post with my buddy Daniel Warfield breaking down why parsing matters so much for RAG and comparing some of the different approaches based on our experience working with Air France, Dartmouth a big online publisher and dozens of other projects with real data&lt;/p&gt; &lt;p&gt;For full transparency, one of the products discussed comes from my firm &lt;a href=&quot;http://EyeLevel.ai&quot;&gt;EyeLevel.ai&lt;/a&gt;, but that&amp;#39;s not the focus. It&amp;#39;s a discussion of how we can all build better RAG on the kind of complex docs we see in the real world.&lt;/p&gt; &lt;p&gt;You can watch it on YT if you prefer... &lt;a href=&quot;https://www.youtube.com/watch?v=7Vv64f1yI0I&quot;&gt;https://www.youtube.com/watch?v=7Vv64f1yI0I&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;The Foundation of RAG: Document Parsing&lt;/h1&gt; &lt;p&gt;Let&amp;#39;s start with a fundamental truth: parsing is the bedrock of any RAG application. &lt;/p&gt; &lt;p&gt;&amp;quot;The first step in any RAG application is parsing your document and extracting the information from it,&amp;quot; says EyeLevel cofounder Neil Katz. &amp;quot;You‚Äôre trying to turn it into something that language models will eventually understand and do something smart with.&amp;quot;&lt;/p&gt; &lt;p&gt;This isn&amp;#39;t just about extracting text. It&amp;#39;s about preserving structure, context, and relationships within the data. Get this wrong, and your entire RAG pipeline suffers. If you don&amp;#39;t get the information out of your giant set of documents in the first place, which is often where RAG starts, it&amp;#39;s ‚Äúgarbage in and garbage out‚Äù and nothing else will work properly.&lt;/p&gt; &lt;h1&gt;The Heart of the Problem&lt;/h1&gt; &lt;p&gt;The basic problem to solve is that language models, at least for now, don&amp;#39;t understand complex visual documents. Anything with tables, forms, graphics, charts, figures and complex formatting will cause downstream hallucinations in a RAG application. Yes you can take a page from a PDF and feed it into ChatGPT and it will understand some of it, sometimes most of it. But try doing this at scale with thousands or millions of pages and you&amp;#39;ve got a mess and eventually downstream hallucinations for your RAG.&lt;/p&gt; &lt;p&gt;So devs need some way of breaking complex documents apart, identifying the text blocks, the tables, the charts and so on, then extracting the information from those positions and converting it into something language models will understand and that you can store in your RAG database. This final output is usually simple text or JSON.&lt;/p&gt; &lt;p&gt;This problem isn&amp;#39;t new btw. There are entire industries devoted to ingesting medical bills, restaurant receipts and so on. That&amp;#39;s typically done with a vision model fine tuned to a very specific set of documents. The model for receipts isn&amp;#39;t good at medical bills. And vice versa.&lt;/p&gt; &lt;p&gt;The new twist is RAG often deals with a highly varied set of content. A legal RAG, for example, might need to understand police reports, medical bills and insurance claims. The second twist is the information needs to be converted into LLM ready data.&lt;/p&gt; &lt;p&gt;So let&amp;#39;s talk about what&amp;#39;s out there.&lt;/p&gt; &lt;h1&gt;Parsing Strategies: Breakdown of Approaches&lt;/h1&gt; &lt;p&gt;Let&amp;#39;s examine some common parsing strategies, their strengths, and their limitations using an example of a medical document showcasing exam dates and fees in a table:&lt;/p&gt; &lt;h1&gt;1. PyPDF&lt;/h1&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/lizhsf8twgfd1.png?width=960&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f8a3fe90f3725c9751fd370dc885fa7d4a4f147b&quot;&gt;Image: PyPDF results showing minimal information extracted from the table in the medical document.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://pypdf.readthedocs.io/en/stable/index.html&quot;&gt;PyPDF&lt;/a&gt; is a longstanding Python library designed for reading and manipulating PDF files. It can be effective for basic text extraction from simple PDFs, but often struggles with complex layouts, tables, and formatted text. &lt;/p&gt; &lt;p&gt;PyPDF is best suited for straightforward, text-heavy documents but may lose critical structural information in more intricate PDFs. It doesn&amp;#39;t process visual objects like images, charts, graphs and figures.&lt;/p&gt; &lt;h1&gt;2. Tesseract (OCR)&lt;/h1&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/ofa0pkawwgfd1.png?width=960&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a9a47fa3defa74939b453236fec4aee0de6dfea4&quot;&gt;Image: Tesseract results showing information extracted from the table in the medical document.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/tesseract-ocr/tesseract&quot;&gt;Tesseract&lt;/a&gt; is an open-source optical character recognition (OCR) engine that can extract text from images and scanned documents. Best known for converting image-based text to machine-readable format, Tesseract can struggle with maintaining document structure, especially in complex layouts or tables. &lt;/p&gt; &lt;p&gt;It&amp;#39;s particularly useful for scanned documents but may require additional post-processing to preserve formatting and structure. Tesseract also doesn&amp;#39;t process visual objects like images, charts, graphs and figures.&lt;/p&gt; &lt;h1&gt;3. Unstructured&lt;/h1&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/yz8gnwvzwgfd1.png?width=960&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a39380072b5beb48547179c463489bd5353ca14d&quot;&gt;Image: Unstructured results showing rich information extracted from the table in the medical document.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://unstructured.io/&quot;&gt;Unstructured&lt;/a&gt; is a modern document parsing library that aims to handle a wide variety of document types and formats. It employs a combination of techniques to extract and structure information from documents, including text extraction, table detection, and layout analysis. &lt;/p&gt; &lt;p&gt;While more robust than traditional parsing tools, Unstructured can still face challenges with highly complex or non-standard document formats. Like the others, it doesn&amp;#39;t process visual objects like images, charts, graphs and figures.&lt;/p&gt; &lt;h1&gt;4. LlamaParse&lt;/h1&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/8ev781z1xgfd1.png?width=960&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2db69039e0567ebe6efb69be07aff2ffec71437e&quot;&gt;Image: LlamaParse results showing a markdown table of information extracted from the table in the medical document.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://docs.llamaindex.ai/en/stable/llama_cloud/llama_parse/&quot;&gt;LlamaParse&lt;/a&gt; is a newer parsing solution developed by the team behind &lt;a href=&quot;https://www.llamaindex.ai/&quot;&gt;LlamaIndex&lt;/a&gt;. It&amp;#39;s designed to handle complex document structures, including tables and formatted text, and outputs results in a markdown format that&amp;#39;s easily interpretable by language models. &lt;/p&gt; &lt;p&gt;It has been seen to preserve document structure and handle tables, though it&amp;#39;s a relatively new tool and its full capabilities and limitations are still being explored in real-world applications.&lt;/p&gt; &lt;h1&gt;5. X-Ray by &lt;a href=&quot;http://EyeLevel.ai&quot;&gt;&lt;strong&gt;EyeLevel.ai&lt;/strong&gt;&lt;/a&gt;&lt;/h1&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/a14hfii3xgfd1.png?width=1942&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=37c7ae239e136201836e3fb44ec33331cf7a92d7&quot;&gt;Image: X-Ray by EyeLevel.ai converts a complex medical bill into clean JSON chunks with both narrative description and data that LLMs prefer&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://www.eyelevel.ai/xray&quot;&gt;X-Ray&lt;/a&gt;, powered by EyeLevel‚Äôs &lt;a href=&quot;https://www.eyelevel.ai/product/apis&quot;&gt;GroundX APIs&lt;/a&gt;, takes a multimodal approach to parsing with industry leading results, especially when parsing complex visuals including charts, graphics and figures. X-Ray is far more than just a table parser.&lt;/p&gt; &lt;p&gt;The X-Ray technology starts with a fine-tuned vision model trained on a million pages of enterprise documents from a wide cross section of industries including health, financial, insurance, legal and government. The system uses the vision model to identify various objects on the page: text blocks, tables, charts and so on. Once the coordinates are known, it extracts the information, chunks it and sends it to different pipelines to be turned into LLM ready data.&lt;/p&gt; &lt;p&gt;The result is a JSON-like output that includes the core data, chunk summary, doc summary, keywords and other metadata, providing richer context for language models. X-Ray is available in a demo format for developers to try for themselves, where they can upload a document to the system and see the semantic objects that are created to translate complex visuals to the LLM. &lt;a href=&quot;https://www.eyelevel.ai/xray&quot;&gt;&lt;strong&gt;You can try X-Ray here&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt; &lt;h1&gt;Performance Impact: The Parsing Difference&lt;/h1&gt; &lt;p&gt;Our tests, along with academic research, show that parsing strategy can significantly impact RAG performance. &lt;/p&gt; &lt;p&gt;We&amp;#39;re talking about substantial gains, as Daniel Warfield, co-host of RAG Masters points out:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;&amp;quot;For some examples, there&amp;#39;s a 10%, even a 20% difference in performance.&amp;quot;&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;This is crucial when you consider the effort that goes into other optimization strategies:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;&amp;quot;People are doing crazy advanced strategies for the difference in 5, 6, 7, even 10 percent performance. And then maybe just completely switching the parser might get you a massive performance increase.&amp;quot;&lt;/p&gt; &lt;/blockquote&gt; &lt;h1&gt;Error Analysis: Common Parsing Pitfalls&lt;/h1&gt; &lt;p&gt;Let&amp;#39;s examine some common parsing errors and their downstream effects:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Table Misinterpretation:&lt;/strong&gt; When parsers fail to correctly identify table structures, it can lead to data being treated as unstructured text. This can result in incorrect answers in question-answering tasks, especially for queries about tabular data.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Loss of Formatting:&lt;/strong&gt; If a document structure isn&amp;#39;t well understood, a text scrape could scramble the pieces up. A header could wind up in body copy. A column label could wind up in the rows of data. You get the parsing equivalent of scrambled eggs.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Image Handling:&lt;/strong&gt; Most parsers struggle with embedded images or diagrams, either ignoring them completely or misinterpreting them as text through OCR.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Header/Footer Confusion:&lt;/strong&gt; Parsers might incorrectly include headers and footers as part of the main content, potentially skewing the context of the extracted information.&lt;/li&gt; &lt;/ol&gt; &lt;h1&gt;Developing Custom Parsing Strategies&lt;/h1&gt; &lt;p&gt;For developers dealing with specific document types or domains, developing custom parsing strategies can be beneficial. Here are some approaches:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Combining Existing Tools:&lt;/strong&gt; Use multiple parsing tools in tandem, leveraging the strengths of each for different parts of your documents.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Regular Expressions:&lt;/strong&gt; Implement custom regex patterns to extract specific types of information consistently found in your documents.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Domain-Specific Rules:&lt;/strong&gt; Incorporate rules based on domain knowledge to improve parsing accuracy for specialized documents.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Machine Learning Augmentation:&lt;/strong&gt; Train models to recognize and extract specific patterns or structures in your documents.&lt;/li&gt; &lt;/ol&gt; &lt;h1&gt;Integration Challenges&lt;/h1&gt; &lt;p&gt;When integrating parsing strategies into existing RAG pipelines, developers often face several challenges:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;API Compatibility:&lt;/strong&gt; Ensure that the chosen parsing strategy can be easily integrated with your existing codebase and infrastructure.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Data Format Consistency:&lt;/strong&gt; The output of your parser should be in a format that&amp;#39;s compatible with the rest of your RAG pipeline, often requiring additional preprocessing steps.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Scalability:&lt;/strong&gt; Consider the computational resources required by different parsing strategies, especially when dealing with large document sets.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Error Handling:&lt;/strong&gt; Implement robust error handling to deal with parsing failures or unexpected document formats.&lt;/li&gt; &lt;/ol&gt; &lt;h1&gt;Best Practices for Selecting a Parsing Strategy&lt;/h1&gt; &lt;p&gt;It‚Äôs recommend to take a two-pronged approach to selecting the right parsing strategy:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;1. Visual Inspection:&lt;/strong&gt; Start by running your documents through different parsers and examining the output. As Warfield advises:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;&amp;quot;Pass your data through a bunch of parsers and look at them. Your brain is still the most powerful model that exists.&amp;quot;&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;&lt;strong&gt;2. End-to-End Testing:&lt;/strong&gt; Once you&amp;#39;ve narrowed down your options, conduct thorough end-to-end testing. This means running your entire RAG pipeline with different parsing strategies and evaluating the final output.&lt;/p&gt; &lt;p&gt;To quantitatively compare parsing strategies, consider the following metrics:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Accuracy in table and graphical extraction&lt;/li&gt; &lt;li&gt;Preservation of document structure&lt;/li&gt; &lt;li&gt;Abiliity to turn extractions into LLM friendly data&lt;/li&gt; &lt;li&gt;Speed of parsing&lt;/li&gt; &lt;li&gt;Consistency across different document types&lt;/li&gt; &lt;li&gt;Ability to handle complex formatting&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;The Challenge of Evaluation&lt;/h1&gt; &lt;p&gt;Here&amp;#39;s the rub: evaluating parsing quality is still a largely manual process. Creating question-answer pairs for evaluation is labor-intensive but crucial for building automated tooling. The need for human evaluation in parsing cannot be completely eliminated, at least not yet.&lt;/p&gt; &lt;p&gt;This presents a significant opportunity in the field, and this post will be updated in the future when a sufficiently advanced solution for automated parsing is discovered.&lt;/p&gt; &lt;h1&gt;Conclusion&lt;/h1&gt; &lt;p&gt;As we continue to push the boundaries of what&amp;#39;s possible with RAG applications, it&amp;#39;s clear that document parsing will remain a critical component. The field is ripe for innovation, particularly in parsing technology and evaluation methods.&lt;/p&gt; &lt;p&gt;For developers building RAG applications, it‚Äôs critical not to overlook the importance of parsing. Take the time to evaluate different parsing strategies and their impact on your specific use case. It could be the difference between a RAG system that merely functions and one that excels.&lt;/p&gt; &lt;p&gt;Remember, in the world of RAG, your system is only as good as the data you feed it. And that all starts with parsing.&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=7Vv64f1yI0I&quot;&gt;You can watch the full episode of RAG Masters here.&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/neilkatz&quot;&gt; /u/neilkatz &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ef12q6/the_rag_engineers_guide_to_document_parsing/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ef12q6/the_rag_engineers_guide_to_document_parsing/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1ef12q6</id><media:thumbnail url="https://external-preview.redd.it/dMnqh7zskubdEjd4AqGra79-CfzcacpPLEPtlfKOTak.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=b61c1baeab07db5c51a79199868d4f269f7650ce" /><link href="https://www.reddit.com/r/LangChain/comments/1ef12q6/the_rag_engineers_guide_to_document_parsing/" /><updated>2024-07-29T14:34:16+00:00</updated><published>2024-07-29T14:34:16+00:00</published><title>The RAG Engineer's Guide to Document Parsing</title></entry><entry><author><name>/u/PromptAwkward7277</name><uri>https://www.reddit.com/user/PromptAwkward7277</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi,&lt;br/&gt; Is there an official discord server for LangChain? Seems like all the links I found are invalid.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/PromptAwkward7277&quot;&gt; /u/PromptAwkward7277 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1efszqi/official_discord/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1efszqi/official_discord/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1efszqi</id><link href="https://www.reddit.com/r/LangChain/comments/1efszqi/official_discord/" /><updated>2024-07-30T13:12:45+00:00</updated><published>2024-07-30T13:12:45+00:00</published><title>Official Discord?</title></entry><entry><author><name>/u/abhinavkimothi</name><uri>https://www.reddit.com/user/abhinavkimothi</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;ve recently created a repository for indexing, generating and evaluating RAG responses. Would love to have some feedback on this. I&amp;#39;ve used LangChain and LangChain benchmarks too.&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/abhinav-kimothi/A-Simple-Guide-to-RAG&quot;&gt;https://github.com/abhinav-kimothi/A-Simple-Guide-to-RAG&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/abhinavkimothi&quot;&gt; /u/abhinavkimothi &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1efnkrw/sharing_a_code_repository_on_rag/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1efnkrw/sharing_a_code_repository_on_rag/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1efnkrw</id><link href="https://www.reddit.com/r/LangChain/comments/1efnkrw/sharing_a_code_repository_on_rag/" /><updated>2024-07-30T07:47:51+00:00</updated><published>2024-07-30T07:47:51+00:00</published><title>Sharing a Code Repository on RAG</title></entry><entry><author><name>/u/Danidre</name><uri>https://www.reddit.com/user/Danidre</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1efvx6d/serializingdeserializing_messages_am_i_using/&quot;&gt; &lt;img src=&quot;https://b.thumbs.redditmedia.com/XItnW8QCzowSUudO50JGRNRonIkjkBMKtal88jBo1wM.jpg&quot; alt=&quot;Serializing/Deserializing Messages? Am I using packages correctly? (LangGraph)&quot; title=&quot;Serializing/Deserializing Messages? Am I using packages correctly? (LangGraph)&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;&lt;strong&gt;[SOLVED]:&lt;/strong&gt; Used &lt;code&gt;_convert_message_to_dict&lt;/code&gt; and stored that dict instead. When loading, convert all dicts with &lt;code&gt;_convert_dict_to_message&lt;/code&gt;. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Preface:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Lest it becomes an XY problem, my initial problem is that I need to count token usage, using the provided &lt;code&gt;model.get_num_tokens_from_messages&lt;/code&gt; from their &lt;code&gt;langchain_openai&lt;/code&gt;. However, due to the way I save and load history, things keep breaking.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Introduction:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I am attempting to handle history management (save/load of conversations and messages) myself, since I can&amp;#39;t quite understand checkpoints yet, and it is also in current heavy development. While LangChain uses their instances of BaseMessage (AIMessage, HumanMessage, etc), I found that doing &lt;code&gt;message.to_json().get(&amp;quot;kwargs&amp;quot;, {})&lt;/code&gt; will return a dict representing the json version that the LLM really sees. I extracted a list of these and fed it to an LLM directly, and realized the LLM was still able to understand everything. So saving these dicts and loading them/feeding them to the LLM worked just fine.&lt;/p&gt; &lt;p&gt;However, now I need to add token count to the mix. I use AzureOpenAIChat which does not support token counts for &lt;code&gt;astream_events&lt;/code&gt;, so I have to get it using &lt;code&gt;model.get_num_tokens_from_messages&lt;/code&gt;. This is where my problems arise.&lt;/p&gt; &lt;p&gt;Here is the process:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Attempt 1)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Error:&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;message does not have attribute &amp;quot;content&amp;quot; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Investigation:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Turns out, &lt;code&gt;get_num_tokens_from_messages&lt;/code&gt; calls a &lt;code&gt;_convert_message_to_dict&lt;/code&gt; function. Some of my messages are already dicts, so when this value is passed to the &lt;code&gt;_format_message_content&lt;/code&gt; function, it crashes and gives an error.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Thoughts:&lt;/strong&gt; It is weird that their &lt;code&gt;_convert_message_to_dict&lt;/code&gt; function doesn&amp;#39;t attempt to account for the case that some in the list may already be dicts.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Solution:&lt;/strong&gt; No worries. I just did an extra step to convert all loaded message dicts back to subclasses of &lt;code&gt;BaseMessage&lt;/code&gt;, using their &lt;code&gt;_convert_dict_to_message&lt;/code&gt; function per message upon load.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Attempt 2)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Error:&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;dict has no property &amp;quot;role&amp;quot; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Investigation:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;That&amp;#39;s right. The function expects a property &amp;quot;role&amp;quot;. Since I stored and loaded the &lt;code&gt;kwargs&lt;/code&gt; from each BaseMessage, they have the property &amp;quot;type&amp;quot; instead of &amp;quot;role&amp;quot;.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Thoughts:&lt;/strong&gt; It is weird that while they themselves use &amp;quot;type&amp;quot; instead of &amp;quot;role&amp;quot;, their &lt;code&gt;_convert_dict_to_message&lt;/code&gt; function only looks for role, and doesn&amp;#39;t try to default to &amp;quot;type&amp;quot;.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Solution:&lt;/strong&gt; No worries, upon saving messages, I just did an extra step to also set the &amp;quot;role&amp;quot; of the dict to the existing &amp;quot;type&amp;quot; value, then store it.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Attempt 3)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Error:&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;role &amp;quot;ai&amp;quot; unknown. expecting &amp;quot;assistant&amp;quot;, &amp;quot;user&amp;quot;, &amp;quot;tool&amp;quot;, &amp;quot;function&amp;quot;, or &amp;quot;system&amp;quot; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Investigation:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Although their json generates additional types, this method only expects these few.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Thoughts:&lt;/strong&gt; It is weird that they use &amp;quot;ai&amp;quot;, &amp;quot;assistant&amp;quot;, &amp;quot;human&amp;quot;, &amp;quot;tool&amp;quot;, &amp;quot;function&amp;quot;, &amp;quot;system&amp;quot;, &amp;quot;chat&amp;quot;, etc, but limit the expected roles.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Solution:&lt;/strong&gt; No worries, upon saving messages, I will do an extra step to also manually convert their types, to align with their expected roles.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Attempt 4)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Error:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/jn5o6f5c2ofd1.png?width=1039&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5d68300fdedf9153ecc336ab594fbb6001e5431e&quot;&gt; none is not an allowed value (type=type_error.none.not_allowed)&lt;/a&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;&lt;em&gt;flips table&lt;/em&gt;&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;&lt;strong&gt;Investigation:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The message trace itself is not very helpful for debugging and determining the exact cause, but based on experience (and it mentioning the tool call) I&amp;#39;m assuming that it is due to the AIMessage with a tool call. Usually that has no content value (an empty string, which somehow becomes a None value internally), but it results in this error.&lt;/p&gt; &lt;p&gt;Deeper investigations show that the to_json method&amp;#39;s kwargs creation omits values that are falsy, which results in its &amp;quot;content&amp;quot; property not being set or saved for AIMessages with tool calls, thus causing the error.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Thoughts:&lt;/strong&gt; It is strange that the BaseMessage&amp;#39;s content attribute is of type &lt;code&gt;Union[str, List[Union[str, Dict]]]&lt;/code&gt;, which gives the error when None is passed, rather than default to an empty string.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Solution?&lt;/strong&gt; I am still experimenting with this. Either to add an extra step when saving to manually add an empty content property to the dict as well, or call &lt;code&gt;message.json&lt;/code&gt; or &lt;code&gt;message.dict&lt;/code&gt; rather than &lt;code&gt;message.to_json&lt;/code&gt;. Then I&amp;#39;ll have to continue testing to see if it will work.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Conclusion:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;All of this, just to get the token usage.&lt;/p&gt; &lt;p&gt;Am I going about this the wrong way? It seems like I am fighting the package more than undergoing development in this case. Are all my thoughts valid? Should the langchain_openai package be updated to support more of the features langgraph provides for better integration? Or is that already handled in another library and I am using the wrong ones?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Danidre&quot;&gt; /u/Danidre &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1efvx6d/serializingdeserializing_messages_am_i_using/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1efvx6d/serializingdeserializing_messages_am_i_using/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1efvx6d</id><media:thumbnail url="https://b.thumbs.redditmedia.com/XItnW8QCzowSUudO50JGRNRonIkjkBMKtal88jBo1wM.jpg" /><link href="https://www.reddit.com/r/LangChain/comments/1efvx6d/serializingdeserializing_messages_am_i_using/" /><updated>2024-07-30T15:14:14+00:00</updated><published>2024-07-30T15:14:14+00:00</published><title>Serializing/Deserializing Messages? Am I using packages correctly? (LangGraph)</title></entry><entry><author><name>/u/PromptAwkward7277</name><uri>https://www.reddit.com/user/PromptAwkward7277</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;This is my code:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;from langchain_google_vertexai import VertexAI from langchain.prompts import PromptTemplate from langchain.chains import LLMChain class GeminiProcessor: def __init__(self, model_name, project): self.model=VertexAI( model_name=model_name, project=project ) class bookProcessor: def __init__(self, topic:str, genai_processor: GeminiProcessor): self.topic = topic self.GeminiProcessor = genai_processor self.system_template = &amp;quot;&amp;quot;&amp;quot; Your task is to generate a general description of a book on the topic of {keyword}. &amp;quot;&amp;quot;&amp;quot; def generate_ebook(self): prompt = PromptTemplate.from_template( template=self.system_template, ) # creating chain chain = prompt | self.GeminiProcessor.model output = chain.invoke(self.topic) print(&amp;quot;output:&amp;quot;, output) return output &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;I don&amp;#39;t really know why this is not working.&lt;/p&gt; &lt;p&gt;I keep getting this weird error: &lt;/p&gt; &lt;pre&gt;&lt;code&gt;line 166, in &amp;lt;listcomp&amp;gt; &amp;quot;category&amp;quot;: rating.category.name, ^^^^^^^^^^^^^^^^^^^^ AttributeError: &amp;#39;int&amp;#39; object has no attribute &amp;#39;name&amp;#39; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Can anyone help with this bug? I&amp;#39;m a beginner at LangChain so any help would be appreciated.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/PromptAwkward7277&quot;&gt; /u/PromptAwkward7277 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1efsssr/i_cant_run_my_simple_model/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1efsssr/i_cant_run_my_simple_model/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1efsssr</id><link href="https://www.reddit.com/r/LangChain/comments/1efsssr/i_cant_run_my_simple_model/" /><updated>2024-07-30T13:03:57+00:00</updated><published>2024-07-30T13:03:57+00:00</published><title>I can't run my simple model</title></entry><entry><author><name>/u/shorty1027</name><uri>https://www.reddit.com/user/shorty1027</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi! I&amp;#39;m trying to implement a RAG service for a chatbot that retrieves information from a document based on text queries. For the most part my code works when implemented in an endpoint, but when executing for a second time i get this: &amp;quot;PermissionError: [WinError 32] The process cannot access the file because it is being used by another process: &amp;#39;chroma\\62bff5e8-b534-41dd-8d3b-e767c1d4b598\\data_level0.bin&amp;#39;&amp;quot;. I tried closing the file with a method, I tried deleting the database everytime before running, deleting the collection, etc. but nothing seems to work. Any suggestions?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/shorty1027&quot;&gt; /u/shorty1027 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1efp23j/chromadb_internal_server_error_when_trying_to/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1efp23j/chromadb_internal_server_error_when_trying_to/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1efp23j</id><link href="https://www.reddit.com/r/LangChain/comments/1efp23j/chromadb_internal_server_error_when_trying_to/" /><updated>2024-07-30T09:32:28+00:00</updated><published>2024-07-30T09:32:28+00:00</published><title>ChromaDB Internal Server Error when trying to implement RAG service on a server</title></entry><entry><author><name>/u/dhj9817</name><uri>https://www.reddit.com/user/dhj9817</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I wanted to share what I built with this community to see what you guys think. I&amp;#39;m curious about any use cases you might have or just general feedback.&lt;/p&gt; &lt;p&gt;I created ParDocs with a simple mission: to make document extraction as painless as possible. I know firsthand how much time and effort can go into pre-training and labeling, and I wanted to build a tool that lets you focus on what really matters -&amp;gt; building and coding.&lt;/p&gt; &lt;p&gt;With ParDocs, you can:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Extract data from any document types with minimal setup.&lt;/li&gt; &lt;li&gt;Customize the JSON format you receive as a response.&lt;/li&gt; &lt;li&gt;Save loads of time on tedious pre-training tasks.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Check out our beta here: &lt;a href=&quot;https://www.traddocs.com/&quot;&gt;https://www.pardocs.com&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;For those who prefer not to click on unknown links, here‚Äôs our YouTube demo video: &lt;a href=&quot;https://youtu.be/LdCC0uBQ-QE&quot;&gt;https://youtu.be/LdCC0uBQ-QE&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;It‚Äôs free to use during this beta phase. After that, I&amp;#39;m considering pricing it at $0.014 for the splitter and $0.075 for the extractor. I‚Äôd love to hear your feedback on this.&lt;/p&gt; &lt;p&gt;Using ParDocs is very simple:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Specify the types of documents you&amp;#39;d like to extract.&lt;/li&gt; &lt;li&gt;Enter the desired JSON format for the response.&lt;/li&gt; &lt;li&gt;Upload your document and receive the data you need!&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;I‚Äôm personally available to answer any questions or help you get started. You can DM me on Reddit or chat with me on Discord: &lt;a href=&quot;https://discord.gg/xgEXkh7Rxk&quot;&gt;https://discord.gg/xgEXkh7Rxk&lt;/a&gt;. I‚Äôd love to hear what you think and how we can make ParDocs even better.&lt;/p&gt; &lt;p&gt;Looking forward to your thoughts and feedback!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/dhj9817&quot;&gt; /u/dhj9817 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ef5f7t/i_built_a_document_parser_that_works_without/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ef5f7t/i_built_a_document_parser_that_works_without/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1ef5f7t</id><link href="https://www.reddit.com/r/LangChain/comments/1ef5f7t/i_built_a_document_parser_that_works_without/" /><updated>2024-07-29T17:29:28+00:00</updated><published>2024-07-29T17:29:28+00:00</published><title>I built a document parser that works without pre-training, unlike google document ai or azure document intelligence. Would love your feedback!</title></entry><entry><author><name>/u/squadfi</name><uri>https://www.reddit.com/user/squadfi</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1efo38c/chat_with_your_sql_database_using_llm/&quot;&gt; &lt;img src=&quot;https://external-preview.redd.it/PnmvP15V-uh1Pwc851H4fA0POs75eSE4v6-7fNfPHlY.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=af04fe831f4115695d3f2d87869c7e14c4c0da7a&quot; alt=&quot;Chat With Your SQL Database Using LLM&quot; title=&quot;Chat With Your SQL Database Using LLM&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/squadfi&quot;&gt; /u/squadfi &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://adrelien.com/blog/chat-with-your-sql-database-using-llm/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1efo38c/chat_with_your_sql_database_using_llm/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1efo38c</id><media:thumbnail url="https://external-preview.redd.it/PnmvP15V-uh1Pwc851H4fA0POs75eSE4v6-7fNfPHlY.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=af04fe831f4115695d3f2d87869c7e14c4c0da7a" /><link href="https://www.reddit.com/r/LangChain/comments/1efo38c/chat_with_your_sql_database_using_llm/" /><updated>2024-07-30T08:24:24+00:00</updated><published>2024-07-30T08:24:24+00:00</published><title>Chat With Your SQL Database Using LLM</title></entry></feed>