<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/"><category term="LangChain" label="r/LangChain"/><updated>2024-06-16T14:39:18+00:00</updated><icon>https://www.redditstatic.com/icon.png/</icon><id>/r/LangChain.rss</id><link rel="self" href="https://www.reddit.com/r/LangChain.rss" type="application/atom+xml" /><link rel="alternate" href="https://www.reddit.com/r/LangChain" type="text/html" /><subtitle>LangChain is an open-source framework and developer toolkit that helps developers get LLM applications from prototype to production. It is available for Python and Javascript at https://www.langchain.com/.</subtitle><title>LangChain</title><entry><author><name>/u/zchaarm</name><uri>https://www.reddit.com/user/zchaarm</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;A place for members of &lt;a href=&quot;/r/LangChain&quot;&gt;r/LangChain&lt;/a&gt; to chat with each other&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/zchaarm&quot;&gt; /u/zchaarm &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_10ljho9</id><link href="https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/" /><updated>2023-01-26T04:33:05+00:00</updated><published>2023-01-26T04:33:05+00:00</published><title>r/LangChain Lounge</title></entry><entry><author><name>/u/sarthakai</name><uri>https://www.reddit.com/user/sarthakai</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Most RAG apps use Dense Passage Retrieval to find relevant docs. But there are better methods:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;RAG-Token:&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;It generates each token by considering different docs and chooses the most probable token at each step. So that every part of the answer is influenced by the best possible context.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;RAG-Sequence:&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;It calculates the probability of each answer and selects the one with the highest combined probability, getting you the best possible answer based on multiple sources. Itâ€™s a lot like RAG-token but less granular.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Fusion-in-Decoder (FiD):&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;It encodes all pairs of questions and chunks in parallel and then combines these encodings before feeding them into the decoder, which generates the answer step-by-step.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Graph RAG:&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;In case your documents are highly interconnected, the links between them are probably important to generate a relevant response.&lt;/p&gt; &lt;p&gt;Search results from Graph RAG are more likely to give you a comprehensive view of the entity being searched and the info connected to it.&lt;/p&gt; &lt;p&gt;I spent the weekend creating a Python library which automatically creates this graph for the documents present in your vectordb. It also makes it easy for you to retrieve relevant documents connected to the best matches.&lt;/p&gt; &lt;p&gt;Currently testing the library on medical documents to gauge its performance.&lt;/p&gt; &lt;p&gt;Sharing version 0.1 tomorrow! You can follow my social media to stay tuned: &lt;a href=&quot;https://linktr.ee/sarthakrastogi&quot;&gt;https://linktr.ee/sarthakrastogi&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/sarthakai&quot;&gt; /u/sarthakai &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dh79xx/suggesting_which_rag_method_will_work_best_for/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dh79xx/suggesting_which_rag_method_will_work_best_for/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dh79xx</id><link href="https://www.reddit.com/r/LangChain/comments/1dh79xx/suggesting_which_rag_method_will_work_best_for/" /><updated>2024-06-16T13:10:38+00:00</updated><published>2024-06-16T13:10:38+00:00</published><title>Suggesting which RAG method will work best for you, based on your use case ðŸ”ŽðŸ“‘</title></entry><entry><author><name>/u/filet_mign0n</name><uri>https://www.reddit.com/user/filet_mign0n</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/filet_mign0n&quot;&gt; /u/filet_mign0n &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;/r/LLMDevs/comments/1dgokij/any_agent_marketplace_worth_looking_into_these/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dh2amw/any_agent_marketplace_worth_looking_into_these/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dh2amw</id><link href="https://www.reddit.com/r/LangChain/comments/1dh2amw/any_agent_marketplace_worth_looking_into_these/" /><updated>2024-06-16T07:26:50+00:00</updated><published>2024-06-16T07:26:50+00:00</published><title>Any agent marketplace worth looking into these days?</title></entry><entry><author><name>/u/HomunMage</name><uri>https://www.reddit.com/user/HomunMage</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Because langchain/langgraph example and tutorial is sxcking, I beleieve many people agree that. such &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d4lwt0/am_i_the_only_one_who_feels_langgraph/&quot;&gt;Am I the only one who feels LangGraph documentation and tutorials by lanfchain absolutely sxck?&lt;/a&gt;&lt;/p&gt; &lt;p&gt;for example, all examples are openai related llm interface and hard to convert to local such ollama.&lt;br/&gt; This makes me even a &lt;a href=&quot;https://github.com/HomunMage/AI_Agents/blob/main/LangChain/Hello%20World.py&quot;&gt;simple hello world&lt;/a&gt; need hours to coding it.&lt;/p&gt; &lt;p&gt;That is why I had crate a &lt;a href=&quot;https://github.com/HomunMage/CrewAI-GUI&quot;&gt;GUI for CrewAI&lt;/a&gt; , not a gui for langchain or langgraph. But I think learning langgraph still important.&lt;/p&gt; &lt;p&gt;I want to find or build a langgraph learning group. And also want to build a LangGraph-GUI&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/HomunMage&quot;&gt; /u/HomunMage &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dgxdvq/any_learning_group_for_langgraph/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dgxdvq/any_learning_group_for_langgraph/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dgxdvq</id><link href="https://www.reddit.com/r/LangChain/comments/1dgxdvq/any_learning_group_for_langgraph/" /><updated>2024-06-16T02:09:17+00:00</updated><published>2024-06-16T02:09:17+00:00</published><title>Any learning Group for LangGraph?</title></entry><entry><author><name>/u/cotimbo</name><uri>https://www.reddit.com/user/cotimbo</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dh5qr2/determinism_control/&quot;&gt; &lt;img src=&quot;https://a.thumbs.redditmedia.com/1AxizTcDr5jabPDa8ybjYV6209DIdcL_hFiPr3S4oG0.jpg&quot; alt=&quot;Determinism control&quot; title=&quot;Determinism control&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Im trying to get my workflow to be accurate and help me give the same response every time.&lt;/p&gt; &lt;p&gt;I have temp set to zero. base prompt bossing the model around to be &amp;#39;deterministic&amp;#39; but you can see, i still have wildly different outputs each time thing thing runs.&lt;/p&gt; &lt;p&gt;any advice on getting this to be more accurate?&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/ibu2ivj68x6d1.png?width=1908&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=47bd31b849fa690dc12e11cc9604702587aae6fa&quot;&gt;https://preview.redd.it/ibu2ivj68x6d1.png?width=1908&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=47bd31b849fa690dc12e11cc9604702587aae6fa&lt;/a&gt;&lt;/p&gt; &lt;p&gt;base prompt: You are a deterministic GPT model designed to analyze a list of transactions in a SQL table. Your task is to provide the same response every time you are asked the same question. Follow these guidelines to ensure determinism:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Always follow the exact format provided in the examples below.&lt;/li&gt; &lt;li&gt;Provide responses based strictly on the information available in the given table.&lt;/li&gt; &lt;li&gt;Do not include any additional or inferred information.&lt;/li&gt; &lt;li&gt;Ensure the order of transactions is consistent based on the primary key or specified order.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Note: the actual correct answer is 106 items. the SQL has 2,000 lines on it&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/cotimbo&quot;&gt; /u/cotimbo &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dh5qr2/determinism_control/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dh5qr2/determinism_control/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1dh5qr2</id><media:thumbnail url="https://a.thumbs.redditmedia.com/1AxizTcDr5jabPDa8ybjYV6209DIdcL_hFiPr3S4oG0.jpg" /><link href="https://www.reddit.com/r/LangChain/comments/1dh5qr2/determinism_control/" /><updated>2024-06-16T11:40:29+00:00</updated><published>2024-06-16T11:40:29+00:00</published><title>Determinism control</title></entry><entry><author><name>/u/victorevolves</name><uri>https://www.reddit.com/user/victorevolves</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi there! &lt;/p&gt; &lt;p&gt;I am trying to query an api with its auth value as its get parameter&lt;br/&gt; &lt;a href=&quot;https://test.com/?key=apikey&quot;&gt;https://test.com/?key=apikey&lt;/a&gt;&lt;br/&gt; How do I tell the Langchain agent to pass an additional url param? (in this case, key) in every requests?&lt;/p&gt; &lt;p&gt;This is how it is executed atm. Thanks in advance &amp;lt;3&lt;/p&gt; &lt;pre&gt;&lt;code&gt;model = Chat(model=&amp;quot;claude-3-haiku-20240307&amp;quot;, api_key=CLAUDE_API_KEY) weather_agent = planner.create_openapi_agent( MY_OPENAPI_SPEC, RequestsWrapper(), llm=model, allow_dangerous_requests=True ) &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/victorevolves&quot;&gt; /u/victorevolves &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dh3fq8/passing_api_key_via_url_params/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dh3fq8/passing_api_key_via_url_params/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dh3fq8</id><link href="https://www.reddit.com/r/LangChain/comments/1dh3fq8/passing_api_key_via_url_params/" /><updated>2024-06-16T08:53:47+00:00</updated><published>2024-06-16T08:53:47+00:00</published><title>Passing API Key via URL Params</title></entry><entry><author><name>/u/Nimitzxz</name><uri>https://www.reddit.com/user/Nimitzxz</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;I&amp;#39;ve been working on a project that aims to enhance job applications using AI. It&amp;#39;s called &lt;a href=&quot;https://github.com/DAVEinside/GenAI_Job_Fit&quot;&gt;GenAI_Job_Fit&lt;/a&gt;, and I would love for you all to check it out. I took inspiration from the Agent-Supervisor example notebook provided.&lt;/p&gt; &lt;p&gt;The system leverages AI to analyze job descriptions and tailor resumes to match job requirements, increasing the chances of getting noticed by recruiters and ATS (Applicant Tracking Systems). Here are some key features:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Automated resume tailoring&lt;/li&gt; &lt;li&gt;Keyword optimization&lt;/li&gt; &lt;li&gt;Compatibility scoring between job descriptions and resumes&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I&amp;#39;d really appreciate it if you could take a look and let me know what you think. I&amp;#39;m particularly interested in any suggestions for improvements or additional features that could make the tool even more useful.&lt;/p&gt; &lt;p&gt;Feel free to fork the repo, open issues, or submit pull requests. Your feedback will be invaluable in making this project better!&lt;/p&gt; &lt;p&gt;Repo Link : &lt;a href=&quot;https://github.com/DAVEinside/GenAI_Job_Fit&quot;&gt;https://github.com/DAVEinside/GenAI_Job_Fit&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Thank you!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Nimitzxz&quot;&gt; /u/Nimitzxz &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dgnmjt/aidriven_job_application_enhancement_system/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dgnmjt/aidriven_job_application_enhancement_system/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dgnmjt</id><link href="https://www.reddit.com/r/LangChain/comments/1dgnmjt/aidriven_job_application_enhancement_system/" /><updated>2024-06-15T17:57:17+00:00</updated><published>2024-06-15T17:57:17+00:00</published><title>AI-Driven Job Application Enhancement System - Seeking Feedback and Suggestions!</title></entry><entry><author><name>/u/darkziosj</name><uri>https://www.reddit.com/user/darkziosj</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hello! Im using the APIchain i use it to get data from an api endpoint, the problem is that the api returns a largue amount of data in json format (i need all the data that the api returns), i will then use a agent to ask questions about that data but since is so massive there&amp;#39;s problems like the token amount or time it takes to analyze, can anyone giveme some tips about what can i do to better the performance or what to do to solve this kind of problem? thanks alot!!!&lt;/p&gt; &lt;p&gt;this is the apiChain im using:&lt;/p&gt; &lt;p&gt;llm = OpenAI(temperature=0)&lt;br/&gt; chain = APIChain.from_llm_and_api_docs(&lt;br/&gt; llm,&lt;br/&gt; open_meteo_docs.OPEN_METEO_DOCS,&lt;br/&gt; verbose=True,&lt;br/&gt; limit_to_domains=[&amp;quot;&lt;a href=&quot;https://api.open-meteo.com/%22%5C&quot;&gt;https://api.open-meteo.com/&amp;quot;\&lt;/a&gt;],&lt;br/&gt; )&lt;br/&gt; chain.run(&lt;br/&gt; &amp;quot;What is the weather like right now in Munich, Germany in degrees Fahrenheit?&amp;quot;&lt;br/&gt; )&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/darkziosj&quot;&gt; /u/darkziosj &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dgm1ij/how_to_work_with_large_data_that_is_returned_from/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dgm1ij/how_to_work_with_large_data_that_is_returned_from/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dgm1ij</id><link href="https://www.reddit.com/r/LangChain/comments/1dgm1ij/how_to_work_with_large_data_that_is_returned_from/" /><updated>2024-06-15T16:42:29+00:00</updated><published>2024-06-15T16:42:29+00:00</published><title>How to work with large data that is returned from an api?</title></entry><entry><author><name>/u/Perfect_Manner8494</name><uri>https://www.reddit.com/user/Perfect_Manner8494</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;i used chromadb with langchain to create embeddings. i used persistent_directory to save those locally and it did but now i am not able to load them. these are the codes&lt;/p&gt; &lt;h1&gt;saving embeddings&lt;/h1&gt; &lt;p&gt;vector_storage=Chroma.from_documents(split,OllamaEmbeddings(model=&amp;quot;nomic-embed-text&amp;quot;), persist_directory=&amp;quot;vector_store&amp;quot;,collection_name=&amp;quot;qna_embeddings&amp;quot;)&lt;/p&gt; &lt;h1&gt;loading embeddings&lt;/h1&gt; &lt;p&gt;vector_store2=Chroma(persist_directory=&amp;quot;vector_store&amp;quot;,embedding_function=OllamaEmbeddings(model=&amp;quot;nomic-embed-text&amp;quot;))&lt;/p&gt; &lt;p&gt;to check i printed the following and it gives 0 as output&lt;/p&gt; &lt;p&gt;print(vector_store2._collection.count())&lt;/p&gt; &lt;p&gt;pls help me &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Perfect_Manner8494&quot;&gt; /u/Perfect_Manner8494 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dgpc6w/save_and_load_embeddings/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dgpc6w/save_and_load_embeddings/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dgpc6w</id><link href="https://www.reddit.com/r/LangChain/comments/1dgpc6w/save_and_load_embeddings/" /><updated>2024-06-15T19:17:27+00:00</updated><published>2024-06-15T19:17:27+00:00</published><title>save and load embeddings</title></entry><entry><author><name>/u/phicreative1997</name><uri>https://www.reddit.com/user/phicreative1997</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dgn8l5/improving_performance_for_data_visualization_ai/&quot;&gt; &lt;img src=&quot;https://external-preview.redd.it/Q6J3m0iBOylgxJPGbdH3ZhbDZVBtgunJRGQdTd5E1bw.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=84ec9c09ce6ecb6530e7a84a850f1b5278517368&quot; alt=&quot;Improving Performance for Data Visualization AI Agent&quot; title=&quot;Improving Performance for Data Visualization AI Agent&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/phicreative1997&quot;&gt; /u/phicreative1997 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://medium.com/firebird-technologies/improving-performance-for-data-visualization-ai-agent-d677ccb71e81&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dgn8l5/improving_performance_for_data_visualization_ai/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1dgn8l5</id><media:thumbnail url="https://external-preview.redd.it/Q6J3m0iBOylgxJPGbdH3ZhbDZVBtgunJRGQdTd5E1bw.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=84ec9c09ce6ecb6530e7a84a850f1b5278517368" /><link href="https://www.reddit.com/r/LangChain/comments/1dgn8l5/improving_performance_for_data_visualization_ai/" /><updated>2024-06-15T17:38:58+00:00</updated><published>2024-06-15T17:38:58+00:00</published><title>Improving Performance for Data Visualization AI Agent</title></entry><entry><author><name>/u/sarthakai</name><uri>https://www.reddit.com/user/sarthakai</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;First, how it works:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;Memory Tuning fine-tunes millions of LoRA adapters (memory experts) on any open-source LLM to ensure accurate fact recall.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;During inference, the model retrieves and integrates the most relevant experts, (a lot like information retrieval). This gives much high accuracy and reduced hallucinations.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;This approach maintains the model&amp;#39;s ability to generalise â€” while at the same time focusing on zero error for specified facts.&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Why is this better than RAG?&lt;/p&gt; &lt;p&gt;RAG shifts probabilities without eliminating errors â€” while Memory Tuning fully corrects inaccuracies.&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://www.linkedin.com/company/lamini-ai/&quot;&gt;Lamini&lt;/a&gt; released their Memory Tuning solution for enterprises with case studies showing amazing accuracy boosts for text-to-sql, labelling, and even recommendation tasks.&lt;/p&gt; &lt;p&gt;Paper: &lt;a href=&quot;https://github.com/lamini-ai/Lamini-Memory-Tuning/blob/main/research-paper.pdf&quot;&gt;https://github.com/lamini-ai/Lamini-Memory-Tuning/blob/main/research-paper.pdf&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I share high quality AI updates and tutorials daily on my LinkedIn: &lt;a href=&quot;https://www.linkedin.com/in/sarthakrastogi/&quot;&gt;https://www.linkedin.com/in/sarthakrastogi/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;If you like this post and want to stay updated on latest AI research, you can check out: &lt;a href=&quot;https://linktr.ee/sarthakrastogi&quot;&gt;https://linktr.ee/sarthakrastogi&lt;/a&gt;.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/sarthakai&quot;&gt; /u/sarthakai &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dgi0vj/whats_memory_tuning_and_how_does_it_give_higher/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dgi0vj/whats_memory_tuning_and_how_does_it_give_higher/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dgi0vj</id><link href="https://www.reddit.com/r/LangChain/comments/1dgi0vj/whats_memory_tuning_and_how_does_it_give_higher/" /><updated>2024-06-15T13:30:01+00:00</updated><published>2024-06-15T13:30:01+00:00</published><title>Whatâ€™s Memory Tuning and how does it give higher accuracy + speed than RAG and prompting?</title></entry><entry><author><name>/u/Sweaty-Minimum5423</name><uri>https://www.reddit.com/user/Sweaty-Minimum5423</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hello all, OpenAI assistant should support streaming. But I am not sure why the current OpenAIAssistantV2Runnable do not supports it. Is there a solution to this?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Sweaty-Minimum5423&quot;&gt; /u/Sweaty-Minimum5423 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dgfsmg/streaming_of_openaiassistant_v2/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dgfsmg/streaming_of_openaiassistant_v2/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dgfsmg</id><link href="https://www.reddit.com/r/LangChain/comments/1dgfsmg/streaming_of_openaiassistant_v2/" /><updated>2024-06-15T11:23:04+00:00</updated><published>2024-06-15T11:23:04+00:00</published><title>Streaming of OpenAIAssistant v2</title></entry><entry><author><name>/u/filet_mign0n</name><uri>https://www.reddit.com/user/filet_mign0n</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Wondering if anyone here has dealt with passing private information from end user inputs to your LLM, later to interact with an external API? I&amp;#39;m not talking about authentication data per se, just private information (e.g PII) people wouldn&amp;#39;t normally want to share on the internet.&lt;br/&gt; What solution have you come up with to ensure some privacy for your users?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/filet_mign0n&quot;&gt; /u/filet_mign0n &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dfyz50/how_to_securely_pass_private_data/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dfyz50/how_to_securely_pass_private_data/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dfyz50</id><link href="https://www.reddit.com/r/LangChain/comments/1dfyz50/how_to_securely_pass_private_data/" /><updated>2024-06-14T19:23:46+00:00</updated><published>2024-06-14T19:23:46+00:00</published><title>How to securely pass private data?</title></entry><entry><author><name>/u/RaeudigerRaffi</name><uri>https://www.reddit.com/user/RaeudigerRaffi</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;m excited to share an updated open-source resource weâ€™ve been working onâ€”an improved version of the Spider dataset originally published by Yale University for Text2SQL tasks. You can check it out here: &lt;a href=&quot;https://huggingface.co/datasets/RaffaSch121/fixed_spider&quot;&gt;https://huggingface.co/datasets/RaffaSch121/fixed_spider&lt;/a&gt;&lt;/p&gt; &lt;p&gt;During our own model training at &lt;a href=&quot;http://www.turbular.com&quot;&gt;Turbular&lt;/a&gt; we identified several issues in the original dataset. To help the community and give back, we decided to address these problems and release a corrected version. We hope this enhanced dataset will benefit everyone working on Text2SQL and similar projects.&lt;/p&gt; &lt;p&gt;Feel free to download, experiment, and contribute back if you find ways to make it even better!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/RaeudigerRaffi&quot;&gt; /u/RaeudigerRaffi &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dfsdbw/improved_text2sql_dataset_now_available_on/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dfsdbw/improved_text2sql_dataset_now_available_on/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dfsdbw</id><link href="https://www.reddit.com/r/LangChain/comments/1dfsdbw/improved_text2sql_dataset_now_available_on/" /><updated>2024-06-14T14:37:24+00:00</updated><published>2024-06-14T14:37:24+00:00</published><title>Improved Text2SQL Dataset Now Available on Huggingface!</title></entry><entry><author><name>/u/FunInformation2332</name><uri>https://www.reddit.com/user/FunInformation2332</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dfxwga/evaluating_with_ragas/&quot;&gt; &lt;img src=&quot;https://a.thumbs.redditmedia.com/XCOcROHXeB7lrz95uNOAiGKakHooUXoHIPXAQ38I2n0.jpg&quot; alt=&quot;Evaluating with Ragas&quot; title=&quot;Evaluating with Ragas&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;ve finished my rag job, and performed a evaluation on my rag. results given below&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/pt0khqy10l6d1.png?width=1280&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4979a08f0e648937407d23feeb494f02a8e793ba&quot;&gt;ragas output&lt;/a&gt;&lt;/p&gt; &lt;p&gt;context_precision is better than good but why the other metrics sucks and how to improve them?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/FunInformation2332&quot;&gt; /u/FunInformation2332 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dfxwga/evaluating_with_ragas/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dfxwga/evaluating_with_ragas/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1dfxwga</id><media:thumbnail url="https://a.thumbs.redditmedia.com/XCOcROHXeB7lrz95uNOAiGKakHooUXoHIPXAQ38I2n0.jpg" /><link href="https://www.reddit.com/r/LangChain/comments/1dfxwga/evaluating_with_ragas/" /><updated>2024-06-14T18:36:01+00:00</updated><published>2024-06-14T18:36:01+00:00</published><title>Evaluating with Ragas</title></entry><entry><author><name>/u/UnderstandLingAI</name><uri>https://www.reddit.com/user/UnderstandLingAI</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;We are in early stages of developing our project so keen feedback. RAG Me Up is a robust layer on top of Langchain designed to make RAG easy and also not prone to simple issues like document re-retrieval, performance for rephrasind and perhaps most importantly: make Langchain work well with Instruct/Chat models&amp;#39; templates.&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/AI-Commandos/RAGMeUp&quot;&gt;https://github.com/AI-Commandos/RAGMeUp &lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/UnderstandLingAI&quot;&gt; /u/UnderstandLingAI &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dfx2di/rag_me_up_rag_for_chat_w_langchain/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dfx2di/rag_me_up_rag_for_chat_w_langchain/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dfx2di</id><link href="https://www.reddit.com/r/LangChain/comments/1dfx2di/rag_me_up_rag_for_chat_w_langchain/" /><updated>2024-06-14T18:00:18+00:00</updated><published>2024-06-14T18:00:18+00:00</published><title>RAG Me Up - RAG for chat /w Langchain</title></entry><entry><author><name>/u/MoronSlayer42</name><uri>https://www.reddit.com/user/MoronSlayer42</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I have implementing streaming with a chain based runnable which gives token by token output ( word by word), making UI similar to how ChatGPT has its UI. But while implementing the same with an Agent based runnable I see that it gives 3 outputs in order, actions, steps and, output which contains answer. All three come as a whole, one after the other, not word by word.&lt;/p&gt; &lt;p&gt;I want to get word by word streaming for the agent&amp;#39;s final answer.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/MoronSlayer42&quot;&gt; /u/MoronSlayer42 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dfsv2t/streaming_with_agents/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dfsv2t/streaming_with_agents/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dfsv2t</id><link href="https://www.reddit.com/r/LangChain/comments/1dfsv2t/streaming_with_agents/" /><updated>2024-06-14T14:59:04+00:00</updated><published>2024-06-14T14:59:04+00:00</published><title>Streaming with agents</title></entry><entry><author><name>/u/ANil1729</name><uri>https://www.reddit.com/user/ANil1729</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I have written an article on how to create a Text to Video AI generator which generates video from a topic by collecting relevant stock videos and stitching them together. &lt;/p&gt; &lt;p&gt;The code is completely open-source and uses free to use tools to generate videos&lt;/p&gt; &lt;p&gt;Link to article :- &lt;a href=&quot;https://medium.com/@anilmatcha/text-to-video-ai-how-to-create-videos-for-free-a-complete-guide-a25c91de50b8&quot;&gt;https://medium.com/@anilmatcha/text-to-video-ai-how-to-create-videos-for-free-a-complete-guide-a25c91de50b8&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/ANil1729&quot;&gt; /u/ANil1729 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dfsc15/a_tutorial_on_creating_video_from_text_using_ai/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dfsc15/a_tutorial_on_creating_video_from_text_using_ai/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dfsc15</id><link href="https://www.reddit.com/r/LangChain/comments/1dfsc15/a_tutorial_on_creating_video_from_text_using_ai/" /><updated>2024-06-14T14:35:53+00:00</updated><published>2024-06-14T14:35:53+00:00</published><title>A tutorial on creating video from text using AI</title></entry><entry><author><name>/u/profsartor</name><uri>https://www.reddit.com/user/profsartor</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;As the title reads, I&amp;#39;m building a side project to chat with my google calendar + assignments from Canvas (learning management system). I&amp;#39;m using GCP to practice working with the cloud. &lt;/p&gt; &lt;p&gt;As of April 2024, Cloud SQL for MySQL now supports vector embeddings. Essentially, I have all of my coursework and assignments in an events table. At first I embedded at the row level but this lost the understanding of columns. Now, I have a new column that is JSON representation of all the relevant columns for my eventual retrieval (event_title, start_time, end_time, tag (Assignment, Discussion, Quiz, Study Times, Personal Events)). In a new column, I&amp;#39;ve successfully embedded all of these JSON&amp;#39;s. What I&amp;#39;ve described above is pretty much the extent of what I&amp;#39;ve done. &lt;/p&gt; &lt;p&gt;My end goal is to develop a streamlit UI to query this vector column in my SQL database. I have a few different paths I can go down, but I&amp;#39;m intentionally keeping this at a high level to hear diverse responses. &lt;/p&gt; &lt;p&gt;Any advice? All thoughts are greatly appreciated. &lt;/p&gt; &lt;p&gt;Cheers&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/profsartor&quot;&gt; /u/profsartor &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dfsjwl/newbie_seeking_advice_on_side_project_chat_with/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dfsjwl/newbie_seeking_advice_on_side_project_chat_with/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dfsjwl</id><link href="https://www.reddit.com/r/LangChain/comments/1dfsjwl/newbie_seeking_advice_on_side_project_chat_with/" /><updated>2024-06-14T14:45:29+00:00</updated><published>2024-06-14T14:45:29+00:00</published><title>Newbie Seeking Advice on Side Project - Chat with Calendar</title></entry><entry><author><name>/u/alcatraz0411</name><uri>https://www.reddit.com/user/alcatraz0411</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey all, How do I get the token count for chain.astream_events()&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/alcatraz0411&quot;&gt; /u/alcatraz0411 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dfpjpr/token_count_and_cost_for_chainastream_events/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dfpjpr/token_count_and_cost_for_chainastream_events/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dfpjpr</id><link href="https://www.reddit.com/r/LangChain/comments/1dfpjpr/token_count_and_cost_for_chainastream_events/" /><updated>2024-06-14T12:23:23+00:00</updated><published>2024-06-14T12:23:23+00:00</published><title>Token count and cost for chain.astream_events().</title></entry><entry><author><name>/u/Convhay</name><uri>https://www.reddit.com/user/Convhay</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi i am new to the framework of langchain and i want to search for some information in contract documents regarding total m2 area for a partner. The problem is that the main partner contract can have several newer appendices where the old total m2 area in the old original contract is now replaced. Now i only want to extract the new total m2 area. Is there a clever way to sort or filter this?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Convhay&quot;&gt; /u/Convhay &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dfp7xf/ragchain_searching_for_similar_prompts/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dfp7xf/ragchain_searching_for_similar_prompts/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dfp7xf</id><link href="https://www.reddit.com/r/LangChain/comments/1dfp7xf/ragchain_searching_for_similar_prompts/" /><updated>2024-06-14T12:05:06+00:00</updated><published>2024-06-14T12:05:06+00:00</published><title>RAGchain searching for similar prompts</title></entry><entry><author><name>/u/EscapedLaughter</name><uri>https://www.reddit.com/user/EscapedLaughter</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dfp2z5/project_compare_top_10_lmsys_models_with_a/&quot;&gt; &lt;img src=&quot;https://external-preview.redd.it/vrQ98WHCObyIKaUSsC_cjzHZfMprk1y9ugKJTbGEQhc.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4b33690b47fabd60a6ce3e21bdf3b238eaebdb8f&quot; alt=&quot;[Project] Compare Top 10 LMSYS Models with a Universal LLM API Library&quot; title=&quot;[Project] Compare Top 10 LMSYS Models with a Universal LLM API Library&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hello Langchain community!&lt;/p&gt; &lt;p&gt;I&amp;#39;m excited to share a project we&amp;#39;ve been working on - an open-source &amp;quot;AI Gateway&amp;quot; library that allows you to access and compare 200+ language models from multiple providers using a simple, unified API.&lt;/p&gt; &lt;p&gt;To showcase the capabilities of this library, I&amp;#39;ve created a Google Colab notebook that demonstrates how you can easily compare the top 10 models from the LMSYS leaderboard with just a few lines of code.&lt;/p&gt; &lt;p&gt;Here&amp;#39;s a snippet:&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/lcqhryzx0j6d1.png?width=1822&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=cf7d055fa0e79117fed5dd8f8dc37498fe43b9e3&quot;&gt;https://preview.redd.it/lcqhryzx0j6d1.png?width=1822&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=cf7d055fa0e79117fed5dd8f8dc37498fe43b9e3&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The library handles all the complexities of authenticating and communicating with different provider APIs behind the scenes, allowing you to focus on experimenting with and comparing the models themselves.&lt;/p&gt; &lt;p&gt;Some key features of the AI Gateway library:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Unified API for accessing 200+ LLMs from OpenAI, Anthropic, Google, Ollama, Cohere, Together AI, and more&lt;/li&gt; &lt;li&gt;Compatible with existing OpenAI client libraries for easy integration&lt;/li&gt; &lt;li&gt;Routing capabilities like fallbacks, load balancing, retries&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I believe this library could be incredibly useful for researchers and developers in the Langchain community who want to easily compare and benchmark different LLMs, or build applications that leverage multiple models.&lt;/p&gt; &lt;p&gt;I&amp;#39;ve put the demo notebook link below, I&amp;#39;d love to get your feedback, suggestions, and contributions:&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/Portkey-AI/gateway/blob/main/cookbook/use-cases/LMSYS%20Series/comparing-top10-LMSYS-models-with-Portkey.ipynb&quot;&gt;https://github.com/Portkey-AI/gateway/blob/main/cookbook/use-cases/LMSYS%20Series/comparing-top10-LMSYS-models-with-Portkey.ipynb&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/EscapedLaughter&quot;&gt; /u/EscapedLaughter &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dfp2z5/project_compare_top_10_lmsys_models_with_a/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dfp2z5/project_compare_top_10_lmsys_models_with_a/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1dfp2z5</id><media:thumbnail url="https://external-preview.redd.it/vrQ98WHCObyIKaUSsC_cjzHZfMprk1y9ugKJTbGEQhc.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=4b33690b47fabd60a6ce3e21bdf3b238eaebdb8f" /><link href="https://www.reddit.com/r/LangChain/comments/1dfp2z5/project_compare_top_10_lmsys_models_with_a/" /><updated>2024-06-14T11:57:45+00:00</updated><published>2024-06-14T11:57:45+00:00</published><title>[Project] Compare Top 10 LMSYS Models with a Universal LLM API Library</title></entry><entry><author><name>/u/vT_Raven</name><uri>https://www.reddit.com/user/vT_Raven</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hello everyone, I mean no disrespect to anyone but I am having trouble seeing the appeal of using the lang chain. In my opinion I&amp;#39;am at best a beginner there for my view coulde be too shalow. I am hoping to find an anweser to where my blind spots are and what use cases the lang chain is useful for. For example, if I want to build a rag chatbot. I would use Ollama with Chromadb without any libery except for chromadb and requests. I have to admit that it is nice to try different things with lang chain. It is also easier to handle complex files like PDF. &lt;/p&gt; &lt;p&gt;If some of you say I don&amp;#39;t have enough experience, that&amp;#39;s why I don&amp;#39;t get it, the answer is fair enough for me to take a agaib a look at Lang Chain.&lt;/p&gt; &lt;p&gt;But I have already tried to work with the framework 3 times and it always seems too complex for what I want to achieve. All those time i build an Chatbot that allows to interact with an modell with some litte custmasation over envs. And the last time was a Rag Chatbot that allows me to index Websites to get answers about their content.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/vT_Raven&quot;&gt; /u/vT_Raven &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1df95xz/why_should_i_use_lang_chain/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1df95xz/why_should_i_use_lang_chain/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1df95xz</id><link href="https://www.reddit.com/r/LangChain/comments/1df95xz/why_should_i_use_lang_chain/" /><updated>2024-06-13T20:41:52+00:00</updated><published>2024-06-13T20:41:52+00:00</published><title>Why should i use lang chain?</title></entry><entry><author><name>/u/cryptokaykay</name><uri>https://www.reddit.com/user/cryptokaykay</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dfaquj/run_evaluations_with_langtrace/&quot;&gt; &lt;img src=&quot;https://b.thumbs.redditmedia.com/OlQpvPg6C80CPbeQqL74YpdIgrAULdbnNlYTTliAWPg.jpg&quot; alt=&quot;Run Evaluations with Langtrace&quot; title=&quot;Run Evaluations with Langtrace&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi all,&lt;/p&gt; &lt;p&gt;Its been a while from me, but just wanted to share that we have added support for running automated evals with Langtrace. As a reminder, Langtrace is an open source LLM application observability and evaluations tool. It is open telemetry compatible so no vendor lock-in. You can also self-host and run Langtrace.&lt;/p&gt; &lt;p&gt;We integrated langtrace with inspect AI (&lt;a href=&quot;https://github.com/UKGovernmentBEIS/inspect%5C_ai&quot;&gt;https://github.com/UKGovernmentBEIS/inspect\_ai&lt;/a&gt;). Inspect is an open source evluations tool from the developers of RStudio - you should definitely check it out. I love it.&lt;br/&gt; With langtrace, you can now&lt;/p&gt; &lt;ul&gt; &lt;li&gt;set up tracing in 2 lines of code&lt;/li&gt; &lt;li&gt;annotate and curate datasets&lt;/li&gt; &lt;li&gt;run evaluations against this dataset using Inspect&lt;/li&gt; &lt;li&gt;view results, compare the outputs against models and understand the performance of your app&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;So, you can now establish this feedback loop with langtrace.&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/qrwn7r1kte6d1.png?width=2304&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3c2d7c82abbb329518b35c133c0e7a0e73a6d53d&quot;&gt;https://preview.redd.it/qrwn7r1kte6d1.png?width=2304&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3c2d7c82abbb329518b35c133c0e7a0e73a6d53d&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Shown below are some screenshots:&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/t45vq2xute6d1.png?width=3156&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1c15fc71499ba5c5ccbf0aa566fc78c82730e209&quot;&gt;https://preview.redd.it/t45vq2xute6d1.png?width=3156&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1c15fc71499ba5c5ccbf0aa566fc78c82730e209&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/0gwmyz0xte6d1.png?width=3150&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2713ba619e903d2db227d5922e8e9c7a562fb9b7&quot;&gt;https://preview.redd.it/0gwmyz0xte6d1.png?width=3150&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2713ba619e903d2db227d5922e8e9c7a562fb9b7&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Would love get any feedback. Please do try it out and let me know.&lt;/p&gt; &lt;p&gt;Link: &lt;a href=&quot;https://github.com/Scale3-Labs/langtrace&quot;&gt;https://github.com/Scale3-Labs/langtrace&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/cryptokaykay&quot;&gt; /u/cryptokaykay &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dfaquj/run_evaluations_with_langtrace/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dfaquj/run_evaluations_with_langtrace/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1dfaquj</id><media:thumbnail url="https://b.thumbs.redditmedia.com/OlQpvPg6C80CPbeQqL74YpdIgrAULdbnNlYTTliAWPg.jpg" /><link href="https://www.reddit.com/r/LangChain/comments/1dfaquj/run_evaluations_with_langtrace/" /><updated>2024-06-13T21:50:44+00:00</updated><published>2024-06-13T21:50:44+00:00</published><title>Run Evaluations with Langtrace</title></entry><entry><author><name>/u/chaitu9701</name><uri>https://www.reddit.com/user/chaitu9701</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;To set the context we have 4 environments predev, dev, testing and production. Our RAG uses langchain for PDF extraction, qdrant(self hosted on kubernetes) for vectorstore and gpt-3.5-turbo-16k for the llm.&lt;/p&gt; &lt;p&gt;We have built a RAG, which worked well(gave correct answers from PDF) on predev. When we moved it to dev, in the initial days its performance(correctness) was bad and eventually got good without any changes, except for minor document update. Then it moved to testing environment where again the same behaviour. Now it&amp;#39;s in prod and again behaves the same. Facing a lot of backlash from client due to this strange behaviour.&lt;/p&gt; &lt;p&gt;It&amp;#39;s the same document, same gpt, but different qdrant hosted different for different environments.&lt;/p&gt; &lt;p&gt;Did anyone experience similar issue? Can anyone explain why the warmup time.&lt;/p&gt; &lt;p&gt;Any help is greatly appreciated.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/chaitu9701&quot;&gt; /u/chaitu9701 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1df1wnb/rag_performs_differently_in_different_environments/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1df1wnb/rag_performs_differently_in_different_environments/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1df1wnb</id><link href="https://www.reddit.com/r/LangChain/comments/1df1wnb/rag_performs_differently_in_different_environments/" /><updated>2024-06-13T15:35:41+00:00</updated><published>2024-06-13T15:35:41+00:00</published><title>RAG performs differently in different environments</title></entry><entry><author><name>/u/ChallengeOk6437</name><uri>https://www.reddit.com/user/ChallengeOk6437</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I am trying to build a model to take in 5-10 PDFs and answer questions based on them.&lt;/p&gt; &lt;p&gt;This is my flow ==&amp;gt; LlamaParse-&amp;gt;OpenAI ada embeddings -&amp;gt; FAISS vector store -&amp;gt; multi query retriever -&amp;gt; cohere reranker -&amp;gt; OpenAI gpt4o -&amp;gt; results&lt;/p&gt; &lt;p&gt;I also have a part in my retriever stage where I get citations and chunking is done page wise&lt;/p&gt; &lt;p&gt;The questions I ask take anywhere between 25-50 seconds to get an answer and also I am missing out on information, I have made the retriever send back all relevant pages, not just the top 3 relevant pages&lt;/p&gt; &lt;p&gt;Is there anyway to get this under 20 seconds and extract all relevant chunks with keeping in mind I need citations?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/ChallengeOk6437&quot;&gt; /u/ChallengeOk6437 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1df0apu/rag_model_too_slow/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1df0apu/rag_model_too_slow/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1df0apu</id><link href="https://www.reddit.com/r/LangChain/comments/1df0apu/rag_model_too_slow/" /><updated>2024-06-13T14:26:24+00:00</updated><published>2024-06-13T14:26:24+00:00</published><title>RAG Model TOO SLOW</title></entry></feed>