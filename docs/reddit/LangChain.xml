<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/"><category term="LangChain" label="r/LangChain"/><updated>2024-03-08T11:18:43+00:00</updated><icon>https://www.redditstatic.com/icon.png/</icon><id>/r/LangChain.rss</id><link rel="self" href="https://www.reddit.com/r/LangChain.rss" type="application/atom+xml" /><link rel="alternate" href="https://www.reddit.com/r/LangChain" type="text/html" /><subtitle>LangChain is an open-source framework and developer toolkit that helps developers get LLM applications from prototype to production. It is available for Python and Javascript at https://www.langchain.com/.</subtitle><title>LangChain</title><entry><author><name>/u/zchaarm</name><uri>https://www.reddit.com/user/zchaarm</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;A place for members of &lt;a href=&quot;/r/LangChain&quot;&gt;r/LangChain&lt;/a&gt; to chat with each other&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/zchaarm&quot;&gt; /u/zchaarm &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_10ljho9</id><link href="https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/" /><updated>2023-01-26T04:33:05+00:00</updated><published>2023-01-26T04:33:05+00:00</published><title>r/LangChain Lounge</title></entry><entry><author><name>/u/jdogbro12</name><uri>https://www.reddit.com/user/jdogbro12</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1b96gl7/tutorial_on_improving_a_langchain_rag_application/&quot;&gt; &lt;img src=&quot;https://external-preview.redd.it/6UJftsJQzqyKErTaqIFpsoeLG17Huo2fRxROga_6eb0.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0891bf8373fe13fba93022c14d24862f1ff0420a&quot; alt=&quot;Tutorial on improving a Langchain RAG application using Evals, Tracing, and Playground.&quot; title=&quot;Tutorial on improving a Langchain RAG application using Evals, Tracing, and Playground.&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/jdogbro12&quot;&gt; /u/jdogbro12 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://docs.parea.ai/tutorials/getting-started-rag&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1b96gl7/tutorial_on_improving_a_langchain_rag_application/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1b96gl7</id><media:thumbnail url="https://external-preview.redd.it/6UJftsJQzqyKErTaqIFpsoeLG17Huo2fRxROga_6eb0.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=0891bf8373fe13fba93022c14d24862f1ff0420a" /><link href="https://www.reddit.com/r/LangChain/comments/1b96gl7/tutorial_on_improving_a_langchain_rag_application/" /><updated>2024-03-07T21:48:09+00:00</updated><published>2024-03-07T21:48:09+00:00</published><title>Tutorial on improving a Langchain RAG application using Evals, Tracing, and Playground.</title></entry><entry><author><name>/u/Money_Mycologist4939</name><uri>https://www.reddit.com/user/Money_Mycologist4939</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1b9k8wf/langserve_for_complex_cases/&quot;&gt; &lt;img src=&quot;https://b.thumbs.redditmedia.com/YkHXV6kGy6IIWyNGr-3RalvwZQEqJPbQsVO4iYrcTYo.jpg&quot; alt=&quot;Langserve for complex cases&quot; title=&quot;Langserve for complex cases&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hello everyone. Langserve is a great tool but it works just for single chains. I have a more complicated code structure where I first extract the docs from the vector store with one function, then for each document I make an llm call. How could I implement this in langserve??&lt;/p&gt; &lt;p&gt;Thanks&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/13j7n760x2nc1.png?width=796&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4c677dd13ac857c3abed8f89baa8d2c5bd55819d&quot;&gt;https://preview.redd.it/13j7n760x2nc1.png?width=796&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4c677dd13ac857c3abed8f89baa8d2c5bd55819d&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/c366b21hw2nc1.png?width=775&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8039203c10a69c1b0d6e8b84906ea39c6b6690c4&quot;&gt;https://preview.redd.it/c366b21hw2nc1.png?width=775&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8039203c10a69c1b0d6e8b84906ea39c6b6690c4&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/eu8v689lw2nc1.png?width=802&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0838356f0f358265831198d3286b22019606142e&quot;&gt;https://preview.redd.it/eu8v689lw2nc1.png?width=802&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0838356f0f358265831198d3286b22019606142e&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/xh9ihb6nw2nc1.png?width=643&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=624c30471ade2421f6b9cb6ff61ebfaaa8544502&quot;&gt;https://preview.redd.it/xh9ihb6nw2nc1.png?width=643&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=624c30471ade2421f6b9cb6ff61ebfaaa8544502&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Money_Mycologist4939&quot;&gt; /u/Money_Mycologist4939 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1b9k8wf/langserve_for_complex_cases/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1b9k8wf/langserve_for_complex_cases/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1b9k8wf</id><media:thumbnail url="https://b.thumbs.redditmedia.com/YkHXV6kGy6IIWyNGr-3RalvwZQEqJPbQsVO4iYrcTYo.jpg" /><link href="https://www.reddit.com/r/LangChain/comments/1b9k8wf/langserve_for_complex_cases/" /><updated>2024-03-08T09:35:58+00:00</updated><published>2024-03-08T09:35:58+00:00</published><title>Langserve for complex cases</title></entry><entry><author><name>/u/MZuc</name><uri>https://www.reddit.com/user/MZuc</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;In my experience developing RAG-based applications with LangChain, I was surprised to find that there aren&amp;#39;t any simple, reliable ways to chunk files. The default &lt;a href=&quot;https://js.langchain.com/docs/modules/data_connection/document_transformers/&quot;&gt;Text Splitters&lt;/a&gt; that LangChain offers employ a naive form of chunking that doesn&amp;#39;t consider positioning data like sections, subsections, paragraphs or tables. &lt;/p&gt; &lt;p&gt;This led me to implement my own chunking service that includes deep positioning data like page index and bounding box coordinates for every chunk.&lt;/p&gt; &lt;p&gt;You can try it out for free here (no account/api key required):&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://filechipper.com&quot;&gt;https://filechipper.com&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Would any of you be interested in something like this? Let me know!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/MZuc&quot;&gt; /u/MZuc &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1b8trzs/after_struggling_with_langchain_text_splitters_i/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1b8trzs/after_struggling_with_langchain_text_splitters_i/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1b8trzs</id><link href="https://www.reddit.com/r/LangChain/comments/1b8trzs/after_struggling_with_langchain_text_splitters_i/" /><updated>2024-03-07T12:45:24+00:00</updated><published>2024-03-07T12:45:24+00:00</published><title>After struggling with LangChain text splitters, I decided to make my own convenient service to chunk files for RAG</title></entry><entry><author><name>/u/AgilePainting931</name><uri>https://www.reddit.com/user/AgilePainting931</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I have created a platform with document upload and search using vector db, embedding models and llm (simple RAG pattern). Now. I want to use gpt-4 multimodel concept in this. For example, user uploads their documents through RAG but at the same time, while asking question, they can upload one more document in model context, or may be an image, and ask question. System should get answers from document uploded in model context as well as vector db. How can i achieve that? Is it a even a real usecase?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/AgilePainting931&quot;&gt; /u/AgilePainting931 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1b9jhm0/multimodel_with_rag_vectordb/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1b9jhm0/multimodel_with_rag_vectordb/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1b9jhm0</id><link href="https://www.reddit.com/r/LangChain/comments/1b9jhm0/multimodel_with_rag_vectordb/" /><updated>2024-03-08T08:43:17+00:00</updated><published>2024-03-08T08:43:17+00:00</published><title>Multimodel with RAG (vectordb)</title></entry><entry><author><name>/u/Inevitable-Judge2642</name><uri>https://www.reddit.com/user/Inevitable-Judge2642</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1b9iz6m/create_a_web_ui_to_use_the_genai_streaming_api/&quot;&gt; &lt;img src=&quot;https://external-preview.redd.it/SQDJ3I5MDarhdchoDokqAiiHVBsG3PKZbJ60ykasqO4.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=60690f7be75eba0e95e850cd2c50be05e507f2dd&quot; alt=&quot;Create a Web UI to use the GenAI streaming API&quot; title=&quot;Create a Web UI to use the GenAI streaming API&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Inevitable-Judge2642&quot;&gt; /u/Inevitable-Judge2642 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://k33g.hashnode.dev/create-a-web-ui-to-use-the-genai-streaming-api&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1b9iz6m/create_a_web_ui_to_use_the_genai_streaming_api/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1b9iz6m</id><media:thumbnail url="https://external-preview.redd.it/SQDJ3I5MDarhdchoDokqAiiHVBsG3PKZbJ60ykasqO4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=60690f7be75eba0e95e850cd2c50be05e507f2dd" /><link href="https://www.reddit.com/r/LangChain/comments/1b9iz6m/create_a_web_ui_to_use_the_genai_streaming_api/" /><updated>2024-03-08T08:09:58+00:00</updated><published>2024-03-08T08:09:58+00:00</published><title>Create a Web UI to use the GenAI streaming API</title></entry><entry><author><name>/u/Xdbao</name><uri>https://www.reddit.com/user/Xdbao</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Some of my pdfs will be loaded with the page_content =&amp;quot;&amp;quot;, why is that so?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Xdbao&quot;&gt; /u/Xdbao &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1b9iphd/pypdfloader/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1b9iphd/pypdfloader/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1b9iphd</id><link href="https://www.reddit.com/r/LangChain/comments/1b9iphd/pypdfloader/" /><updated>2024-03-08T07:53:09+00:00</updated><published>2024-03-08T07:53:09+00:00</published><title>PyPDFLoader</title></entry><entry><author><name>/u/o3omoomin</name><uri>https://www.reddit.com/user/o3omoomin</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;If ask a question other than what is in the document, will not receive an answer.&lt;/p&gt; &lt;p&gt;I want to create a chatbot with rag function using llama index.&lt;/p&gt; &lt;p&gt;How can I implement it?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/o3omoomin&quot;&gt; /u/o3omoomin &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1b9ihfy/how_do_create_a_chatbot_that_uses_rag_in_the/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1b9ihfy/how_do_create_a_chatbot_that_uses_rag_in_the/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1b9ihfy</id><link href="https://www.reddit.com/r/LangChain/comments/1b9ihfy/how_do_create_a_chatbot_that_uses_rag_in_the/" /><updated>2024-03-08T07:38:16+00:00</updated><published>2024-03-08T07:38:16+00:00</published><title>How do create a chatbot that uses RAG in the llama index?</title></entry><entry><author><name>/u/jzone3</name><uri>https://www.reddit.com/user/jzone3</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1b944qj/tutorial_evaluating_rag_systems_endtoend/&quot;&gt; &lt;img src=&quot;https://external-preview.redd.it/Tj0wNjySJ4Gj9r_GPnV3W8L_NkewWpcd0Dkjn61x2Ws.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=619a386a0bf5f670e00181cebedfb76d4cbee2d1&quot; alt=&quot;[Tutorial] Evaluating RAG systems end-to-end&quot; title=&quot;[Tutorial] Evaluating RAG systems end-to-end&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/jzone3&quot;&gt; /u/jzone3 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://youtu.be/DhKtNpvxawU&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1b944qj/tutorial_evaluating_rag_systems_endtoend/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1b944qj</id><media:thumbnail url="https://external-preview.redd.it/Tj0wNjySJ4Gj9r_GPnV3W8L_NkewWpcd0Dkjn61x2Ws.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=619a386a0bf5f670e00181cebedfb76d4cbee2d1" /><link href="https://www.reddit.com/r/LangChain/comments/1b944qj/tutorial_evaluating_rag_systems_endtoend/" /><updated>2024-03-07T20:15:57+00:00</updated><published>2024-03-07T20:15:57+00:00</published><title>[Tutorial] Evaluating RAG systems end-to-end</title></entry><entry><author><name>/u/jzone3</name><uri>https://www.reddit.com/user/jzone3</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1b944o2/tutorial_evaluating_rag_systems_endtoend/&quot;&gt; &lt;img src=&quot;https://external-preview.redd.it/Tj0wNjySJ4Gj9r_GPnV3W8L_NkewWpcd0Dkjn61x2Ws.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=619a386a0bf5f670e00181cebedfb76d4cbee2d1&quot; alt=&quot;[Tutorial] Evaluating RAG systems end-to-end&quot; title=&quot;[Tutorial] Evaluating RAG systems end-to-end&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/jzone3&quot;&gt; /u/jzone3 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://youtu.be/DhKtNpvxawU&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1b944o2/tutorial_evaluating_rag_systems_endtoend/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1b944o2</id><media:thumbnail url="https://external-preview.redd.it/Tj0wNjySJ4Gj9r_GPnV3W8L_NkewWpcd0Dkjn61x2Ws.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=619a386a0bf5f670e00181cebedfb76d4cbee2d1" /><link href="https://www.reddit.com/r/LangChain/comments/1b944o2/tutorial_evaluating_rag_systems_endtoend/" /><updated>2024-03-07T20:15:52+00:00</updated><published>2024-03-07T20:15:52+00:00</published><title>[Tutorial] Evaluating RAG systems end-to-end</title></entry><entry><author><name>/u/Defih</name><uri>https://www.reddit.com/user/Defih</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi folks, I&amp;#39;m putting together a simple Github RAG chatbot using `GithubRepoLoader` and Pinecone vector store. However when upserting the embeddings to Pinecone, the upsert process fails with the error: `PineconeBadRequestError: Missing low surrogate.`&lt;br/&gt; After a bit of research this seem to be due to how UTF-16 encodes the Unicode Characters outside the BMP (Basic Multilingual Plane) are represented using a pair of surrogate code points in UTF-16 encoding. The error message indicates that a high surrogate code point (D800–DBFF) was found without a corresponding low surrogate (DC00–DFFF) following it.&lt;br/&gt; Now with that being said, my question is:&lt;br/&gt; Would it be feasible to use UTF-8 instead? Where would be a good place (at what level of the stack) in the code would you make some changes to inject a parameter for a different (i.e. UTF-8) encoding?&lt;br/&gt; TYIA&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Defih&quot;&gt; /u/Defih &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1b8xh9g/github_web_loader_pinecone_index_fail_due_to/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1b8xh9g/github_web_loader_pinecone_index_fail_due_to/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1b8xh9g</id><link href="https://www.reddit.com/r/LangChain/comments/1b8xh9g/github_web_loader_pinecone_index_fail_due_to/" /><updated>2024-03-07T15:30:32+00:00</updated><published>2024-03-07T15:30:32+00:00</published><title>Github web loader + Pinecone Index fail due to UTF-16 encoding when trying to Upsert vector embeddings</title></entry><entry><author><name>/u/o3omoomin</name><uri>https://www.reddit.com/user/o3omoomin</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;So far, I have implemented RAG using Langchain. In the case of Langchain’s RAG,&lt;/p&gt; &lt;p&gt;It was like this: &amp;quot;Load document -&amp;gt; Text split -&amp;gt; Chroma vector DB embedding -&amp;gt; llm.&amp;quot;&lt;/p&gt; &lt;p&gt;However, in the case of llama index, it looks like there is no vector DB embedding. Am I misunderstanding it?&lt;/p&gt; &lt;p&gt;And setting the embedding model doesn&amp;#39;t seem to exist in the llama index.&lt;/p&gt; &lt;p&gt;I would appreciate it if you could explain. or git code please&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/o3omoomin&quot;&gt; /u/o3omoomin &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1b8iaac/what_is_the_difference_between_llama_index_and/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1b8iaac/what_is_the_difference_between_llama_index_and/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1b8iaac</id><link href="https://www.reddit.com/r/LangChain/comments/1b8iaac/what_is_the_difference_between_llama_index_and/" /><updated>2024-03-07T02:00:55+00:00</updated><published>2024-03-07T02:00:55+00:00</published><title>What is the difference between llama index and Langchain?</title></entry><entry><author><name>/u/esraaatmeh</name><uri>https://www.reddit.com/user/esraaatmeh</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1b8p37d/stop_agent_from_generate_new_input/&quot;&gt; &lt;img src=&quot;https://b.thumbs.redditmedia.com/SRU7iXjD5f-clqYQ1WEYKTpCbg5ZF3at_ONvwFUkIpw.jpg&quot; alt=&quot;stop agent from generate new input.&quot; title=&quot;stop agent from generate new input.&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;How I can stop LLM Agent new input, I want just to stop the generation process and extract the first AI answer. &lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/xz8t0cdpbvmc1.png?width=765&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b249549b5be459acfbc4aed3b3181023fb2298dd&quot;&gt;https://preview.redd.it/xz8t0cdpbvmc1.png?width=765&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b249549b5be459acfbc4aed3b3181023fb2298dd&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/fjsfyv4abvmc1.png?width=1102&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0a5f8df8e756e7cfa6580d014bbcbf7270822af1&quot;&gt;https://preview.redd.it/fjsfyv4abvmc1.png?width=1102&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0a5f8df8e756e7cfa6580d014bbcbf7270822af1&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/esraaatmeh&quot;&gt; /u/esraaatmeh &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1b8p37d/stop_agent_from_generate_new_input/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1b8p37d/stop_agent_from_generate_new_input/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1b8p37d</id><media:thumbnail url="https://b.thumbs.redditmedia.com/SRU7iXjD5f-clqYQ1WEYKTpCbg5ZF3at_ONvwFUkIpw.jpg" /><link href="https://www.reddit.com/r/LangChain/comments/1b8p37d/stop_agent_from_generate_new_input/" /><updated>2024-03-07T07:55:43+00:00</updated><published>2024-03-07T07:55:43+00:00</published><title>stop agent from generate new input.</title></entry><entry><author><name>/u/martingoodall</name><uri>https://www.reddit.com/user/martingoodall</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I am building a RAG chatbot on internal corporate documents. My specific architecture is Amazon Kendra for document retrieval, Amazon Bedrock using foundation models (llama or cohere), and LangChain as the orchestrator. I have purposely asked it a question that is not in the corporate documentation. The bot correctly returns that it doesn’t know, but it is still returning documents from Kendra. I’m using AmazonKendraRetriever as the retriever and ConversationalRetrievalChain as the chain. Trying to figure out how to not return any src documents when it’s an out of scope question. Any help is appreciated!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/martingoodall&quot;&gt; /u/martingoodall &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1b8sex2/chatbot_via_kendrabedrocklangchain_returning/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1b8sex2/chatbot_via_kendrabedrocklangchain_returning/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1b8sex2</id><link href="https://www.reddit.com/r/LangChain/comments/1b8sex2/chatbot_via_kendrabedrocklangchain_returning/" /><updated>2024-03-07T11:30:17+00:00</updated><published>2024-03-07T11:30:17+00:00</published><title>Chatbot via Kendra/Bedrock/LangChain returning non-relevant documents</title></entry><entry><author><name>/u/ronittsainii</name><uri>https://www.reddit.com/user/ronittsainii</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey everyone, I have written a new blog that explains how you can create a custom AI-powered chatbot using LangChain with code examples.&lt;/p&gt; &lt;p&gt;At the end of this blog, I have also given a working chatbot, that has been developed using LangChain, OpenAI API, and Pinecone that you can use and test.&lt;/p&gt; &lt;p&gt;You can read it at &lt;a href=&quot;https://www.deligence.com/langchain-chatbot/&quot;&gt;LangChain Chatbot&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Feedback appreciated!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/ronittsainii&quot;&gt; /u/ronittsainii &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1b8rxtu/how_to_build_a_custom_chatbot_using_langchain/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1b8rxtu/how_to_build_a_custom_chatbot_using_langchain/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1b8rxtu</id><link href="https://www.reddit.com/r/LangChain/comments/1b8rxtu/how_to_build_a_custom_chatbot_using_langchain/" /><updated>2024-03-07T11:01:37+00:00</updated><published>2024-03-07T11:01:37+00:00</published><title>How To Build a Custom Chatbot Using LangChain With Examples</title></entry><entry><author><name>/u/Top_Raccoon_1493</name><uri>https://www.reddit.com/user/Top_Raccoon_1493</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Could someone kindly assist me with this issue?&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/langchain-ai/langchain/discussions/18722&quot;&gt;https://github.com/langchain-ai/langchain/discussions/18722&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Top_Raccoon_1493&quot;&gt; /u/Top_Raccoon_1493 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1b8prsz/cant_make_the_chat_to_understand_previous_context/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1b8prsz/cant_make_the_chat_to_understand_previous_context/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1b8prsz</id><link href="https://www.reddit.com/r/LangChain/comments/1b8prsz/cant_make_the_chat_to_understand_previous_context/" /><updated>2024-03-07T08:39:44+00:00</updated><published>2024-03-07T08:39:44+00:00</published><title>Can't make the chat to understand previous context</title></entry><entry><author><name>/u/Apprehensive_Act_707</name><uri>https://www.reddit.com/user/Apprehensive_Act_707</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi. Just starting a new journey on this, and Need some clarification. I’m building a complex rag system for many different kind of documents. The way I understand between the many commercially available vector stores, some have different strengths and advantages depending on what king of data you retrieving. There is some good comparison between then in regards of kind of data and chunk sizes? Ro help on which to choose, or this difference is negligible and we can choose whatever is easier to implement?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Apprehensive_Act_707&quot;&gt; /u/Apprehensive_Act_707 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1b8po5e/doubts_about_choosing_vet_for_storage/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1b8po5e/doubts_about_choosing_vet_for_storage/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1b8po5e</id><link href="https://www.reddit.com/r/LangChain/comments/1b8po5e/doubts_about_choosing_vet_for_storage/" /><updated>2024-03-07T08:32:59+00:00</updated><published>2024-03-07T08:32:59+00:00</published><title>Doubts about choosing vet for storage</title></entry><entry><author><name>/u/Inevitable-Judge2642</name><uri>https://www.reddit.com/user/Inevitable-Judge2642</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1b8nwv8/genai_streaming_api_with_langchainjs_ollama_and/&quot;&gt; &lt;img src=&quot;https://external-preview.redd.it/K3poCqV0RmekR4MCYAgcxwNtedRSjQKYQF_31nRlj_w.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7a99ffb7374d2cff74c9ccf3b039ff8ae6b3a4e5&quot; alt=&quot;GenAI streaming API with LangChainJS, Ollama and Fastify&quot; title=&quot;GenAI streaming API with LangChainJS, Ollama and Fastify&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Inevitable-Judge2642&quot;&gt; /u/Inevitable-Judge2642 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://k33g.hashnode.dev/genai-streaming-api-with-langchainjs-ollama-and-fastify&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1b8nwv8/genai_streaming_api_with_langchainjs_ollama_and/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1b8nwv8</id><media:thumbnail url="https://external-preview.redd.it/K3poCqV0RmekR4MCYAgcxwNtedRSjQKYQF_31nRlj_w.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=7a99ffb7374d2cff74c9ccf3b039ff8ae6b3a4e5" /><link href="https://www.reddit.com/r/LangChain/comments/1b8nwv8/genai_streaming_api_with_langchainjs_ollama_and/" /><updated>2024-03-07T06:44:20+00:00</updated><published>2024-03-07T06:44:20+00:00</published><title>GenAI streaming API with LangChainJS, Ollama and Fastify</title></entry><entry><author><name>/u/Diligent_Tonight3232</name><uri>https://www.reddit.com/user/Diligent_Tonight3232</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;As the text suggests, can I build an application for creating responses for emails when i provide them with a email text? Any advice or heads up in this direction would help me start with the project.&lt;/p&gt; &lt;p&gt;Thanks in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Diligent_Tonight3232&quot;&gt; /u/Diligent_Tonight3232 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1b8mfdw/building_a_email_responder_with_langchain/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1b8mfdw/building_a_email_responder_with_langchain/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1b8mfdw</id><link href="https://www.reddit.com/r/LangChain/comments/1b8mfdw/building_a_email_responder_with_langchain/" /><updated>2024-03-07T05:22:15+00:00</updated><published>2024-03-07T05:22:15+00:00</published><title>Building a email responder with langchain?</title></entry><entry><author><name>/u/A_Feyn-man</name><uri>https://www.reddit.com/user/A_Feyn-man</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;So I&amp;#39;m using an LLM = AzureChatOpenAI() and i have a .json data file that has the content of various APIs in the form : &amp;quot;2&amp;quot;: { &amp;quot;Method&amp;quot;: &amp;quot;POST&amp;quot;, &amp;quot;Path&amp;quot;: &amp;quot;&amp;quot;, &amp;quot;FunctionName&amp;quot;: &amp;quot;&amp;quot;, &amp;quot;FunctionCode&amp;quot;: &amp;quot;{...}&amp;quot;, &amp;quot;Queries&amp;quot;: [] } &lt;/p&gt; &lt;p&gt;I have a list of these and I&amp;#39;m using AzureOpenAI() as my embeddings model to create a vector store db and retrieve. The I&amp;#39;m initialising my agent= AutoGPT with memory as vectorstore.as_retriever.&lt;/p&gt; &lt;p&gt;My end goal is to generate end to end flow of these APIs for testing purposes for the given api and original prompt is agent.run(&amp;quot;What are the different API sequences that are possible to test the end to end flow of the API for the given APIs. The different fields that are present in the json are path, method, queries,FunctionName and FunctionCode. You cannot ask for human input.Start by using APIs wivh no prerequisitesand authentication&amp;quot;) &lt;/p&gt; &lt;p&gt;Also I&amp;#39;ve defined custom tool which @tool def get_api_based_on_index(index: int) -&amp;gt; dict: &amp;quot;&amp;quot;&amp;quot;Get API details based on the given index number&amp;quot;&amp;quot;&amp;quot; if str(index) in data: return data[str(index)]&lt;br/&gt; else: raise ValueError(f&amp;quot;Index {index} not found in the data.&amp;quot;)&lt;/p&gt; &lt;p&gt;But right now my langchain agent is hallucinating and not able to get api values and fields and is only looping around 3-4 APIs .&lt;/p&gt; &lt;p&gt;Can anyone look into this and help me such that I can get this agent to retrieve the json data as vectors and go through all of that data and generate me test flow sequences of various APIs that is generally done in end to end software testing.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/A_Feyn-man&quot;&gt; /u/A_Feyn-man &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1b8mans/auto_gpt_is_hallucinatinghow_to_make_autogpt_read/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1b8mans/auto_gpt_is_hallucinatinghow_to_make_autogpt_read/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1b8mans</id><link href="https://www.reddit.com/r/LangChain/comments/1b8mans/auto_gpt_is_hallucinatinghow_to_make_autogpt_read/" /><updated>2024-03-07T05:15:06+00:00</updated><published>2024-03-07T05:15:06+00:00</published><title>Auto GPT is hallucinating.How to make AutoGPT read json data and work onto it to generate test sequences.</title></entry><entry><author><name>/u/sushilkhadakaanon</name><uri>https://www.reddit.com/user/sushilkhadakaanon</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Does indexing happen sequentially in llamindex/langchain? I mean say I&amp;#39;ve a pdf containing images and text. When I store the embeddings in the vector database, the order of text and images matters (text just below a fig. might be explaining about the fig), so is it good to go with default implementation?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/sushilkhadakaanon&quot;&gt; /u/sushilkhadakaanon &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1b86gzl/how_does_indexing_work/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1b86gzl/how_does_indexing_work/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1b86gzl</id><link href="https://www.reddit.com/r/LangChain/comments/1b86gzl/how_does_indexing_work/" /><updated>2024-03-06T18:01:46+00:00</updated><published>2024-03-06T18:01:46+00:00</published><title>How does indexing work?</title></entry><entry><author><name>/u/Sweaty-Minimum5423</name><uri>https://www.reddit.com/user/Sweaty-Minimum5423</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I am using assistant api as agent in the LangChain framework. I’m using gpt4-0125-preview for this agent. The reason why I use agent is because I do not want every query to search the database. And I find the assistant api agent is smarter than the ReAct agent in terms of generating responses.&lt;/p&gt; &lt;p&gt;I have only one tool for linking to a retrieval chain. When the user asks certain question, the agent invokes a chain and pass query into the retrieval system. I pass in gpt-4-preview-0125 for the retrieval chain. To improve the retrieval process, I use multi-query to help generate sub questions from the original questions to dig into the details. I use gpt3.5-1106 for the multi query retriever as this doesn’t require much reasoning. So essentially, I use the assistant api(gpt-4-0125) as agent, gpt4-0125 for the retrieval chain and gpt3.5 1106 as the multi query retriever.&lt;/p&gt; &lt;p&gt;In terms of data preparation. I use manual chunking. By manual chunking I mean I manually gather relevant content into one ‘document’. This is because I see splitter does not consider context so it’s better to do the chunking on my own. &lt;/p&gt; &lt;p&gt;The problem is that the average response time for 1000-2000 token is ranging from 10s to 30s. I tried using gpt3.5 as the agent in assistant api. Speed cuts down to 3-10s. But the generation is way worse. This one I’m not sure why as I keep the gpt4-0125 in the retrieval chain. I assume the generation should be good…&lt;/p&gt; &lt;p&gt;How should I improve my architecture to enhance speed?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Sweaty-Minimum5423&quot;&gt; /u/Sweaty-Minimum5423 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1b87p0i/how_to_improve_rag_speed_with_openai/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1b87p0i/how_to_improve_rag_speed_with_openai/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1b87p0i</id><link href="https://www.reddit.com/r/LangChain/comments/1b87p0i/how_to_improve_rag_speed_with_openai/" /><updated>2024-03-06T18:47:54+00:00</updated><published>2024-03-06T18:47:54+00:00</published><title>How to Improve RAG speed with OpenAI?</title></entry><entry><author><name>/u/Not-That-rpg</name><uri>https://www.reddit.com/user/Not-That-rpg</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I saw the previous discussion about using double curly braces inside a template to avoid expansion in a prompt&amp;#39;s format method.&lt;/p&gt; &lt;p&gt;However, I&amp;#39;m still having a problem with this, even with double curly-braces, when I use a `FewShotPrompt`. I have *examples* in the FSP that contain code. So I suspect what is going wrong is that the double curly-braces help when the examples are folded into the prompt, but then when the *input* is folded in, the double curly braces have been stripped and I get an error.&lt;/p&gt; &lt;p&gt;This seems like a numbskull problem on my part, so even just a pointer to some tutorial about using a FewShotPrompt with code (surely there must be one?) would be great.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Not-That-rpg&quot;&gt; /u/Not-That-rpg &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1b8cpcz/curly_braces_in_prompts_again/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1b8cpcz/curly_braces_in_prompts_again/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1b8cpcz</id><link href="https://www.reddit.com/r/LangChain/comments/1b8cpcz/curly_braces_in_prompts_again/" /><updated>2024-03-06T22:04:40+00:00</updated><published>2024-03-06T22:04:40+00:00</published><title>Curly braces in prompts again...</title></entry><entry><author><name>/u/RepresentativeNo547</name><uri>https://www.reddit.com/user/RepresentativeNo547</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Which library comes out on top? I am building a multi-agent system in production but still have not decided which framework to use&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/RepresentativeNo547&quot;&gt; /u/RepresentativeNo547 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1b7q44y/autogen_vs_langgraph/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1b7q44y/autogen_vs_langgraph/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1b7q44y</id><link href="https://www.reddit.com/r/LangChain/comments/1b7q44y/autogen_vs_langgraph/" /><updated>2024-03-06T04:00:13+00:00</updated><published>2024-03-06T04:00:13+00:00</published><title>Autogen vs. LangGraph</title></entry><entry><author><name>/u/EscapedLaughter</name><uri>https://www.reddit.com/user/EscapedLaughter</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1b7q6u1/switch_to_and_fro_claude3_gpt4_by_changing_2/&quot;&gt; &lt;img src=&quot;https://preview.redd.it/km0k8hnc1nmc1.gif?width=640&amp;amp;crop=smart&amp;amp;s=181d7cff9061d16d502dd5fd826a87ed4a82e3a5&quot; alt=&quot;Switch to and fro Claude-3 &amp;lt;—&amp;gt; GPT-4 by changing 2 lines of code&quot; title=&quot;Switch to and fro Claude-3 &amp;lt;—&amp;gt; GPT-4 by changing 2 lines of code&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/EscapedLaughter&quot;&gt; /u/EscapedLaughter &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://i.redd.it/km0k8hnc1nmc1.gif&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1b7q6u1/switch_to_and_fro_claude3_gpt4_by_changing_2/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1b7q6u1</id><media:thumbnail url="https://preview.redd.it/km0k8hnc1nmc1.gif?width=640&amp;crop=smart&amp;s=181d7cff9061d16d502dd5fd826a87ed4a82e3a5" /><link href="https://www.reddit.com/r/LangChain/comments/1b7q6u1/switch_to_and_fro_claude3_gpt4_by_changing_2/" /><updated>2024-03-06T04:03:58+00:00</updated><published>2024-03-06T04:03:58+00:00</published><title>Switch to and fro Claude-3 &lt;—&gt; GPT-4 by changing 2 lines of code</title></entry><entry><author><name>/u/Over_Fun6759</name><uri>https://www.reddit.com/user/Over_Fun6759</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;React Native is incompatible with langchain, when are we going to get an update?&lt;/p&gt; &lt;p&gt;i found this but there is no solution even the one provided with langchain maintainer himself &lt;a href=&quot;https://stackoverflow.com/questions/77307779/react-native-issue-while-implementing-langchain/77313089#77313089?newreg=6af4405652b844fd81c2d0735b49c25f&quot;&gt;https://stackoverflow.com/questions/77307779/react-native-issue-while-implementing-langchain/77313089#77313089?newreg=6af4405652b844fd81c2d0735b49c25f&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Over_Fun6759&quot;&gt; /u/Over_Fun6759 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1b815jv/react_native_langchain_support/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1b815jv/react_native_langchain_support/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1b815jv</id><link href="https://www.reddit.com/r/LangChain/comments/1b815jv/react_native_langchain_support/" /><updated>2024-03-06T14:33:12+00:00</updated><published>2024-03-06T14:33:12+00:00</published><title>React Native Langchain support?</title></entry></feed>