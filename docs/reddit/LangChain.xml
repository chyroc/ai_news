<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/"><category term="LangChain" label="r/LangChain"/><updated>2024-03-17T13:49:15+00:00</updated><icon>https://www.redditstatic.com/icon.png/</icon><id>/r/LangChain.rss</id><link rel="self" href="https://www.reddit.com/r/LangChain.rss" type="application/atom+xml" /><link rel="alternate" href="https://www.reddit.com/r/LangChain" type="text/html" /><subtitle>LangChain is an open-source framework and developer toolkit that helps developers get LLM applications from prototype to production. It is available for Python and Javascript at https://www.langchain.com/.</subtitle><title>LangChain</title><entry><author><name>/u/zchaarm</name><uri>https://www.reddit.com/user/zchaarm</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;A place for members of &lt;a href=&quot;/r/LangChain&quot;&gt;r/LangChain&lt;/a&gt; to chat with each other&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/zchaarm&quot;&gt; /u/zchaarm &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_10ljho9</id><link href="https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/" /><updated>2023-01-26T04:33:05+00:00</updated><published>2023-01-26T04:33:05+00:00</published><title>r/LangChain Lounge</title></entry><entry><author><name>/u/Silver_Equivalent_58</name><uri>https://www.reddit.com/user/Silver_Equivalent_58</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I have a word document that is basically like a self guide manual, which has a heading, below procedure to perform the operation.&lt;/p&gt; &lt;p&gt;Now the problem is ive tried lots of chunking methods, even semantic chunking, but the heading gets attached to a different chunk and retrieval system goes crazy, whats an optimal way to chunk so that the heading + context gets retained?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Silver_Equivalent_58&quot;&gt; /u/Silver_Equivalent_58 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bgqc2o/optimal_way_to_chunk_word_document_for/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bgqc2o/optimal_way_to_chunk_word_document_for/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bgqc2o</id><link href="https://www.reddit.com/r/LangChain/comments/1bgqc2o/optimal_way_to_chunk_word_document_for/" /><updated>2024-03-17T05:42:41+00:00</updated><published>2024-03-17T05:42:41+00:00</published><title>Optimal way to chunk word document for RAG(semantic chunking giving bad results)</title></entry><entry><author><name>/u/jimmy02020</name><uri>https://www.reddit.com/user/jimmy02020</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Langchain API naming is confusing, for some reason, it&amp;#39;s focusing on where things are coming from:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;PromptTemplate.from_template ChatPromptTemplate.from_template ChatPromptTemplate.from_messages &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Why not using more descriptive name like:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;prompt.chat.template prompt.chat.message prompt.template &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/jimmy02020&quot;&gt; /u/jimmy02020 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bgvu2f/am_i_missing_something_langchain_api_naming_seems/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bgvu2f/am_i_missing_something_langchain_api_naming_seems/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bgvu2f</id><link href="https://www.reddit.com/r/LangChain/comments/1bgvu2f/am_i_missing_something_langchain_api_naming_seems/" /><updated>2024-03-17T11:59:05+00:00</updated><published>2024-03-17T11:59:05+00:00</published><title>Am I Missing Something? Langchain API Naming Seems Odd</title></entry><entry><author><name>/u/gurjant123</name><uri>https://www.reddit.com/user/gurjant123</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Firstly I connected db then converted db to text then with embedding I stored in chromadb now I want query execute by my chromdb I want to sql chain intialization with vector db can anyone help me please dm &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/gurjant123&quot;&gt; /u/gurjant123 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bgwflw/create_sql_agent_with_langchain_for_query_sql_db/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bgwflw/create_sql_agent_with_langchain_for_query_sql_db/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bgwflw</id><link href="https://www.reddit.com/r/LangChain/comments/1bgwflw/create_sql_agent_with_langchain_for_query_sql_db/" /><updated>2024-03-17T12:32:07+00:00</updated><published>2024-03-17T12:32:07+00:00</published><title>Create sql agent with langchain for query sql db</title></entry><entry><author><name>/u/BladexJS</name><uri>https://www.reddit.com/user/BladexJS</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi everyone, I am trying to start learning how to implement langchain. &lt;/p&gt; &lt;p&gt;Could anyone give recommendations on books/online courses/youtube videos I can watch to get started? &lt;/p&gt; &lt;p&gt;Also, are there any prerequisites I need to have practiced/learnt before starting on learning langchain?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/BladexJS&quot;&gt; /u/BladexJS &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bgsok2/how_to_start_learning_langchain/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bgsok2/how_to_start_learning_langchain/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bgsok2</id><link href="https://www.reddit.com/r/LangChain/comments/1bgsok2/how_to_start_learning_langchain/" /><updated>2024-03-17T08:24:34+00:00</updated><published>2024-03-17T08:24:34+00:00</published><title>How to start learning langchain?</title></entry><entry><author><name>/u/Educational-Run674</name><uri>https://www.reddit.com/user/Educational-Run674</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;automation or gaining followers or crawling and finding followers etc. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Educational-Run674&quot;&gt; /u/Educational-Run674 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bgvk62/question_have_any_of_you_built_anything_related/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bgvk62/question_have_any_of_you_built_anything_related/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bgvk62</id><link href="https://www.reddit.com/r/LangChain/comments/1bgvk62/question_have_any_of_you_built_anything_related/" /><updated>2024-03-17T11:42:35+00:00</updated><published>2024-03-17T11:42:35+00:00</published><title>Question: Have any of you built anything related to social media</title></entry><entry><author><name>/u/Cool_Bhidu</name><uri>https://www.reddit.com/user/Cool_Bhidu</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;There&amp;#39;s a famous Semantic Chunker (by Greg) which splits the document and group together sentences which are semantically similar. (&lt;a href=&quot;https://www.youtube.com/watch?v=8OJC21T2SL4&amp;amp;t=1933s&amp;amp;ab_channel=GregKamradt%28DataIndy%29&quot;&gt;YouTube Video of Greg&lt;/a&gt;) &lt;/p&gt; &lt;p&gt;I am using it in langchain, to split my HTML page. But its not working, it supposed to have html table in the same chunk but it is not keeping it into same chunk. &lt;/p&gt; &lt;p&gt;How to solve this? Anything? &lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Cool_Bhidu&quot;&gt; /u/Cool_Bhidu &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bgv8g6/does_semantic_chunker_works_with_html_documents/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bgv8g6/does_semantic_chunker_works_with_html_documents/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bgv8g6</id><link href="https://www.reddit.com/r/LangChain/comments/1bgv8g6/does_semantic_chunker_works_with_html_documents/" /><updated>2024-03-17T11:22:06+00:00</updated><published>2024-03-17T11:22:06+00:00</published><title>Does Semantic Chunker works with HTML documents</title></entry><entry><author><name>/u/shiv11afk</name><uri>https://www.reddit.com/user/shiv11afk</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I recently delved into using agents and noticed there are different types of messages available: SystemMessages,Ai messages, Human Messages, Prefix, and Suffix. I&amp;#39;m curious about their distinctions and how they aid in agent initialization.&lt;/p&gt; &lt;p&gt;Could someone provide a breakdown with examples? Let&amp;#39;s say I have an assistant named Shibi who works with my data in Python DataFrame/CSV format. How would each message type play out in the initialization process, especially when using an inbuilt agent(pandas,csv) vs a custom one?&lt;/p&gt; &lt;p&gt;Looking forward to your insights!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/shiv11afk&quot;&gt; /u/shiv11afk &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bgui26/understanding_message_types_in_agent/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bgui26/understanding_message_types_in_agent/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bgui26</id><link href="https://www.reddit.com/r/LangChain/comments/1bgui26/understanding_message_types_in_agent/" /><updated>2024-03-17T10:33:21+00:00</updated><published>2024-03-17T10:33:21+00:00</published><title>Understanding Message Types in Agent Initialization</title></entry><entry><author><name>/u/callmegetafix</name><uri>https://www.reddit.com/user/callmegetafix</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;New to langchain, and trying to get used to the tooling. However, I am struggling with hallucinations in conversationsummarymemory. I am using llama2 via Ollama. I am using the smaller 3.8GB (llama2:latest 78e26419b446) model. My leading hypothesis is that the model itself is not very good at summarizing, and is just corrupting the memory with its hallucinations.&lt;/p&gt; &lt;p&gt;The post might be long, but I thought it is best to illustrate my problem using this small demo I have prepared.&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;Here&amp;#39;s how I instantiate,&lt;/p&gt; &lt;pre&gt;&lt;code&gt;template = &amp;quot;&amp;quot;&amp;quot; Act as an AI assistant and follow user instructions and answer questions in a precise manner. User conversation history for context whenever applicable. Current conversation: {history} Human: {input} &amp;quot;&amp;quot;&amp;quot; llm = Ollama(model=&amp;quot;llama2&amp;quot;, temperature=0.9) memory = ConversationSummaryMemory(llm=Ollama(model=&amp;quot;llama2&amp;quot;, temperature=0.0)) # set temperature=0. for memory for determinism. convo = ConversationChain( llm = llm, verbose = True, memory=memory, prompt = PromptTemplate(input_variables=[&amp;quot;history&amp;quot;, &amp;quot;input&amp;quot;], template=template) ) convo.predict(input=&amp;quot;Introduce yourself.&amp;quot;) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Output : so far reasonable.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;&amp;gt; Entering new ConversationChain chain... Prompt after formatting: Act as an AI assistant and follow user instructions and answer questions in a precise manner. User conversation history for context whenever applicable. Current conversation: Human: Introduce yourself. &amp;gt; Finished chain. &amp;quot;Hello! I&amp;#39;m just an AI assistant, here to help you with any questions or tasks you may have. My name is Assistant, and I&amp;#39;m here to assist you in a precise and helpful manner. How can I assist you today?&amp;quot; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Then I provide some new inputs.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;convo.predict(input=&amp;quot;Memorize the following words, bat, cat dog&amp;quot;) convo.predict(input=&amp;quot;Try to remember the following words: coffee and tea&amp;quot;) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Now it has completely gone off the rails. Here&amp;#39;s the outputs.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;&amp;gt; Entering new ConversationChain chain... Prompt after formatting: Act as an AI assistant and follow user instructions and answer questions in a precise manner. User conversation history for context whenever applicable. Current conversation: Current summary: The human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good because it will help humans reach their full potential. New lines of conversation: Human: What do you mean by &amp;quot;reach their full potential&amp;quot;? AI: Well, with the help of artificial intelligence, humans can accomplish more than they ever thought possible. It can help them solve complex problems, make better decisions, and achieve their goals more efficiently. New summary: The human asks what the AI thinks of artificial intelligence, and the AI replies that it believes artificial intelligence is a force for good because it will help humans reach their full potential by enabling them to accomplish more than they ever thought possible, solve complex problems, make better decisions, and achieve their goals more efficiently. Human: Memorize the following words, bat, cat dog &amp;gt; Finished chain. &amp;gt; Entering new ConversationChain chain... Prompt after formatting: Act as an AI assistant and follow user instructions and answer questions in a precise manner. User conversation history for context whenever applicable. Current conversation: Current summary: The human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good because it will help humans reach their full potential. New lines of conversation: Human: What do you mean by &amp;quot;reach their full potential&amp;quot;? AI: Well, with the help of artificial intelligence, humans can accomplish more than they ever thought possible. It can help them solve complex problems, make better decisions, and achieve their goals more efficiently. New summary: The human asks what the AI thinks of artificial intelligence, and the AI replies that it believes artificial intelligence is a force for good because it will help humans reach their full potential by enabling them to accomplish more than they ever thought possible, solve complex problems, make better decisions, and achieve their goals more efficiently. New lines of conversation: AI: I&amp;#39;m glad you asked! *smirks* In my opinion, the future of artificial intelligence is bright, and it will continue to revolutionize various industries. With the advancements in machine learning and natural language processing, AI will become even more integrated into our daily lives. New summary: The human asks what the AI thinks of artificial intelligence, and the AI replies that it believes artificial intelligence is a force for good because it will help humans reach their full potential by enabling them to accomplish more than they ever thought possible, solve complex problems, make better decisions, and achieve their goals more efficiently. The AI then goes on to express its excitement about the future of artificial intelligence and how it will continue to revolutionize various industries with advancements in machine learning and natural language processing. Human: Try to remember the following words: coffee and tea &amp;gt; Finished chain. &amp;quot;Of course, I&amp;#39;d be happy to help! *smirks* I have a great memory when it comes to storing information, so I will definitely keep those words in mind for future conversations. By the way, do you want to know a fun fact about artificial intelligence? Did you know that AI can now create art that is almost indistinguishable from that created by humans? It&amp;#39;s fascinating to see how far the technology has come!&amp;quot; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The new summary lines contain things that have not been brought up in conversation at all.&lt;/p&gt; &lt;p&gt;Testing again,&lt;/p&gt; &lt;pre&gt;&lt;code&gt;convo.predict(input=&amp;quot;Out of the 5 words I shared with you. Repeat the third and fourth words.&amp;quot;) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Yields total nonsense.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;&amp;gt; Entering new ConversationChain chain... Prompt after formatting: Act as an AI assistant and follow user instructions and answer questions in a precise manner. User conversation history for context whenever applicable. Current conversation: Current summary: The human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good because it will help humans reach their full potential. New lines of conversation: Human: Why do you think artificial intelligence is a force for good? AI: Because artificial intelligence will help humans solve complex problems, make better decisions, and achieve their goals more efficiently. Human: That&amp;#39;s interesting. Can you give me an example? AI: Sure! For instance, AI can help doctors diagnose diseases more accurately and quickly than ever before. It can also assist businesses in making better investment decisions and optimizing their operations. New summary: The human asks what the AI thinks of artificial intelligence, and the AI replies that it believes artificial intelligence is a force for good because it will help humans solve complex problems, make better decisions, and achieve their goals more efficiently. The AI then provides an example of how AI can help doctors diagnose diseases more accurately and quickly than ever before, as well as assist businesses in making better investment decisions and optimizing their operations. New lines of conversation: Human: That&amp;#39;s impressive. Can AI also help with creative tasks? AI: Absolutely! Artificial intelligence can assist with creative tasks such as writing, music composition, and even art creation. In fact, there are already many AI-generated art pieces that are highly regarded and sold for thousands of dollars. New summary: The human asks what the AI thinks of artificial intelligence, and the AI replies that it believes artificial intelligence is a force for good because it will help humans solve complex problems, make better decisions, and achieve their goals more efficiently. The AI then provides an example of how AI can help doctors diagnose diseases more accurately and quickly than ever before, as well as assist businesses in making better investment decisions and optimizing their operations. The AI also mentions that artificial intelligence can assist with creative tasks such as writing, music composition, and even art creation, with many AI-generated art pieces already highly regarded and sold for thousands of dollars. Human: Out of the 5 words I shared with you. Repeat the third and fourth words. &amp;gt; Finished chain. &amp;#39;Of course! The third word is &amp;quot;ai&amp;quot; and the fourth word is &amp;quot;intelligence&amp;quot;.&amp;#39; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/callmegetafix&quot;&gt; /u/callmegetafix &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bgdh3n/struggling_with_hallucinations_in/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bgdh3n/struggling_with_hallucinations_in/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bgdh3n</id><link href="https://www.reddit.com/r/LangChain/comments/1bgdh3n/struggling_with_hallucinations_in/" /><updated>2024-03-16T19:04:43+00:00</updated><published>2024-03-16T19:04:43+00:00</published><title>Struggling with hallucinations in ConversationSummaryMemory</title></entry><entry><author><name>/u/worldender999</name><uri>https://www.reddit.com/user/worldender999</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;m an experienced software engineer who is new to AI-based development. I am working on a practice RAG project with LangChain and Milvus. &lt;/p&gt; &lt;p&gt;Right now I am just re-creating the example found here: &lt;a href=&quot;https://github.com/langchain4j/langchain4j-examples/blob/main/milvus-example/src/main/java/MilvusEmbeddingStoreExample.java&quot;&gt;https://github.com/langchain4j/langchain4j-examples/blob/main/milvus-example/src/main/java/MilvusEmbeddingStoreExample.java&lt;/a&gt;. No more, no less. I am doing this on my M1 Pro MacBook.&lt;/p&gt; &lt;p&gt;When I try performing the query, it runs incredibly slow. I can&amp;#39;t tell you exactly how long, because after waiting several minutes I always abort. There are no exceptions, it just sits there, trying to process.&lt;/p&gt; &lt;p&gt;I have allocated 8 CPU cores and 16GB of RAM to the docker engine on my laptop. Based on the stats I am seeing, milvus itself is heavily CPU bound. The tiny quantity of data in the vector store at the time means the RAM requirements are minimal. Yet at the end of the day, it is still incredibly slow.&lt;/p&gt; &lt;p&gt;There are a few possibilities I am considering at this point. The first is that Milvus benefits from GPU optimization. That&amp;#39;s my least-preferred scenario, as my MacBook and my home server are lacking in GPU hardware.&lt;/p&gt; &lt;p&gt;The second scenario is indexing. This is an area that I know from working with traditional databases, but with vector databases it&amp;#39;s all new to me. Specifically, I&amp;#39;m using the default FLAT index which I know doesn&amp;#39;t perform well. I&amp;#39;m beginning to read about alternative indexes to see what options I have there.&lt;/p&gt; &lt;p&gt;Anyway, I&amp;#39;m hoping that folks here can offer advice on my existing ideas and any other general improvements I can make. Thanks in advance.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/worldender999&quot;&gt; /u/worldender999 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bg81iw/new_to_langchain_how_to_make_my_vector_store/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bg81iw/new_to_langchain_how_to_make_my_vector_store/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bg81iw</id><link href="https://www.reddit.com/r/LangChain/comments/1bg81iw/new_to_langchain_how_to_make_my_vector_store/" /><updated>2024-03-16T15:02:39+00:00</updated><published>2024-03-16T15:02:39+00:00</published><title>New to LangChain, how to make my vector store query faster?</title></entry><entry><author><name>/u/danipudani</name><uri>https://www.reddit.com/user/danipudani</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bgbvmc/future_of_nlp_chris_manning_stanford_corenlp/&quot;&gt; &lt;img src=&quot;https://external-preview.redd.it/MEJbeXJfv7_J1XfMubk4lFohPs9G6cM15TgeICfNphk.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6fbdb05b21a1af6537ce1177458bece843b1ae53&quot; alt=&quot;Future of NLP - Chris Manning Stanford CoreNLP&quot; title=&quot;Future of NLP - Chris Manning Stanford CoreNLP&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/danipudani&quot;&gt; /u/danipudani &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://youtu.be/xk01kx_klOE?si=TiBA2XyhPuuDpjMn&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bgbvmc/future_of_nlp_chris_manning_stanford_corenlp/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1bgbvmc</id><media:thumbnail url="https://external-preview.redd.it/MEJbeXJfv7_J1XfMubk4lFohPs9G6cM15TgeICfNphk.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=6fbdb05b21a1af6537ce1177458bece843b1ae53" /><link href="https://www.reddit.com/r/LangChain/comments/1bgbvmc/future_of_nlp_chris_manning_stanford_corenlp/" /><updated>2024-03-16T17:53:32+00:00</updated><published>2024-03-16T17:53:32+00:00</published><title>Future of NLP - Chris Manning Stanford CoreNLP</title></entry><entry><author><name>/u/Downtown_Repeat7455</name><uri>https://www.reddit.com/user/Downtown_Repeat7455</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;My team got requirement from a client and client wants do this using any LLM. Its about a workflow based on serious questions. If user say yes to a particular question some, One set of questions will be triggered. If user say no the same question another set of questions should be followed. &lt;/p&gt; &lt;p&gt;Is there any framework open/closed to achieve this. Its more of a decision trees kind of problem. So my client thinks if we use LLm then questions will more creative and conversational. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Downtown_Repeat7455&quot;&gt; /u/Downtown_Repeat7455 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bg7kfm/llm_workflows/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bg7kfm/llm_workflows/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bg7kfm</id><link href="https://www.reddit.com/r/LangChain/comments/1bg7kfm/llm_workflows/" /><updated>2024-03-16T14:40:43+00:00</updated><published>2024-03-16T14:40:43+00:00</published><title>LLM workflows</title></entry><entry><author><name>/u/Queasy_Role2723</name><uri>https://www.reddit.com/user/Queasy_Role2723</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi all, &lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;I am trying to deploy a chainlit app in our k8 cluster. &lt;/p&gt; &lt;p&gt;When deployed my http:baseurl/app shows up as a blank white page. &lt;/p&gt; &lt;p&gt;on the container logs it shows your app is available at localhost:8080 . &lt;/p&gt; &lt;p&gt;I feel like I will have to write my own framework instead of chainlit to deploy in prod. &lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;Any advice is welcome. &lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;Thanks &lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Queasy_Role2723&quot;&gt; /u/Queasy_Role2723 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bga3qk/chainlit_deployment_for_prod_in_kubernetes_cluster/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bga3qk/chainlit_deployment_for_prod_in_kubernetes_cluster/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bga3qk</id><link href="https://www.reddit.com/r/LangChain/comments/1bga3qk/chainlit_deployment_for_prod_in_kubernetes_cluster/" /><updated>2024-03-16T16:34:58+00:00</updated><published>2024-03-16T16:34:58+00:00</published><title>Chainlit deployment for prod in Kubernetes cluster</title></entry><entry><author><name>/u/ugh_madlad</name><uri>https://www.reddit.com/user/ugh_madlad</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I am working on a project to convert non-fiction book PDFs (300 pages max) to a high quality crisp summary. &lt;/p&gt; &lt;p&gt;Now, I&amp;#39;ve a standard structure for this summary: 1. It should be condensed to 10 &amp;#39;slides&amp;#39; 2. It should be high quality without omitting key aspects of the book. 3. I want each slide to have a title and a description below it. Title should be engaging for the reader and description should be 200 words Max. 4. should include a mindmap of all core ideas (optional)&lt;/p&gt; &lt;p&gt;My question to the experts on language model folks here is: a. Is this a fair expectation? If not what is the closest I can get? a. If yes. What is the best and cheapest (free) way to go about executing it as of today? fast, free, high quality option. b. How can I get started to achive the above task.&lt;/p&gt; &lt;p&gt;Thanks a lot.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/ugh_madlad&quot;&gt; /u/ugh_madlad &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bg2qgo/help_me_find_the_state_of_the_art_for_my_usecase/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bg2qgo/help_me_find_the_state_of_the_art_for_my_usecase/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bg2qgo</id><link href="https://www.reddit.com/r/LangChain/comments/1bg2qgo/help_me_find_the_state_of_the_art_for_my_usecase/" /><updated>2024-03-16T10:04:48+00:00</updated><published>2024-03-16T10:04:48+00:00</published><title>Help me find the state of the art for my usecase</title></entry><entry><author><name>/u/WanderingMerchant74</name><uri>https://www.reddit.com/user/WanderingMerchant74</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hello everyone,&lt;/p&gt; &lt;p&gt;I am new to this subreddit and to reddit more in general. I am using LangChain and LLaMa2 a lot for my research lately, and I have some general questions about usage. &lt;/p&gt; &lt;p&gt;1) To begin with, is there any way to verify that I am working with the right model? I am loading a quantized version of LLaMa2 though HF pipelines, and when I inspect the model it calls it GPT2. Not sure if I should be worried. &lt;/p&gt; &lt;p&gt;2) Regarding memory: how does the ConversationBufferMemory for LLM chains work? Is it loaded on GPU? Does it count as context, meaning that a memory with too many messages will cause the model to start spouting gibberish? How persistent is it (i.e.: after how many messages will the earliest message be forgotten/deleted)?&lt;/p&gt; &lt;p&gt;3) Regarding RAG using Chroma (which I have seen being used in the LangChain docs): is there a tutorial for how to conduct it? I am especially interested in verifying if one can iteratively add to the database the LLM is drawing from without having to &amp;quot;retrain&amp;#39; the model. &lt;/p&gt; &lt;p&gt;4) Related to questions 2 and 3. Suppose that I have an script running that iteratively produces text in batches. I would then like to feed batches one after the other to an LLM in order for the algorithm to evaluate the content of these batches. I have considered two options: the first is to feed each part of the batch directly into the LLM&amp;#39;s memory, but I am not sure if this would overload its GPU or context window (more worried about the latter, presently). The other option is, you guessed it, performing RAG on the batches. But I am not sure if RAG can help me with something beyond simple retrieval and more in depth. I don&amp;#39;t want a summary of what is in a given batch of text, I want some manner of inference on the batch conditional on my request, for instance: if the batch of text says that person X took a series of action that resulted in person Y dying, I want to ask the LLM if it can find a causal link between X&amp;#39;s actions and Y&amp;#39;s death.&lt;/p&gt; &lt;p&gt;5) Finally, is there a way to use LLMs for flow control? Say that I have a very basic counting loop, something that simply enumerates all natural numbers until it is stopped. Is there a way, or maybe a tool, to tell an LLM to stop the loop when a number exceeds a certain threshold? I realize that this is killing a rat with a bazooka, but it&amp;#39;s the smallest working example I could produce. A more fitting and complex example would be: let us return to the text in question 4. I want the LLM to stop the script that is producing text if it can identify a causal link between Y&amp;#39;s death and X&amp;#39;s action. Is there a way to do this? Simply asking LLaMa2-7b to type &amp;quot;stop&amp;quot; if it thinks the loop should stop does not work, despite using very wordy prompts and CoT.&lt;/p&gt; &lt;p&gt;I am sorry for the very basic questions, I do not really know where to turn for help. I likewise apologize for being vague but I cannot disclose too much about my research. Any link, resource or manner of assistance will be very much appreciated. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/WanderingMerchant74&quot;&gt; /u/WanderingMerchant74 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bg7ojh/langchain_for_pretty_obscure_task_i_suppose/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bg7ojh/langchain_for_pretty_obscure_task_i_suppose/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bg7ojh</id><link href="https://www.reddit.com/r/LangChain/comments/1bg7ojh/langchain_for_pretty_obscure_task_i_suppose/" /><updated>2024-03-16T14:46:03+00:00</updated><published>2024-03-16T14:46:03+00:00</published><title>LangChain for... pretty obscure task, I suppose.</title></entry><entry><author><name>/u/Oldsixstring</name><uri>https://www.reddit.com/user/Oldsixstring</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;&lt;strong&gt;Not sure what to do next. Help Appreciated&lt;/strong&gt;&lt;br/&gt; Tried to install flowise, getting these errors. &lt;/p&gt; &lt;p&gt;The npm list -g puppeteer&lt;br/&gt; command output indicates that puppeteer&lt;br/&gt; is being used by flowise&lt;br/&gt; and its sub-dependencies at versions 19.11.1&lt;br/&gt; and 20.9.0&lt;br/&gt; , both of which are deprecated as they are below the supported version 21.5.0&lt;br/&gt; . Here&amp;#39;s what you can consider doing next:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Direct Dependency Update&lt;/strong&gt;: If a package directly depends on an outdated version, you could try updating that dependency. However, since puppeteer&lt;br/&gt; is a nested dependency in your case (used by flowise-components&lt;br/&gt; and langchain&lt;br/&gt; ), direct intervention isn&amp;#39;t straightforward.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Contact the Maintainers&lt;/strong&gt;: Since the outdated puppeteer&lt;br/&gt; versions are dependencies of flowise-components&lt;br/&gt; and langchain&lt;br/&gt; , the ideal approach would be to contact the maintainers of these packages and request them to update their puppeteer&lt;br/&gt; dependencies. This way, when you update flowise&lt;br/&gt; , it would use the updated versions of these dependencies.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Manual Override (Advanced)&lt;/strong&gt;: If you&amp;#39;re comfortable with manual intervention and understand the potential risks, you could consider using npm&amp;#39;s shrinkwrap&lt;br/&gt; feature or resolutions&lt;br/&gt; in package.json&lt;br/&gt; (if using Yarn) to force the use of a newer puppeteer&lt;br/&gt; version. This is more complex and can lead to compatibility issues, so it&amp;#39;s typically recommended only if you&amp;#39;re experienced with Node.js and npm&amp;#39;s inner workings.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Monitor and Update&lt;/strong&gt;: If the current functionality isn&amp;#39;t affected and you&amp;#39;re not using puppeteer&lt;br/&gt; in security-critical environments, you may choose to monitor the situation while waiting for the maintainers to update their packages. Ensure to regularly check for new versions of flowise&lt;br/&gt; and its dependencies that might resolve this issue.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Assess Usage&lt;/strong&gt;: Consider how you&amp;#39;re using flowise&lt;br/&gt; . If puppeteer&lt;br/&gt; &amp;#39;s role is not critical for your use case, the deprecated warnings might be less concerning. However, if you&amp;#39;re using puppeteer&lt;br/&gt; features extensively, especially in a production or security-sensitive environment, addressing this becomes more urgent.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;In summary, the best course of action is typically to reach out to the package maintainers or monitor for updates that resolve the dependency concerns. Direct intervention is possible but should be approached with caution.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Oldsixstring&quot;&gt; /u/Oldsixstring &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bfza1v/flowise_pupeteer_version_dependencies_help_needed/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bfza1v/flowise_pupeteer_version_dependencies_help_needed/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bfza1v</id><link href="https://www.reddit.com/r/LangChain/comments/1bfza1v/flowise_pupeteer_version_dependencies_help_needed/" /><updated>2024-03-16T05:55:06+00:00</updated><published>2024-03-16T05:55:06+00:00</published><title>Flowise - Pupeteer version dependencies - Help needed</title></entry><entry><author><name>/u/GritsNGreens</name><uri>https://www.reddit.com/user/GritsNGreens</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I have an existing RAG flow that uses ConversationBufferMemory and a prompt that summarizes the history to answer questions. I want to add extraction to the response to pull out some data I&amp;#39;m interested in (like the standard examples, names of people, places, etc) so I can use that in my own code (not an agent).&lt;/p&gt; &lt;p&gt;Has anyone implemented something like this? I&amp;#39;m not seeing how I should integrate the llm.with_structured_output with the existing ConversationBufferMemory flow. Does anyone have an example that does all of those things? For reference I&amp;#39;m using OpenAI and FAISS in Python. Thanks!&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://python.langchain.com/docs/use_cases/extraction/?ref=blog.langchain.dev&quot;&gt;Extraction | ü¶úÔ∏èüîó Langchain&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://python.langchain.com/docs/modules/memory/types/buffer&quot;&gt;Conversation Buffer | ü¶úÔ∏èüîó Langchain&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/GritsNGreens&quot;&gt; /u/GritsNGreens &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bftk5y/perform_extraction_on_the_answer_to_a_prompt_with/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bftk5y/perform_extraction_on_the_answer_to_a_prompt_with/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bftk5y</id><link href="https://www.reddit.com/r/LangChain/comments/1bftk5y/perform_extraction_on_the_answer_to_a_prompt_with/" /><updated>2024-03-16T00:49:27+00:00</updated><published>2024-03-16T00:49:27+00:00</published><title>Perform extraction on the answer to a prompt with ConversationBufferMemory</title></entry><entry><author><name>/u/worldender999</name><uri>https://www.reddit.com/user/worldender999</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;m very new to working with LLMs, but I&amp;#39;m an experienced software engineer. I&amp;#39;ve read that OpenAI embeddings work best with vector stores that are configured to use cosine similarity when doing searches. Milvus, the vector store I&amp;#39;m using for my POC right now, supports cosine similarity. However it is a very recent feature.&lt;/p&gt; &lt;p&gt;The docs I&amp;#39;m looking at for LangChain show it auto-configuring the collection, for the most part. What I&amp;#39;m asking is if LangChain will use cosine similarity when I configure it with milvus and OpenAI embeddings&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/worldender999&quot;&gt; /u/worldender999 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bfrguy/does_langchain_support_the_new_milvus_cosine/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bfrguy/does_langchain_support_the_new_milvus_cosine/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bfrguy</id><link href="https://www.reddit.com/r/LangChain/comments/1bfrguy/does_langchain_support_the_new_milvus_cosine/" /><updated>2024-03-15T23:14:23+00:00</updated><published>2024-03-15T23:14:23+00:00</published><title>Does LangChain support the new milvus cosine similarity?</title></entry><entry><author><name>/u/AbortingMission</name><uri>https://www.reddit.com/user/AbortingMission</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;First day using langchain, and already 3 patches to the official repo. 10 years ago this would have been called alpha software, and invitation only for qa. Absolute amatuer hour. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/AbortingMission&quot;&gt; /u/AbortingMission &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bfve1k/if_you_like_langchain_get_familiar_with/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bfve1k/if_you_like_langchain_get_familiar_with/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bfve1k</id><link href="https://www.reddit.com/r/LangChain/comments/1bfve1k/if_you_like_langchain_get_familiar_with/" /><updated>2024-03-16T02:19:21+00:00</updated><published>2024-03-16T02:19:21+00:00</published><title>If you like langchain, get familiar with patch-package too</title></entry><entry><author><name>/u/datascienceharp</name><uri>https://www.reddit.com/user/datascienceharp</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bfd4od/i_was_struggling_to_create_a_custom_llm_for_an/&quot;&gt; &lt;img src=&quot;https://external-preview.redd.it/0-fRWqjlLadVXj5pfYp4_Oe3xgBWE-_rdjVSn7hlohI.jpg?width=216&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2817183828c9747b960cb2e55c59cfa41f4f9ded&quot; alt=&quot;I was struggling to create a custom LLM for an API provider. This notebook is my best attempt. It works! Feedback is welcome.&quot; title=&quot;I was struggling to create a custom LLM for an API provider. This notebook is my best attempt. It works! Feedback is welcome.&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/datascienceharp&quot;&gt; /u/datascienceharp &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://colab.research.google.com/drive/1PMwMovV-ji1mp0yl0qYDTI-gdG6SjOnZ?usp=sharing&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bfd4od/i_was_struggling_to_create_a_custom_llm_for_an/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1bfd4od</id><media:thumbnail url="https://external-preview.redd.it/0-fRWqjlLadVXj5pfYp4_Oe3xgBWE-_rdjVSn7hlohI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=2817183828c9747b960cb2e55c59cfa41f4f9ded" /><link href="https://www.reddit.com/r/LangChain/comments/1bfd4od/i_was_struggling_to_create_a_custom_llm_for_an/" /><updated>2024-03-15T12:46:08+00:00</updated><published>2024-03-15T12:46:08+00:00</published><title>I was struggling to create a custom LLM for an API provider. This notebook is my best attempt. It works! Feedback is welcome.</title></entry><entry><author><name>/u/FunnyMathematician77</name><uri>https://www.reddit.com/user/FunnyMathematician77</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bffy3w/rerankers_and_twostage_retrieval_pinecone/&quot;&gt; &lt;img src=&quot;https://external-preview.redd.it/cHA3acF8ZZgUd0il8kFYGuUq8is1-EshROO0qp_J7iQ.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=aae6bea1750e26efd15409c922e51da12721e5bd&quot; alt=&quot;Rerankers and Two-Stage Retrieval | Pinecone&quot; title=&quot;Rerankers and Two-Stage Retrieval | Pinecone&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/FunnyMathematician77&quot;&gt; /u/FunnyMathematician77 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.pinecone.io/learn/series/rag/rerankers/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bffy3w/rerankers_and_twostage_retrieval_pinecone/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1bffy3w</id><media:thumbnail url="https://external-preview.redd.it/cHA3acF8ZZgUd0il8kFYGuUq8is1-EshROO0qp_J7iQ.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=aae6bea1750e26efd15409c922e51da12721e5bd" /><link href="https://www.reddit.com/r/LangChain/comments/1bffy3w/rerankers_and_twostage_retrieval_pinecone/" /><updated>2024-03-15T14:59:24+00:00</updated><published>2024-03-15T14:59:24+00:00</published><title>Rerankers and Two-Stage Retrieval | Pinecone</title></entry><entry><author><name>/u/basicdude13</name><uri>https://www.reddit.com/user/basicdude13</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hello, I am new to RAG.&lt;/p&gt; &lt;p&gt;I am making a chatbot for the Beeline application for my company. &lt;/p&gt; &lt;p&gt;The Beeline team at my company has put together two FAQs to use as context: one for contractors/vendors and one for internal employees. Each FAQ is a word document with various questions and answers that describe processes like onboarding, submitting time cards, etc.. Each is about 3000 tokens long.&lt;/p&gt; &lt;p&gt;I have written two system prompts based on whether the user is a contractor/vendor or internal users. Each is about 2000 tokens‚Ä¶ does this seem like too much?&lt;/p&gt; &lt;p&gt;The chatbot application uses the correct faq document and system prompt based on the user. Basically, I just grab the correct ones and send to my deployed LLM alongside the user query. The total prompt is usually like 5000 tokens.I use the faq document in its entirety given I am using a gpt-4-32k deployment as my LLM.&lt;/p&gt; &lt;p&gt;The responses are accurate every time. However, they are slow. Is this due to token size? Should I be chunking? How can I ensure a given chunk would have all the right sentences in it if I chunked? Some processes are 5 sentences long and what if they are captured in different chunks?&lt;/p&gt; &lt;p&gt;Any insight is super helpful. I am coding everything rather than using langchain or llama2index or whatever. Should I not do this? I found the documentation for both hard to follow honestly so I immediately abandoned trying to use them üòÖ&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/basicdude13&quot;&gt; /u/basicdude13 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bf9ps7/when_to_simply_feed_whole_document_in_rag/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bf9ps7/when_to_simply_feed_whole_document_in_rag/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bf9ps7</id><link href="https://www.reddit.com/r/LangChain/comments/1bf9ps7/when_to_simply_feed_whole_document_in_rag/" /><updated>2024-03-15T09:13:55+00:00</updated><published>2024-03-15T09:13:55+00:00</published><title>When to simply feed whole document in RAG?</title></entry><entry><author><name>/u/danipudani</name><uri>https://www.reddit.com/user/danipudani</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bfej72/chomsky_vs_shannon_approaches_to_nlp_and_ai_chris/&quot;&gt; &lt;img src=&quot;https://external-preview.redd.it/imqh4WJkkuk7nM24-TElTHcRbuOshFr34JgZf-Lfh5k.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7db10bd65cf06a58299f4a4bde51a588b4d6142b&quot; alt=&quot;Chomsky vs Shannon approaches to NLP and AI - Chris Manning Stanford OpenNLP creator&quot; title=&quot;Chomsky vs Shannon approaches to NLP and AI - Chris Manning Stanford OpenNLP creator&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/danipudani&quot;&gt; /u/danipudani &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://youtu.be/9PqOWu2_0MA?si=hCkx0-v8hRlelZpy&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bfej72/chomsky_vs_shannon_approaches_to_nlp_and_ai_chris/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1bfej72</id><media:thumbnail url="https://external-preview.redd.it/imqh4WJkkuk7nM24-TElTHcRbuOshFr34JgZf-Lfh5k.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=7db10bd65cf06a58299f4a4bde51a588b4d6142b" /><link href="https://www.reddit.com/r/LangChain/comments/1bfej72/chomsky_vs_shannon_approaches_to_nlp_and_ai_chris/" /><updated>2024-03-15T13:56:27+00:00</updated><published>2024-03-15T13:56:27+00:00</published><title>Chomsky vs Shannon approaches to NLP and AI - Chris Manning Stanford OpenNLP creator</title></entry><entry><author><name>/u/ThaiosX0195</name><uri>https://www.reddit.com/user/ThaiosX0195</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hello everyone,&lt;/p&gt; &lt;p&gt;This is my first post here and I hope it is clear and correct for you all :)&lt;/p&gt; &lt;p&gt;Currently, I am working on an AI project where the idea is to &amp;quot;teach&amp;quot; a large language model thousands of english PDFs (around 100k, all about the same topic) and then be able to chat with it.&lt;/p&gt; &lt;p&gt;I followed several tutorials, using different LLMs (Zephyr, Llama2, Mistral) from HuggingFace and giving a proper prompt for the topic.&lt;/p&gt; &lt;p&gt;In the last weeks I tried RAG and unfortunately it could not give me good results :( I suppose due to the huge amount of data, but it is really really slow and to answer it takes 10 min or more (on a cluster Tesla V100-SXM2-32GB GPU)! Is there a way to speed it up?&lt;/p&gt; &lt;p&gt;Moreover, sometimes it gives very good answers, while sometimes it gives erroneous information and a sort of &amp;quot;hallucination&amp;quot; :( I really tried everything: change the prompt, change the embedding model, change the LLM, change the chunk size, but nothing could help.&lt;br/&gt; Finally, I am afraid that RAG is not good at all for deployment since you need a lot of memory for the &amp;quot;external knowledge&amp;quot; and for the model itself.&lt;/p&gt; &lt;p&gt;Is there another solution to RAG? Should I use fine tuning? If yes, how can I let my model to assimilate all these data? Should I turn the PDFs in JSONs made of Questions/Answers?&lt;/p&gt; &lt;p&gt;Did you succed in doing a project like this? If yes, can you help me, please? I do not know how to properly solve this issue. Any help would be really appreciated! &lt;/p&gt; &lt;p&gt;Thank you so much in advance!&lt;/p&gt; &lt;p&gt;EDIT:&lt;br/&gt; I am really really happy to see so much people trying to help me! I THANK YOU ALL! :)&lt;br/&gt; Apologies if it is a long text and maybe with too general information, but I am still a beginner and I am trying to explain my problem as best as I can :(&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/ThaiosX0195&quot;&gt; /u/ThaiosX0195 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1benf40/rag_is_too_slow_with_100k_pdfs_what_do_you/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1benf40/rag_is_too_slow_with_100k_pdfs_what_do_you/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1benf40</id><link href="https://www.reddit.com/r/LangChain/comments/1benf40/rag_is_too_slow_with_100k_pdfs_what_do_you/" /><updated>2024-03-14T15:09:16+00:00</updated><published>2024-03-14T15:09:16+00:00</published><title>RAG is too slow with 100k PDFs! What do you suggest? LLM fine-tuning?</title></entry><entry><author><name>/u/Sweaty-Minimum5423</name><uri>https://www.reddit.com/user/Sweaty-Minimum5423</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I have a rag system that is used to answer customer questions. But to optimise the retrieval, I want the agent ask some follow up questions and instruct user to provide answer before retrieval.&lt;/p&gt; &lt;p&gt;I have a few questions. Which type of langchain agent is good at this task? How to prompt the agent to ask follow up one or more question dynamically? Do I need multiple agent to do it? I haven‚Äôt touched on langgraph yet but I want to see one agent can do this task.&lt;/p&gt; &lt;p&gt;Thanks. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Sweaty-Minimum5423&quot;&gt; /u/Sweaty-Minimum5423 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bf2iya/rag_agent_in_langchain/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bf2iya/rag_agent_in_langchain/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bf2iya</id><link href="https://www.reddit.com/r/LangChain/comments/1bf2iya/rag_agent_in_langchain/" /><updated>2024-03-15T01:50:19+00:00</updated><published>2024-03-15T01:50:19+00:00</published><title>RAG agent in langchain</title></entry><entry><author><name>/u/G_S_7_wiz</name><uri>https://www.reddit.com/user/G_S_7_wiz</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I am using the below sql_agent to query through my database:&lt;br/&gt; agent = create_sql_agent(llm=llm, toolkit=sql_toolkit, agent_type=AgentType.ZERO_SHOT_REACT_DESCRIPTION,&lt;br/&gt; handle_parsing_errors=True, verbose=True, max_execution_time=200,&lt;br/&gt; max_iterations=1000)&lt;br/&gt; Is there any way to count the total number of tokens(prompt tokens+query tokens+query_result tokens+thought tokens+action tokens+observation tokens+tool tokens) used by this agent.&lt;br/&gt; I have used the below approach without good results:&lt;br/&gt; &lt;a href=&quot;https://python.langchain.com/docs/modules/model_io/llms/token_usage_tracking&quot;&gt;https://python.langchain.com/docs/modules/model_io/llms/token_usage_tracking&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/G_S_7_wiz&quot;&gt; /u/G_S_7_wiz &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bf9mkq/knowing_number_of_tokens_in_sql_agent/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bf9mkq/knowing_number_of_tokens_in_sql_agent/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bf9mkq</id><link href="https://www.reddit.com/r/LangChain/comments/1bf9mkq/knowing_number_of_tokens_in_sql_agent/" /><updated>2024-03-15T09:07:21+00:00</updated><published>2024-03-15T09:07:21+00:00</published><title>Knowing number of tokens in SQL agent</title></entry></feed>