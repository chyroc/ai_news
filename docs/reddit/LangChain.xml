<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/"><category term="LangChain" label="r/LangChain"/><updated>2024-03-21T07:43:39+00:00</updated><icon>https://www.redditstatic.com/icon.png/</icon><id>/r/LangChain.rss</id><link rel="self" href="https://www.reddit.com/r/LangChain.rss" type="application/atom+xml" /><link rel="alternate" href="https://www.reddit.com/r/LangChain" type="text/html" /><subtitle>LangChain is an open-source framework and developer toolkit that helps developers get LLM applications from prototype to production. It is available for Python and Javascript at https://www.langchain.com/.</subtitle><title>LangChain</title><entry><author><name>/u/zchaarm</name><uri>https://www.reddit.com/user/zchaarm</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;A place for members of &lt;a href=&quot;/r/LangChain&quot;&gt;r/LangChain&lt;/a&gt; to chat with each other&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/zchaarm&quot;&gt; /u/zchaarm &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_10ljho9</id><link href="https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/" /><updated>2023-01-26T04:33:05+00:00</updated><published>2023-01-26T04:33:05+00:00</published><title>r/LangChain Lounge</title></entry><entry><author><name>/u/ninja24x7</name><uri>https://www.reddit.com/user/ninja24x7</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I am implementing RAG and trying to understand what&amp;#39;s the advantage of Overlapping.&lt;br/&gt; Consider this text:&lt;br/&gt; &lt;code&gt;&amp;quot;One of the most important things I didn&amp;#39;t understand about the world when I was a child is the degree to which the returns for performance are superlinear.&amp;quot;&lt;/code&gt;&lt;br/&gt; which is chunked and overlapped as using Naive or any strategy :&lt;br/&gt; &lt;code&gt;chunk 1 : One of the most important things&lt;/code&gt;&lt;br/&gt; &lt;code&gt;Chunk 2 : things I didn&amp;#39;t understand about&lt;/code&gt;&lt;br/&gt; &lt;code&gt;chunk 3: about the world when I was a child&lt;/code&gt;&lt;br/&gt; and so on..&lt;br/&gt; As you can see there is a word overlap with the chunks.&lt;br/&gt; What advantage does LLM get when you feed overlapping &lt;code&gt;chunk2&lt;/code&gt; and &lt;code&gt;chunk3&lt;/code&gt; to execute RAG prompt against a user query. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/ninja24x7&quot;&gt; /u/ninja24x7 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bjxvov/what_is_the_advantage_of_overlapping_in_chunking/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bjxvov/what_is_the_advantage_of_overlapping_in_chunking/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bjxvov</id><link href="https://www.reddit.com/r/LangChain/comments/1bjxvov/what_is_the_advantage_of_overlapping_in_chunking/" /><updated>2024-03-21T04:12:54+00:00</updated><published>2024-03-21T04:12:54+00:00</published><title>what is the advantage of overlapping in chunking strategy</title></entry><entry><author><name>/u/redditforgets</name><uri>https://www.reddit.com/user/redditforgets</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bjlldg/got_the_accuracy_of_gpt4_function_calling_from_35/&quot;&gt; &lt;img src=&quot;https://b.thumbs.redditmedia.com/EVm_c1rJmIBCr6_4BoHQygB8rtJ_kZYrD-LWEoWIYPQ.jpg&quot; alt=&quot;Got the accuracy of GPT4 Function Calling from 35% to 75% by tweaking function definitions.&quot; title=&quot;Got the accuracy of GPT4 Function Calling from 35% to 75% by tweaking function definitions.&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;ul&gt; &lt;li&gt;Adding function definitions in the system prompt of functions (Clickup&amp;#39;s API calls).&lt;/li&gt; &lt;li&gt;Flattening the Schema of the function&lt;/li&gt; &lt;li&gt;Adding system prompts&lt;/li&gt; &lt;li&gt;Adding function definitions in system prompt&lt;/li&gt; &lt;li&gt;Adding individual parameter examples&lt;/li&gt; &lt;li&gt;Adding function examples&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Wrote a nice blog with an &lt;a href=&quot;https://blog.composio.dev/improving-function-calling-accuracy-for-agentic-integrations/&quot;&gt;Indepth explanation&lt;/a&gt; here.&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/rmxgt35zfjpc1.png?width=816&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=934eddf839e17f2324c590157943a92ebbdedffa&quot;&gt;https://preview.redd.it/rmxgt35zfjpc1.png?width=816&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=934eddf839e17f2324c590157943a92ebbdedffa&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/redditforgets&quot;&gt; /u/redditforgets &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bjlldg/got_the_accuracy_of_gpt4_function_calling_from_35/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bjlldg/got_the_accuracy_of_gpt4_function_calling_from_35/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1bjlldg</id><media:thumbnail url="https://b.thumbs.redditmedia.com/EVm_c1rJmIBCr6_4BoHQygB8rtJ_kZYrD-LWEoWIYPQ.jpg" /><link href="https://www.reddit.com/r/LangChain/comments/1bjlldg/got_the_accuracy_of_gpt4_function_calling_from_35/" /><updated>2024-03-20T19:10:44+00:00</updated><published>2024-03-20T19:10:44+00:00</published><title>Got the accuracy of GPT4 Function Calling from 35% to 75% by tweaking function definitions.</title></entry><entry><author><name>/u/Yintorion</name><uri>https://www.reddit.com/user/Yintorion</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I have the biography of a fictional character. It is about 160 pages long. How do I create a chatbot of this character with memory using RAG? I am using Gemini btw. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Yintorion&quot;&gt; /u/Yintorion &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bk0bo3/creating_chatbot_of_characters_using_rag/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bk0bo3/creating_chatbot_of_characters_using_rag/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bk0bo3</id><link href="https://www.reddit.com/r/LangChain/comments/1bk0bo3/creating_chatbot_of_characters_using_rag/" /><updated>2024-03-21T06:48:41+00:00</updated><published>2024-03-21T06:48:41+00:00</published><title>Creating chatbot of characters using RAG</title></entry><entry><author><name>/u/XhoniShollaj</name><uri>https://www.reddit.com/user/XhoniShollaj</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Has anyone here succesfully deployed LangChain in production? If yes, what were the main issues enountered and how did you approach them?&lt;/p&gt; &lt;p&gt;If not, what alternatives did you use or considering (e.g. Haystack etc.) ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/XhoniShollaj&quot;&gt; /u/XhoniShollaj &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bjxx32/langchain_in_production_alternatives/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bjxx32/langchain_in_production_alternatives/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bjxx32</id><link href="https://www.reddit.com/r/LangChain/comments/1bjxx32/langchain_in_production_alternatives/" /><updated>2024-03-21T04:15:01+00:00</updated><published>2024-03-21T04:15:01+00:00</published><title>Langchain in Production (&amp; Alternatives)</title></entry><entry><author><name>/u/Livid-Violinist-7652</name><uri>https://www.reddit.com/user/Livid-Violinist-7652</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I have a word doc and an excel file whose information is interconnected? The excel file outlines the process steps and the word file has process specifics. &lt;/p&gt; &lt;p&gt;I want to build a RAG by leveraging these two files to generate a document based on some prompts. What is the best strategy to do it?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Livid-Violinist-7652&quot;&gt; /u/Livid-Violinist-7652 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bk0kyo/how_to_build_a_rag_on_structed_data/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bk0kyo/how_to_build_a_rag_on_structed_data/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bk0kyo</id><link href="https://www.reddit.com/r/LangChain/comments/1bk0kyo/how_to_build_a_rag_on_structed_data/" /><updated>2024-03-21T07:06:27+00:00</updated><published>2024-03-21T07:06:27+00:00</published><title>How to build a RAG on structed data?</title></entry><entry><author><name>/u/Electronic_Key_8235</name><uri>https://www.reddit.com/user/Electronic_Key_8235</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Electronic_Key_8235&quot;&gt; /u/Electronic_Key_8235 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;/r/LLMDevs/comments/1bjzhja/ideal_toolchain_for_embedding_employee_training/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bjzzx8/ideal_toolchain_for_embedding_employee_training/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bjzzx8</id><link href="https://www.reddit.com/r/LangChain/comments/1bjzzx8/ideal_toolchain_for_embedding_employee_training/" /><updated>2024-03-21T06:25:59+00:00</updated><published>2024-03-21T06:25:59+00:00</published><title>Ideal Toolchain for Embedding Employee Training Documents</title></entry><entry><author><name>/u/Zealousideal-Fall705</name><uri>https://www.reddit.com/user/Zealousideal-Fall705</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I’m a Ph.D. student who recently try to switch from hugging face to langchain. It feels like huggingface organize their libraries the research way (or the PyTorch way? It just feel like I can use them the same way I use research papers’ code), but langchain is more like something developed by JavaScript engineers and designed with no research user cases. &lt;/p&gt; &lt;p&gt;For example, all the “batch inference “ requirements on GitHub are ignored. The interface for customized functions (e.g., chat history post processing) are ill-designed. &lt;/p&gt; &lt;p&gt;I chose langchain in the beginning because the LLMs hosted by langchain responds faster than my local ones. But it seems that it’s really hard to customize the functionalities for research purposes.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Zealousideal-Fall705&quot;&gt; /u/Zealousideal-Fall705 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bjks1c/do_researchers_like_langchain/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bjks1c/do_researchers_like_langchain/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bjks1c</id><link href="https://www.reddit.com/r/LangChain/comments/1bjks1c/do_researchers_like_langchain/" /><updated>2024-03-20T18:37:14+00:00</updated><published>2024-03-20T18:37:14+00:00</published><title>Do researchers like langchain?</title></entry><entry><author><name>/u/Bhaag_Jaa</name><uri>https://www.reddit.com/user/Bhaag_Jaa</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;m working on a project to generate insights from the data.&lt;/p&gt; &lt;p&gt;Extracting data from DB and passing it as context to LLM&lt;/p&gt; &lt;p&gt;I am using AWS bedrock service , antropic claude v2 as LLM(coz 100k tokken limit.)&lt;/p&gt; &lt;p&gt;Data comprises audiences and one base Audience with multiple attributes related to demographics,geography and employment.&lt;br/&gt; Each attribute have multiple attribute values&lt;/p&gt; &lt;p&gt;Issue I&amp;#39;m facing is some attributes have 20-35k rows of unique attribute values, i want to generate the insghits from them.&lt;/p&gt; &lt;p&gt;I tried passing 400-500 rows at a time with a loop to cover 20k rows and store the output in a list and passing that list again through LLM to generate summary of all the 400 rows loop output.. something like Mapdreduce.&lt;/p&gt; &lt;p&gt;but it is taking a lot of time (40-min to generate insights from 20k rows)and there is loss of information, LLM ignores some of the important rows as every row is unique and important.&lt;/p&gt; &lt;p&gt;Pls suggest some better way to solve this problem.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Bhaag_Jaa&quot;&gt; /u/Bhaag_Jaa &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bjmv5j/how_to_pass_30k_rows_as_context_to_llm/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bjmv5j/how_to_pass_30k_rows_as_context_to_llm/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bjmv5j</id><link href="https://www.reddit.com/r/LangChain/comments/1bjmv5j/how_to_pass_30k_rows_as_context_to_llm/" /><updated>2024-03-20T20:03:07+00:00</updated><published>2024-03-20T20:03:07+00:00</published><title>How to pass 30k rows as context to LLM.</title></entry><entry><author><name>/u/Forward-Tip8621</name><uri>https://www.reddit.com/user/Forward-Tip8621</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi all, was going through the search tools available via langchain. Just wanted to check which is the best one to use? What are the key aspects to consider other than cost? If anyone who has used/compared these APIs that would be a great value add to my research &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Forward-Tip8621&quot;&gt; /u/Forward-Tip8621 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bjsx89/best_search_tool_in_langchain/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bjsx89/best_search_tool_in_langchain/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bjsx89</id><link href="https://www.reddit.com/r/LangChain/comments/1bjsx89/best_search_tool_in_langchain/" /><updated>2024-03-21T00:13:03+00:00</updated><published>2024-03-21T00:13:03+00:00</published><title>Best Search Tool in Langchain</title></entry><entry><author><name>/u/tisi3000</name><uri>https://www.reddit.com/user/tisi3000</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;When building LLM workflows with LangChain/LangGraph what&amp;#39;s the best way to build a node in the workflow &lt;strong&gt;where a human can validate/approve/reject&lt;/strong&gt; a flow? I know there is a Human-in-the-loop component in LangGraph that will prompt the user for input. But what if I&amp;#39;m not creating a user-initiated chat conversation, but a flow that reacts to e.g. incoming emails?&lt;/p&gt; &lt;p&gt;I guess I&amp;#39;d have to design my UI so that it&amp;#39;s not only a simple single-threaded chat interface, but some sort of inbox, right? Or is there any standard way that comes to mind?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/tisi3000&quot;&gt; /u/tisi3000 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bjnmu4/human_intervention_in_agent_workflows/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bjnmu4/human_intervention_in_agent_workflows/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bjnmu4</id><link href="https://www.reddit.com/r/LangChain/comments/1bjnmu4/human_intervention_in_agent_workflows/" /><updated>2024-03-20T20:34:23+00:00</updated><published>2024-03-20T20:34:23+00:00</published><title>Human intervention in agent workflows</title></entry><entry><author><name>/u/redfuel2</name><uri>https://www.reddit.com/user/redfuel2</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hello everyone,&lt;/p&gt; &lt;p&gt;I&amp;#39;m embarking on a project that requires a fresh start, and I find myself at a crossroads trying to decide on the optimal technology stack. The core objective is to enable conversations with a database using natural language, aiming for precise outcomes. This involves working with tabular data, applying filters, and conducting semantic searches.&lt;/p&gt; &lt;p&gt;Given the plethora of options out there, from graph databases and SQLCoder models to Retrieval-Augmented Generation (RAG) techniques, making a choice feels overwhelming. Each of these technologies brings something unique to the table, but I&amp;#39;m looking for a solution that balances ease of integration, scalability, and, most importantly, the ability to understand and process natural language queries effectively.&lt;/p&gt; &lt;p&gt;I would greatly appreciate your insights, experiences, or any advice you could share on this matter. Which stack or combination of technologies have you found to be the most effective for interacting with databases through natural language? Any pitfalls or success stories you could share would also be incredibly helpful as I navigate through these options.&lt;/p&gt; &lt;p&gt;Thank you in advance for your time and help!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/redfuel2&quot;&gt; /u/redfuel2 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bjf4xd/seeking_the_ideal_stack_for_natural_language/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bjf4xd/seeking_the_ideal_stack_for_natural_language/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bjf4xd</id><link href="https://www.reddit.com/r/LangChain/comments/1bjf4xd/seeking_the_ideal_stack_for_natural_language/" /><updated>2024-03-20T14:43:52+00:00</updated><published>2024-03-20T14:43:52+00:00</published><title>Seeking the Ideal Stack for Natural Language Database Interactions</title></entry><entry><author><name>/u/HappyDataGuy</name><uri>https://www.reddit.com/user/HappyDataGuy</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/HappyDataGuy&quot;&gt; /u/HappyDataGuy &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;/r/LLMDevs/comments/1bjctuz/has_anyone_used_dspy_for_rag_how_does_it_compare/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bjcun4/has_anyone_used_dspy_for_rag_how_does_it_compare/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bjcun4</id><link href="https://www.reddit.com/r/LangChain/comments/1bjcun4/has_anyone_used_dspy_for_rag_how_does_it_compare/" /><updated>2024-03-20T13:00:30+00:00</updated><published>2024-03-20T13:00:30+00:00</published><title>Has anyone used dspy for RAG? how does it compare to langchain/llama-index? and how does it &quot;train&quot; an LLM?</title></entry><entry><author><name>/u/Mediocre-Card8046</name><uri>https://www.reddit.com/user/Mediocre-Card8046</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;i have built a Langchain RAG app with a local model and now want to be able to run it on a Laptop. I am using a quantized Mixtral Model (Q5_0) and for this I want to conntect 2 GeoForce RTX 4090 to my laptop. As I am a newby (and nooby) in the Hardware topic, is it even possible to connect 2 RTX 4090 to a more or less &amp;quot;normal&amp;quot; Laptop?&lt;/p&gt; &lt;p&gt;The use case would be that the customer tries the (local) application on a standalone device and if he is happy with it he buys more Hardware to host it for production.&lt;/p&gt; &lt;p&gt;At the moment I am running everything on my Macbook with 64GB RAM but I need a solution for a customer with a Windows PC.&lt;/p&gt; &lt;p&gt;One other option would be that the customer just buys a Macbook, but the 2 GeForece RTX 4090 would be a better investment I think because these could further be used for a prodcution setting.&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;Thanks for you suggestions!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Mediocre-Card8046&quot;&gt; /u/Mediocre-Card8046 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bji0np/is_it_possible_to_connect_2_geforce_rtx_4090_to_a/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bji0np/is_it_possible_to_connect_2_geforce_rtx_4090_to_a/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bji0np</id><link href="https://www.reddit.com/r/LangChain/comments/1bji0np/is_it_possible_to_connect_2_geforce_rtx_4090_to_a/" /><updated>2024-03-20T16:44:18+00:00</updated><published>2024-03-20T16:44:18+00:00</published><title>is it possible to connect 2 GeForce RTX 4090 to a Laptop?</title></entry><entry><author><name>/u/fish2079</name><uri>https://www.reddit.com/user/fish2079</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I have built a simple RAG chain with message history using Mistral-7b model with 4bit quantization. &lt;/p&gt; &lt;p&gt;Whenever I build this chain using a model from the dockerized Ollama, everything works fine and I can have a long conversation with the chain. &lt;/p&gt; &lt;p&gt;However, as soon as I switch to HF model, only the first message goes through, everything else gets the OOM memory. In fact, the memory usage seems to increase with each subsequent invoke. &lt;/p&gt; &lt;p&gt;In both cases, I am using the Mistral-7b model with quantization. So I am confused as to where the memory issue comes from. &lt;/p&gt; &lt;p&gt;Here are the code snippets: &lt;/p&gt; &lt;p&gt;Using HF model:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;model_name = &amp;quot;mistralai/Mistral-7B-Instruct-v0.2&amp;quot; bnb_config = BitsAndBytesConfig( load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=&amp;quot;nf4&amp;quot;, bnb_4bit_compute_dtype=torch.bfloat16 ) model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=bnb_config) tokenizer = AutoTokenizer.from_pretrained(model_name) text_generation_pipeline = pipeline( model=model, tokenizer=tokenizer, task=&amp;quot;text-generation&amp;quot;, temperature=0.2, do_sample=True, repetition_penalty=1.1, max_new_tokens=400, ) chat_llm = HuggingFacePipeline(pipeline=text_generation_pipeline) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Using Ollama model:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;chat_llm = ChatOllama(model=&amp;quot;mistral:7b&amp;quot;) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Overall chain setup&lt;/p&gt; &lt;pre&gt;&lt;code&gt;chatbot_conversation_with_context_chain = &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;RunnablePassthrough.assign(standalone_message=standalone_message_chain).assign(context= itemgetter(&amp;#39;standalone_message&amp;#39;) | retriever).assign(output= question_answering_prompt | chat_llm | StrOutputParser())&lt;/p&gt; &lt;pre&gt;&lt;code&gt;chatbot = RunnableWithMessageHistory( chatbot_conversation_with_context_chain, get_session_history=get_session_history, input_messages_key=&amp;quot;messages&amp;quot;, history_messages_key=&amp;quot;chat_history&amp;quot;, history_factory_config=[ ConfigurableFieldSpec( id=&amp;quot;user_id&amp;quot;, annotation=str, name=&amp;quot;User ID&amp;quot;, description=&amp;quot;Unique identifier for the user.&amp;quot;, default=&amp;quot;&amp;quot;, is_shared=True, ), ConfigurableFieldSpec( id=&amp;quot;conversation_id&amp;quot;, annotation=str, name=&amp;quot;Conversation ID&amp;quot;, description=&amp;quot;Unique identifier for the conversation.&amp;quot;, default=&amp;quot;&amp;quot;, is_shared=True, ), ], &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;) response = chatbot.invoke( {&amp;#39;messages&amp;#39;: &amp;quot;Can you give me the basic Java code for reading a CSV file?&amp;quot;}, config={ &amp;quot;configurable&amp;quot;: {&amp;quot;user_id&amp;quot;: &amp;quot;test&amp;quot;, &amp;quot;conversation_id&amp;quot;: &amp;quot;dummy&amp;quot;} }, )&lt;/p&gt; &lt;p&gt;print(response.keys()) for key in response.keys(): print(key+&amp;quot;: &amp;quot;, end=&amp;quot;&amp;quot;) print(response[key])&lt;/p&gt; &lt;p&gt;response = chatbot.invoke( {&amp;#39;messages&amp;#39;: &amp;quot;Can you elaborate on the first function?&amp;quot;}, config={&amp;quot;configurable&amp;quot;: {&amp;quot;user_id&amp;quot;: &amp;quot;test&amp;quot;, &amp;quot;conversation_id&amp;quot;: &amp;quot;dummy&amp;quot;} }, )&lt;/p&gt; &lt;p&gt;print(response.keys()) for key in response.keys(): print(key+&amp;quot;: &amp;quot;, end=&amp;quot;&amp;quot;) print(response[key])&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/fish2079&quot;&gt; /u/fish2079 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bjm5rp/rag_chain_with_hf_model_works_fine_for_first/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bjm5rp/rag_chain_with_hf_model_works_fine_for_first/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bjm5rp</id><link href="https://www.reddit.com/r/LangChain/comments/1bjm5rp/rag_chain_with_hf_model_works_fine_for_first/" /><updated>2024-03-20T19:33:59+00:00</updated><published>2024-03-20T19:33:59+00:00</published><title>RAG chain with HF model works fine for first quest, then OOM for subsequent chain. No OOM issue when using Ollama model instead</title></entry><entry><author><name>/u/VegetableAddendum888</name><uri>https://www.reddit.com/user/VegetableAddendum888</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;If any Colab notebook or github repo available then it will be helpful&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/VegetableAddendum888&quot;&gt; /u/VegetableAddendum888 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bjiabv/can_anyone_suggest_a_idea_to_implement_rag_with/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bjiabv/can_anyone_suggest_a_idea_to_implement_rag_with/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bjiabv</id><link href="https://www.reddit.com/r/LangChain/comments/1bjiabv/can_anyone_suggest_a_idea_to_implement_rag_with/" /><updated>2024-03-20T16:55:26+00:00</updated><published>2024-03-20T16:55:26+00:00</published><title>Can anyone suggest a idea to implement RAG with LLm.Like if the searched query not in RAG data then LLm responses to the query</title></entry><entry><author><name>/u/stargazer1Q84</name><uri>https://www.reddit.com/user/stargazer1Q84</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi everyone, I hope it is fine to post questions here. &lt;/p&gt; &lt;p&gt;I am just getting started with output-parsers and I&amp;#39;m impressed with their usefulness when they work properly. I have, however, run into a case where every now and then, a chain returns an error that seems to be related to the JsonOutputParser that I use, as indicated by the following (condensed) error message:&lt;/p&gt; &lt;p&gt;&lt;code&gt;JSONDecodeError&lt;/code&gt;&lt;br/&gt; &lt;code&gt;JsonOutputParser.parse_result(self, result, partial)&lt;/code&gt;&lt;br/&gt; &lt;code&gt;156 # Parse the JSON string into a Python dictionary&lt;/code&gt;&lt;br/&gt; &lt;code&gt;--&amp;gt; 157 parsed = parser(json_str)&lt;/code&gt;&lt;br/&gt; &lt;code&gt;159 return parsed&lt;/code&gt;&lt;br/&gt; &lt;code&gt;122 # If we got here, we ran out of characters to remove&lt;/code&gt;&lt;br/&gt; &lt;code&gt;123 # and still couldn&amp;#39;t parse the string as JSON, so return the parse error&lt;/code&gt;&lt;br/&gt; &lt;code&gt;124 # for the original string.&lt;/code&gt;&lt;br/&gt; &lt;code&gt;--&amp;gt; 125 return json.loads(s, strict=strict)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;According to &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/17hep0o/comment/k6na6nd/?utm_source=share&amp;amp;utm_medium=web3x&amp;amp;utm_name=web3xcss&amp;amp;utm_term=1&amp;amp;utm_content=share_button&quot;&gt;this post here&lt;/a&gt; this could be related to there not being &amp;quot;enough tokens left to fully generate my output&amp;quot;, which seems to be in line with the error message above:&lt;/p&gt; &lt;p&gt;&amp;gt;&lt;code&gt;122 # If we got here, we ran out of characters to remove&lt;/code&gt; &lt;/p&gt; &lt;p&gt;although I am not fully sure what that means or how it can be fixed. &lt;/p&gt; &lt;p&gt;Has anybody encountered this problem before and could offer some guidance? I must admit that I&amp;#39;m feeling kind of stumped, especially since the error can&amp;#39;t be reproduced reliably and only occurs every other time I run my script. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/stargazer1Q84&quot;&gt; /u/stargazer1Q84 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bjdjk0/understanding_jsondecodeerror_when_using/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bjdjk0/understanding_jsondecodeerror_when_using/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bjdjk0</id><link href="https://www.reddit.com/r/LangChain/comments/1bjdjk0/understanding_jsondecodeerror_when_using/" /><updated>2024-03-20T13:33:06+00:00</updated><published>2024-03-20T13:33:06+00:00</published><title>Understanding JSONDecodeError when using JsonOutputParser</title></entry><entry><author><name>/u/mehul_gupta1997</name><uri>https://www.reddit.com/user/mehul_gupta1997</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/mehul_gupta1997&quot;&gt; /u/mehul_gupta1997 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;/r/ArtificialInteligence/comments/1bjbd36/multiagent_conversation_using_crewai_genai/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bjbe0j/multiagent_conversation_using_crewai_genai/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bjbe0j</id><link href="https://www.reddit.com/r/LangChain/comments/1bjbe0j/multiagent_conversation_using_crewai_genai/" /><updated>2024-03-20T11:40:44+00:00</updated><published>2024-03-20T11:40:44+00:00</published><title>Multi-Agent Conversation using CrewAI (GenAI)</title></entry><entry><author><name>/u/Thegunsmith98</name><uri>https://www.reddit.com/user/Thegunsmith98</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I am trying to build an application that takes templates of things like a cover letter , resume , medical research document. Now based on this template I will upload another document containing information to be used to fill the template. However after the model generates a new document following the template and information , the whole alignment of the document is wrong and it doesnt bold the necessary parts. Is there any way to ensure that a model can follow the format for a template like center allignment , bolding the headers , etc. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Thegunsmith98&quot;&gt; /u/Thegunsmith98 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bj6xl3/langchain_usage_doubt_for_document_generation/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bj6xl3/langchain_usage_doubt_for_document_generation/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bj6xl3</id><link href="https://www.reddit.com/r/LangChain/comments/1bj6xl3/langchain_usage_doubt_for_document_generation/" /><updated>2024-03-20T06:25:51+00:00</updated><published>2024-03-20T06:25:51+00:00</published><title>Langchain Usage doubt for document generation</title></entry><entry><author><name>/u/Mediocre-Card8046</name><uri>https://www.reddit.com/user/Mediocre-Card8046</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;when thinking about RAG evaluation, everybody talks about RAGAS. It is generally nice to have a framework where you can evaluate your RAG workflows. However I tried it with an own local LLM as well as with the gpt-4-turbo model and the results really are not reliable. &lt;/p&gt; &lt;p&gt;I adapted prompts to my language (german) and with my test dataset, the answer_correctness, answer_relevancy scores are often times very low, zero or NaN, even if the answer is completely correct. &lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;Does anyone have similar experiences? &lt;/p&gt; &lt;p&gt;With my experience, I am not feeling comfortable using ragas as results differ heavenly from run to run, so all the evaluation doesn&amp;#39;t really help me. &lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Mediocre-Card8046&quot;&gt; /u/Mediocre-Card8046 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bijg75/why_is_everyone_using_ragas_for_rag_evaluation/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bijg75/why_is_everyone_using_ragas_for_rag_evaluation/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bijg75</id><link href="https://www.reddit.com/r/LangChain/comments/1bijg75/why_is_everyone_using_ragas_for_rag_evaluation/" /><updated>2024-03-19T12:49:43+00:00</updated><published>2024-03-19T12:49:43+00:00</published><title>Why is everyone using RAGAS for RAG evaluation? For me it looks very unreliable</title></entry><entry><author><name>/u/PreparationSad1717</name><uri>https://www.reddit.com/user/PreparationSad1717</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi folks, If you are building with LangChain and want to turn your work into sharable chat app in minutes and in pure python, then join the waitlist &lt;a href=&quot;https://cycls.typeform.com/waitlist&quot;&gt;https://cycls.typeform.com/waitlist&lt;/a&gt; .&lt;br/&gt; We&amp;#39;re gearing up for a release in just a few weeks. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/PreparationSad1717&quot;&gt; /u/PreparationSad1717 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bj3qjq/want_to_turn_your_work_into_sharable_chat_app_in/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bj3qjq/want_to_turn_your_work_into_sharable_chat_app_in/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bj3qjq</id><link href="https://www.reddit.com/r/LangChain/comments/1bj3qjq/want_to_turn_your_work_into_sharable_chat_app_in/" /><updated>2024-03-20T03:13:12+00:00</updated><published>2024-03-20T03:13:12+00:00</published><title>Want to turn your work into sharable chat app in minutes ?</title></entry><entry><author><name>/u/TheBroWhoLifts</name><uri>https://www.reddit.com/user/TheBroWhoLifts</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I work in education and envision a way of using RAG to help my students develop their writing. &lt;/p&gt; &lt;p&gt;My vision is to have students keep a digital portfolio of all their writing over the course of the semester or the year. I&amp;#39;d like to then use a RAG/LLM setup to provide students with feedback regarding their writing development over the course of the year. Ultimately, I&amp;#39;d like to load all of their writing into a RAG for my own analysis. I would be running this on an local LM Studio LLM for student privacy. &lt;/p&gt; &lt;p&gt;Is Langchain an appropriate tool to achieve this? Would I be able to set it up to analyze thousands of pages of student work? And can I design it so that it is a conversational interaction with a history window? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/TheBroWhoLifts&quot;&gt; /u/TheBroWhoLifts &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bimg0l/high_school_teacher_here_with_a_use_case_question/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bimg0l/high_school_teacher_here_with_a_use_case_question/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bimg0l</id><link href="https://www.reddit.com/r/LangChain/comments/1bimg0l/high_school_teacher_here_with_a_use_case_question/" /><updated>2024-03-19T15:06:18+00:00</updated><published>2024-03-19T15:06:18+00:00</published><title>High school teacher here with a use case question for the educational setting.</title></entry><entry><author><name>/u/logandarknight</name><uri>https://www.reddit.com/user/logandarknight</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hello! I have a question I haven’t been able to find online, and was hoping someone could explain it to me. &lt;/p&gt; &lt;p&gt;I need to build a “chatbot” where the user asks questions about history, and the agent must reply with the correct answer. Here’s the thing: The context needs to be fed from certain books, and some pdfs.&lt;/p&gt; &lt;p&gt;Why’s the best way to do this? The objective is to outperform in replying correctly to OpenAI models. Or, which model would be the best (or combination of model + RAG) to get the best result?&lt;/p&gt; &lt;p&gt;Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/logandarknight&quot;&gt; /u/logandarknight &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1biwtil/use_case_doubt/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1biwtil/use_case_doubt/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1biwtil</id><link href="https://www.reddit.com/r/LangChain/comments/1biwtil/use_case_doubt/" /><updated>2024-03-19T22:05:22+00:00</updated><published>2024-03-19T22:05:22+00:00</published><title>Use case doubt</title></entry><entry><author><name>/u/DXVA</name><uri>https://www.reddit.com/user/DXVA</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Is there a good way to integrate LangChain with a personal LLM RESTAPI yet?&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/17v1rhv/integrating_llm_rest_api_into_a_langchain/&quot;&gt;https://www.reddit.com/r/LangChain/comments/17v1rhv/integrating_llm_rest_api_into_a_langchain/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I saw this post, but it doesn&amp;#39;t explain any of the integration with basic chains like LLMChain. There&amp;#39;re so many integrations, but nothing I see so far for interacting with your own tooling?&lt;/p&gt; &lt;p&gt;The API just has the basic response structure for a LLM, but that should just be one piece of the connection right? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/DXVA&quot;&gt; /u/DXVA &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1billcu/integration_with_restapis/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1billcu/integration_with_restapis/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1billcu</id><link href="https://www.reddit.com/r/LangChain/comments/1billcu/integration_with_restapis/" /><updated>2024-03-19T14:28:38+00:00</updated><published>2024-03-19T14:28:38+00:00</published><title>Integration with RESTAPIs?</title></entry><entry><author><name>/u/heybigeyes123</name><uri>https://www.reddit.com/user/heybigeyes123</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Where are you guys finding customers to sell your RAG products? What do thesr customers look like?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/heybigeyes123&quot;&gt; /u/heybigeyes123 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bitjgv/rag_customers/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bitjgv/rag_customers/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bitjgv</id><link href="https://www.reddit.com/r/LangChain/comments/1bitjgv/rag_customers/" /><updated>2024-03-19T19:54:58+00:00</updated><published>2024-03-19T19:54:58+00:00</published><title>RAG customers</title></entry><entry><author><name>/u/Not-That-rpg</name><uri>https://www.reddit.com/user/Not-That-rpg</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I was just changing an existing langchain workflow from using an OpenAI model to using one from Replicate.&lt;/p&gt; &lt;p&gt;This showed the value of using &lt;code&gt;langchain&lt;/code&gt; because it pretty much Just Worked to change the LLM model constructor I had used originally, and then rerun all my code (in a Jupyter Notebook).&lt;/p&gt; &lt;p&gt;But it only &amp;quot;pretty much&amp;quot; worked: in particular, when I invoked the OpenAI models, I would get an &lt;code&gt;AIMessage&lt;/code&gt; object out of the chain. When I invoke a Replicate model, I am just getting a string.&lt;/p&gt; &lt;p&gt;I imagine that this could cause issues if trying to extend a chain past the Replicate LLM to something like an Output Parser couldn&amp;#39;t it?&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Not-That-rpg&quot;&gt; /u/Not-That-rpg &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bish94/should_i_report_this_as_a_bug/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bish94/should_i_report_this_as_a_bug/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bish94</id><link href="https://www.reddit.com/r/LangChain/comments/1bish94/should_i_report_this_as_a_bug/" /><updated>2024-03-19T19:12:17+00:00</updated><published>2024-03-19T19:12:17+00:00</published><title>Should I report this as a bug?</title></entry></feed>