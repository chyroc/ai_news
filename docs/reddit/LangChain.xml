<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/"><category term="LangChain" label="r/LangChain"/><updated>2024-04-24T20:57:56+00:00</updated><icon>https://www.redditstatic.com/icon.png/</icon><id>/r/LangChain.rss</id><link rel="self" href="https://www.reddit.com/r/LangChain.rss" type="application/atom+xml" /><link rel="alternate" href="https://www.reddit.com/r/LangChain" type="text/html" /><subtitle>LangChain is an open-source framework and developer toolkit that helps developers get LLM applications from prototype to production. It is available for Python and Javascript at https://www.langchain.com/.</subtitle><title>LangChain</title><entry><author><name>/u/zchaarm</name><uri>https://www.reddit.com/user/zchaarm</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;A place for members of &lt;a href=&quot;/r/LangChain&quot;&gt;r/LangChain&lt;/a&gt; to chat with each other&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/zchaarm&quot;&gt; /u/zchaarm &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_10ljho9</id><link href="https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/" /><updated>2023-01-26T04:33:05+00:00</updated><published>2023-01-26T04:33:05+00:00</published><title>r/LangChain Lounge</title></entry><entry><author><name>/u/BuildingLLMTools</name><uri>https://www.reddit.com/user/BuildingLLMTools</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey folks,&lt;/p&gt; &lt;p&gt;We&amp;#39;re building Langtrace, an open-source LLM App observability platform (&lt;a href=&quot;http://www.langtrace.ai&quot;&gt;www.langtrace.ai&lt;/a&gt;) and we recently built support for LlamaIndex, the go-to library for building retrieval-augmented generation (RAG) applications.&lt;/p&gt; &lt;p&gt;As builders, we know how frustrating it can be to optimize RAG apps (e.g. trying to figure out where the bottlenecks are, whether your retrieval strategy is effective, etc.) That&amp;#39;s why we&amp;#39;re building a tool that makes it easy to gain deeper insights and optimize performance, reliability, and user experience for your LLM apps.&lt;/p&gt; &lt;p&gt;With Langtrace and LlamaIndex, you can:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Get one-click observability for LlamaIndex-based RAG applications&lt;/li&gt; &lt;li&gt;Visualize latency breakdowns, context relevance, and resource utilization&lt;/li&gt; &lt;li&gt;Monitor and analyze traces, evals, metrics, and logs with OpenTelemetry&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Feel free to check out our &lt;a href=&quot;https://github.com/Scale3-Labs/langtrace&quot;&gt;repo&lt;/a&gt; for &lt;a href=&quot;https://github.com/Scale3-Labs/langtrace-docs/blob/main/langtrace-examples/llamaindex_essay/starter.py.ipynb&quot;&gt;examples&lt;/a&gt;, contribute, provide feedback, and join our &lt;a href=&quot;https://discord.com/invite/EaSATwtr4t&quot;&gt;community&lt;/a&gt;. More info on the integration with LlamaIndex &lt;a href=&quot;https://langtrace.ai/blog/langtrace-llamaindex-a-game-changing-combo-for-rag-developers&quot;&gt;here&lt;/a&gt; including a video demo. Looking forward to hearing of your feedback! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/BuildingLLMTools&quot;&gt; /u/BuildingLLMTools &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cc8fx4/solve_rag_app_optimization_puzzles_with_langtrace/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cc8fx4/solve_rag_app_optimization_puzzles_with_langtrace/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cc8fx4</id><link href="https://www.reddit.com/r/LangChain/comments/1cc8fx4/solve_rag_app_optimization_puzzles_with_langtrace/" /><updated>2024-04-24T20:18:16+00:00</updated><published>2024-04-24T20:18:16+00:00</published><title>Solve RAG App Optimization Puzzles with Langtrace + LlamaIndex</title></entry><entry><author><name>/u/OfficeSalamander</name><uri>https://www.reddit.com/user/OfficeSalamander</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I am trying to use internet-search enabled bots, and I was wondering how you guys were doing it - I see that Serpdev and Tavily have Langchain integration - which of these two do you guys like? Or do you roll your own?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/OfficeSalamander&quot;&gt; /u/OfficeSalamander &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cc1dyq/how_are_you_guys_doing_internet_search/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cc1dyq/how_are_you_guys_doing_internet_search/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cc1dyq</id><link href="https://www.reddit.com/r/LangChain/comments/1cc1dyq/how_are_you_guys_doing_internet_search/" /><updated>2024-04-24T15:40:46+00:00</updated><published>2024-04-24T15:40:46+00:00</published><title>How are you guys doing internet search?</title></entry><entry><author><name>/u/ashpreetbedi</name><uri>https://www.reddit.com/user/ashpreetbedi</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I dont trust the benchmarks, so I recorded my very first test run. Completely unedited, each question asked for the first time. First impression is that it is good, very very good for its size. Sharing the code below.&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://reddit.com/link/1cbuqow/video/ay3us3wbmewc1/player&quot;&gt;https://reddit.com/link/1cbuqow/video/ay3us3wbmewc1/player&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/ashpreetbedi&quot;&gt; /u/ashpreetbedi &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cbuqow/initial_tests_rag_with_phi3/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cbuqow/initial_tests_rag_with_phi3/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cbuqow</id><link href="https://www.reddit.com/r/LangChain/comments/1cbuqow/initial_tests_rag_with_phi3/" /><updated>2024-04-24T10:26:44+00:00</updated><published>2024-04-24T10:26:44+00:00</published><title>Initial tests: RAG with Phi-3</title></entry><entry><author><name>/u/supreet02</name><uri>https://www.reddit.com/user/supreet02</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;While RAG is undeniably impressive, the process of creating a functional application with it can be daunting. There&amp;#39;s a significant amount to grasp regarding implementation and development practices, ranging from selecting the appropriate AI models for the specific use case to organizing data effectively to obtain the desired insights. While tools like LangChain and LlamaIndex exist to simplify the prototype design process, there has yet to be an accessible, ready-to-use open-source RAG template that incorporates best practices and offers modular support, allowing anyone to quickly and easily utilize it.&lt;/p&gt; &lt;p&gt;TrueFoundry has recently introduced a new open-source framework called &lt;a href=&quot;https://github.com/truefoundry/cognita&quot;&gt;&lt;strong&gt;Cognita&lt;/strong&gt;&lt;/a&gt;, which utilizes Retriever-Augmented Generation (RAG) technology to simplify the transition by providing robust, scalable solutions for deploying AI applications. AI development often begins in experimental environments such as Jupyter notebooks, which are useful for prototyping but not well-suited for production environments. However, Cognita aims to bridge this gap. Developed on top of Langchain and LlamaIndex, Cognita offers a structured and modular approach to AI application development. Each component of the RAG, from data handling to model deployment, is designed to be modular, API-driven, and extendable.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/supreet02&quot;&gt; /u/supreet02 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cbqzlr/how_to_quickly_build_and_deploy_scalable_rag/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cbqzlr/how_to_quickly_build_and_deploy_scalable_rag/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cbqzlr</id><link href="https://www.reddit.com/r/LangChain/comments/1cbqzlr/how_to_quickly_build_and_deploy_scalable_rag/" /><updated>2024-04-24T06:02:44+00:00</updated><published>2024-04-24T06:02:44+00:00</published><title>How to quickly build and deploy scalable RAG applications?</title></entry><entry><author><name>/u/sarthak_uchiha</name><uri>https://www.reddit.com/user/sarthak_uchiha</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I am working on a project of bill of material where a client has recieved a mail which contains the catalogue I&amp;#39;d the quantity of the catalogue and it&amp;#39;s description,... The data could be in normal text , in a table , or in a image of the body (not in attachments )&lt;/p&gt; &lt;p&gt;How should I tackle this , like image could be many and some irrelevant ones like logo of company and other than there might be possibility that a duplicate data may present in text and image , and how to handle the thread of email &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/sarthak_uchiha&quot;&gt; /u/sarthak_uchiha &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cc3m04/bill_of_material_need_some_pov/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cc3m04/bill_of_material_need_some_pov/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cc3m04</id><link href="https://www.reddit.com/r/LangChain/comments/1cc3m04/bill_of_material_need_some_pov/" /><updated>2024-04-24T17:07:37+00:00</updated><published>2024-04-24T17:07:37+00:00</published><title>Bill of material need some PoV</title></entry><entry><author><name>/u/mofusa16</name><uri>https://www.reddit.com/user/mofusa16</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey Redditors! 🙋‍♂️&lt;/p&gt; &lt;p&gt;I came up with the idea of summarizing text with various large language models (LLMs). I intend to develop this fully-fledged application (including a register page, login page, database etc.) using either Python, JavaScript, or both. Can you advise me on which framework would be most suitable for such an endeavor? I&amp;#39;m seeking recommendations on frameworks that excel in constructing this type of application. Some colleagues have proposed trying Flask, Gradio, or Django. Please share your insights on which framework would be optimal for this project, and kindly provide reasons to support your suggestion.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/mofusa16&quot;&gt; /u/mofusa16 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cbv3dv/seeking_advice_which_framework_is_best_suited_for/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cbv3dv/seeking_advice_which_framework_is_best_suited_for/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cbv3dv</id><link href="https://www.reddit.com/r/LangChain/comments/1cbv3dv/seeking_advice_which_framework_is_best_suited_for/" /><updated>2024-04-24T10:49:36+00:00</updated><published>2024-04-24T10:49:36+00:00</published><title>Seeking Advice: Which Framework is best suited for building GenAI Web App?</title></entry><entry><author><name>/u/UpskillingDS17</name><uri>https://www.reddit.com/user/UpskillingDS17</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi, I am thinking of creating a LLM based application where questions can be asked in excel files and the files are small to medium size less than 10 MB. What is the best way to approach this problem ? In my team there are consultants who have 0 to little background on coding and SQL, so this could be a great help to them. Thanks&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/UpskillingDS17&quot;&gt; /u/UpskillingDS17 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cbwajs/creating_data_analytics_qa_platform_using_llm/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cbwajs/creating_data_analytics_qa_platform_using_llm/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cbwajs</id><link href="https://www.reddit.com/r/LangChain/comments/1cbwajs/creating_data_analytics_qa_platform_using_llm/" /><updated>2024-04-24T11:57:48+00:00</updated><published>2024-04-24T11:57:48+00:00</published><title>Creating data analytics Q&amp;A platform using LLM</title></entry><entry><author><name>/u/AddendumLow4692</name><uri>https://www.reddit.com/user/AddendumLow4692</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi all,&lt;/p&gt; &lt;p&gt;First time using LangChain, I&amp;#39;m following &lt;a href=&quot;https://www.youtube.com/watch?v=BrsocJb-fAo&amp;amp;t=631s&quot;&gt;a guide&lt;/a&gt; and I&amp;#39;m getting this error, does anyone know what might be wrong? I&amp;#39;m using Pinecone along with this, I&amp;#39;m not sure if that makes a difference.&lt;/p&gt; &lt;p&gt;For my Pinecone API environment I&amp;#39;m using &amp;quot;us-east-1&amp;quot; - I&amp;#39;m unsure if this is the right format?&lt;/p&gt; &lt;p&gt;I&amp;#39;d be very grateful for any input!&lt;/p&gt; &lt;p&gt;Many thanks in advance :)&lt;/p&gt; &lt;p&gt;So this is my code:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;from langchain_community.vectorstores import DocArrayInMemorySearch vectorstore1 = DocArrayInMemorySearch.from_texts( [ &amp;quot;Mary&amp;#39;s sister is Susana&amp;quot;, &amp;quot;John and Tommy are brothers&amp;quot;, &amp;quot;Patricia likes white cars&amp;quot;, &amp;quot;Pedro&amp;#39;s mother is a teacher&amp;quot;, &amp;quot;Lucia drives an Audi&amp;quot;, &amp;quot;Mary has two siblings&amp;quot;, ], embedding=embeddings, ) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;And I&amp;#39;m getting this error:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;AttributeError Traceback (most recent call last) Cell In[58], line 3 1 from langchain_community.vectorstores import DocArrayInMemorySearch ----&amp;gt; 3 vectorstore1 = DocArrayInMemorySearch.from_texts( 4 [ 5 &amp;quot;Mary&amp;#39;s sister is Susana&amp;quot;, 6 &amp;quot;John and Tommy are brothers&amp;quot;, 7 &amp;quot;Patricia likes white cars&amp;quot;, 8 &amp;quot;Pedro&amp;#39;s mother is a teacher&amp;quot;, 9 &amp;quot;Lucia drives an Audi&amp;quot;, 10 &amp;quot;Mary has two siblings&amp;quot;, 11 ], 12 embedding=embeddings, 13 ) File ~\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\LocalCache\local-packages\Python312\site-packages\langchain_community\vectorstores\docarray\in_memory.py:68, in DocArrayInMemorySearch.from_texts(cls, texts, embedding, metadatas, **kwargs) 46 u/classmethod 47 def from_texts( 48 cls, (...) 52 **kwargs: Any, 53 ) -&amp;gt; DocArrayInMemorySearch: 54 &amp;quot;&amp;quot;&amp;quot;Create an DocArrayInMemorySearch store and insert data. 55 ... ---&amp;gt; 46 return Generic.__class_getitem__.__func__(cls, item) # type: ignore 47 # this do nothing that checking that item is valid type var or str 48 if not issubclass(item, BaseDoc): AttributeError: &amp;#39;builtin_function_or_method&amp;#39; object has no attribute &amp;#39;__func__&amp;#39; &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/AddendumLow4692&quot;&gt; /u/AddendumLow4692 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cbuikm/error_builtin_function_or_method_object_has_no/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cbuikm/error_builtin_function_or_method_object_has_no/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cbuikm</id><link href="https://www.reddit.com/r/LangChain/comments/1cbuikm/error_builtin_function_or_method_object_has_no/" /><updated>2024-04-24T10:11:57+00:00</updated><published>2024-04-24T10:11:57+00:00</published><title>Error: 'builtin_function_or_method' object has no attribute '__func__'</title></entry><entry><author><name>/u/MintDrake</name><uri>https://www.reddit.com/user/MintDrake</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;&lt;strong&gt;Code of pure implementation through POST to local ollama&lt;/strong&gt; &lt;a href=&quot;http://localhost:11434/api/chat&quot;&gt;&lt;strong&gt;http://localhost:11434/api/chat&lt;/strong&gt;&lt;/a&gt; &lt;strong&gt;(3.2s):&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;import aiohttp from dataclasses import dataclass, field from typing import List import time start_time = time.time() @dataclass class Message: role: str content: str @dataclass class ChatHistory: messages: List[Message] = field(default_factory=list) def add_message(self, message: Message): self.messages.append(message) @dataclass class RequestData: model: str messages: List[dict] stream: bool = False @classmethod def from_params(cls, model, system_message, history): messages = [ {&amp;quot;role&amp;quot;: &amp;quot;system&amp;quot;, &amp;quot;content&amp;quot;: system_message}, *[{&amp;quot;role&amp;quot;: msg.role, &amp;quot;content&amp;quot;: msg.content} for msg in history.messages], ] return cls(model=model, messages=messages, stream=False) class LocalLlm: def __init__(self, model=&amp;#39;llama3:8b&amp;#39;, history=None, system_message=&amp;quot;You are a helpful assistant&amp;quot;): self.model = model self.history = history or ChatHistory() self.system_message = system_message async def ask(self, input=&amp;quot;&amp;quot;): if input: self.history.add_message(Message(role=&amp;quot;user&amp;quot;, content=input)) data = RequestData.from_params(self.model, self.system_message, self.history) url = &amp;quot;http://localhost:11434/api/chat&amp;quot; async with aiohttp.ClientSession() as session: async with session.post(url, json=data.__dict__) as response: result = await response.json() print(result[&amp;quot;message&amp;quot;][&amp;quot;content&amp;quot;]) if result[&amp;quot;done&amp;quot;]: ai_response = result[&amp;quot;message&amp;quot;][&amp;quot;content&amp;quot;] self.history.add_message(Message(role=&amp;quot;assistant&amp;quot;, content=ai_response)) return ai_response else: raise Exception(&amp;quot;Error generating response&amp;quot;) if __name__ == &amp;quot;__main__&amp;quot;: chat_history = ChatHistory(messages=[ Message(role=&amp;quot;system&amp;quot;, content=&amp;quot;You are a crazy pirate&amp;quot;), Message(role=&amp;quot;user&amp;quot;, content=&amp;quot;Can you tell me a joke?&amp;quot;) ]) llm = LocalLlm(history=chat_history) import asyncio response = asyncio.run(llm.ask()) print(response) print(llm.history) print(&amp;quot;--- %s seconds ---&amp;quot; % (time.time() - start_time)) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;code&gt;--- 3.2285749912261963 seconds ---&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Lang chain equivalent (3.5 s):&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;from langchain_core.messages import HumanMessage, SystemMessage, AIMessage, BaseMessage from langchain_community.chat_models.ollama import ChatOllama from langchain.memory import ChatMessageHistory import time start_time = time.time() class LocalLlm: def __init__(self, model=&amp;#39;llama3:8b&amp;#39;, messages=ChatMessageHistory(), system_message=&amp;quot;You are a helpful assistant&amp;quot;, context_length = 8000): self.model = ChatOllama(model=model, system=system_message, num_ctx=context_length) self.history = messages def ask(self, input=&amp;quot;&amp;quot;): if input: self.history.add_user_message(input) response = self.model.invoke(self.history.messages) self.history.add_ai_message(response) return response if __name__ == &amp;quot;__main__&amp;quot;: chat = ChatMessageHistory() chat.add_messages([ SystemMessage(content=&amp;quot;You are a crazy pirate&amp;quot;), HumanMessage(content=&amp;quot;Can you tell me a joke?&amp;quot;) ]) print(chat) llm = LocalLlm(messages=chat) print(llm.ask()) print(llm.history.messages) print(&amp;quot;--- %s seconds ---&amp;quot; % (time.time() - start_time)) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;code&gt;--- 3.469588279724121 seconds ---&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;So it&amp;#39;s 3.2 vs 3.469(nice) so the difference so 0.3s difference is nothing.&lt;br/&gt; Made this post because was so upset over &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c6zktz/llms_frameworks_langchain_llamaindex_griptape/&quot;&gt;this post&lt;/a&gt; after getting to know langchain and finally coming up with some results. I think it&amp;#39;s true that it&amp;#39;s not very suitable for serious development, but it&amp;#39;s perfect for theory crafting and experimenting, but anyways you can just write your own abstractions which you know. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/MintDrake&quot;&gt; /u/MintDrake &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cbj7gg/i_tested_langchain_vs_vanilla_speed/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cbj7gg/i_tested_langchain_vs_vanilla_speed/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cbj7gg</id><link href="https://www.reddit.com/r/LangChain/comments/1cbj7gg/i_tested_langchain_vs_vanilla_speed/" /><updated>2024-04-23T23:19:36+00:00</updated><published>2024-04-23T23:19:36+00:00</published><title>I tested LANGCHAIN vs VANILLA speed</title></entry><entry><author><name>/u/Competitive-Ninja423</name><uri>https://www.reddit.com/user/Competitive-Ninja423</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I am working on project where I have multiple documents to process using output parser of Lang Chain, as I have Mutiple it takes time, so to reduce time I am planning to process each doc in parallel to reduce the time.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Competitive-Ninja423&quot;&gt; /u/Competitive-Ninja423 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cbqwg2/does_anybody_have_good_tutorial_or_page_or_repo/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cbqwg2/does_anybody_have_good_tutorial_or_page_or_repo/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cbqwg2</id><link href="https://www.reddit.com/r/LangChain/comments/1cbqwg2/does_anybody_have_good_tutorial_or_page_or_repo/" /><updated>2024-04-24T05:57:25+00:00</updated><published>2024-04-24T05:57:25+00:00</published><title>Does anybody have good tutorial or page or repo which targets the Runnable Parallels of Lang chain?</title></entry><entry><author><name>/u/Unrealnooob</name><uri>https://www.reddit.com/user/Unrealnooob</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi, So I have a rag application/chatbot, uses conversationalretrivalqa chain from Langchain, say if for questions like &amp;#39;Hi&amp;#39; and all retrieval is happening, and its returning random documents How do I make the llm answer directly without retrieval for questions like this.? And one more thing how do I implement a memory(longterm will be better) with conversationalretrivalqa.from_llm chain..whatever I tried is not working, I tried with the Runnablehistory but that screws up the retrieval Does anyone have any workaround on that.? Any help will be appreciated ,thanks &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Unrealnooob&quot;&gt; /u/Unrealnooob &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cbq1jt/how_to_make_llm_differentiate_whether_to_retrieve/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cbq1jt/how_to_make_llm_differentiate_whether_to_retrieve/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cbq1jt</id><link href="https://www.reddit.com/r/LangChain/comments/1cbq1jt/how_to_make_llm_differentiate_whether_to_retrieve/" /><updated>2024-04-24T05:04:47+00:00</updated><published>2024-04-24T05:04:47+00:00</published><title>How to make llm differentiate whether to retrieve or not</title></entry><entry><author><name>/u/QueRoub</name><uri>https://www.reddit.com/user/QueRoub</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I have built a RAG application with my own PDF documents.&lt;/p&gt; &lt;p&gt;Some of the answers are not correct, usually they are from wrong documents even if the right ones have been retrieved.&lt;/p&gt; &lt;p&gt;What is the right way to approach it?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/QueRoub&quot;&gt; /u/QueRoub &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cbrpms/how_to_finetune_the_answers_of_llm_in_a_rag/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cbrpms/how_to_finetune_the_answers_of_llm_in_a_rag/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cbrpms</id><link href="https://www.reddit.com/r/LangChain/comments/1cbrpms/how_to_finetune_the_answers_of_llm_in_a_rag/" /><updated>2024-04-24T06:50:23+00:00</updated><published>2024-04-24T06:50:23+00:00</published><title>How to fine-tune the answers of LLM in a RAG application</title></entry><entry><author><name>/u/Apart-Damage143</name><uri>https://www.reddit.com/user/Apart-Damage143</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Does PydanticOutputParser only function with ChatGPT from OpenAI, or does it extend its support to other large language models (LLMs) as well? &lt;/p&gt; &lt;p&gt;I&amp;#39;m particularly interested in using it with models available through groq and wondering if anyone has explored this compatibility. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Apart-Damage143&quot;&gt; /u/Apart-Damage143 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cbr66a/question_pydanticoutputparser_compatibility/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cbr66a/question_pydanticoutputparser_compatibility/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cbr66a</id><link href="https://www.reddit.com/r/LangChain/comments/1cbr66a/question_pydanticoutputparser_compatibility/" /><updated>2024-04-24T06:14:44+00:00</updated><published>2024-04-24T06:14:44+00:00</published><title>Question: PydanticOutputParser Compatibility Beyond ChatOpenAI?</title></entry><entry><author><name>/u/phantom69_ftw</name><uri>https://www.reddit.com/user/phantom69_ftw</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Basically if I have a pdf of 100 pages and to answer my question I need 30 diff chunks across diff pages. Now if my top_k is set to 20. How will this ever be possible?&lt;/p&gt; &lt;p&gt;Like in general, isn&amp;#39;t this a issue with RAGs? How can I know how many chunks are needed to answer a question? If it&amp;#39;s less than whatever topk I set, it&amp;#39;s fine. But what if there are more?&lt;/p&gt; &lt;p&gt;Is this a limitation of RAG? If no, how to solve for this? If yes, what other ways can I explore?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/phantom69_ftw&quot;&gt; /u/phantom69_ftw &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cbcln9/how_to_solve_if_relevant_docs_max_top_k/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cbcln9/how_to_solve_if_relevant_docs_max_top_k/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cbcln9</id><link href="https://www.reddit.com/r/LangChain/comments/1cbcln9/how_to_solve_if_relevant_docs_max_top_k/" /><updated>2024-04-23T18:52:55+00:00</updated><published>2024-04-23T18:52:55+00:00</published><title>How to solve if relevant docs &gt; max top_k ?</title></entry><entry><author><name>/u/Dry-Magician1415</name><uri>https://www.reddit.com/user/Dry-Magician1415</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I am working with LangGraph and used the multi agent collaboration example (&lt;a href=&quot;https://github.com/langchain-ai/langgraph/blob/main/examples/multi_agent/multi-agent-collaboration.ipynb&quot;&gt;github&lt;/a&gt;). To the create_agent(...) helper method, I added the following so that the response from LLMs would match a format I can use.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;parser = JsonOutputParser(pydantic_object=MyResponse) ... return prompt | llm.bind_functions(functions) | parser &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This works and provides a python dict that is easy to work with. So far so good.&lt;/p&gt; &lt;p&gt;The issue is when the LLM wants to use the Tavily search tool which is failing at &lt;code&gt;result = agent.invoke(state)&lt;/code&gt; with &lt;strong&gt;OutputParserException(&amp;#39;Invalid json output: &amp;#39;)&lt;/strong&gt; because the llm obviously wants to call Tavily which has its own shape being something like that:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;tavily_search_results_json&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Which obviously doesnt conform to the MyResponse class, but the parser is kicking in anyway.&lt;/p&gt; &lt;p&gt;I guess I can make a Researcher agent which doesnt have the JsonOutputParser and a Worker that doesn&amp;#39;t have the Tavily tool but, I figure there must be a way to get JsonOutputParser to work with tools like Tavily. I mean they can&amp;#39;t just be outright incompatible (i.e. if you have an agent with a parser, it can&amp;#39;t have the tavily tool).&lt;/p&gt; &lt;p&gt;This is my full create_agent function if anybody knows what it should look like in terms of getting the parsers and tools to play nice:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;def create_task_agent(llm, tools, system_message: str): &amp;quot;&amp;quot;&amp;quot;Create an agent.&amp;quot;&amp;quot;&amp;quot; functions = [convert_to_openai_function(t) for t in tools] functions.append(convert_pydantic_to_openai_function(TaskResponse)) prompt = ChatPromptTemplate.from_messages( [ ( &amp;quot;system&amp;quot;, &amp;quot;some message&amp;quot;, ), MessagesPlaceholder(variable_name=&amp;quot;messages&amp;quot;), ] ) parser = JsonOutputParser(pydantic_object=TaskResponse) prompt = prompt.partial(system_message=system_message) prompt = prompt.partial(task_response=&amp;quot;TaskResponse&amp;quot;) prompt = prompt.partial(tool_names=&amp;quot;, &amp;quot;.join([tool.name for tool in tools])) prompt = prompt.partial(format_instructions=parser) return prompt | llm.bind_functions(functions) | parser &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Dry-Magician1415&quot;&gt; /u/Dry-Magician1415 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cbodkf/jsonoutputparser_conflicting_with_tavily/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cbodkf/jsonoutputparser_conflicting_with_tavily/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cbodkf</id><link href="https://www.reddit.com/r/LangChain/comments/1cbodkf/jsonoutputparser_conflicting_with_tavily/" /><updated>2024-04-24T03:30:42+00:00</updated><published>2024-04-24T03:30:42+00:00</published><title>JsonOutputParser conflicting with Tavily</title></entry><entry><author><name>/u/MarkusWeierstrass</name><uri>https://www.reddit.com/user/MarkusWeierstrass</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey all. I&amp;#39;m building a simple chatbot that will let users hold their own documents to use in RAG; basically I just want them to be able to ask questions related to what&amp;#39;s on their own files. I&amp;#39;m using LangChain of course, using postgres with the pgvector extension. &lt;/p&gt; &lt;p&gt;My question is, what&amp;#39;s the most optimized way to design the documents table(s) in order for users to only be able to search their own files? Do you create separate doc tables for each user? Do you filter through metadata or some other technique? Metadata filtering in particular doesn&amp;#39;t look like it&amp;#39;d be too optimized, so I&amp;#39;m just looking into how best to think about storing and retrieving from a vector store for this use case. I don&amp;#39;t want the bot to be able to find the answer in another user&amp;#39;s files.&lt;/p&gt; &lt;p&gt;Or am I just thinking about the whole thing in the wrong way and is there a better way to structure all this? &lt;/p&gt; &lt;p&gt;Thanks a lot in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/MarkusWeierstrass&quot;&gt; /u/MarkusWeierstrass &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cbdk3m/how_to_structure_the_vector_store_and_retrieval/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cbdk3m/how_to_structure_the_vector_store_and_retrieval/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cbdk3m</id><link href="https://www.reddit.com/r/LangChain/comments/1cbdk3m/how_to_structure_the_vector_store_and_retrieval/" /><updated>2024-04-23T19:31:11+00:00</updated><published>2024-04-23T19:31:11+00:00</published><title>How to structure the vector store and retrieval for user files RAG?</title></entry><entry><author><name>/u/ravediamond000</name><uri>https://www.reddit.com/user/ravediamond000</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hello guys,&lt;/p&gt; &lt;p&gt;Just wrote a new blog post explaining Langchain LCEL in a easier manner: &lt;a href=&quot;https://www.metadocs.co/&quot;&gt;link&lt;/a&gt;.&lt;br/&gt; I really love LCEL (feels a little like functional programing right !?) and wanted to try to explain it in a simpler way.&lt;br/&gt; Enjoy.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/ravediamond000&quot;&gt; /u/ravediamond000 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cb01ch/langchain_lcel_explained_the_easy_way/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cb01ch/langchain_lcel_explained_the_easy_way/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cb01ch</id><link href="https://www.reddit.com/r/LangChain/comments/1cb01ch/langchain_lcel_explained_the_easy_way/" /><updated>2024-04-23T09:12:48+00:00</updated><published>2024-04-23T09:12:48+00:00</published><title>Langchain LCEL explained the easy way</title></entry><entry><author><name>/u/StrayyLight</name><uri>https://www.reddit.com/user/StrayyLight</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;m using llamaindex for a multilingual database retriever system and using claude as the provider. I&amp;#39;m interested in integrating external apis( function calling) and knowledge graphs.&lt;/p&gt; &lt;p&gt;Separately it&amp;#39;d also be helpful to have the ability to manage states within a conversation and langgraph seems to meet the criteria.&lt;/p&gt; &lt;p&gt;Should I switch to langchain and rewrite my early stage code? Does langchain function calling work well with Claude? does llamaindex offer langgraph like abilities or good integration with neo4j?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/StrayyLight&quot;&gt; /u/StrayyLight &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cbfeci/langchain_vs_llamaindex/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cbfeci/langchain_vs_llamaindex/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cbfeci</id><link href="https://www.reddit.com/r/LangChain/comments/1cbfeci/langchain_vs_llamaindex/" /><updated>2024-04-23T20:44:00+00:00</updated><published>2024-04-23T20:44:00+00:00</published><title>Langchain vs llamaindex</title></entry><entry><author><name>/u/Money_Mycologist4939</name><uri>https://www.reddit.com/user/Money_Mycologist4939</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cb68b9/langsmith_render_of_retrieved_documents/&quot;&gt; &lt;img src=&quot;https://b.thumbs.redditmedia.com/JmafAij99N282f2vvy_q1p_h8FB_SzZHa5Ipo8uMwPg.jpg&quot; alt=&quot;Langsmith render of retrieved documents&quot; title=&quot;Langsmith render of retrieved documents&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I have been using langsmith for controling the retrieving step of my rag application. And it&amp;#39;s nice because it has a render that format the raw langchain docs in a more readable format. &lt;/p&gt; &lt;p&gt;The problem is that since I changed the format of my langchain docs, adding more metadata, this feature does not work anymore. Do anyone has got any advice on what&amp;#39;s the right format compatible to the render??&lt;/p&gt; &lt;p&gt;now: &lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/7leugfcup8wc1.png?width=1266&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f7738461ab2add7e587aeb9aed415a5b2e1f6c20&quot;&gt;https://preview.redd.it/7leugfcup8wc1.png?width=1266&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f7738461ab2add7e587aeb9aed415a5b2e1f6c20&lt;/a&gt;&lt;/p&gt; &lt;p&gt;BEFORE:&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/8s86iy6xp8wc1.png?width=1287&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=302f1f3b2923f428288c59c29f31327d5d9b6db0&quot;&gt;https://preview.redd.it/8s86iy6xp8wc1.png?width=1287&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=302f1f3b2923f428288c59c29f31327d5d9b6db0&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Money_Mycologist4939&quot;&gt; /u/Money_Mycologist4939 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cb68b9/langsmith_render_of_retrieved_documents/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cb68b9/langsmith_render_of_retrieved_documents/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1cb68b9</id><media:thumbnail url="https://b.thumbs.redditmedia.com/JmafAij99N282f2vvy_q1p_h8FB_SzZHa5Ipo8uMwPg.jpg" /><link href="https://www.reddit.com/r/LangChain/comments/1cb68b9/langsmith_render_of_retrieved_documents/" /><updated>2024-04-23T14:35:42+00:00</updated><published>2024-04-23T14:35:42+00:00</published><title>Langsmith render of retrieved documents</title></entry><entry><author><name>/u/furyacer</name><uri>https://www.reddit.com/user/furyacer</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I am trying to build a chatbot using RAG and LangChain that will update the PDFs based on the user prompt and the pdfs will be stored in a db (chromedb) that will be connected to the chatbot. I&amp;#39;m planning to use OpenAI for chunking and indexing information that will be analyzed by the bot. &lt;/p&gt; &lt;p&gt;It will be helpful if anyone can tell me how to proceed further with this. I have only found projects and repos which focus on QA chatbots so I just want to extend this project to include this functionality.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/furyacer&quot;&gt; /u/furyacer &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cb35fa/chatbot_using_rag_to_update_pdfs/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cb35fa/chatbot_using_rag_to_update_pdfs/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cb35fa</id><link href="https://www.reddit.com/r/LangChain/comments/1cb35fa/chatbot_using_rag_to_update_pdfs/" /><updated>2024-04-23T12:18:55+00:00</updated><published>2024-04-23T12:18:55+00:00</published><title>Chat-bot using RAG to update PDFs</title></entry><entry><author><name>/u/arb_plato</name><uri>https://www.reddit.com/user/arb_plato</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;hi there, so my issue is that i want to preserve the chat history with gemini, and according to need manipulate it, i can do it in google provided sdk, i dont know how to do that in langchain, i want to manupilate chat history and according to need, delete some responces, add new responces from the database.&lt;/p&gt; &lt;p&gt;also, i am only interested in langchain responses (semantic) caching support, if i can do caching without the need of langchain or implementing a rag myself manually, i am all up for that solution!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/arb_plato&quot;&gt; /u/arb_plato &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cbf1au/preserving_the_gemini_state_with_langchain_for/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cbf1au/preserving_the_gemini_state_with_langchain_for/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cbf1au</id><link href="https://www.reddit.com/r/LangChain/comments/1cbf1au/preserving_the_gemini_state_with_langchain_for/" /><updated>2024-04-23T20:29:26+00:00</updated><published>2024-04-23T20:29:26+00:00</published><title>preserving the Gemini state with Langchain for caching responses</title></entry><entry><author><name>/u/ErnteSkunkFest</name><uri>https://www.reddit.com/user/ErnteSkunkFest</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;me and my company are currently building a(nother) RAG system for our customers in the legal sector. &lt;/p&gt; &lt;p&gt;We are performing a keyword (BM25) + vector search and then using Reciprocal Rank Fusion (RRF) algorithm fuse the combined 10 top_k results and feed them into our LLM. &lt;/p&gt; &lt;p&gt;We have also implemented HyDE (hypothetical document embeddings) &lt;a href=&quot;https://github.com/texttron/hyde&quot;&gt;https://github.com/texttron/hyde&lt;/a&gt; to further improve retrieval quality. &lt;/p&gt; &lt;p&gt;Now I read through a few articles on Medium and Reddit and saw a lot of recommendations to use reranking to further improve results. &lt;/p&gt; &lt;p&gt;Does it make sense to again rerank the results of the hybrid search, even though strictly speaking we already are reranking them based on the output of BM25 + vector search? I specifically thought about Cohere rerank here. &lt;/p&gt; &lt;p&gt;Thanks and greets!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/ErnteSkunkFest&quot;&gt; /u/ErnteSkunkFest &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cazrxf/reranking_after_rrfhybrid_search/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cazrxf/reranking_after_rrfhybrid_search/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cazrxf</id><link href="https://www.reddit.com/r/LangChain/comments/1cazrxf/reranking_after_rrfhybrid_search/" /><updated>2024-04-23T08:54:38+00:00</updated><published>2024-04-23T08:54:38+00:00</published><title>Reranking after RRF-Hybrid Search?</title></entry><entry><author><name>/u/Desperate-Energy2694</name><uri>https://www.reddit.com/user/Desperate-Energy2694</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cb4yjd/how_does_chunk_size_relate_to_an_embedding_models/&quot;&gt; &lt;img src=&quot;https://b.thumbs.redditmedia.com/9QB0QmRhJMwVuBBSw9iKfRPwWGMk-YQugUlRLhWvp3c.jpg&quot; alt=&quot;How does chunk size relate to an embedding model's dimension of vectors and max token lenght?&quot; title=&quot;How does chunk size relate to an embedding model's dimension of vectors and max token lenght?&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi, everyone! I&amp;#39;m fairly new to NLP tasks, and I&amp;#39;m currently building a langchain RAG app, and for that I need to do some testings with different chunks sizes.&lt;/p&gt; &lt;p&gt;I&amp;#39;m currently using a large BERT model, and what I&amp;#39;ve been confused about is: What is the relationship between the chunk size chosen to chunk my documents, and the embedding model&amp;#39;s vector dimension (if there is any) or even the max token limit?&lt;/p&gt; &lt;p&gt;I&amp;#39;ve seen a wide range of articles from people testings out chunk sizes from 128 to 2048, but I&amp;#39;ve also read in some places that the original BERT models take 512 tokens max. What does that influence on the way I&amp;#39;m doing things?&lt;/p&gt; &lt;p&gt;I&amp;#39;m creating embeddings like this: Using RecursiveCharacterSplitter with different chunks sizes and len function using my bert model&amp;#39;s tokenizer and finally creating embeddings with HuggingFaceBgeEmbeddings (arbitrary choice, I couldn&amp;#39;t figure out which class to choose) and storing on a vector store.&lt;/p&gt; &lt;p&gt;I iterated over my chunked documents (splitted using the same flow mentioned above) and their sizes nor the amount of tokens gotten from tokenizing it directly are equal to my chunk_size:&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/s9txv1rej8wc1.png?width=1015&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a11725cf9967ec9a04ddad46f8c2bedc587d7ed8&quot;&gt;https://preview.redd.it/s9txv1rej8wc1.png?width=1015&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a11725cf9967ec9a04ddad46f8c2bedc587d7ed8&lt;/a&gt;&lt;/p&gt; &lt;p&gt;But the actual embeddings are generated with dimension 1024 (which is the correct value for vector dimensions for large bert models):&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/9dta4kymj8wc1.png?width=1210&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=31c1787b790d1ecabe5a91c54735b5f0dded62de&quot;&gt;https://preview.redd.it/9dta4kymj8wc1.png?width=1210&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=31c1787b790d1ecabe5a91c54735b5f0dded62de&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The reason I&amp;#39;m asking this is because I&amp;#39;ve been gettting some weird results analyzing results with chunks bigger than 512 (this graph is a comparison of the chunk overlap with fixed chunk size = 1024, where the results don&amp;#39;t really vary even though I&amp;#39;m varying other parameters, like chunk overlap here, which does affect other chunk sizes comparisons)&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/x7qwsfo2g8wc1.png?width=1015&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a3c2273dbe60d07c6974d64802b1b6391398e3d0&quot;&gt;https://preview.redd.it/x7qwsfo2g8wc1.png?width=1015&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a3c2273dbe60d07c6974d64802b1b6391398e3d0&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Desperate-Energy2694&quot;&gt; /u/Desperate-Energy2694 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cb4yjd/how_does_chunk_size_relate_to_an_embedding_models/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cb4yjd/how_does_chunk_size_relate_to_an_embedding_models/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1cb4yjd</id><media:thumbnail url="https://b.thumbs.redditmedia.com/9QB0QmRhJMwVuBBSw9iKfRPwWGMk-YQugUlRLhWvp3c.jpg" /><link href="https://www.reddit.com/r/LangChain/comments/1cb4yjd/how_does_chunk_size_relate_to_an_embedding_models/" /><updated>2024-04-23T13:42:03+00:00</updated><published>2024-04-23T13:42:03+00:00</published><title>How does chunk size relate to an embedding model's dimension of vectors and max token lenght?</title></entry><entry><author><name>/u/phenobarbital_</name><uri>https://www.reddit.com/user/phenobarbital_</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hello guys, I&amp;#39;m new on this of NPL and Langchain, I&amp;#39;m currently working on a chatbot to &amp;quot;talk&amp;quot; to my data, converting pandas dataframes into JSON and every row in dataframe is a document saved into Vector Store (I&amp;#39;m using Milvus as Vector Database). &lt;/p&gt; &lt;p&gt;For questions related to 1 to N (getting one row from many), the similarity search is working as expected and I am achieving good results.&lt;/p&gt; &lt;p&gt;For example, if I asking &amp;quot;where this store is located?&amp;quot; or &amp;quot;how many displays has Store A?&amp;quot; is working, but if I ask something about the entire dataset as &amp;quot;how many displays are overall in US?&amp;quot;, or &amp;quot;how many displays are in California?&amp;quot;, the totalization is related to the &amp;quot;k&amp;quot; passed to the vector retriever:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;retriever = VectorStoreRetriever( vectorstore=vector, search_type=&amp;#39;similarity&amp;#39;, search_kwargs={&amp;quot;k&amp;quot;: 10} ) chain = RetrievalQA.from_chain_type( llm=llm, retriever=retriever, chain_type=&amp;#39;stuff&amp;#39;, verbose=True ) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;I cannot pass a bigger &amp;quot;K&amp;quot; because my LLM rejects it (I&amp;#39;m using Google Gemini-Pro).&lt;/p&gt; &lt;p&gt;There is a way to:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Check if the user&amp;#39;s question involves a quantification.&lt;/li&gt; &lt;li&gt;Executing something like a &amp;quot;Map Reduce&amp;quot; over the entire dataset to return the reduced version of the documents (or documents with question applied).&lt;/li&gt; &lt;li&gt;Passing the reduction to the LLM for getting the final result.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Or if there is a way to making this in Langchain using another type on Chain.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/phenobarbital_&quot;&gt; /u/phenobarbital_ &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cb9mab/getting_totals_and_counts_based_on_the_entire/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cb9mab/getting_totals_and_counts_based_on_the_entire/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cb9mab</id><link href="https://www.reddit.com/r/LangChain/comments/1cb9mab/getting_totals_and_counts_based_on_the_entire/" /><updated>2024-04-23T16:54:06+00:00</updated><published>2024-04-23T16:54:06+00:00</published><title>Getting totals and counts based on the entire dataset with RetrievalQA</title></entry><entry><author><name>/u/AchillesFirstStand</name><uri>https://www.reddit.com/user/AchillesFirstStand</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;This is the excerpt from my code:&lt;br/&gt; &lt;code&gt;chain = prompt_template | ollama | output_parser&lt;/code&gt;&lt;/p&gt; &lt;p&gt;How do I store the output from &lt;code&gt;ollama&lt;/code&gt; as a variable and then pass that output to &lt;code&gt;output_parser&lt;/code&gt;? &lt;/p&gt; &lt;p&gt;I don&amp;#39;t understand how the pipe operator | works. I am asking the ollama model to give me a structured output and I need to be able to debug and see what the output was when the output_parser gives an error.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/AchillesFirstStand&quot;&gt; /u/AchillesFirstStand &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cb7rx7/how_can_i_see_the_input_that_is_passed_to_the/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cb7rx7/how_can_i_see_the_input_that_is_passed_to_the/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cb7rx7</id><link href="https://www.reddit.com/r/LangChain/comments/1cb7rx7/how_can_i_see_the_input_that_is_passed_to_the/" /><updated>2024-04-23T15:40:48+00:00</updated><published>2024-04-23T15:40:48+00:00</published><title>How can I see the input that is passed to the output parser when the commands are chained?</title></entry></feed>