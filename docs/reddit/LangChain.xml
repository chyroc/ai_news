<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/"><category term="LangChain" label="r/LangChain"/><updated>2024-04-07T17:32:39+00:00</updated><icon>https://www.redditstatic.com/icon.png/</icon><id>/r/LangChain.rss</id><link rel="self" href="https://www.reddit.com/r/LangChain.rss" type="application/atom+xml" /><link rel="alternate" href="https://www.reddit.com/r/LangChain" type="text/html" /><subtitle>LangChain is an open-source framework and developer toolkit that helps developers get LLM applications from prototype to production. It is available for Python and Javascript at https://www.langchain.com/.</subtitle><title>LangChain</title><entry><author><name>/u/zchaarm</name><uri>https://www.reddit.com/user/zchaarm</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;A place for members of &lt;a href=&quot;/r/LangChain&quot;&gt;r/LangChain&lt;/a&gt; to chat with each other&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/zchaarm&quot;&gt; /u/zchaarm &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_10ljho9</id><link href="https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/" /><updated>2023-01-26T04:33:05+00:00</updated><published>2023-01-26T04:33:05+00:00</published><title>r/LangChain Lounge</title></entry><entry><author><name>/u/Zealousideal_Wolf624</name><uri>https://www.reddit.com/user/Zealousideal_Wolf624</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I just read their &lt;a href=&quot;https://blog.langchain.dev/langchain-documentation-refresh/&quot;&gt;new blog post&lt;/a&gt;, about the new documentation website. It&amp;#39;s very curious and funny.&lt;/p&gt; &lt;p&gt;It goes through the Diataxis taxonomy for documentation, which I find useful and aligns with how my brain works.&lt;/p&gt; &lt;p&gt;Just to throw everything out of the window and say: we mixed and matched every section of Diataxis and you can find tutorials spread all over the place, mixed with reference and explanations!&lt;/p&gt; &lt;p&gt;Take a look at this section of the post:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;This section should contain mostly conceptual Tutorials, References, and Explanations of the components they cover.&lt;/p&gt; &lt;p&gt;Note: As a general rule of thumb, everything covered in the Expression Language and Components sections (with the exception of the Composition section of components) should cover only components that exist in langchain_core.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Impressive! They need to explain what&amp;#39;s where, and even introduce a rule about langchain_core that is broken from the get go. And when you go to the socs the components section isn&amp;#39;t even in the menu to be selected!&lt;/p&gt; &lt;p&gt;I mean, just make it simple:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Tutorials (quick start, use cases with in depth explanations, etc)&lt;/li&gt; &lt;li&gt;How to guides (terse, context free guides such as how to create a chain, new runnable from scratch, new agent from scratch, how to visualize a chain, how to pass a system prompt to a model, how to make models spit structured output, etc)&lt;/li&gt; &lt;li&gt;Explanation (langchain purpose, package organization, what is LCEL, what is a chain/agent/runnable/etc, model vs chat model, what is a tool/toolkit, what is a function call etc). Accept a small amount of repetition from what we have in tutorials.&lt;/li&gt; &lt;li&gt;Reference (API docs)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Wouldn&amp;#39;t that be simpler? I&amp;#39;m so frustrated with this...&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Zealousideal_Wolf624&quot;&gt; /u/Zealousideal_Wolf624 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1by72bo/new_documentation_is_still_bad/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1by72bo/new_documentation_is_still_bad/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1by72bo</id><link href="https://www.reddit.com/r/LangChain/comments/1by72bo/new_documentation_is_still_bad/" /><updated>2024-04-07T15:24:32+00:00</updated><published>2024-04-07T15:24:32+00:00</published><title>New documentation is still bad</title></entry><entry><author><name>/u/Calm_Pea_2428</name><uri>https://www.reddit.com/user/Calm_Pea_2428</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;m thinking about writing a detailed blog on the Challenges you face while scaling your RAG apps. Please comment some suggestions you would like me to discuss in the blog. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Calm_Pea_2428&quot;&gt; /u/Calm_Pea_2428 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1by7s2m/challenges_of_scaling_rag_applications/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1by7s2m/challenges_of_scaling_rag_applications/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1by7s2m</id><link href="https://www.reddit.com/r/LangChain/comments/1by7s2m/challenges_of_scaling_rag_applications/" /><updated>2024-04-07T15:55:01+00:00</updated><published>2024-04-07T15:55:01+00:00</published><title>Challenges of Scaling RAG applications</title></entry><entry><author><name>/u/UpvoteBeast</name><uri>https://www.reddit.com/user/UpvoteBeast</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;We are trying to get our feet wet with RAG with a small engineering team. I want to build a RAG system querying an extensive internal documents system. With the available choice of LLMs, embedding models, vector databases, hyperparameters it&amp;#39;s easy to get overwhelmed. So what I want is to create a test dataset manually with like ten-twenty questions and answers we would like to receive (or multiple answer options for each question??) and automate deployment of several combinations of different LLMs, hyperparameters, embedding models, etc and compare the actuals against the gold standard answers (using ROUGE score maybe??). Does that make sense? Are there any tools/frameworks I need to be aware of to do something like that for me? Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/UpvoteBeast&quot;&gt; /u/UpvoteBeast &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1by54rq/evaluating_rag_on_custom_qas/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1by54rq/evaluating_rag_on_custom_qas/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1by54rq</id><link href="https://www.reddit.com/r/LangChain/comments/1by54rq/evaluating_rag_on_custom_qas/" /><updated>2024-04-07T13:58:11+00:00</updated><published>2024-04-07T13:58:11+00:00</published><title>Evaluating RAG on custom Q&amp;As</title></entry><entry><author><name>/u/No_Garbage9512</name><uri>https://www.reddit.com/user/No_Garbage9512</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I have been working on the large data. The data consist of talks from the different anchor persons, also there are comments in numerical representation like for example &amp;quot;&lt;em&gt;how many people agreed and how many are disagreed&amp;quot;.&lt;/em&gt; and on which session they are disscusing the point of agenda and on which bill number they place a talk. &lt;/p&gt; &lt;p&gt;So my question is: I want to generate the document a large document which contains multiple sections almost 20 sections. Each section has diverse and different instructions so how do I manage my vectordb calls. ? because each section of the document is different so how to make a calls to retreival automatically based on the sections conditions. ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/No_Garbage9512&quot;&gt; /u/No_Garbage9512 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1by9iqn/working_with_diverse_data_to_create_30_to_35/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1by9iqn/working_with_diverse_data_to_create_30_to_35/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1by9iqn</id><link href="https://www.reddit.com/r/LangChain/comments/1by9iqn/working_with_diverse_data_to_create_30_to_35/" /><updated>2024-04-07T17:08:55+00:00</updated><published>2024-04-07T17:08:55+00:00</published><title>Working with diverse data to create 30 to 35 pages document and Managing the retrieval.</title></entry><entry><author><name>/u/alexndr2022</name><uri>https://www.reddit.com/user/alexndr2022</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;m currently working with AWS Bedrock and Langchain while it retrieves good answers when I want to ask it stuff like comparing documents or listing the documents on its data sources Its unable to do it. It seems like its not conscious of its environment. Does anyone has some experience working with this? Like I want it to be a typical RAG application based on some documents but I want it to be conscious of the data it have like comparing versions of the same documents...&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/alexndr2022&quot;&gt; /u/alexndr2022 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1by8u01/how_to_make_a_rag_conscious_of_the_documents_it/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1by8u01/how_to_make_a_rag_conscious_of_the_documents_it/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1by8u01</id><link href="https://www.reddit.com/r/LangChain/comments/1by8u01/how_to_make_a_rag_conscious_of_the_documents_it/" /><updated>2024-04-07T16:39:39+00:00</updated><published>2024-04-07T16:39:39+00:00</published><title>How to make a RAG conscious of the documents it have? Amazon Bedrock</title></entry><entry><author><name>/u/Crazy_Cut_7250</name><uri>https://www.reddit.com/user/Crazy_Cut_7250</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1by66py/quota_exceeded_error/&quot;&gt; &lt;img src=&quot;https://preview.redd.it/5gxw1qi4l2tc1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=de243bd8dbaaf01ba810158680c37480a924fc4a&quot; alt=&quot;Quota exceeded error&quot; title=&quot;Quota exceeded error&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Crazy_Cut_7250&quot;&gt; /u/Crazy_Cut_7250 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://i.redd.it/5gxw1qi4l2tc1.png&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1by66py/quota_exceeded_error/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1by66py</id><media:thumbnail url="https://preview.redd.it/5gxw1qi4l2tc1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=de243bd8dbaaf01ba810158680c37480a924fc4a" /><link href="https://www.reddit.com/r/LangChain/comments/1by66py/quota_exceeded_error/" /><updated>2024-04-07T14:45:55+00:00</updated><published>2024-04-07T14:45:55+00:00</published><title>Quota exceeded error</title></entry><entry><author><name>/u/redd-dev</name><uri>https://www.reddit.com/user/redd-dev</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey guys, so I am building a chatbot which uses a RAG-tuned LLM in AWS Bedrock (and deployed using AWS Lambda endpoints).&lt;/p&gt; &lt;p&gt;How do I avoid my LLM from being having to be RAG-tuned every single time a user asks his/her first question? I am thinking of storing the RAG-tuned LLM in an AWS S3 bucket. If I do this, I believe I will have to store the LLM model parameters and the vector store index in the S3 bucket. Doing this would mean every single time a user asks his/her first question (and subsequent questions), I will just be loading the the RAG-tuned LLM from the S3 bucket (rather than having to run RAG-tuning every single time when a user asks his/her first question, which will save me RAG-tuning costs and latency).&lt;/p&gt; &lt;p&gt;Would this design work? I have a sample of my script below:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;import os import json import boto3 from langchain.document_loaders import PyPDFLoader from langchain.text_splitter import RecursiveCharacterTextSplitter from langchain.embeddings import BedrockEmbeddings from langchain.vectorstores import FAISS from langchain.indexes import VectorstoreIndexCreator from langchain.llms.bedrock import Bedrock def save_to_s3(model_params, vector_store_index, bucket_name, model_key, index_key): s3 = boto3.client(&amp;#39;s3&amp;#39;) # Save model parameters to S3 s3.put_object(Body=model_params, Bucket=bucket_name, Key=model_key) # Save vector store index to S3 s3.put_object(Body=vector_store_index, Bucket=bucket_name, Key=index_key) def load_from_s3(bucket_name, model_key, index_key): s3 = boto3.client(&amp;#39;s3&amp;#39;) # Load model parameters from S3 model_params = s3.get_object(Bucket=bucket_name, Key=model_key)[&amp;#39;Body&amp;#39;].read() # Load vector store index from S3 vector_store_index = s3.get_object(Bucket=bucket_name, Key=index_key)[&amp;#39;Body&amp;#39;].read() return model_params, vector_store_index def initialize_hr_system(bucket_name, model_key, index_key): s3 = boto3.client(&amp;#39;s3&amp;#39;) try: # Check if model parameters and vector store index exist in S3 s3.head_object(Bucket=bucket_name, Key=model_key) s3.head_object(Bucket=bucket_name, Key=index_key) # Load model parameters and vector store index from S3 model_params, vector_store_index = load_from_s3(bucket_name, model_key, index_key) # Deserialize and reconstruct the RAG-tuned LLM and vector store index llm = Bedrock.deserialize(json.loads(model_params)) index = VectorstoreIndexCreator.deserialize(json.loads(vector_store_index)) except s3.exceptions.ClientError: # Model parameters and vector store index don&amp;#39;t exist in S3 # Create them and save to S3 data_load = PyPDFLoader(&amp;#39;Glossary_of_Terms.pdf&amp;#39;) data_split = RecursiveCharacterTextSplitter(separators=[&amp;quot;\n\n&amp;quot;, &amp;quot;\n&amp;quot;, &amp;quot; &amp;quot;, &amp;quot;&amp;quot;], chunk_size=100, chunk_overlap=10) data_embeddings = BedrockEmbeddings(credentials_profile_name=&amp;#39;default&amp;#39;, model_id=&amp;#39;amazon.titan-embed-text-v1&amp;#39;) data_index = VectorstoreIndexCreator(text_splitter=data_split, embedding=data_embeddings, vectorstore_cls=FAISS) index = data_index.from_loaders([data_load]) llm = Bedrock( credentials_profile_name=&amp;#39;default&amp;#39;, model_id=&amp;#39;mistral.mixtral-8x7b-instruct-v0:1&amp;#39;, model_kwargs={ &amp;quot;max_tokens_to_sample&amp;quot;: 3000, &amp;quot;temperature&amp;quot;: 0.1, &amp;quot;top_p&amp;quot;: 0.9 } ) # Serialize model parameters and vector store index serialized_model_params = json.dumps(llm.serialize()) serialized_vector_store_index = json.dumps(index.serialize()) # Save model parameters and vector store index to S3 save_to_s3(serialized_model_params, serialized_vector_store_index, bucket_name, model_key, index_key) return index, llm def hr_rag_response(index, llm, question): hr_rag_query = index.query(question=question, llm=llm) return hr_rag_query # S3 bucket configuration bucket_name = &amp;#39;your-bucket-name&amp;#39; model_key = &amp;#39;models/chatbot_model.json&amp;#39; index_key = &amp;#39;indexes/chatbot_index.json&amp;#39; # Initialize the system index, llm = initialize_hr_system(bucket_name, model_key, index_key) # Serve user requests while True: user_question = input(&amp;quot;User: &amp;quot;) response = hr_rag_response(index, llm, user_question) print(&amp;quot;Chatbot:&amp;quot;, response) &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/redd-dev&quot;&gt; /u/redd-dev &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1by17qh/how_to_deploy_a_ragtuned_ai_chatbotllm_using_aws/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1by17qh/how_to_deploy_a_ragtuned_ai_chatbotllm_using_aws/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1by17qh</id><link href="https://www.reddit.com/r/LangChain/comments/1by17qh/how_to_deploy_a_ragtuned_ai_chatbotllm_using_aws/" /><updated>2024-04-07T10:23:23+00:00</updated><published>2024-04-07T10:23:23+00:00</published><title>How to deploy a RAG-tuned AI chatbot/LLM using AWS Bedrock (with Langchain functions)</title></entry><entry><author><name>/u/ZuckyFox</name><uri>https://www.reddit.com/user/ZuckyFox</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bxs70y/gemini/&quot;&gt; &lt;img src=&quot;https://external-preview.redd.it/MTd0eWtwZnZueXNjMY4zffG6ljuKgIwhcpPfTvcb1KD0RlSYVhImNFkRFBXF.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a49cb05502dd40e63d0e9349ef7de5ad9cbcbbdd&quot; alt=&quot;Gemini 👍🌚&quot; title=&quot;Gemini 👍🌚&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Never knew 14 yrs ago, Rick astley taught about LangChain through his songs. 🤯😂😂&lt;/p&gt; &lt;h1&gt;aiml #aiforfun #rofl #gemini #google&lt;/h1&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/ZuckyFox&quot;&gt; /u/ZuckyFox &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://v.redd.it/um4exjmvnysc1&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bxs70y/gemini/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1bxs70y</id><media:thumbnail url="https://external-preview.redd.it/MTd0eWtwZnZueXNjMY4zffG6ljuKgIwhcpPfTvcb1KD0RlSYVhImNFkRFBXF.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=a49cb05502dd40e63d0e9349ef7de5ad9cbcbbdd" /><link href="https://www.reddit.com/r/LangChain/comments/1bxs70y/gemini/" /><updated>2024-04-07T01:34:04+00:00</updated><published>2024-04-07T01:34:04+00:00</published><title>Gemini 👍🌚</title></entry><entry><author><name>/u/ramirez_tn</name><uri>https://www.reddit.com/user/ramirez_tn</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;ol&gt; &lt;li&gt;I loaded and split a PDF document using PDFMiner (I also tried a couple of other loaders)&lt;/li&gt; &lt;li&gt;I embedded the result and stored it in VectorDB&lt;/li&gt; &lt;li&gt;I retrieved the Data with RetrievalQA and a question like &amp;quot;What did this document say about Eye safety ?&amp;quot; which is mentioned a couple of times in the 80 pages document&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;The LLM always answers with : &amp;quot;it looks like there nothing mentioned about Eye safety &amp;quot;&lt;/p&gt; &lt;p&gt;FYI: When I check how the PDF is loaded it shows the content related to eye safety in the pages but it has a lot of \n and it include headers. I don&amp;#39;t know if this is contributing to the bad behavior&lt;/p&gt; &lt;p&gt;I am new to Langchain and it is driving me crazy, please help !&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/ramirez_tn&quot;&gt; /u/ramirez_tn &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bxs6g9/retriever_is_not_returning_proper_answers_to/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bxs6g9/retriever_is_not_returning_proper_answers_to/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bxs6g9</id><link href="https://www.reddit.com/r/LangChain/comments/1bxs6g9/retriever_is_not_returning_proper_answers_to/" /><updated>2024-04-07T01:33:13+00:00</updated><published>2024-04-07T01:33:13+00:00</published><title>retriever is not returning proper answers to obvious questions</title></entry><entry><author><name>/u/chaitu9701</name><uri>https://www.reddit.com/user/chaitu9701</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I have one banking related document with several overlapping topics. Say one topic is related to credit card request, another related to cheque book request, another relating to account deactivation request. Mind that each of topic in itself are lengthy.&lt;/p&gt; &lt;p&gt;When in the retrieval chain, I ask a question &amp;quot;how to raise requests&amp;quot;, the result is a mixture from all of the above topics. First few lines describe credit card procedure and then bridge to checkbook. Which is wrong as each process has a different steps.&lt;/p&gt; &lt;p&gt;I&amp;#39;m using chunking strategy of 1000, default sentence transformers embedding, qdrant for as retriever, and gpt3.5 turbo 16k for llm.&lt;/p&gt; &lt;p&gt;Also the llm gives a disclaimer/note at the end saying that steps vary per organisation. Tried several prompts to remove disclaimer but nothing seems to work.&lt;/p&gt; &lt;p&gt;Any help / prompt would be greatly appreciated.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/chaitu9701&quot;&gt; /u/chaitu9701 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bxwxtu/rag_returns_concocted_results/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bxwxtu/rag_returns_concocted_results/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bxwxtu</id><link href="https://www.reddit.com/r/LangChain/comments/1bxwxtu/rag_returns_concocted_results/" /><updated>2024-04-07T05:48:32+00:00</updated><published>2024-04-07T05:48:32+00:00</published><title>RAG returns concocted results</title></entry><entry><author><name>/u/crookedhell</name><uri>https://www.reddit.com/user/crookedhell</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;This is a code snippet from my chatbot model&lt;/p&gt; &lt;pre&gt;&lt;code&gt;def create_embeddings(): embeddings = HuggingFaceEmbeddings(model_name=&amp;#39;sentence-transformers/all-MiniLM-L6-v2&amp;#39;, model_kwargs={&amp;#39;device&amp;#39;: &amp;#39;cuda&amp;#39;}) return embeddings &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Initially I ran it using &amp;#39;device&amp;#39; : &amp;#39;cpu&amp;#39; but the chatbot was extremely slow.&lt;br/&gt; So I installed the cuda toolkit along with nsight. The code gave me a &amp;quot;torch not compiled with cuda enabled&amp;quot; error.&lt;br/&gt; So I uninstalled and reinstalled torch with cuda and the code started working just fine.&lt;br/&gt; But the chatbot was giving outputs as slow as it was earlier, when I checked the task manager, python was still heavily utilizing my cpu and not utilizing the gpu at all.&lt;br/&gt; I have a gtx1650 and this is a code snippet from a chatbot in a virtual environment (all libraries installed there). Am I making a stupid error?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/crookedhell&quot;&gt; /u/crookedhell &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bxmka8/python_not_utilizing_gpu_even_with_cuda_enabled/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bxmka8/python_not_utilizing_gpu_even_with_cuda_enabled/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bxmka8</id><link href="https://www.reddit.com/r/LangChain/comments/1bxmka8/python_not_utilizing_gpu_even_with_cuda_enabled/" /><updated>2024-04-06T21:19:16+00:00</updated><published>2024-04-06T21:19:16+00:00</published><title>Python not utilizing GPU even with CUDA enabled</title></entry><entry><author><name>/u/BrilliantNose2000</name><uri>https://www.reddit.com/user/BrilliantNose2000</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I have a SQL database and want users to be able to query it using English sentences. Currently I have implemented it using a simple NET application, calling OpenAPI. In short:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;User enter a query&lt;/li&gt; &lt;li&gt;The query is converted to SQL using OpenAPI&lt;/li&gt; &lt;li&gt;The query is run towards a SQL database&lt;/li&gt; &lt;li&gt;In case of syntax errors, the application feeds them back to OpenAPI and asks for a fix &lt;/li&gt; &lt;li&gt;The data is shown to the user&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Works maybe 80-90% of the time. &lt;/p&gt; &lt;p&gt;I have been reading about Langchain and watching various tutorials on YouTube but I don&amp;#39;t really get what it is adding.&lt;/p&gt; &lt;p&gt;Cound someone help me understand how Langchain would help implementing the above? Would it add something which I&amp;#39;m just not seeing? The application is very simple so far, about 100 lines of code including presentation. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/BrilliantNose2000&quot;&gt; /u/BrilliantNose2000 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bxm44q/should_i_be_using_langchain/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bxm44q/should_i_be_using_langchain/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bxm44q</id><link href="https://www.reddit.com/r/LangChain/comments/1bxm44q/should_i_be_using_langchain/" /><updated>2024-04-06T21:00:10+00:00</updated><published>2024-04-06T21:00:10+00:00</published><title>Should I be using Langchain?</title></entry><entry><author><name>/u/KarbohJorneKraft</name><uri>https://www.reddit.com/user/KarbohJorneKraft</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;When integrating a Retriever-Augmented Generation (RAG) model with a Large Language Model (LLM) to process documents containing tabular data and embedded decision trees, the goal is to respond to user prompts that necessitate traversing the documents (retrieved by the RAG) and evaluating a decision tree. is anyone working on this? it is non-trivial &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/KarbohJorneKraft&quot;&gt; /u/KarbohJorneKraft &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bxf6bi/using_rag_nlp_llm_for_decision_tree_evaluation/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bxf6bi/using_rag_nlp_llm_for_decision_tree_evaluation/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bxf6bi</id><link href="https://www.reddit.com/r/LangChain/comments/1bxf6bi/using_rag_nlp_llm_for_decision_tree_evaluation/" /><updated>2024-04-06T16:07:23+00:00</updated><published>2024-04-06T16:07:23+00:00</published><title>using RAG, NLP, LLM for decision tree evaluation when embedded in tabular data within otherwise unstructured documents</title></entry><entry><author><name>/u/Silver_Equivalent_58</name><uri>https://www.reddit.com/user/Silver_Equivalent_58</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I have a simple RAG pipeline, but now say the user is not satisfied with the response(basically a thumbs up or down), how can i incorporate this feedback to improve my RAG in a continuous manner? Thanks&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Silver_Equivalent_58&quot;&gt; /u/Silver_Equivalent_58 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bx74lo/how_to_incorporate_user_feedback_in_rag/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bx74lo/how_to_incorporate_user_feedback_in_rag/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bx74lo</id><link href="https://www.reddit.com/r/LangChain/comments/1bx74lo/how_to_incorporate_user_feedback_in_rag/" /><updated>2024-04-06T09:05:31+00:00</updated><published>2024-04-06T09:05:31+00:00</published><title>How to incorporate user feedback in RAG?</title></entry><entry><author><name>/u/vishal2045ks</name><uri>https://www.reddit.com/user/vishal2045ks</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;ol&gt; &lt;li&gt;You have to solve a multi-label classification problem statement.&lt;/li&gt; &lt;li&gt;It contains two files: train.csv and test.csv.&lt;/li&gt; &lt;li&gt;The dataset contains the following columns: &lt;ul&gt; &lt;li&gt;LossDescription: Description of Event&lt;/li&gt; &lt;li&gt;ResultingInjuryDesc: Injury Description&lt;/li&gt; &lt;li&gt;PartInjuredDesc: Body Part Injured Description&lt;/li&gt; &lt;li&gt;Cause - Hierarchy 1: Cause Hierarchy 1&lt;/li&gt; &lt;li&gt;Body Part - Hierarchy 1: Body Part Hierarchy 1&lt;/li&gt; &lt;li&gt;Index: Identifier&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Tasks:&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;Perform exploratory data analysis (EDA) on the dataset.&lt;/li&gt; &lt;li&gt;Train multi-label classification models to predict &amp;quot;Cause - Hierarchy 1&amp;quot; and &amp;quot;Body Part - Hierarchy 1&amp;quot; when other columns are given. Two models will be required to predict each target variable. please tell the approach to solve this problem&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/vishal2045ks&quot;&gt; /u/vishal2045ks &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bxp7p6/need_help_regarding_llm_project/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bxp7p6/need_help_regarding_llm_project/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bxp7p6</id><link href="https://www.reddit.com/r/LangChain/comments/1bxp7p6/need_help_regarding_llm_project/" /><updated>2024-04-06T23:15:00+00:00</updated><published>2024-04-06T23:15:00+00:00</published><title>Need help regarding LLM project</title></entry><entry><author><name>/u/sarthak_uchiha</name><uri>https://www.reddit.com/user/sarthak_uchiha</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I am building a RAG of patients clinical trial and over which preventing prompt injection also , first I am finding some data on clinical trials can some suggest ,where can I get a sample data like that &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/sarthak_uchiha&quot;&gt; /u/sarthak_uchiha &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bx8l2f/need_clinical_trials_dataset/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bx8l2f/need_clinical_trials_dataset/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bx8l2f</id><link href="https://www.reddit.com/r/LangChain/comments/1bx8l2f/need_clinical_trials_dataset/" /><updated>2024-04-06T10:43:46+00:00</updated><published>2024-04-06T10:43:46+00:00</published><title>Need clinical trials dataset</title></entry><entry><author><name>/u/yashdeep1929</name><uri>https://www.reddit.com/user/yashdeep1929</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I have a dataset of 400 resumes in .txt format. I want to build a model that can generate responses to specific candidate queries like &amp;#39;Tell me the skillset of XYZ,&amp;#39; but also handle generic queries like &amp;#39;Tell me the names of people who went to Ivy League schools.&amp;#39; While RAG using OpenAI works well for candidate-specific queries, it struggles with generic ones.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/yashdeep1929&quot;&gt; /u/yashdeep1929 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bwz6op/need_help_regarding_a_llm_project/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bwz6op/need_help_regarding_a_llm_project/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bwz6op</id><link href="https://www.reddit.com/r/LangChain/comments/1bwz6op/need_help_regarding_a_llm_project/" /><updated>2024-04-06T01:28:09+00:00</updated><published>2024-04-06T01:28:09+00:00</published><title>Need help regarding a LLM project</title></entry><entry><author><name>/u/Bubbly-Platypus-8602</name><uri>https://www.reddit.com/user/Bubbly-Platypus-8602</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;here the query of mongodb generated and query queried on mongodb response data is then visualized&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Bubbly-Platypus-8602&quot;&gt; /u/Bubbly-Platypus-8602 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bx3upb/required_open_source_librariespackage_in_python/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bx3upb/required_open_source_librariespackage_in_python/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bx3upb</id><link href="https://www.reddit.com/r/LangChain/comments/1bx3upb/required_open_source_librariespackage_in_python/" /><updated>2024-04-06T05:34:13+00:00</updated><published>2024-04-06T05:34:13+00:00</published><title>Required open source libraries/package in python for visualizing the data fetched from mongodb via prior prompt ,</title></entry><entry><author><name>/u/Important_Hamster171</name><uri>https://www.reddit.com/user/Important_Hamster171</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;m trying to work my way through the &lt;a href=&quot;https://github.com/langchain-ai/langgraph/blob/main/examples/multi_agent/hierarchical_agent_teams.ipynb&quot;&gt;Hierarchical Agent Teams&lt;/a&gt; example in the LangGraph documentation using LMStudio but am getting this error:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;OutputParserException: Could not parse function call: &amp;#39;function_call&amp;#39; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;When I run this block of code:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;for s in research_chain.stream( &amp;quot;when is Taylor Swift&amp;#39;s next tour?&amp;quot;, {&amp;quot;recursion_limit&amp;quot;: 100} ): if &amp;quot;__end__&amp;quot; not in s: print(s) print(&amp;quot;---&amp;quot;) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;function_call comes from this function at the bottom:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;def create_team_supervisor(llm: ChatOpenAI, system_prompt, members) -&amp;gt; str: &amp;quot;&amp;quot;&amp;quot;An LLM-based router.&amp;quot;&amp;quot;&amp;quot; options = [&amp;quot;FINISH&amp;quot;] + members function_def = { &amp;quot;name&amp;quot;: &amp;quot;route&amp;quot;, &amp;quot;description&amp;quot;: &amp;quot;Select the next role.&amp;quot;, &amp;quot;parameters&amp;quot;: { &amp;quot;title&amp;quot;: &amp;quot;routeSchema&amp;quot;, &amp;quot;type&amp;quot;: &amp;quot;object&amp;quot;, &amp;quot;properties&amp;quot;: { &amp;quot;next&amp;quot;: { &amp;quot;title&amp;quot;: &amp;quot;Next&amp;quot;, &amp;quot;anyOf&amp;quot;: [ {&amp;quot;enum&amp;quot;: options}, ], }, }, &amp;quot;required&amp;quot;: [&amp;quot;next&amp;quot;], }, } prompt = ChatPromptTemplate.from_messages( [ (&amp;quot;system&amp;quot;, system_prompt), MessagesPlaceholder(variable_name=&amp;quot;messages&amp;quot;), ( &amp;quot;system&amp;quot;, &amp;quot;Given the conversation above, who should act next?&amp;quot; &amp;quot; Or should we FINISH? Select one of: {options}&amp;quot;, ), ] ).partial(options=str(options), team_members=&amp;quot;, &amp;quot;.join(members)) return ( prompt | llm.bind_functions(functions=[function_def], function_call=&amp;quot;route&amp;quot;) | JsonOutputFunctionsParser() ) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Here&amp;#39;s my chat model:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;llm = ChatOpenAI(temperature=0.0, base_url=&amp;quot;http://localhost:1234/v1&amp;quot;, api_key=&amp;quot;not-needed&amp;quot;, model=&amp;quot;mistral-7b-instruct&amp;quot;) &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Important_Hamster171&quot;&gt; /u/Important_Hamster171 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bwq86t/langgraph_function_call_error_using_lmstudio/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bwq86t/langgraph_function_call_error_using_lmstudio/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bwq86t</id><link href="https://www.reddit.com/r/LangChain/comments/1bwq86t/langgraph_function_call_error_using_lmstudio/" /><updated>2024-04-05T19:07:05+00:00</updated><published>2024-04-05T19:07:05+00:00</published><title>LangGraph 'function_call' error using LMStudio</title></entry><entry><author><name>/u/Transit-Strike</name><uri>https://www.reddit.com/user/Transit-Strike</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Currently I’m training an LLM and that can only handle one input string at a time for summarization. Since it is running with map reduce, it’s also very slow splitting each text into small chunks of size 1024 tokens. I have access four GPUs and my plan is to define a training function that can create 4 GPUs. Load my data structures into batches containing 4 strings. And pass them all to a different GPU I tried that and my GPU utilization for each is still very low. Around 11%, same as last time. However last time each model had partial models loaded onto them. Is map reduce just that slow that it can’t fully utilize the GPU?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Transit-Strike&quot;&gt; /u/Transit-Strike &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bwqhj9/achieving_model_parallelism_and_n_langchain/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bwqhj9/achieving_model_parallelism_and_n_langchain/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bwqhj9</id><link href="https://www.reddit.com/r/LangChain/comments/1bwqhj9/achieving_model_parallelism_and_n_langchain/" /><updated>2024-04-05T19:18:08+00:00</updated><published>2024-04-05T19:18:08+00:00</published><title>Achieving model parallelism and n Langchain</title></entry><entry><author><name>/u/BlindingLT</name><uri>https://www.reddit.com/user/BlindingLT</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I don&amp;#39;t agree with a lot of the hate LangChain gets. I actually quite like LCEL and the core/community distinctions they&amp;#39;ve made. LCEL is also well documented, so if I&amp;#39;m working with LCEL I know I won&amp;#39;t have to deal with outdated docs.&lt;/p&gt; &lt;p&gt;Having said that, I&amp;#39;m trying to understand why there are inconsistencies even in their most bread and butter classes. If I implement &lt;code&gt;ChatOpenAI&lt;/code&gt; and set the api key with &lt;code&gt;api_key&lt;/code&gt;, I would expect that &lt;code&gt;ChatAnthropic&lt;/code&gt; would work the same way. But nope - that&amp;#39;s &lt;code&gt;anthropic_api_key&lt;/code&gt;. If I set the timeout on &lt;code&gt;ChatOpenAI&lt;/code&gt; with &lt;code&gt;request_timeout&lt;/code&gt;, I would assume I could implement it the same way in &lt;code&gt;ChatAnthropic&lt;/code&gt;. But nope, that&amp;#39;s &lt;code&gt;default_request_timeout&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;I know these are minor annoyances, but they bother me because inconsistencies like these should be the first ones addressed, and I&amp;#39;d like to believe they care enough about their core-est of features to be diligent here.&lt;/p&gt; &lt;p&gt;I&amp;#39;m far from the world&amp;#39;s greatest developer, so perhaps there&amp;#39;s a good reason for the inconsistencies and I&amp;#39;m just missing it?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/BlindingLT&quot;&gt; /u/BlindingLT &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bw79ca/dear_langchain_why/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bw79ca/dear_langchain_why/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bw79ca</id><link href="https://www.reddit.com/r/LangChain/comments/1bw79ca/dear_langchain_why/" /><updated>2024-04-05T02:56:37+00:00</updated><published>2024-04-05T02:56:37+00:00</published><title>Dear LangChain, why?</title></entry><entry><author><name>/u/traderprof</name><uri>https://www.reddit.com/user/traderprof</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I did alredy the process, but i cannot register the tool to invoke it&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/traderprof&quot;&gt; /u/traderprof &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bwpi9u/how_to_create_a_custom_tool_in_langchain_i_get/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bwpi9u/how_to_create_a_custom_tool_in_langchain_i_get/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bwpi9u</id><link href="https://www.reddit.com/r/LangChain/comments/1bwpi9u/how_to_create_a_custom_tool_in_langchain_i_get/" /><updated>2024-04-05T18:36:55+00:00</updated><published>2024-04-05T18:36:55+00:00</published><title>How to create a custom tool in LangChain. i get this error. from langchain_core.tools import register_tool, tool ImportError: cannot import name 'register_tool' from 'langchain_core.tools' Does anybody know to solve this?</title></entry><entry><author><name>/u/iclickedca</name><uri>https://www.reddit.com/user/iclickedca</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;e.g. flashrank&amp;#39;s models?&lt;br/&gt; &lt;a href=&quot;https://python.langchain.com/docs/integrations/retrievers/flashrank-reranker/&quot;&gt;https://python.langchain.com/docs/integrations/retrievers/flashrank-reranker/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/iclickedca&quot;&gt; /u/iclickedca &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bwm4is/whats_your_favourite_reranker_any_best_for/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bwm4is/whats_your_favourite_reranker_any_best_for/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bwm4is</id><link href="https://www.reddit.com/r/LangChain/comments/1bwm4is/whats_your_favourite_reranker_any_best_for/" /><updated>2024-04-05T16:18:24+00:00</updated><published>2024-04-05T16:18:24+00:00</published><title>What's your favourite reranker? Any best for reranking chat history?</title></entry><entry><author><name>/u/ZuckyFox</name><uri>https://www.reddit.com/user/ZuckyFox</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bwsgi7/explore_langchain_basics/&quot;&gt; &lt;img src=&quot;https://external-preview.redd.it/627lQQYd7OQMrFcfUEeTy8fr7pvyA-wXJGaqeWyi3k0.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=08d5bb045150177ccd3af9cd17f25ea0a5e58206&quot; alt=&quot;Explore Langchain basics&quot; title=&quot;Explore Langchain basics&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/ZuckyFox&quot;&gt; /u/ZuckyFox &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://youtu.be/veDJ3zKcWd4&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bwsgi7/explore_langchain_basics/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1bwsgi7</id><media:thumbnail url="https://external-preview.redd.it/627lQQYd7OQMrFcfUEeTy8fr7pvyA-wXJGaqeWyi3k0.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=08d5bb045150177ccd3af9cd17f25ea0a5e58206" /><link href="https://www.reddit.com/r/LangChain/comments/1bwsgi7/explore_langchain_basics/" /><updated>2024-04-05T20:39:45+00:00</updated><published>2024-04-05T20:39:45+00:00</published><title>Explore Langchain basics</title></entry><entry><author><name>/u/Adammm101</name><uri>https://www.reddit.com/user/Adammm101</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey all,&lt;/p&gt; &lt;p&gt;has anyone figured out how to calculate the tokens consumption when invoking chains via Langchain when using Claude models?&lt;/p&gt; &lt;p&gt;Sorry if it can be figured out easily, I just wasn&amp;#39;t able to find it.&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;We&amp;#39;re using the ChatAnthropic fn as our llm.&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;THanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Adammm101&quot;&gt; /u/Adammm101 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bwdr62/claude_tokens_consumption/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bwdr62/claude_tokens_consumption/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bwdr62</id><link href="https://www.reddit.com/r/LangChain/comments/1bwdr62/claude_tokens_consumption/" /><updated>2024-04-05T09:29:07+00:00</updated><published>2024-04-05T09:29:07+00:00</published><title>Claude Tokens consumption</title></entry></feed>