<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/"><category term="LangChain" label="r/LangChain"/><updated>2024-01-07T02:17:20+00:00</updated><icon>https://www.redditstatic.com/icon.png/</icon><id>/r/LangChain.rss</id><link rel="self" href="https://www.reddit.com/r/LangChain.rss" type="application/atom+xml" /><link rel="alternate" href="https://www.reddit.com/r/LangChain" type="text/html" /><subtitle>LangChain is an open-source framework and developer toolkit that helps developers get LLM applications from prototype to production. It is available for Python and Javascript at https://www.langchain.com/.</subtitle><title>LangChain</title><entry><author><name>/u/zchaarm</name><uri>https://www.reddit.com/user/zchaarm</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;A place for members of &lt;a href=&quot;/r/LangChain&quot;&gt;r/LangChain&lt;/a&gt; to chat with each other&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/zchaarm&quot;&gt; /u/zchaarm &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_10ljho9</id><link href="https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/" /><updated>2023-01-26T04:33:05+00:00</updated><published>2023-01-26T04:33:05+00:00</published><title>r/LangChain Lounge</title></entry><entry><author><name>/u/greycrab</name><uri>https://www.reddit.com/user/greycrab</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Okay so my question is - if I am making a pdf(or url) summariser app , do I need to use vector DBs to store embeddings? &lt;/p&gt; &lt;p&gt;I have been watching a lot of tutorials of chat to pdf apps and they use vector db to store the chunks - but do I need it since I dont wanna make a conversational app? I just want to take pdf or an url extract the text and summarise it one time only .&lt;/p&gt; &lt;p&gt;I know embeddings are necessary so that we don&amp;#39;t have to send whole large text to llm model - is what I wanna do possible without databases?&lt;/p&gt; &lt;p&gt;Thanks&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/greycrab&quot;&gt; /u/greycrab &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1908cyz/need_help_regarding_vector_dbs/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1908cyz/need_help_regarding_vector_dbs/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1908cyz</id><link href="https://www.reddit.com/r/LangChain/comments/1908cyz/need_help_regarding_vector_dbs/" /><updated>2024-01-06T19:48:44+00:00</updated><published>2024-01-06T19:48:44+00:00</published><title>Need help regarding vector DBs</title></entry><entry><author><name>/u/Glass_Journalist6022</name><uri>https://www.reddit.com/user/Glass_Journalist6022</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey Reddit community! 🚀&lt;/p&gt; &lt;p&gt;Whilst building various things with LLMs I’ve found one of the most time-consuming, annoying things to be writing &amp;amp; optimising prompts, then testing them across different models. That&amp;#39;s why I want to share something I&amp;#39;ve been working on: &lt;a href=&quot;http://composo.ai/&quot;&gt;Composo.ai&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Automated Prompt Writing&lt;/strong&gt;: Wish GPT4 would just write its prompts for itself? Now you can 🙂. Great for those moments when you&amp;#39;re stuck, just starting out or are trying to do the final bit of refining to a well-tuned prompt.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Compare Gemini vs GPT4 &amp;amp; more&lt;/strong&gt;: Want to test how Gemini stacks up against Claude or GPT-4 for a certain prompt? Our platform lets you compare them side by side.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;No sign-up to try it out&lt;/strong&gt;: Jump straight into action at &lt;a href=&quot;http://www.composo.ai/&quot;&gt;www.composo.ai&lt;/a&gt; - no sign-up, no barriers.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Robust testing &amp;amp; evaluation platform too:&lt;/strong&gt; We’re also building a robust testing &amp;amp; evaluation platform for production applications, so you can automate all that subjective ‘testing by vibes’ of your application outputs.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The automated prompt writing is the easiest to get started with, and you can also test those prompts across models without having to sign up too.&lt;/p&gt; &lt;p&gt;Super keen to hear what you think!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Glass_Journalist6022&quot;&gt; /u/Glass_Journalist6022 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/19051ef/automating_prompt_writing_llm_evaluation/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/19051ef/automating_prompt_writing_llm_evaluation/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_19051ef</id><link href="https://www.reddit.com/r/LangChain/comments/19051ef/automating_prompt_writing_llm_evaluation/" /><updated>2024-01-06T17:28:26+00:00</updated><published>2024-01-06T17:28:26+00:00</published><title>Automating prompt writing &amp; LLM evaluation</title></entry><entry><author><name>/u/leandrosq</name><uri>https://www.reddit.com/user/leandrosq</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey folks,&lt;/p&gt; &lt;p&gt;So I am new to LLMs but as a learning I want to rewrite the ending of Game of Thrones using a LLM.&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;So far here&amp;#39;s what I got:&lt;/p&gt; &lt;p&gt;- Made my own dataset with the scripts for each episode on all seasons. (I was going to use the books... but they are just too much text for the LLM)&lt;/p&gt; &lt;p&gt;- Simple inference using LLamaCpp, Mistral and Langchain&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;But I can&amp;#39;t feed all the episodes into one prompt, as it exceeds the token window size. I played with document splitting and Chroma as a Vector database, but it does not solve the problem.&lt;/p&gt; &lt;p&gt;So how can I proceed to rewrite it?&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;My code so far:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;# %% !pip install langchain langchain-community chromadb # %% !CMAKE_ARGS=&amp;quot;-DLLAMA_METAL=on&amp;quot; FORCE_CMAKE=1 pip install --upgrade --force-reinstall llama-cpp-python # %% from langchain.callbacks.manager import CallbackManager from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler from langchain.chains import LLMChain from langchain.prompts import PromptTemplate from langchain.prompts.few_shot import FewShotPromptTemplate from langchain_community.llms import LlamaCpp # %% import os examples = [] # Read .txt files under &amp;quot;./dataset/&amp;quot; DATASET_PATH = &amp;quot;./scripts/&amp;quot; files = os.listdir(DATASET_PATH) for file in files: if file.endswith(&amp;quot;.txt&amp;quot;): with open(os.path.join(DATASET_PATH, file), &amp;quot;r&amp;quot;) as f: content = f.read() title = content[:content.find(&amp;quot;\n&amp;quot;)] # Start content on the third line content = content[content.find(&amp;quot;\n&amp;quot;, content.find(&amp;quot;\n&amp;quot;) + 1) + 1 :] season = file[: file.find(&amp;quot;x&amp;quot;)] episode = file[file.find(&amp;quot;x&amp;quot;) + 1 : file.find(&amp;quot;.&amp;quot;)] examples.append({ &amp;quot;title&amp;quot;: title, &amp;quot;content&amp;quot;: content, &amp;quot;season&amp;quot;: season, &amp;quot;episode&amp;quot;: episode }) # examples.append([&amp;quot;title: &amp;quot; + title, &amp;quot;book content: &amp;quot; + content, &amp;quot;season: &amp;quot; + season, &amp;quot;episode: &amp;quot; + episode]) print(examples[0]) # %% from langchain.schema import Document # prompt = &amp;quot;Given the following scripts, rewrite the final two seasons so that the plots are more consistent with the seasons before.&amp;quot; # source = &amp;quot;&amp;quot; # currentSeason = 0 # for example in examples: # if example[&amp;quot;season&amp;quot;] != currentSeason: # if currentSeason != 0: # source += &amp;quot;&amp;lt;END OF SEASON&amp;gt;\n&amp;quot; # source += &amp;quot;\nSEASON &amp;quot; + str(example[&amp;quot;season&amp;quot;]) + &amp;quot;\n&amp;quot; # currentSeason = example[&amp;quot;season&amp;quot;] # source += example[&amp;quot;content&amp;quot;] + &amp;quot;&amp;lt;END OF EPISODE&amp;gt;\n&amp;quot; documents = [] for example in examples: documents.append( Document( page_content=example[&amp;quot;content&amp;quot;], metadata={ &amp;quot;title&amp;quot;: example[&amp;quot;title&amp;quot;], &amp;quot;season&amp;quot;: example[&amp;quot;season&amp;quot;], &amp;quot;episode&amp;quot;: example[&amp;quot;episode&amp;quot;] } ) ) # %% # from llama_cpp import Llama # model = &amp;quot;./mistral-7b-v0.1.Q6_K.gguf&amp;quot; # llm = Llama( # model_path=model, # n_ctx=8196, # n_batch=512, # n_threads=7, # n_gpu_layers=2, # verbose=True, # seed=42 # ) # message = f&amp;quot;&amp;lt;s&amp;gt;[INST] {prompt} [/INST]&amp;lt;/s&amp;gt;{source}&amp;quot; # # output = llm(message, echo=True, stream=True, max_tokens=4096) # stream = llm.create_completion( # message, # stream=True, # echo=True, # repeat_penalty=1.1, # max_tokens=4096, # stop=[&amp;quot;&amp;lt;END OF SEASON&amp;gt;&amp;quot;], #) # for output in stream: # print(output[&amp;quot;choices&amp;quot;][0][&amp;quot;text&amp;quot;].replace(message, &amp;quot;&amp;quot;)) # print(output[&amp;quot;usage&amp;quot;]) # output = output[&amp;quot;choices&amp;quot;][0][&amp;quot;text&amp;quot;].replace(message, &amp;quot;&amp;quot;) # print(output) # %% callbackManager = CallbackManager([StreamingStdOutCallbackHandler()]) llm = LlamaCpp( model_path=&amp;quot;./mistral-7b-v0.1.Q6_K.gguf&amp;quot;, n_ctx=8196, n_batch=512, n_threads=7, n_gpu_layers=1, f16_kv=True, verbose=False, seed=42, callback_manager=callbackManager ) pmt = PromptTemplate( template=&amp;quot;Question: {input}\nAnswer: Hello!&amp;lt;&amp;quot;, input_variables=[&amp;quot;input&amp;quot;] ) llm_chain = LLMChain(llm=llm, prompt=pmt) # %% from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter chunkSize = 4096 chunkOverlap = 4 splitter = RecursiveCharacterTextSplitter( chunk_size=chunkSize, chunk_overlap=chunkOverlap ) docs = splitter.split_documents(documents) print(f&amp;quot;from {len(documents)} to {len(docs)}&amp;quot;) # %% from langchain.vectorstores import Chroma from langchain_community.embeddings import LlamaCppEmbeddings storagePath = &amp;quot;./storage/chroma&amp;quot; if not os.path.exists(storagePath): os.makedirs(storagePath) embeddings = LlamaCppEmbeddings( model_path=&amp;quot;./mistral-7b-v0.1.Q6_K.gguf&amp;quot;, n_ctx=8196, n_batch=512, n_threads=7, n_gpu_layers=1, f16_kv=True, verbose=False, seed=42 ) # %% db = Chroma.from_documents( documents=docs, embedding=embeddings, persist_directory=storagePath, ) # %% %%capture captured --no-stdout llm_chain.run(&amp;quot;Hey, what&amp;#39;s up?&amp;quot;) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/leandrosq&quot;&gt; /u/leandrosq &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18zpl3w/book_rewrite_with_large_context_window/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18zpl3w/book_rewrite_with_large_context_window/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18zpl3w</id><link href="https://www.reddit.com/r/LangChain/comments/18zpl3w/book_rewrite_with_large_context_window/" /><updated>2024-01-06T02:55:24+00:00</updated><published>2024-01-06T02:55:24+00:00</published><title>Book rewrite with large context window</title></entry><entry><author><name>/u/Adventurous_Key_5341</name><uri>https://www.reddit.com/user/Adventurous_Key_5341</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Let&amp;#39;s say I have a CSV file with 100,000 comments related to a product. I would like to summarize all the comments and provide some recommendations on how to improve the product. &lt;/p&gt; &lt;p&gt;What is the best way to summarize this? Using RAG or chunking and feeding the entire list of comments to the LLM to process?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Adventurous_Key_5341&quot;&gt; /u/Adventurous_Key_5341 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18z51mu/rag_vs_batch_processing_in_large_document/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18z51mu/rag_vs_batch_processing_in_large_document/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18z51mu</id><link href="https://www.reddit.com/r/LangChain/comments/18z51mu/rag_vs_batch_processing_in_large_document/" /><updated>2024-01-05T11:46:34+00:00</updated><published>2024-01-05T11:46:34+00:00</published><title>RAG Vs. Batch Processing in Large Document Summarization</title></entry><entry><author><name>/u/tatyanaaaaaa</name><uri>https://www.reddit.com/user/tatyanaaaaaa</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18z5pch/endtoend_observability_for_langchain_script/&quot;&gt; &lt;img src=&quot;https://external-preview.redd.it/mO85YzUfer9KVyiJT_eCFGAbSyXAkrbUsykXu0xdDqk.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bced1e137c9eeaf99b33937b2bda15adac7124f4&quot; alt=&quot;End-to-end observability for LangChain script&quot; title=&quot;End-to-end observability for LangChain script&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Prompt: &amp;quot;What are the number of parameters in GPT5 and GPT4? What is the logarithm (base e) of the difference between the number of parameters?&amp;quot;&lt;/p&gt; &lt;p&gt;AimOS providing essential information about the trace:&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/u4walgoh5mac1.jpg?width=2880&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=ecdef81eb609552d333134a962397b4b483761dd&quot;&gt;AimOS&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Exploring the process: Steps tab provides a detailed walkthrough of the sequence of actions undertaken throughout the pipeline to achieve the specified goal. &lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/ju843lmi4mac1.jpg?width=2880&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=6c7c44b724f584d1f70bdf0027a2a17cbbdc9418&quot;&gt;AimOS&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The final answer to our question: The logarithm (base e) of the difference between the number of parameters in GPT5 and GPT4 is 4.605170185988092.&lt;/p&gt; &lt;p&gt;Cost tab including three graphs showing token-usage, token-usage-input, and token-usage-output, providing a detailed breakdown of the computational costs associated LangChain activities.&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/w3xgkjcv5mac1.jpg?width=2880&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=7e9460d625fd1a2c30291ba599ccd0ed2b33d1ad&quot;&gt;AimOS&lt;/a&gt;&lt;/p&gt; &lt;p&gt;AimOS has a Debugger for LangChain that logs LLMs prompts and generations, tools inputs/outputs, and chains metadata. &lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/aimhubio/aimos&quot;&gt;https://github.com/aimhubio/aimos&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/tatyanaaaaaa&quot;&gt; /u/tatyanaaaaaa &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18z5pch/endtoend_observability_for_langchain_script/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18z5pch/endtoend_observability_for_langchain_script/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_18z5pch</id><media:thumbnail url="https://external-preview.redd.it/mO85YzUfer9KVyiJT_eCFGAbSyXAkrbUsykXu0xdDqk.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=bced1e137c9eeaf99b33937b2bda15adac7124f4" /><link href="https://www.reddit.com/r/LangChain/comments/18z5pch/endtoend_observability_for_langchain_script/" /><updated>2024-01-05T12:24:47+00:00</updated><published>2024-01-05T12:24:47+00:00</published><title>End-to-end observability for LangChain script</title></entry><entry><author><name>/u/sarthak_uchiha</name><uri>https://www.reddit.com/user/sarthak_uchiha</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Is there any library or any way which helps in extracting pdf containing complex tables data and store , and how can we chunk that pdf data such that table data preserves in vector db ? Assuming each pdf contains around 5-10 pages&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/sarthak_uchiha&quot;&gt; /u/sarthak_uchiha &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18yxacm/extracting_data_from_pdf_containing_complex_tables/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18yxacm/extracting_data_from_pdf_containing_complex_tables/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18yxacm</id><link href="https://www.reddit.com/r/LangChain/comments/18yxacm/extracting_data_from_pdf_containing_complex_tables/" /><updated>2024-01-05T03:54:45+00:00</updated><published>2024-01-05T03:54:45+00:00</published><title>Extracting data from pdf containing complex tables</title></entry><entry><author><name>/u/Useful_Ad_7882</name><uri>https://www.reddit.com/user/Useful_Ad_7882</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;While it is easy to create streamlit/hosted apps using vector databases; i am looking to create a solution which ensures that user data [including vector database information] never leaves user device, leading to utmost privacy [unless search results for a RAG solution are sent to an LLM]&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;Anyone has had luck running chromaDB on mobile ? or any other vector databases that would work accordingly ? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Useful_Ad_7882&quot;&gt; /u/Useful_Ad_7882 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18yxpdg/chromadb_or_any_vector_database_for_mobile_devices/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18yxpdg/chromadb_or_any_vector_database_for_mobile_devices/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18yxpdg</id><link href="https://www.reddit.com/r/LangChain/comments/18yxpdg/chromadb_or_any_vector_database_for_mobile_devices/" /><updated>2024-01-05T04:15:53+00:00</updated><published>2024-01-05T04:15:53+00:00</published><title>ChromaDB or any vector database for mobile devices</title></entry><entry><author><name>/u/SustainedSuspense</name><uri>https://www.reddit.com/user/SustainedSuspense</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I want an LLM like GPT to be the brain of my automated workflow. I would define an SOP (standard operating procedure) for how to handle a transaction from start to finish. The transaction will be added to a Postgres DB and trigger an event via Amazon SNS, at which point id like my LangChain app to start processing the transaction (which has a JSON structure). There will be points in the processing of this transaction where API requests will need to be made based on certain conditions in the data. Ill need a way to store the current state of the transaction to know what should happen next (or possibly have the transaction in the db be the source of truth). There will be a step where ill need to send reminder emails to users once a day until they complete an action. &lt;/p&gt; &lt;p&gt;Can LangChain handle most of the thinking in my automated workflow? Eg “based on the current state of the transaction you should probably do X next.”&lt;/p&gt; &lt;p&gt;Any tutorials or reading you could recommend would be helpful. Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/SustainedSuspense&quot;&gt; /u/SustainedSuspense &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18ypaes/would_i_use_langchain_for_this/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18ypaes/would_i_use_langchain_for_this/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18ypaes</id><link href="https://www.reddit.com/r/LangChain/comments/18ypaes/would_i_use_langchain_for_this/" /><updated>2024-01-04T22:01:45+00:00</updated><published>2024-01-04T22:01:45+00:00</published><title>Would i use LangChain for this?</title></entry><entry><author><name>/u/modularmindapp</name><uri>https://www.reddit.com/user/modularmindapp</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18z5q0l/automate_marketing_content_generation_effortlessly/&quot;&gt; &lt;img src=&quot;https://external-preview.redd.it/MDllbzZ0bHY2bWFjMRVzvsqQW8F5a8H9mQhX4DL55qVzfrzQ7DPsaV4qySwB.png?width=140&amp;amp;height=140&amp;amp;crop=140:140,smart&amp;amp;format=jpg&amp;amp;v=enabled&amp;amp;lthumb=true&amp;amp;s=048153d7b3cff3ebe75b083a63527fd3b2acc727&quot; alt=&quot;Automate marketing content generation effortlessly&quot; title=&quot;Automate marketing content generation effortlessly&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/modularmindapp&quot;&gt; /u/modularmindapp &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://v.redd.it/5h7apd4t6mac1&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18z5q0l/automate_marketing_content_generation_effortlessly/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_18z5q0l</id><media:thumbnail url="https://external-preview.redd.it/MDllbzZ0bHY2bWFjMRVzvsqQW8F5a8H9mQhX4DL55qVzfrzQ7DPsaV4qySwB.png?width=140&amp;height=140&amp;crop=140:140,smart&amp;format=jpg&amp;v=enabled&amp;lthumb=true&amp;s=048153d7b3cff3ebe75b083a63527fd3b2acc727" /><link href="https://www.reddit.com/r/LangChain/comments/18z5q0l/automate_marketing_content_generation_effortlessly/" /><updated>2024-01-05T12:25:47+00:00</updated><published>2024-01-05T12:25:47+00:00</published><title>Automate marketing content generation effortlessly</title></entry><entry><author><name>/u/NickWang_</name><uri>https://www.reddit.com/user/NickWang_</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I have been developing based on langchain for a period of time recently, and later I came into contact with the service langsmith provided by it. The overall feeling is that the trace capability is quite powerful, but the test set management is not convenient to use. What did you use during the testing process? Test methods or dataset management tools.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/NickWang_&quot;&gt; /u/NickWang_ &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18yws8j/how_do_you_test_and_manage_use_cases_based_on_the/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18yws8j/how_do_you_test_and_manage_use_cases_based_on_the/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18yws8j</id><link href="https://www.reddit.com/r/LangChain/comments/18yws8j/how_do_you_test_and_manage_use_cases_based_on_the/" /><updated>2024-01-05T03:29:00+00:00</updated><published>2024-01-05T03:29:00+00:00</published><title>How do you test and manage use cases based on the langchain framework?</title></entry><entry><author><name>/u/InternationalMail954</name><uri>https://www.reddit.com/user/InternationalMail954</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi everyone I am trying to use langchain to reference and answer questions about documents and keep memory during the conversation about my document questions. I keep running into issues where it only references the documents in my Pinecone data base but has no conversation memory. Any help would be greatly appreciated. Code below:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;import streamlit as st from io import StringIO import pinecone pinecone.init(api_key=&amp;quot;&amp;quot;, environment=&amp;quot;gcp-starter&amp;quot;) from langchain import PromptTemplate from langchain.chat_models import ChatOpenAI from langchain.chains import LLMChain from langchain.chains import RetrievalQA from langchain.embeddings import OpenAIEmbeddings from langchain.vectorstores import Pinecone from langchain.text_splitter import RecursiveCharacterTextSplitter from langchain.memory import ConversationBufferMemory OPENAI_API_KEY = &amp;quot;&amp;quot; OPENAI_DIMENSION = 1536 embedding = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY) st.reupload_file = False vector_store = Pinecone.from_existing_index(&amp;quot;legal-cases&amp;quot;, embedding) index = pinecone.Index(&amp;#39;legal-cases&amp;#39;) if not hasattr(st.session_state, &amp;quot;convo_memory&amp;quot;): st.session_state.convo_memory = ConversationBufferMemory(return_messages=True) def upload_new_file_to_pinecone(text): embedding = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY) text_splitter = RecursiveCharacterTextSplitter( chunk_size=100, chunk_overlap=0, length_function=len ) chunks = text_splitter.create_documents([text]) result = Pinecone.from_documents(chunks, embedding, index_name=&amp;quot;legal-cases&amp;quot;) st.title(&amp;quot;📝 File Q&amp;amp;A&amp;quot;) uploaded_file = st.file_uploader(&amp;quot;Upload an article&amp;quot;, type=(&amp;quot;txt&amp;quot;, &amp;quot;md&amp;quot;)) if st.reupload_file: print(&amp;quot;updated file&amp;quot;) # To read file as bytes: bytes_data = uploaded_file.getvalue() # To convert to a string based IO: stringio = StringIO(uploaded_file.getvalue().decode(&amp;quot;utf-8&amp;quot;)) # To read file as string: string_data = stringio.read() index.delete(delete_all=True) upload_new_file_to_pinecone(string_data) def ask_and_get_answer(vector_store, q, k=3): from langchain.chains import RetrievalQA from langchain.chat_models import ChatOpenAI llm = ChatOpenAI(model=&amp;#39;gpt-3.5-turbo&amp;#39;, temperature=1, api_key=OPENAI_API_KEY) retriever = vector_store.as_retriever(search_type=&amp;#39;similarity&amp;#39;, search_kwargs={&amp;#39;k&amp;#39;: k}) chain = RetrievalQA.from_chain_type(llm=llm, chain_type=&amp;quot;stuff&amp;quot;, retriever=retriever, memory=st.session_state.convo_memory, ) answer = chain.run(q) return answer st.session_state.vs = vector_store k = 3 # user&amp;#39;s question text input widget q = st.text_input(&amp;#39;Ask a question about the content of your file:&amp;#39;) if q: # if the user entered a question and hit enter standard_answer = &amp;quot;&amp;quot; q = f&amp;quot;{q} {standard_answer}&amp;quot; if &amp;#39;vs&amp;#39; in st.session_state: # if there&amp;#39;s the vector store (user uploaded, split and embedded a file) vector_store = st.session_state.vs st.write(f&amp;#39;k: {k}&amp;#39;) answer = ask_and_get_answer(vector_store, q, k) # text area widget for the LLM answer st.text_area(&amp;#39;LLM Answer: &amp;#39;, value=answer) st.divider() # if there&amp;#39;s no chat history in the session state, create it if &amp;#39;history&amp;#39; not in st.session_state: st.session_state.history = &amp;#39;&amp;#39; # the current question and answer value = f&amp;#39;Q: {q} \nA: {answer}&amp;#39; st.session_state.history = f&amp;#39;{value} \n {&amp;quot;-&amp;quot; * 100} \n {st.session_state.history}&amp;#39; h = st.session_state.history # text area widget for the chat history st.text_area(label=&amp;#39;Chat History&amp;#39;, value=h, key=&amp;#39;history&amp;#39;, height=400) print(st.session_state.convo_memory) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/InternationalMail954&quot;&gt; /u/InternationalMail954 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18yovcm/please_help_with_langchain_want_both_document/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18yovcm/please_help_with_langchain_want_both_document/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18yovcm</id><link href="https://www.reddit.com/r/LangChain/comments/18yovcm/please_help_with_langchain_want_both_document/" /><updated>2024-01-04T21:44:41+00:00</updated><published>2024-01-04T21:44:41+00:00</published><title>Please help with langchain, want both document retrieval and conversation memory</title></entry><entry><author><name>/u/Ill_Bodybuilder3499</name><uri>https://www.reddit.com/user/Ill_Bodybuilder3499</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;I have built a RAG App with Langchain and used the &lt;a href=&quot;https://huggingface.co/intfloat/multilingual-e5-large&quot;&gt;intfloat/multilingual-e5-large&lt;/a&gt; embeddings so far. At the moment I tried oout different chunk sizes (100-2000) and I am wondering if the embedding model is relevant for the chunk size? I was wondering because I saw that the embedding size is 1024.&lt;/p&gt; &lt;p&gt;Thanks for suggestions!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Ill_Bodybuilder3499&quot;&gt; /u/Ill_Bodybuilder3499 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18yd77i/rag_embedding_model_relevant_to_chunk_size/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18yd77i/rag_embedding_model_relevant_to_chunk_size/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18yd77i</id><link href="https://www.reddit.com/r/LangChain/comments/18yd77i/rag_embedding_model_relevant_to_chunk_size/" /><updated>2024-01-04T13:29:22+00:00</updated><published>2024-01-04T13:29:22+00:00</published><title>RAG Embedding Model relevant to Chunk Size?</title></entry><entry><author><name>/u/HiddenMushroom11</name><uri>https://www.reddit.com/user/HiddenMushroom11</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I get responses from my model, but only if I don&amp;#39;t use the streaming=True parameter. I can also stream the model directly from my local server when I use curl, but not when I use langchain.&lt;/p&gt; &lt;p&gt;Context to the issue here with code example:&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/langchain-ai/langchain/issues/15516&quot;&gt;https://github.com/langchain-ai/langchain/issues/15516&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/HiddenMushroom11&quot;&gt; /u/HiddenMushroom11 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18yfh3p/langchain_streaming_is_broken_with_local_hugging/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18yfh3p/langchain_streaming_is_broken_with_local_hugging/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18yfh3p</id><link href="https://www.reddit.com/r/LangChain/comments/18yfh3p/langchain_streaming_is_broken_with_local_hugging/" /><updated>2024-01-04T15:15:45+00:00</updated><published>2024-01-04T15:15:45+00:00</published><title>Langchain streaming is broken with Local Hugging Face models</title></entry><entry><author><name>/u/Manasi1208</name><uri>https://www.reddit.com/user/Manasi1208</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;&lt;a href=&quot;https://github.com/ManasiTilak/TweetGen&quot;&gt;https://github.com/ManasiTilak/TweetGen&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Manasi1208&quot;&gt; /u/Manasi1208 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18yj9j5/tried_to_make_a_tweet_generator_using_openai_and/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18yj9j5/tried_to_make_a_tweet_generator_using_openai_and/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18yj9j5</id><link href="https://www.reddit.com/r/LangChain/comments/18yj9j5/tried_to_make_a_tweet_generator_using_openai_and/" /><updated>2024-01-04T17:55:48+00:00</updated><published>2024-01-04T17:55:48+00:00</published><title>Tried to make a tweet generator using openai and chatgpt. Please check it and add suggestions. I want to automate these tweets and tweet them at certain times.</title></entry><entry><author><name>/u/No-Tailor-6633</name><uri>https://www.reddit.com/user/No-Tailor-6633</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi,&lt;br/&gt; Wrote an article on how to autonomously perform semantic chunking for a wide variety of documents. Please give it a read : &lt;a href=&quot;https://medium.com/@boudhayan-dev/semantic-chunking-in-practice-23a8bc33d56d&quot;&gt;Semantic chunking using LLM&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/No-Tailor-6633&quot;&gt; /u/No-Tailor-6633 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18y44fp/semantic_chunking/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18y44fp/semantic_chunking/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18y44fp</id><link href="https://www.reddit.com/r/LangChain/comments/18y44fp/semantic_chunking/" /><updated>2024-01-04T04:23:20+00:00</updated><published>2024-01-04T04:23:20+00:00</published><title>Semantic chunking</title></entry><entry><author><name>/u/Ill_Bodybuilder3499</name><uri>https://www.reddit.com/user/Ill_Bodybuilder3499</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;I have put my application into a Docker and therefore I have created a requirements.txt file. Now I need to install &amp;quot;llama-cpp-python&amp;quot; for Mac, as I am loading my LLM with &lt;code&gt;from langchain.llms import LlamaCpp&lt;/code&gt;. &lt;/p&gt; &lt;p&gt;My installation command specifically for Mac is: &amp;quot;&lt;code&gt;CMAKE_ARGS=&amp;quot;-DLLAMA_METAL=on&amp;quot; FORCE_CMAKE=1 pip install llama-cpp-python&lt;/code&gt;&amp;quot;, but it does not work if I put this in my &amp;quot;requirements.txt&amp;quot; file.&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;How can I do this?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Ill_Bodybuilder3499&quot;&gt; /u/Ill_Bodybuilder3499 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18ybano/install_llamacpppython_in_requirementstxt_file/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18ybano/install_llamacpppython_in_requirementstxt_file/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18ybano</id><link href="https://www.reddit.com/r/LangChain/comments/18ybano/install_llamacpppython_in_requirementstxt_file/" /><updated>2024-01-04T11:47:53+00:00</updated><published>2024-01-04T11:47:53+00:00</published><title>Install &quot;llama-cpp-python&quot; in requirements.txt file</title></entry><entry><author><name>/u/silent-spiral</name><uri>https://www.reddit.com/user/silent-spiral</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;How to use gpt3.5-turbo-instruct with Langchain? I get: &lt;/p&gt; &lt;p&gt;&amp;quot;Error code: 404 - {&amp;#39;error&amp;#39;: {&amp;#39;message&amp;#39;: &amp;#39;This is not a chat model and thus not supported in the v1/chat/completions endpoint. Did you mean to use v1/completions?&amp;#39;, &amp;#39;type&amp;#39;: &amp;#39;invalid_request_error&amp;#39;, &amp;#39;param&amp;#39;: &amp;#39;model&amp;#39;, &amp;#39;code&amp;#39;: None}} &lt;/p&gt; &lt;p&gt;Is there any way to get langchain to hit the /completions endpoint instead?&lt;/p&gt; &lt;p&gt;Or is langchain not meant for use with instruct models? I might have to abandon langchain for this project?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/silent-spiral&quot;&gt; /u/silent-spiral &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18y4pyj/how_to_use_gpt35turboinstruct_with_langchain/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18y4pyj/how_to_use_gpt35turboinstruct_with_langchain/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18y4pyj</id><link href="https://www.reddit.com/r/LangChain/comments/18y4pyj/how_to_use_gpt35turboinstruct_with_langchain/" /><updated>2024-01-04T04:54:57+00:00</updated><published>2024-01-04T04:54:57+00:00</published><title>How to use gpt3.5-turbo-instruct with Langchain?</title></entry><entry><author><name>/u/HiddenMushroom11</name><uri>https://www.reddit.com/user/HiddenMushroom11</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;m wondering if streaming even works with the HuggingFaceTextGenInference llm code.&lt;/p&gt; &lt;p&gt;I get responses from my model, but only when I run the model normally, and don&amp;#39;t use the streaming=True parameter. I can also stream the model directly from my local server when I use curl, but not when I use langchain.&lt;/p&gt; &lt;p&gt;Context to the issue here with code example:&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/langchain-ai/langchain/issues/15516&quot;&gt;https://github.com/langchain-ai/langchain/issues/15516&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Any help is appreciated. :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/HiddenMushroom11&quot;&gt; /u/HiddenMushroom11 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18y2coy/has_anyone_gotten_langchain_to_stream_hugging/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18y2coy/has_anyone_gotten_langchain_to_stream_hugging/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18y2coy</id><link href="https://www.reddit.com/r/LangChain/comments/18y2coy/has_anyone_gotten_langchain_to_stream_hugging/" /><updated>2024-01-04T02:56:14+00:00</updated><published>2024-01-04T02:56:14+00:00</published><title>Has anyone gotten Langchain to stream Hugging Face models with FastAPI?</title></entry><entry><author><name>/u/EducatorDiligent5114</name><uri>https://www.reddit.com/user/EducatorDiligent5114</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I want to built a RAG system for science documents which contains the theory texts along withn equations, tables, and labelled diagrams. Questions can be from understanding of theory, equations and information about tables. How should I proceed? Have idea of building a naive rag system only. Any resources will be helpful.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/EducatorDiligent5114&quot;&gt; /u/EducatorDiligent5114 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18xp9xi/rag_for_pdf_with_tables/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18xp9xi/rag_for_pdf_with_tables/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18xp9xi</id><link href="https://www.reddit.com/r/LangChain/comments/18xp9xi/rag_for_pdf_with_tables/" /><updated>2024-01-03T17:53:53+00:00</updated><published>2024-01-03T17:53:53+00:00</published><title>RAG for Pdf with tables</title></entry><entry><author><name>/u/mohan-aditya05</name><uri>https://www.reddit.com/user/mohan-aditya05</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18y1tic/wrote_an_article_on_using_langchain_for/&quot;&gt; &lt;img src=&quot;https://external-preview.redd.it/QTgTUY4iz7H4Wx3iBA3KGtWV3mVKJraN3u-HNXk9rz4.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=189c3a145c156156a705a4ef4936e0712c6b965f&quot; alt=&quot;Wrote an article on using Langchain for information extraction and building a UI around it using Streamlit. Check it out!&quot; title=&quot;Wrote an article on using Langchain for information extraction and building a UI around it using Streamlit. Check it out!&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/mohan-aditya05&quot;&gt; /u/mohan-aditya05 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://medium.com/p/f1a551f01f66&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18y1tic/wrote_an_article_on_using_langchain_for/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_18y1tic</id><media:thumbnail url="https://external-preview.redd.it/QTgTUY4iz7H4Wx3iBA3KGtWV3mVKJraN3u-HNXk9rz4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=189c3a145c156156a705a4ef4936e0712c6b965f" /><link href="https://www.reddit.com/r/LangChain/comments/18y1tic/wrote_an_article_on_using_langchain_for/" /><updated>2024-01-04T02:31:10+00:00</updated><published>2024-01-04T02:31:10+00:00</published><title>Wrote an article on using Langchain for information extraction and building a UI around it using Streamlit. Check it out!</title></entry><entry><author><name>/u/modularmindapp</name><uri>https://www.reddit.com/user/modularmindapp</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18y6u49/summarize_dozens_of_youtube_videos_in_seconds/&quot;&gt; &lt;img src=&quot;https://external-preview.redd.it/MzNmN2xzZDlmZGFjMS4Blkz_LHpwwPWvzOzcEePvPFOHfAVlVD8x7DHC7HNm.png?width=140&amp;amp;height=140&amp;amp;crop=140:140,smart&amp;amp;format=jpg&amp;amp;v=enabled&amp;amp;lthumb=true&amp;amp;s=10ab1465aa4654d743390cb24ff4c99f4f722798&quot; alt=&quot;Summarize dozens of YouTube videos in seconds&quot; title=&quot;Summarize dozens of YouTube videos in seconds&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/modularmindapp&quot;&gt; /u/modularmindapp &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://v.redd.it/80gjyg98fdac1&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18y6u49/summarize_dozens_of_youtube_videos_in_seconds/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_18y6u49</id><media:thumbnail url="https://external-preview.redd.it/MzNmN2xzZDlmZGFjMS4Blkz_LHpwwPWvzOzcEePvPFOHfAVlVD8x7DHC7HNm.png?width=140&amp;height=140&amp;crop=140:140,smart&amp;format=jpg&amp;v=enabled&amp;lthumb=true&amp;s=10ab1465aa4654d743390cb24ff4c99f4f722798" /><link href="https://www.reddit.com/r/LangChain/comments/18y6u49/summarize_dozens_of_youtube_videos_in_seconds/" /><updated>2024-01-04T06:56:00+00:00</updated><published>2024-01-04T06:56:00+00:00</published><title>Summarize dozens of YouTube videos in seconds</title></entry><entry><author><name>/u/jawbuster</name><uri>https://www.reddit.com/user/jawbuster</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;promptT = PromptTemplate(template=prompt_template, input_variables=[&amp;quot;context&amp;quot;, &amp;quot;question&amp;quot;]) &lt;/p&gt; &lt;p&gt;llm = ChatGoogleGenerativeAI(model=&amp;quot;gemini-pro&amp;quot;,google_api_key=google_api_key) &lt;/p&gt; &lt;p&gt;chain = load_qa_chain(llm=llm, chain_type=&amp;quot;stuff&amp;quot;,prompt=promptT) &lt;/p&gt; &lt;p&gt;keeps giving me below error:&lt;/p&gt; &lt;p&gt;File &amp;quot;pydantic\&lt;a href=&quot;https://main.py&quot;&gt;main.py&lt;/a&gt;&amp;quot;, line 341, in pydantic.main.BaseModel.__init__&lt;/p&gt; &lt;p&gt;pydantic.error_wrappers.ValidationError: 2 validation errors for LLMChain&lt;/p&gt; &lt;p&gt;llm&lt;/p&gt; &lt;p&gt;instance of Runnable expected (type=type_error.arbitrary_type; expected_arbitrary_type=Runnable)&lt;/p&gt; &lt;p&gt;llm&lt;/p&gt; &lt;p&gt;instance of Runnable expected (type=type_error.arbitrary_type; expected_arbitrary_type=Runnable)&lt;/p&gt; &lt;p&gt;I am fed up with this..I tried without the prompt template and I am sure it has something to do with the model but not sure how to correct..&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/jawbuster&quot;&gt; /u/jawbuster &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18xydx7/please_help_with_this/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18xydx7/please_help_with_this/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18xydx7</id><link href="https://www.reddit.com/r/LangChain/comments/18xydx7/please_help_with_this/" /><updated>2024-01-04T00:00:15+00:00</updated><published>2024-01-04T00:00:15+00:00</published><title>Please help with this</title></entry><entry><author><name>/u/Notchampa</name><uri>https://www.reddit.com/user/Notchampa</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;m working on a student project that involves a FastAPI backend serving a RAG (Retrieval-Augmented Generation) application, which interfaces with a frontend already hosted on Netlify. The app leverages the LLaMA index, and I recently made some enhancements following the &amp;quot;small to big retrieval&amp;quot; strategies outlined &lt;a href=&quot;https://docs.llamaindex.ai/en/stable/optimizing/advanced_retrieval/advanced_retrieval.html&quot;&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;While these improvements have significantly boosted the app&amp;#39;s performance, they&amp;#39;ve also led to a new challenge: my current hosting solution on a Digital Ocean droplet isn&amp;#39;t cutting it anymore, as I&amp;#39;m consistently running into out-of-memory issues.&lt;/p&gt; &lt;p&gt;I&amp;#39;m now in the market for a hosting platform that can comfortably handle the heavier memory requirements of my updated backend. Key requirements include:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Robust enough to support a memory-intensive FastAPI app.&lt;/li&gt; &lt;li&gt;HTTPS support for security.&lt;/li&gt; &lt;li&gt;Preferably developer-friendly and cost-effective, considering it&amp;#39;s for a student project.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Does anyone have recommendations for hosting providers or services that can meet these needs? Or, if you&amp;#39;ve worked on similar projects, I&amp;#39;d love to hear how you tackled the hosting challenges. Any insights, tips, or shared experiences would be greatly appreciated!&lt;/p&gt; &lt;p&gt;Thank you all in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Notchampa&quot;&gt; /u/Notchampa &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18xe8di/where_to_host_rag_app/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18xe8di/where_to_host_rag_app/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18xe8di</id><link href="https://www.reddit.com/r/LangChain/comments/18xe8di/where_to_host_rag_app/" /><updated>2024-01-03T08:08:48+00:00</updated><published>2024-01-03T08:08:48+00:00</published><title>Where to host RAG app?</title></entry><entry><author><name>/u/Intern_MSFT</name><uri>https://www.reddit.com/user/Intern_MSFT</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hello, I have been trying to join LCEL with StreamlitChatMessageHistory to store chat messages. This is my code:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;llm_chain = RunnablePassthrough.assign( \ history=RunnableLambda(memory.load_memory_variables) | itemgetter(&amp;quot;history&amp;quot;)) | prompt_to_ai | llm_openai | output_parser &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;I got the code snippet from here: &lt;a href=&quot;https://python.langchain.com/docs/expression_language/cookbook/memory&quot;&gt;https://python.langchain.com/docs/expression_language/cookbook/memory&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Can someone please assist? Thank you.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Intern_MSFT&quot;&gt; /u/Intern_MSFT &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18xu7jd/lcel_with_streamlit_fails_to_include_memory_in/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18xu7jd/lcel_with_streamlit_fails_to_include_memory_in/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18xu7jd</id><link href="https://www.reddit.com/r/LangChain/comments/18xu7jd/lcel_with_streamlit_fails_to_include_memory_in/" /><updated>2024-01-03T21:10:52+00:00</updated><published>2024-01-03T21:10:52+00:00</published><title>LCEL with Streamlit fails to include memory in conversations.</title></entry><entry><author><name>/u/Ill_Bodybuilder3499</name><uri>https://www.reddit.com/user/Ill_Bodybuilder3499</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;I have built a RAG app with Langchain and Streamlit and now want to share it with my team. As I am dealing with confidential data, I don&amp;#39;t want to use a cloud solution, so my idea was to convert the Ap to an .EXE-File, so that they can load and use it with no programming expertise. I saw this tutorial, but it is not working for me. &lt;/p&gt; &lt;p&gt;Does anyone have a good resource for this?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Ill_Bodybuilder3499&quot;&gt; /u/Ill_Bodybuilder3499 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18xlzmg/deploy_langchainstreamlit_app_to_exefile/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18xlzmg/deploy_langchainstreamlit_app_to_exefile/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18xlzmg</id><link href="https://www.reddit.com/r/LangChain/comments/18xlzmg/deploy_langchainstreamlit_app_to_exefile/" /><updated>2024-01-03T15:28:12+00:00</updated><published>2024-01-03T15:28:12+00:00</published><title>Deploy Langchain-Streamlit App to EXE-File</title></entry></feed>