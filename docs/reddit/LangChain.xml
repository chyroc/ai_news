<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/"><category term="LangChain" label="r/LangChain"/><updated>2024-04-25T22:34:19+00:00</updated><icon>https://www.redditstatic.com/icon.png/</icon><id>/r/LangChain.rss</id><link rel="self" href="https://www.reddit.com/r/LangChain.rss" type="application/atom+xml" /><link rel="alternate" href="https://www.reddit.com/r/LangChain" type="text/html" /><subtitle>LangChain is an open-source framework and developer toolkit that helps developers get LLM applications from prototype to production. It is available for Python and Javascript at https://www.langchain.com/.</subtitle><title>LangChain</title><entry><author><name>/u/zchaarm</name><uri>https://www.reddit.com/user/zchaarm</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;A place for members of &lt;a href=&quot;/r/LangChain&quot;&gt;r/LangChain&lt;/a&gt; to chat with each other&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/zchaarm&quot;&gt; /u/zchaarm &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_10ljho9</id><link href="https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/" /><updated>2023-01-26T04:33:05+00:00</updated><published>2023-01-26T04:33:05+00:00</published><title>r/LangChain Lounge</title></entry><entry><author><name>/u/ravediamond000</name><uri>https://www.reddit.com/user/ravediamond000</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hello everyone, &lt;/p&gt; &lt;p&gt;Just wrote and article on two underestimated (and mostly unknown) features of Langchain to create completely configurable chains while still being production ready. This is actually what I use in my own production chains.&lt;br/&gt; Here&amp;#39;s the link: &lt;a href=&quot;https://www.metadocs.co/2024/04/25/two-underestimated-langchain-features-to-create-production-ready-configurable-chains/&quot;&gt;https://www.metadocs.co/2024/04/25/two-underestimated-langchain-features-to-create-production-ready-configurable-chains/&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Enjoy!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/ravediamond000&quot;&gt; /u/ravediamond000 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ccxg2l/two_underestimated_langchain_features_to_create/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ccxg2l/two_underestimated_langchain_features_to_create/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1ccxg2l</id><link href="https://www.reddit.com/r/LangChain/comments/1ccxg2l/two_underestimated_langchain_features_to_create/" /><updated>2024-04-25T17:47:37+00:00</updated><published>2024-04-25T17:47:37+00:00</published><title>Two underestimated Langchain features to create production-ready configurable chains</title></entry><entry><author><name>/u/fokke2508</name><uri>https://www.reddit.com/user/fokke2508</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi All,&lt;/p&gt; &lt;p&gt;I work for a startup that is developing a platform to easily build GenAI-infused applications. As part of our platform, we are starting a community-based building blocks library (sort of like an app store).&lt;/p&gt; &lt;p&gt;We are about to release the community-based components and I would love to fill it up a bit more with great building blocks. Wondering if there are people here that would want to contribute?&lt;/p&gt; &lt;p&gt;I can provide you with the resources to build anything you want, including vector stores, LLMs etc.&lt;br/&gt; The idea is that each building block should help you and others build LLM apps more easily. For example, we might have a building block that provides a specific RAG task or one that converts a PDF into vectors. Could be langchain based but does not have to. &lt;/p&gt; &lt;p&gt;I don&amp;#39;t want to turn this too much into a sales pitch so I&amp;#39;ll stop there, would love to hear if anyone is interested in contributing.&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/fokke2508&quot;&gt; /u/fokke2508 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cd1roj/community_created_building_blocks_for_llms/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cd1roj/community_created_building_blocks_for_llms/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cd1roj</id><link href="https://www.reddit.com/r/LangChain/comments/1cd1roj/community_created_building_blocks_for_llms/" /><updated>2024-04-25T20:27:55+00:00</updated><published>2024-04-25T20:27:55+00:00</published><title>Community created building blocks for LLMs</title></entry><entry><author><name>/u/Any-Demand-2928</name><uri>https://www.reddit.com/user/Any-Demand-2928</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;For those of you who build your frontend UI in React, what library are you using to create the actual chat part of the website? For example, displaying messages, being able to send messages using a chat box, etc...&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Any-Demand-2928&quot;&gt; /u/Any-Demand-2928 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cd31v4/what_react_library_do_you_use_to_build_the_actual/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cd31v4/what_react_library_do_you_use_to_build_the_actual/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cd31v4</id><link href="https://www.reddit.com/r/LangChain/comments/1cd31v4/what_react_library_do_you_use_to_build_the_actual/" /><updated>2024-04-25T21:16:41+00:00</updated><published>2024-04-25T21:16:41+00:00</published><title>What React Library do you use to build the actual Chat Interface?</title></entry><entry><author><name>/u/neodyme4</name><uri>https://www.reddit.com/user/neodyme4</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Complete noob in AI, deep learning, machine learning, everything with &amp;quot;intelligent something&amp;quot;.&lt;/p&gt; &lt;p&gt;I would love some advice to start understanding how it works, and understand my mistakes.&lt;/p&gt; &lt;p&gt;I started to write code for a very simple task: &lt;/p&gt; &lt;p&gt;- I have a text file in Spanish (but Spanish is not important), and there is no necessarily a relationship between the lines - meaning by now I do not need to handle the context (maybe later!)&lt;/p&gt; &lt;p&gt;- I read it line by line&lt;/p&gt; &lt;p&gt;- I write a prompt asking to translate it for a towerinstruct model &lt;/p&gt; &lt;p&gt;- Then I print the result.&lt;/p&gt; &lt;p&gt;To be honest, the behavior of the machine seems very strange to me. At first it works (first lines), but after few lines it starts to write text by himself as such as &amp;quot;The translation you entered is as follows: &amp;quot; , &amp;quot;Translation in English&amp;quot; or &amp;quot;Spanish: &amp;quot;. I tried to add some system prompt, without significant success.&lt;/p&gt; &lt;p&gt;Here is my dumb code. Any comment will be so helpful to me! &lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;import sys import os import re from langchain.callbacks.manager import CallbackManager from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler from langchain.chains import LLMChain from langchain.prompts import PromptTemplate from langchain_community.llms import LlamaCpp MODEL=&amp;quot;/home/dani/AI-models/towerinstruct-7b-v0.1.Q8_0.gguf&amp;quot; TEMPLATE = &amp;quot;&amp;quot;&amp;quot; &amp;lt;|im_start|&amp;gt;system {system_message}&amp;lt;|im_end|&amp;gt; &amp;lt;|im_start|&amp;gt;user {prompt}&amp;lt;|im_end|&amp;gt; &amp;lt;|im_start|&amp;gt;assistant &amp;quot;&amp;quot;&amp;quot; PROMPT = PromptTemplate( input_variables=[&amp;quot;prompt&amp;quot;, &amp;quot;system_message&amp;quot;], template=TEMPLATE, ) SYSTEM_MESSAGE = &amp;quot;&amp;quot; CALLBACK_MANAGER = CallbackManager([StreamingStdOutCallbackHandler()]) LLM = LlamaCpp( model_path=MODEL, temperature=0.5, max_tokens=500, top_p=1, callback_manager=CALLBACK_MANAGER, verbose=False, ) def prompt_tr(txt, in_lang=&amp;#39;Spanish&amp;#39;, out_lang=&amp;#39;English&amp;#39;): return &amp;quot;Translate the following text from {lang1} into {lang2}.\n{lang1}: {prompt}\n{lang2}:&amp;quot;.format( lang1=in_lang, lang2=out_lang, prompt=txt ) def translate_sp_en(txt): text = prompt_tr(txt) #print(PROMPT.format(prompt=text, system_message=SYSTEM_MESSAGE)) output = LLM.invoke(PROMPT.format(prompt=text, system_message=SYSTEM_MESSAGE)) print(output) def usage(): print(&amp;quot;Usage: {} @filepath&amp;quot;.format(sys.argv[0])) if __name__ == &amp;#39;__main__&amp;#39;: if len(sys.argv) &amp;lt; 2: usage() sys.exit(1) if not os.path.isfile(sys.argv[1]): print(&amp;quot;Wrong path &amp;#39;{}&amp;#39;&amp;quot;.format(sys.argv[1])) usage() sys.exit(2) with open(sys.argv[1],&amp;#39;r&amp;#39;) as f: for line in f: translate_sp_en(line.rstrip()) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/neodyme4&quot;&gt; /u/neodyme4 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cd4na0/noob_asking_code_review_and_advice_langchain_and/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cd4na0/noob_asking_code_review_and_advice_langchain_and/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cd4na0</id><link href="https://www.reddit.com/r/LangChain/comments/1cd4na0/noob_asking_code_review_and_advice_langchain_and/" /><updated>2024-04-25T22:26:03+00:00</updated><published>2024-04-25T22:26:03+00:00</published><title>Noob asking code review and advice: langchain and translation</title></entry><entry><author><name>/u/svenjacobs3</name><uri>https://www.reddit.com/user/svenjacobs3</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I am trying to wrap my head around Langchain and streaming content from an agent to the frontend token after token (to mitigate long response times). I&amp;#39;m really just looking for something barebones to grasp how best to do this. AsyncIteratorCallbackHandler() looked promising since it appears to create a queue of tokens I should be able to iterate through. I get a &lt;em&gt;&amp;quot;TypeError: &amp;#39;async_generator&amp;#39; object is not iterable&amp;quot;&lt;/em&gt; error however, and I&amp;#39;m not sure how to remedy the problem to bring about the solution I&amp;#39;m looking for: &lt;/p&gt; &lt;pre&gt;&lt;code&gt;@app.route(&amp;#39;/stream&amp;#39;) async def stream_chunks(): CSV_PROMPT_PREFIX = &amp;quot;&amp;quot;&amp;quot; - First set the pandas display options to show all the columns, get the column names, then answer the question. &amp;quot;&amp;quot;&amp;quot; handler = AsyncIteratorCallbackHandler() llm = AzureChatOpenAI(deployment_name=os.environ[&amp;quot;GPT35_DEPLOYMENT_NAME&amp;quot;], temperature=0.1, max_tokens=1000, streaming=True, callbacks=[handler] ) agent_executor = create_csv_agent(llm=llm, path=&amp;quot;static/DemographicCompilation.csv&amp;quot;, prefix=CSV_PROMPT_PREFIX, verbose=True, agent_type=AgentType.OPENAI_FUNCTIONS, return_intermediate_steps=False ) agent = agent_executor(&amp;quot;Tell me a long story&amp;quot;) async def generate(): async for token in handler.aiter(): yield token return Response(generate(), content_type=&amp;quot;text/plain&amp;quot;) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;If anyone could help I&amp;#39;d be much obliged. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/svenjacobs3&quot;&gt; /u/svenjacobs3 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cd3dot/langchain_and_asynciteratorcallbackhandler/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cd3dot/langchain_and_asynciteratorcallbackhandler/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cd3dot</id><link href="https://www.reddit.com/r/LangChain/comments/1cd3dot/langchain_and_asynciteratorcallbackhandler/" /><updated>2024-04-25T21:29:32+00:00</updated><published>2024-04-25T21:29:32+00:00</published><title>Langchain and AsyncIteratorCallbackHandler()</title></entry><entry><author><name>/u/WesEd178</name><uri>https://www.reddit.com/user/WesEd178</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I am building a RAG application from 400+ XML documents, half of the content are tables which I am converting to csv and then extracting all text from the xml tags. A document before being added to the retriever contains both text and csv. Currently I am using an ensemble retriever combining bm25, tfidf and vectorstore (FAISS, chunk_size=2000, overlap=100). I have around 4000 test questions for these documents along with human labeled ground truth for each question and I also have a reference to the document that contains the answer. Right now I am able to get 91 questions out of 100 correctly in a random sample. &lt;/p&gt; &lt;p&gt;model: gpt-4&lt;br/&gt; embeddings: OpenAI text-embedding-3-large retriever: ensemble (bm25, tfidf, FAISS(hunk_size=2000, overlap=100)) additional: -RAPTOR clustering -sort by date then reordered using Long-Context Reorder&lt;/p&gt; &lt;p&gt;Is this a good &amp;quot;accuracy&amp;quot;? How can I improve? Is there such thing as 100% accurate RAG? How are your RAG applications doing?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/WesEd178&quot;&gt; /u/WesEd178 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cclcns/how_accurate_are_your_rag_applications/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cclcns/how_accurate_are_your_rag_applications/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cclcns</id><link href="https://www.reddit.com/r/LangChain/comments/1cclcns/how_accurate_are_your_rag_applications/" /><updated>2024-04-25T06:51:10+00:00</updated><published>2024-04-25T06:51:10+00:00</published><title>How accurate are your RAG applications?</title></entry><entry><author><name>/u/TinyZoro</name><uri>https://www.reddit.com/user/TinyZoro</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Can Langchain chains be imported / exported and therefore easily shared?&lt;/p&gt; &lt;p&gt;What does Langchain really give that you can‚Äôt easily do with something like pipedream or buildship?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/TinyZoro&quot;&gt; /u/TinyZoro &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cd1i0j/couple_of_noob_questions/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cd1i0j/couple_of_noob_questions/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cd1i0j</id><link href="https://www.reddit.com/r/LangChain/comments/1cd1i0j/couple_of_noob_questions/" /><updated>2024-04-25T20:17:45+00:00</updated><published>2024-04-25T20:17:45+00:00</published><title>Couple of Noob questions</title></entry><entry><author><name>/u/Calm_Pea_2428</name><uri>https://www.reddit.com/user/Calm_Pea_2428</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I want to ask natural-language questions to collections. For example: for sales collection, ‚ÄúWhats the average quantity sold in the past 3 months?&amp;quot;. I got about 10 collections. About 100K rows each and 25 columns each and this data is updated daily. Apart from mongo, If you have developed this kind of application using any database please add your suggestions. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Calm_Pea_2428&quot;&gt; /u/Calm_Pea_2428 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ccv7qg/build_a_rag_application_with_large_knowledge_base/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ccv7qg/build_a_rag_application_with_large_knowledge_base/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1ccv7qg</id><link href="https://www.reddit.com/r/LangChain/comments/1ccv7qg/build_a_rag_application_with_large_knowledge_base/" /><updated>2024-04-25T15:43:16+00:00</updated><published>2024-04-25T15:43:16+00:00</published><title>Build a RAG application with large knowledge base</title></entry><entry><author><name>/u/hodl_and_chill</name><uri>https://www.reddit.com/user/hodl_and_chill</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi builders,&lt;/p&gt; &lt;p&gt;Figuring out to set up the most productive code environment. GitHub Co-Pilot seems to outperform Google Duet, but GitHub co-pilot doesn&amp;#39;t have the documentation of LangChain integrated.&lt;/p&gt; &lt;p&gt;Is there a way to do this? And in general how to get external docs as extra context for AI coding co-pilots? Imagine being able to drop any documentation of API&amp;#39;s/external tools you are trying to connect, and quickly leveraging those abstractions to spit out working code.&lt;/p&gt; &lt;p&gt;Using VSCode as IDE but open to switch in case there is a workflow that increases my output and allows me to ship prototypes faster :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/hodl_and_chill&quot;&gt; /u/hodl_and_chill &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ccp13d/feeding_langchain_documentation_in_a_copilot_for/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ccp13d/feeding_langchain_documentation_in_a_copilot_for/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1ccp13d</id><link href="https://www.reddit.com/r/LangChain/comments/1ccp13d/feeding_langchain_documentation_in_a_copilot_for/" /><updated>2024-04-25T10:59:21+00:00</updated><published>2024-04-25T10:59:21+00:00</published><title>Feeding LangChain documentation in a co-pilot for VSCode</title></entry><entry><author><name>/u/HappyDataGuy</name><uri>https://www.reddit.com/user/HappyDataGuy</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Lets say question is - What was my top performing post?&lt;br/&gt; the actual question here is - which post has highest summation of likes,comment and shares?&lt;br/&gt; in second question LLM knows what columns to use, in first question it doesn&amp;#39;t. &lt;/p&gt; &lt;p&gt;do any one of you have experience with using knowledge graph for this? or any other way to solve this? any tutorial or paper would be amazing. Open source solutions are welcome as well. &lt;/p&gt; &lt;p&gt;If you&amp;#39;re working on a similar project im down for sharing ideas. Thanks. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/HappyDataGuy&quot;&gt; /u/HappyDataGuy &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ccovds/how_to_knowledge_graphs_can_be_used_to_improve/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ccovds/how_to_knowledge_graphs_can_be_used_to_improve/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1ccovds</id><link href="https://www.reddit.com/r/LangChain/comments/1ccovds/how_to_knowledge_graphs_can_be_used_to_improve/" /><updated>2024-04-25T10:49:27+00:00</updated><published>2024-04-25T10:49:27+00:00</published><title>How to knowledge graphs can be used to improve SQL generation? [text to sql]</title></entry><entry><author><name>/u/patcher99</name><uri>https://www.reddit.com/user/patcher99</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ccjozm/openlit_preview_opentelemetrynative_llm/&quot;&gt; &lt;img src=&quot;https://b.thumbs.redditmedia.com/jllXvUlBtTTyc1aFLMyhzX7p3v8xZJMp2-Zweq5w5oQ.jpg&quot; alt=&quot;OpenLIT Preview: OpenTelemetry-native LLM Application Observability&quot; title=&quot;OpenLIT Preview: OpenTelemetry-native LLM Application Observability&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey folks! My friend and I were working on an LLM-based legal helper but got really stuck trying to figure out our tweaked GPT-3.5. So, we came up with a tool named Doku to keep an monitor on our LLM apps and make them more trusty. It got stars fairly quickly, but folks found it a bit tricky since they had to set up things before diving into the analysis.&lt;/p&gt; &lt;p&gt;Here&amp;#39;s what we did next:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;We switched our tech to OpenTelemetry for easier tracking.&lt;/li&gt; &lt;li&gt;We made it so you can see costs and how many tokens you&amp;#39;re using straight from your console ‚Äì no extra Infra needed for basic debugging.&lt;/li&gt; &lt;li&gt;We decided to call it OpenLIT (short for Learning Interpretability Tool, shining a light on model behavior and data visualization, inspired by a term from &lt;a href=&quot;https://developers.google.com/machine-learning/glossary#learning-interpretability-tool-lit&quot;&gt;Google&lt;/a&gt;).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;We&amp;#39;ve just put out our new Python library, OpenLIT, in preview. You can check it out here: &lt;a href=&quot;https://pypi.org/project/openlit/&quot;&gt;https://pypi.org/project/openlit/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This library is provides OpenTelemetry Auto-Instrumentation for LLM Applications, It integrates monitoring for&lt;/p&gt; &lt;ul&gt; &lt;li&gt;LLM providers like OpenAI, Anthropic, HuggingFace, Cohere, and Mistral.&lt;/li&gt; &lt;li&gt;Vector DBs including Pinecone and ChromaDB.&lt;/li&gt; &lt;li&gt;Frameworks like LangChain&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This library will work even if you are using frameworks like LiteLLM!&lt;/p&gt; &lt;p&gt;It&amp;#39;s the first of its kind to align with the OTEL &lt;a href=&quot;https://github.com/open-telemetry/semantic-conventions/tree/main/docs/gen-ai&quot;&gt;Semcov for GenAI Applications&lt;/a&gt;, allowing you to forward all collected metrics to any OTEL-compatible backend.&lt;/p&gt; &lt;p&gt;We&amp;#39;re also working on an open-source, self-hosted UI - Attached a couple pictures for you to get a feel of it.&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/8toxeja56kwc1.png?width=2553&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6704818740f9e76f953e951351c06698a21396c9&quot;&gt;https://preview.redd.it/8toxeja56kwc1.png?width=2553&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6704818740f9e76f953e951351c06698a21396c9&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/k5b3cka56kwc1.png?width=2552&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=266b290b1548ff85ae5730bc7b81b855cfe71fd2&quot;&gt;https://preview.redd.it/k5b3cka56kwc1.png?width=2552&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=266b290b1548ff85ae5730bc7b81b855cfe71fd2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Follow our project here for updates - &lt;a href=&quot;https://github.com/openlit/openlit&quot;&gt;https://github.com/openlit/openlit&lt;/a&gt;. The stable release drops tomorrow for both the SDK and UI. Let me know if there&amp;#39;s something specific you‚Äôd love to see!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/patcher99&quot;&gt; /u/patcher99 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ccjozm/openlit_preview_opentelemetrynative_llm/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ccjozm/openlit_preview_opentelemetrynative_llm/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1ccjozm</id><media:thumbnail url="https://b.thumbs.redditmedia.com/jllXvUlBtTTyc1aFLMyhzX7p3v8xZJMp2-Zweq5w5oQ.jpg" /><link href="https://www.reddit.com/r/LangChain/comments/1ccjozm/openlit_preview_opentelemetrynative_llm/" /><updated>2024-04-25T05:06:40+00:00</updated><published>2024-04-25T05:06:40+00:00</published><title>OpenLIT Preview: OpenTelemetry-native LLM Application Observability</title></entry><entry><author><name>/u/ashpreetbedi</name><uri>https://www.reddit.com/user/ashpreetbedi</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Here&amp;#39;s an unedited video testing tools with llama3 running locally (at 1.5x speed). The good, bad and ugly. &lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://reddit.com/link/1ccdexb/video/a47qddncliwc1/player&quot;&gt;https://reddit.com/link/1ccdexb/video/a47qddncliwc1/player&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/ashpreetbedi&quot;&gt; /u/ashpreetbedi &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ccdexb/testing_local_llama3_at_function_calling_and_tool/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ccdexb/testing_local_llama3_at_function_calling_and_tool/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1ccdexb</id><link href="https://www.reddit.com/r/LangChain/comments/1ccdexb/testing_local_llama3_at_function_calling_and_tool/" /><updated>2024-04-24T23:48:18+00:00</updated><published>2024-04-24T23:48:18+00:00</published><title>üßôTesting local llama3 at function calling and tool use.</title></entry><entry><author><name>/u/SashaBaych</name><uri>https://www.reddit.com/user/SashaBaych</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Any ideas how to add memory/persistence to a StateGraph when doing a langgraph? There is tutorial on MessageGraph, but what would I do with the StateGraph with multiple chains? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/SashaBaych&quot;&gt; /u/SashaBaych &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cco1yb/stategraph_persistencehistory/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cco1yb/stategraph_persistencehistory/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cco1yb</id><link href="https://www.reddit.com/r/LangChain/comments/1cco1yb/stategraph_persistencehistory/" /><updated>2024-04-25T09:58:11+00:00</updated><published>2024-04-25T09:58:11+00:00</published><title>StateGraph persistence/history</title></entry><entry><author><name>/u/Travolta1984</name><uri>https://www.reddit.com/user/Travolta1984</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;LangChain recently added Semantic Chunker as an option for splitting documents, and from my experience it performs better than RecursiveCharacterSplitter (although it&amp;#39;s more expensive due to the sentence embeddings). &lt;/p&gt; &lt;p&gt;One thing that I noticed though, is that there&amp;#39;s no pre-defined limit to the size of the result chunks: I have seen chunks that are just a couple of words (i.e. section headers), and also very long chunks (5k+ characters). Which makes total sense, given the logic: if all sentences in that chunk are semantically similar, they should all be grouped together, regardless of how long that chunk will be. But that can lead to issues downstream: document context gets too large for the LLM, or small chunks that add no context at all.&lt;/p&gt; &lt;p&gt;Based on that, I wrote my custom version of the Semantic Chunker that optionally respects the character count limit (both minimum and maximum). The logic I am using is: a chunk split happens when either the semantic distance between the sentences becomes too large and the chunk is at least &amp;lt;MIN\_SIZE&amp;gt; long, or when the chunk becomes larger than &amp;lt;MAX\_SIZE&amp;gt;.&lt;/p&gt; &lt;p&gt;My question to the community is: &lt;/p&gt; &lt;p&gt;- Does the above make sense? I feel like this approach can be useful, but it kind of goes against the idea of chunking your texts semantically.&lt;/p&gt; &lt;p&gt;- I thought about creating a PR to add this option to the official code. Has anyone contributed to LangChain&amp;#39;s repo? What has been your experience doing so?&lt;/p&gt; &lt;p&gt;Thanks.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Travolta1984&quot;&gt; /u/Travolta1984 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cca0h0/question_about_semantic_chunker/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cca0h0/question_about_semantic_chunker/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cca0h0</id><link href="https://www.reddit.com/r/LangChain/comments/1cca0h0/question_about_semantic_chunker/" /><updated>2024-04-24T21:21:33+00:00</updated><published>2024-04-24T21:21:33+00:00</published><title>Question about Semantic Chunker</title></entry><entry><author><name>/u/OfficeSalamander</name><uri>https://www.reddit.com/user/OfficeSalamander</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I am trying to use internet-search enabled bots, and I was wondering how you guys were doing it - I see that Serpdev and Tavily have Langchain integration - which of these two do you guys like? Or do you roll your own?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/OfficeSalamander&quot;&gt; /u/OfficeSalamander &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cc1dyq/how_are_you_guys_doing_internet_search/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cc1dyq/how_are_you_guys_doing_internet_search/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cc1dyq</id><link href="https://www.reddit.com/r/LangChain/comments/1cc1dyq/how_are_you_guys_doing_internet_search/" /><updated>2024-04-24T15:40:46+00:00</updated><published>2024-04-24T15:40:46+00:00</published><title>How are you guys doing internet search?</title></entry><entry><author><name>/u/Moochiberico</name><uri>https://www.reddit.com/user/Moochiberico</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I would like to know if this is possible, I&amp;#39;m fairly new to langchain, I want to split into text chunks by different paragraphs and after reading doc, still seem to be stuck on this one. Some help would be much appreciated. Thanks!&lt;/p&gt; &lt;p&gt;Edit: Nevermind, think I got it, posting it in case anyone else has this question.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;text_splitter = CharacterTextSplitter(separator=&amp;quot;\n&amp;quot;,chunk_size=400, chunk_overlap=20) &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Moochiberico&quot;&gt; /u/Moochiberico &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ccccw6/text_split_by_paragraphs/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ccccw6/text_split_by_paragraphs/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1ccccw6</id><link href="https://www.reddit.com/r/LangChain/comments/1ccccw6/text_split_by_paragraphs/" /><updated>2024-04-24T23:01:12+00:00</updated><published>2024-04-24T23:01:12+00:00</published><title>Text Split by paragraphs?</title></entry><entry><author><name>/u/BuildingLLMTools</name><uri>https://www.reddit.com/user/BuildingLLMTools</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey folks,&lt;/p&gt; &lt;p&gt;We&amp;#39;re building Langtrace, an open-source LLM App observability platform (&lt;a href=&quot;http://www.langtrace.ai&quot;&gt;www.langtrace.ai&lt;/a&gt;) and we recently built support for LlamaIndex, the go-to library for building retrieval-augmented generation (RAG) applications.&lt;/p&gt; &lt;p&gt;As builders, we know how frustrating it can be to optimize RAG apps (e.g. trying to figure out where the bottlenecks are, whether your retrieval strategy is effective, etc.) That&amp;#39;s why we&amp;#39;re building a tool that makes it easy to gain deeper insights and optimize performance, reliability, and user experience for your LLM apps.&lt;/p&gt; &lt;p&gt;With Langtrace and LlamaIndex, you can:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Get one-click observability for LlamaIndex-based RAG applications&lt;/li&gt; &lt;li&gt;Visualize latency breakdowns, context relevance, and resource utilization&lt;/li&gt; &lt;li&gt;Monitor and analyze traces, evals, metrics, and logs with OpenTelemetry&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Feel free to check out our &lt;a href=&quot;https://github.com/Scale3-Labs/langtrace&quot;&gt;repo&lt;/a&gt; for &lt;a href=&quot;https://github.com/Scale3-Labs/langtrace-docs/blob/main/langtrace-examples/llamaindex_essay/starter.py.ipynb&quot;&gt;examples&lt;/a&gt;, contribute, provide feedback, and join our &lt;a href=&quot;https://discord.com/invite/EaSATwtr4t&quot;&gt;community&lt;/a&gt;. More info on the integration with LlamaIndex &lt;a href=&quot;https://langtrace.ai/blog/langtrace-llamaindex-a-game-changing-combo-for-rag-developers&quot;&gt;here&lt;/a&gt; including a video demo. Looking forward to hearing of your feedback! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/BuildingLLMTools&quot;&gt; /u/BuildingLLMTools &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cc8fx4/solve_rag_app_optimization_puzzles_with_langtrace/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cc8fx4/solve_rag_app_optimization_puzzles_with_langtrace/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cc8fx4</id><link href="https://www.reddit.com/r/LangChain/comments/1cc8fx4/solve_rag_app_optimization_puzzles_with_langtrace/" /><updated>2024-04-24T20:18:16+00:00</updated><published>2024-04-24T20:18:16+00:00</published><title>Solve RAG App Optimization Puzzles with Langtrace + LlamaIndex</title></entry><entry><author><name>/u/ashpreetbedi</name><uri>https://www.reddit.com/user/ashpreetbedi</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I dont trust the benchmarks, so I recorded my very first test run. Completely unedited, each question asked for the first time. First impression is that it is good, very very good for its size. Sharing the code below.&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://reddit.com/link/1cbuqow/video/ay3us3wbmewc1/player&quot;&gt;https://reddit.com/link/1cbuqow/video/ay3us3wbmewc1/player&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/ashpreetbedi&quot;&gt; /u/ashpreetbedi &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cbuqow/initial_tests_rag_with_phi3/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cbuqow/initial_tests_rag_with_phi3/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cbuqow</id><link href="https://www.reddit.com/r/LangChain/comments/1cbuqow/initial_tests_rag_with_phi3/" /><updated>2024-04-24T10:26:44+00:00</updated><published>2024-04-24T10:26:44+00:00</published><title>Initial tests: RAG with Phi-3</title></entry><entry><author><name>/u/supreet02</name><uri>https://www.reddit.com/user/supreet02</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;While RAG is undeniably impressive, the process of creating a functional application with it can be daunting. There&amp;#39;s a significant amount to grasp regarding implementation and development practices, ranging from selecting the appropriate AI models for the specific use case to organizing data effectively to obtain the desired insights. While tools like LangChain and LlamaIndex exist to simplify the prototype design process, there has yet to be an accessible, ready-to-use open-source RAG template that incorporates best practices and offers modular support, allowing anyone to quickly and easily utilize it.&lt;/p&gt; &lt;p&gt;TrueFoundry has recently introduced a new open-source framework called &lt;a href=&quot;https://github.com/truefoundry/cognita&quot;&gt;&lt;strong&gt;Cognita&lt;/strong&gt;&lt;/a&gt;, which utilizes Retriever-Augmented Generation (RAG) technology to simplify the transition by providing robust, scalable solutions for deploying AI applications. AI development often begins in experimental environments such as Jupyter notebooks, which are useful for prototyping but not well-suited for production environments. However, Cognita aims to bridge this gap. Developed on top of Langchain and LlamaIndex, Cognita offers a structured and modular approach to AI application development. Each component of the RAG, from data handling to model deployment, is designed to be modular, API-driven, and extendable.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/supreet02&quot;&gt; /u/supreet02 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cbqzlr/how_to_quickly_build_and_deploy_scalable_rag/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cbqzlr/how_to_quickly_build_and_deploy_scalable_rag/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cbqzlr</id><link href="https://www.reddit.com/r/LangChain/comments/1cbqzlr/how_to_quickly_build_and_deploy_scalable_rag/" /><updated>2024-04-24T06:02:44+00:00</updated><published>2024-04-24T06:02:44+00:00</published><title>How to quickly build and deploy scalable RAG applications?</title></entry><entry><author><name>/u/sarthak_uchiha</name><uri>https://www.reddit.com/user/sarthak_uchiha</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I am working on a project of bill of material where a client has recieved a mail which contains the catalogue I&amp;#39;d the quantity of the catalogue and it&amp;#39;s description,... The data could be in normal text , in a table , or in a image of the body (not in attachments )&lt;/p&gt; &lt;p&gt;How should I tackle this , like image could be many and some irrelevant ones like logo of company and other than there might be possibility that a duplicate data may present in text and image , and how to handle the thread of email &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/sarthak_uchiha&quot;&gt; /u/sarthak_uchiha &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cc3m04/bill_of_material_need_some_pov/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cc3m04/bill_of_material_need_some_pov/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cc3m04</id><link href="https://www.reddit.com/r/LangChain/comments/1cc3m04/bill_of_material_need_some_pov/" /><updated>2024-04-24T17:07:37+00:00</updated><published>2024-04-24T17:07:37+00:00</published><title>Bill of material need some PoV</title></entry><entry><author><name>/u/mofusa16</name><uri>https://www.reddit.com/user/mofusa16</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey Redditors! üôã‚Äç‚ôÇÔ∏è&lt;/p&gt; &lt;p&gt;I came up with the idea of summarizing text with various large language models (LLMs). I intend to develop this fully-fledged application (including a register page, login page, database etc.) using either Python, JavaScript, or both. Can you advise me on which framework would be most suitable for such an endeavor? I&amp;#39;m seeking recommendations on frameworks that excel in constructing this type of application. Some colleagues have proposed trying Flask, Gradio, or Django. Please share your insights on which framework would be optimal for this project, and kindly provide reasons to support your suggestion.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/mofusa16&quot;&gt; /u/mofusa16 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cbv3dv/seeking_advice_which_framework_is_best_suited_for/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cbv3dv/seeking_advice_which_framework_is_best_suited_for/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cbv3dv</id><link href="https://www.reddit.com/r/LangChain/comments/1cbv3dv/seeking_advice_which_framework_is_best_suited_for/" /><updated>2024-04-24T10:49:36+00:00</updated><published>2024-04-24T10:49:36+00:00</published><title>Seeking Advice: Which Framework is best suited for building GenAI Web App?</title></entry><entry><author><name>/u/UpskillingDS17</name><uri>https://www.reddit.com/user/UpskillingDS17</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi, I am thinking of creating a LLM based application where questions can be asked in excel files and the files are small to medium size less than 10 MB. What is the best way to approach this problem ? In my team there are consultants who have 0 to little background on coding and SQL, so this could be a great help to them. Thanks&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/UpskillingDS17&quot;&gt; /u/UpskillingDS17 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cbwajs/creating_data_analytics_qa_platform_using_llm/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cbwajs/creating_data_analytics_qa_platform_using_llm/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cbwajs</id><link href="https://www.reddit.com/r/LangChain/comments/1cbwajs/creating_data_analytics_qa_platform_using_llm/" /><updated>2024-04-24T11:57:48+00:00</updated><published>2024-04-24T11:57:48+00:00</published><title>Creating data analytics Q&amp;A platform using LLM</title></entry><entry><author><name>/u/MintDrake</name><uri>https://www.reddit.com/user/MintDrake</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;&lt;strong&gt;Code of pure implementation through POST to local ollama&lt;/strong&gt; &lt;a href=&quot;http://localhost:11434/api/chat&quot;&gt;&lt;strong&gt;http://localhost:11434/api/chat&lt;/strong&gt;&lt;/a&gt; &lt;strong&gt;(3.2s):&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;import aiohttp from dataclasses import dataclass, field from typing import List import time start_time = time.time() @dataclass class Message: role: str content: str @dataclass class ChatHistory: messages: List[Message] = field(default_factory=list) def add_message(self, message: Message): self.messages.append(message) @dataclass class RequestData: model: str messages: List[dict] stream: bool = False @classmethod def from_params(cls, model, system_message, history): messages = [ {&amp;quot;role&amp;quot;: &amp;quot;system&amp;quot;, &amp;quot;content&amp;quot;: system_message}, *[{&amp;quot;role&amp;quot;: msg.role, &amp;quot;content&amp;quot;: msg.content} for msg in history.messages], ] return cls(model=model, messages=messages, stream=False) class LocalLlm: def __init__(self, model=&amp;#39;llama3:8b&amp;#39;, history=None, system_message=&amp;quot;You are a helpful assistant&amp;quot;): self.model = model self.history = history or ChatHistory() self.system_message = system_message async def ask(self, input=&amp;quot;&amp;quot;): if input: self.history.add_message(Message(role=&amp;quot;user&amp;quot;, content=input)) data = RequestData.from_params(self.model, self.system_message, self.history) url = &amp;quot;http://localhost:11434/api/chat&amp;quot; async with aiohttp.ClientSession() as session: async with session.post(url, json=data.__dict__) as response: result = await response.json() print(result[&amp;quot;message&amp;quot;][&amp;quot;content&amp;quot;]) if result[&amp;quot;done&amp;quot;]: ai_response = result[&amp;quot;message&amp;quot;][&amp;quot;content&amp;quot;] self.history.add_message(Message(role=&amp;quot;assistant&amp;quot;, content=ai_response)) return ai_response else: raise Exception(&amp;quot;Error generating response&amp;quot;) if __name__ == &amp;quot;__main__&amp;quot;: chat_history = ChatHistory(messages=[ Message(role=&amp;quot;system&amp;quot;, content=&amp;quot;You are a crazy pirate&amp;quot;), Message(role=&amp;quot;user&amp;quot;, content=&amp;quot;Can you tell me a joke?&amp;quot;) ]) llm = LocalLlm(history=chat_history) import asyncio response = asyncio.run(llm.ask()) print(response) print(llm.history) print(&amp;quot;--- %s seconds ---&amp;quot; % (time.time() - start_time)) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;code&gt;--- 3.2285749912261963 seconds ---&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Lang chain equivalent (3.5 s):&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;from langchain_core.messages import HumanMessage, SystemMessage, AIMessage, BaseMessage from langchain_community.chat_models.ollama import ChatOllama from langchain.memory import ChatMessageHistory import time start_time = time.time() class LocalLlm: def __init__(self, model=&amp;#39;llama3:8b&amp;#39;, messages=ChatMessageHistory(), system_message=&amp;quot;You are a helpful assistant&amp;quot;, context_length = 8000): self.model = ChatOllama(model=model, system=system_message, num_ctx=context_length) self.history = messages def ask(self, input=&amp;quot;&amp;quot;): if input: self.history.add_user_message(input) response = self.model.invoke(self.history.messages) self.history.add_ai_message(response) return response if __name__ == &amp;quot;__main__&amp;quot;: chat = ChatMessageHistory() chat.add_messages([ SystemMessage(content=&amp;quot;You are a crazy pirate&amp;quot;), HumanMessage(content=&amp;quot;Can you tell me a joke?&amp;quot;) ]) print(chat) llm = LocalLlm(messages=chat) print(llm.ask()) print(llm.history.messages) print(&amp;quot;--- %s seconds ---&amp;quot; % (time.time() - start_time)) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;code&gt;--- 3.469588279724121 seconds ---&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;So it&amp;#39;s 3.2 vs 3.469(nice) so the difference so 0.3s difference is nothing.&lt;br/&gt; Made this post because was so upset over &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c6zktz/llms_frameworks_langchain_llamaindex_griptape/&quot;&gt;this post&lt;/a&gt; after getting to know langchain and finally coming up with some results. I think it&amp;#39;s true that it&amp;#39;s not very suitable for serious development, but it&amp;#39;s perfect for theory crafting and experimenting, but anyways you can just write your own abstractions which you know. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/MintDrake&quot;&gt; /u/MintDrake &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cbj7gg/i_tested_langchain_vs_vanilla_speed/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cbj7gg/i_tested_langchain_vs_vanilla_speed/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cbj7gg</id><link href="https://www.reddit.com/r/LangChain/comments/1cbj7gg/i_tested_langchain_vs_vanilla_speed/" /><updated>2024-04-23T23:19:36+00:00</updated><published>2024-04-23T23:19:36+00:00</published><title>I tested LANGCHAIN vs VANILLA speed</title></entry><entry><author><name>/u/Unrealnooob</name><uri>https://www.reddit.com/user/Unrealnooob</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi, So I have a rag application/chatbot, uses conversationalretrivalqa chain from Langchain, say if for questions like &amp;#39;Hi&amp;#39; and all retrieval is happening, and its returning random documents How do I make the llm answer directly without retrieval for questions like this.? And one more thing how do I implement a memory(longterm will be better) with conversationalretrivalqa.from_llm chain..whatever I tried is not working, I tried with the Runnablehistory but that screws up the retrieval Does anyone have any workaround on that.? Any help will be appreciated ,thanks &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Unrealnooob&quot;&gt; /u/Unrealnooob &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cbq1jt/how_to_make_llm_differentiate_whether_to_retrieve/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cbq1jt/how_to_make_llm_differentiate_whether_to_retrieve/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cbq1jt</id><link href="https://www.reddit.com/r/LangChain/comments/1cbq1jt/how_to_make_llm_differentiate_whether_to_retrieve/" /><updated>2024-04-24T05:04:47+00:00</updated><published>2024-04-24T05:04:47+00:00</published><title>How to make llm differentiate whether to retrieve or not</title></entry><entry><author><name>/u/QueRoub</name><uri>https://www.reddit.com/user/QueRoub</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I have built a RAG application with my own PDF documents.&lt;/p&gt; &lt;p&gt;Some of the answers are not correct, usually they are from wrong documents even if the right ones have been retrieved.&lt;/p&gt; &lt;p&gt;What is the right way to approach it?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/QueRoub&quot;&gt; /u/QueRoub &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cbrpms/how_to_finetune_the_answers_of_llm_in_a_rag/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cbrpms/how_to_finetune_the_answers_of_llm_in_a_rag/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cbrpms</id><link href="https://www.reddit.com/r/LangChain/comments/1cbrpms/how_to_finetune_the_answers_of_llm_in_a_rag/" /><updated>2024-04-24T06:50:23+00:00</updated><published>2024-04-24T06:50:23+00:00</published><title>How to fine-tune the answers of LLM in a RAG application</title></entry></feed>