<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/"><category term="LangChain" label="r/LangChain"/><updated>2024-06-22T15:33:23+00:00</updated><icon>https://www.redditstatic.com/icon.png/</icon><id>/r/LangChain.rss</id><link rel="self" href="https://www.reddit.com/r/LangChain.rss" type="application/atom+xml" /><link rel="alternate" href="https://www.reddit.com/r/LangChain" type="text/html" /><subtitle>LangChain is an open-source framework and developer toolkit that helps developers get LLM applications from prototype to production. It is available for Python and Javascript at https://www.langchain.com/.</subtitle><title>LangChain</title><entry><author><name>/u/zchaarm</name><uri>https://www.reddit.com/user/zchaarm</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;A place for members of &lt;a href=&quot;/r/LangChain&quot;&gt;r/LangChain&lt;/a&gt; to chat with each other&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/zchaarm&quot;&gt; /u/zchaarm &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_10ljho9</id><link href="https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/" /><updated>2023-01-26T04:33:05+00:00</updated><published>2023-01-26T04:33:05+00:00</published><title>r/LangChain Lounge</title></entry><entry><author><name>/u/tuantruong84</name><uri>https://www.reddit.com/user/tuantruong84</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;As much as i like LangChain, there is some actual good points from this article &lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://www.octomind.dev/blog/why-we-no-longer-use-langchain-for-building-our-ai-agents&quot;&gt;https://www.octomind.dev/blog/why-we-no-longer-use-langchain-for-building-our-ai-agents&lt;/a&gt;&lt;/p&gt; &lt;p&gt;What you guys think ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/tuantruong84&quot;&gt; /u/tuantruong84 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dlu5t9/an_article_on_why_moving_away_from_langchain/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dlu5t9/an_article_on_why_moving_away_from_langchain/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dlu5t9</id><link href="https://www.reddit.com/r/LangChain/comments/1dlu5t9/an_article_on_why_moving_away_from_langchain/" /><updated>2024-06-22T12:03:37+00:00</updated><published>2024-06-22T12:03:37+00:00</published><title>An article on why moving away from langchain</title></entry><entry><author><name>/u/sarthakai</name><uri>https://www.reddit.com/user/sarthakai</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;So first, here&amp;#39;s what I understand of how they did it:&lt;/p&gt; &lt;p&gt;They made the KG by parsing customer support tickets into structured tree representations, preserving their internal relationships.&lt;/p&gt; &lt;p&gt;Tickets are linked based on contextual similarities, dependencies, and references — all of these make up a comprehensive graph.&lt;/p&gt; &lt;p&gt;Each node in the KG is embedded so they can do semantic search and retrieval.&lt;/p&gt; &lt;p&gt;The RAG QA system identifies relevant sub-graphs by doing traversal and searching by semantic similarity.&lt;/p&gt; &lt;p&gt;Then, it generates contextually aware answers from the KG, evaluating by MRR, which saw a significant improvement.&lt;/p&gt; &lt;p&gt;Paper: &lt;a href=&quot;https://arxiv.org/pdf/2404.17723&quot;&gt;https://arxiv.org/pdf/2404.17723&lt;/a&gt;&lt;/p&gt; &lt;p&gt;If you’d like to implement Graph RAG too, I’m creating a Python library which automatically creates this graph for the documents in your vectordb. It also makes it easy for you to retrieve relevant documents connected to the best matches.&lt;/p&gt; &lt;p&gt;If you&amp;#39;re interested in contributing or have suggestions please raise them on Github.&lt;/p&gt; &lt;p&gt;Here’s the repo for the library: &lt;a href=&quot;https://github.com/sarthakrastogi/graph-rag/tree/main&quot;&gt;https://github.com/sarthakrastogi/graph-rag/tree/main&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/sarthakai&quot;&gt; /u/sarthakai &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dlwc39/linkedin_used_graph_rag_to_cut_down_their_ticket/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dlwc39/linkedin_used_graph_rag_to_cut_down_their_ticket/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dlwc39</id><link href="https://www.reddit.com/r/LangChain/comments/1dlwc39/linkedin_used_graph_rag_to_cut_down_their_ticket/" /><updated>2024-06-22T14:00:46+00:00</updated><published>2024-06-22T14:00:46+00:00</published><title>LinkedIn used Graph RAG to cut down their ticket resolution time from 40 hrs to 15 hrs. Let's make a library to make it accessible to everyone?</title></entry><entry><author><name>/u/CodingButStillAlive</name><uri>https://www.reddit.com/user/CodingButStillAlive</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I know that streamlit was popular, but neither optimized for chatbot interactivity, nor ready to set up for production.&lt;/p&gt; &lt;p&gt;I assume some TypeScript + REACT is state of the art, but I am a Data Scientist and no frontend developer.&lt;/p&gt; &lt;p&gt;Are there any new libraries that nicely integrate with LangGraph and also FastAPI?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/CodingButStillAlive&quot;&gt; /u/CodingButStillAlive &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dlrouj/what_is_the_best_python_library_for_chatbot_uis/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dlrouj/what_is_the_best_python_library_for_chatbot_uis/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dlrouj</id><link href="https://www.reddit.com/r/LangChain/comments/1dlrouj/what_is_the_best_python_library_for_chatbot_uis/" /><updated>2024-06-22T09:18:19+00:00</updated><published>2024-06-22T09:18:19+00:00</published><title>What is the best python library for chatbot UIs?</title></entry><entry><author><name>/u/AnEdgeLordWeeb</name><uri>https://www.reddit.com/user/AnEdgeLordWeeb</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey guys!&lt;br/&gt; Has anyone tried and managed to find a successful solution, as to how I can messages in LangGraph through the usage of FastAPI and React?&lt;br/&gt; I have multiple nodes in my LangGraph app, and each one is appending a message to the &amp;quot;messages&amp;quot; list attribute. I want these messages&amp;#39; content to be streamed as a single message in my React app, until I reach the END node.&lt;br/&gt; Does anyone have any idea as to how to do that?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/AnEdgeLordWeeb&quot;&gt; /u/AnEdgeLordWeeb &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dltljb/how_to_stream_messages_with_fastapi_and_react/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dltljb/how_to_stream_messages_with_fastapi_and_react/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dltljb</id><link href="https://www.reddit.com/r/LangChain/comments/1dltljb/how_to_stream_messages_with_fastapi_and_react/" /><updated>2024-06-22T11:29:52+00:00</updated><published>2024-06-22T11:29:52+00:00</published><title>How to stream messages with FastAPI and React? - LangGraph</title></entry><entry><author><name>/u/Informal-Victory8655</name><uri>https://www.reddit.com/user/Informal-Victory8655</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Informal-Victory8655&quot;&gt; /u/Informal-Victory8655 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dlsxte/how_to_use_rabbitmq_or_any_other_broker_with/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dlsxte/how_to_use_rabbitmq_or_any_other_broker_with/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dlsxte</id><link href="https://www.reddit.com/r/LangChain/comments/1dlsxte/how_to_use_rabbitmq_or_any_other_broker_with/" /><updated>2024-06-22T10:47:26+00:00</updated><published>2024-06-22T10:47:26+00:00</published><title>How to Use RabbitMQ or any other Broker with LangChain FastApi chatbot</title></entry><entry><author><name>/u/diptanuc</name><uri>https://www.reddit.com/user/diptanuc</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi folks, I often see questions about which open source pdf model or APIs are best for extraction from PDF. We attempt to help people make data-driven decisions by comparing the various models on their private documents.&lt;/p&gt; &lt;p&gt;We benchmarked several PDF models - Marker, EasyOCR, Unstructured and OCRMyPDF.&lt;/p&gt; &lt;p&gt;Marker is better than the others in terms of accuracy. EasyOCR comes second, and OCRMyPDF is pretty close.&lt;/p&gt; &lt;p&gt;You can run these benchmarks on your documents using our code - &lt;a href=&quot;https://github.com/tensorlakeai/indexify-extractors/tree/main/pdf/benchmark&quot;&gt;https://github.com/tensorlakeai/indexify-extractors/tree/main/pdf/benchmark&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The benchmark tool is using Indexify behind the scenes - &lt;a href=&quot;https://github.com/tensorlakeai/indexify&quot;&gt;https://github.com/tensorlakeai/indexify&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Indexify is a scalable unstructured data extraction engine for building multi-stage inference pipelines. The pipelines can handle extraction from 1000s of documents in parallel when deployed in a real cluster on the cloud.&lt;/p&gt; &lt;p&gt;I would love your feedback on what models and document layouts to benchmark next. &lt;/p&gt; &lt;p&gt;For some reason Reddit is marking this post as spam when I add pictures, so here is a link to the docs with some charts - &lt;a href=&quot;https://docs.getindexify.ai/usecases/pdf_extraction/#extractor-performance-analysis&quot;&gt;https://docs.getindexify.ai/usecases/pdf_extraction/#extractor-performance-analysis&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/diptanuc&quot;&gt; /u/diptanuc &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dlfth6/benchmarking_pdf_models_for_parsing_accuracy/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dlfth6/benchmarking_pdf_models_for_parsing_accuracy/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dlfth6</id><link href="https://www.reddit.com/r/LangChain/comments/1dlfth6/benchmarking_pdf_models_for_parsing_accuracy/" /><updated>2024-06-21T21:57:01+00:00</updated><published>2024-06-21T21:57:01+00:00</published><title>Benchmarking PDF models for parsing accuracy</title></entry><entry><author><name>/u/sarthakai</name><uri>https://www.reddit.com/user/sarthakai</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Suppose in your LLM you have the original weight matrix W of dimensions d x k.&lt;/p&gt; &lt;p&gt;Your traditional training process would update W directly -- that’s a huge number of parameters if d x k is large, needing a lot of compute.&lt;/p&gt; &lt;p&gt;So, we use Low-Rank Decomposition to break it down before weight update. Here’s how —We represent the weight update (Delta W) as a product of two lower-rank matrices A and B, such that Delta W = BA.&lt;/p&gt; &lt;p&gt;Here, A is a matrix of dimensions r x k and B is a matrix of dimensions d x r. And here, r (rank) is much smaller than both d and k.&lt;/p&gt; &lt;p&gt;Now, Matrix A is initialised with some random Gaussian values and matrix B is initialised with zeros.&lt;/p&gt; &lt;p&gt;Why? So that initially Delta W = BA can be 0.&lt;/p&gt; &lt;p&gt;Now comes the training process:&lt;/p&gt; &lt;p&gt;During weight update, only the smaller matrices A and B are updated — this reduces the number of parameters to be tuned by a huge margin.&lt;/p&gt; &lt;p&gt;The effective update to the original weight matrix W is Delta W = BA, which approximates the changes in W using fewer parameters.&lt;/p&gt; &lt;p&gt;Let’s compare the params to be updated before and after LoRA:&lt;/p&gt; &lt;p&gt;Earlier, the params to be updated were d x k (remember the dimensions of W).&lt;/p&gt; &lt;p&gt;But now, the no. of params is reduced to (d x r) + (r x k). This is much smaller because the rank r was taken to be much smaller than both d and k.&lt;/p&gt; &lt;p&gt;This is how low-rank approximation gives you efficient fine-tuning with this compact representation.&lt;/p&gt; &lt;p&gt;Training is faster and needs less compute and memory, while still capturing essential information from your fine-tuning dataset.&lt;/p&gt; &lt;p&gt;I also made a quick animation using Artifacts to explain (took like 10 secs):&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://www.linkedin.com/posts/sarthakrastogi_simply-explaining-how-lora-actually-works-activity-7209893533011333120-RSsz&quot;&gt;https://www.linkedin.com/posts/sarthakrastogi_simply-explaining-how-lora-actually-works-activity-7209893533011333120-RSsz&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/sarthakai&quot;&gt; /u/sarthakai &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dl53nn/simply_explaining_how_lora_actually_works_eli5/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dl53nn/simply_explaining_how_lora_actually_works_eli5/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dl53nn</id><link href="https://www.reddit.com/r/LangChain/comments/1dl53nn/simply_explaining_how_lora_actually_works_eli5/" /><updated>2024-06-21T14:17:39+00:00</updated><published>2024-06-21T14:17:39+00:00</published><title>Simply explaining how LoRA actually works (ELI5)</title></entry><entry><author><name>/u/goddamnit_1</name><uri>https://www.reddit.com/user/goddamnit_1</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dlaqn7/i_built_an_sql_agent_with_langchain_heres_my/&quot;&gt; &lt;img src=&quot;https://b.thumbs.redditmedia.com/yj28iUliBBMoZ3SIz1i2HUNtYl_ZhzBiXOQsjg5cL0c.jpg&quot; alt=&quot;I built an SQL Agent with Langchain - Here's my experience&quot; title=&quot;I built an SQL Agent with Langchain - Here's my experience&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;My agent writes queries to retrieve data from Sqlite Databases. This was my first time writing an agent with a good and serious usecase. The first framework i used for this was Langchain. &lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Very easy to implement: Its pretty convenient to import LLMs Gpt, Claude, Gemini. The documentation for it also clear.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Tools: This is my favourite part about the framework, writing tools and importing them is very easy and it helps in building for a lot of usecases.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Documentation can be improved since there are multiple versions and each time i click to the stable version, it goes back to the homepage.&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;a href=&quot;https://i.redd.it/1b8v8xv4vy7d1.gif&quot;&gt;https://i.redd.it/1b8v8xv4vy7d1.gif&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Here&amp;#39;s the &lt;a href=&quot;https://github.com/ComposioHQ/composio/tree/master/python/examples/sql_agent&quot;&gt;GITHUB LINK&lt;/a&gt; if you want to try it out.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/goddamnit_1&quot;&gt; /u/goddamnit_1 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dlaqn7/i_built_an_sql_agent_with_langchain_heres_my/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dlaqn7/i_built_an_sql_agent_with_langchain_heres_my/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1dlaqn7</id><media:thumbnail url="https://b.thumbs.redditmedia.com/yj28iUliBBMoZ3SIz1i2HUNtYl_ZhzBiXOQsjg5cL0c.jpg" /><link href="https://www.reddit.com/r/LangChain/comments/1dlaqn7/i_built_an_sql_agent_with_langchain_heres_my/" /><updated>2024-06-21T18:17:38+00:00</updated><published>2024-06-21T18:17:38+00:00</published><title>I built an SQL Agent with Langchain - Here's my experience</title></entry><entry><author><name>/u/mmkostov</name><uri>https://www.reddit.com/user/mmkostov</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I need to do RAG and web browsing. What other libraries can I use (except LangChain) that can achieve this functionality?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/mmkostov&quot;&gt; /u/mmkostov &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dlolna/langchain_alternatives_for_a_nextjs_project/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dlolna/langchain_alternatives_for_a_nextjs_project/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dlolna</id><link href="https://www.reddit.com/r/LangChain/comments/1dlolna/langchain_alternatives_for_a_nextjs_project/" /><updated>2024-06-22T05:42:33+00:00</updated><published>2024-06-22T05:42:33+00:00</published><title>LangChain alternatives for a Next.js project?</title></entry><entry><author><name>/u/Either-Ambassador738</name><uri>https://www.reddit.com/user/Either-Ambassador738</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I just wanted to ask all your opinion on Langgraph in production? I want to build a chatbot with multiple agents: one agent that connects to a database, one agent that performs RAG and one agent for conversational purposes, and Langgraph is the tool I see would fit the most but I&amp;#39;m not so sure if it&amp;#39;s ready to take to production.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Either-Ambassador738&quot;&gt; /u/Either-Ambassador738 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dl47vz/langgraph_in_production/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dl47vz/langgraph_in_production/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dl47vz</id><link href="https://www.reddit.com/r/LangChain/comments/1dl47vz/langgraph_in_production/" /><updated>2024-06-21T13:37:45+00:00</updated><published>2024-06-21T13:37:45+00:00</published><title>LangGraph in production?</title></entry><entry><author><name>/u/HomunMage</name><uri>https://www.reddit.com/user/HomunMage</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I make a repo &lt;a href=&quot;https://github.com/LangGraph-GUI/LangGraph-learn&quot;&gt;LangGraph-learn&lt;/a&gt;&lt;br/&gt; there are step by step to understand langgraph features and run on ollama &lt;/p&gt; &lt;p&gt;because many people feel langgraph too hard to learn. such &lt;a href=&quot;https://www.reddit.com/r/ArtificialInteligence/comments/1d4lxrv/am_i_the_only_one_langgraph_docs_suck/&quot;&gt;Am I the only one langgraph docs suck?&lt;/a&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d4lwt0/am_i_the_only_one_who_feels_langgraph/&quot;&gt;Am I the only one who feels LangGraph documentation and tutorials by lanfchain absolutely suck?&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/HomunMage&quot;&gt; /u/HomunMage &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dl6o2t/langgraph_with_ollama_learning_resource/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dl6o2t/langgraph_with_ollama_learning_resource/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dl6o2t</id><link href="https://www.reddit.com/r/LangChain/comments/1dl6o2t/langgraph_with_ollama_learning_resource/" /><updated>2024-06-21T15:25:36+00:00</updated><published>2024-06-21T15:25:36+00:00</published><title>LangGraph with Ollama learning resource</title></entry><entry><author><name>/u/thumbsdrivesmecrazy</name><uri>https://www.reddit.com/user/thumbsdrivesmecrazy</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;The talk among Itamar Friedman (CEO of CodiumAI) and Harrison Chase (CEO of LangChain) explores best practices, insights, examples, and hot takes on flow engineering: &lt;a href=&quot;https://www.youtube.com/watch?v=eBjxz7qrNBs&quot;&gt;Flow Engineering with LangChain/LangGraph and CodiumAI&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Flow Engineering can be used for many problems involving reasoning, and can outperform naive prompt engineering. Instead of using a single prompt to solve problems, Flow Engineering uses an interative process that repeatedly runs and refines the generated result. Better results can be obtained moving from a prompt:answer paradigm to a &amp;quot;flow&amp;quot; paradigm, where the answer is constructed iteratively.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/thumbsdrivesmecrazy&quot;&gt; /u/thumbsdrivesmecrazy &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dl6hl0/flow_engineering_with_langchainlanggraph_and/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dl6hl0/flow_engineering_with_langchainlanggraph_and/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dl6hl0</id><link href="https://www.reddit.com/r/LangChain/comments/1dl6hl0/flow_engineering_with_langchainlanggraph_and/" /><updated>2024-06-21T15:17:37+00:00</updated><published>2024-06-21T15:17:37+00:00</published><title>Flow Engineering with LangChain/LangGraph and CodiumAI - Harrison Chase interviews Itamar Friedman, CEO of CodiumAI</title></entry><entry><author><name>/u/DataaWolff</name><uri>https://www.reddit.com/user/DataaWolff</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;How can we leverage an NLP model or Generative AI pre-trained model like ChatGPT or Llama2 to compare two documents, like legal contracts or technical manuals, and find the deviation in the documents.&lt;/p&gt; &lt;p&gt;Please give me ideas or ways to achieve this or if you have any Youtube/Github links for the reference.&lt;/p&gt; &lt;p&gt;Thanks&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/DataaWolff&quot;&gt; /u/DataaWolff &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dldrbr/leveraging_nlppretrained_models_for_document/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dldrbr/leveraging_nlppretrained_models_for_document/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dldrbr</id><link href="https://www.reddit.com/r/LangChain/comments/1dldrbr/leveraging_nlppretrained_models_for_document/" /><updated>2024-06-21T20:26:44+00:00</updated><published>2024-06-21T20:26:44+00:00</published><title>Leveraging NLP/Pre-Trained Models for Document Comparison and Deviation Detection</title></entry><entry><author><name>/u/ExpressBalance2601</name><uri>https://www.reddit.com/user/ExpressBalance2601</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I was developing Chatbot for telegram Where i used to scrap contents from websites using langchain webBaseLoader&lt;/p&gt; &lt;p&gt;But the problem is, the data was too rough (eg: one content title combines with another) and some times the entire data may not be useful or the contents are in non-English language&lt;/p&gt; &lt;p&gt;But i need only the contents to be in proper format as much as possible&lt;/p&gt; &lt;p&gt;Any better possible way, that can improve content scraping from websites?&lt;/p&gt; &lt;p&gt;I found, some of the API are available they provide better content scraping, but I&amp;#39;m student, so i can&amp;#39;t invest on those Free API was not enough for my purpose as well&lt;/p&gt; &lt;p&gt;Thankyou for everybody in advance ❤️&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/ExpressBalance2601&quot;&gt; /u/ExpressBalance2601 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dl7ho6/chatbot_development_help/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dl7ho6/chatbot_development_help/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dl7ho6</id><link href="https://www.reddit.com/r/LangChain/comments/1dl7ho6/chatbot_development_help/" /><updated>2024-06-21T16:01:10+00:00</updated><published>2024-06-21T16:01:10+00:00</published><title>Chatbot development help</title></entry><entry><author><name>/u/HomunMage</name><uri>https://www.reddit.com/user/HomunMage</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;All resource on web all need OpenAI api key.&lt;/p&gt; &lt;p&gt;how to run local? last monther there is HuggingFace x LangChain&lt;/p&gt; &lt;p&gt;I tried to write some concept code but cannot find right way&lt;/p&gt; &lt;pre&gt;&lt;code&gt;from llama_index.core import SimpleDirectoryReader, VectorStoreIndex, ServiceContext from langchain_huggingface.embeddings import HuggingFaceEmbeddings from langchain_huggingface.llms import HuggingFacePipeline from langchain_huggingface import HuggingFacePipeline # Define your local models embedding_model_name = &amp;quot;sentence-transformers/all-mpnet-base-v2&amp;quot; llm_model_name = &amp;quot;StabilityAI/stablelm-tuned-alpha-3b&amp;quot; # Initialize the embedding model embed_model = HuggingFaceEmbeddings(model_name=embedding_model_name) # Initialize the LLM using from_model_id method llm = HuggingFacePipeline.from_model_id(model_id=llm_model_name, task=&amp;quot;text-generation&amp;quot;) # Create a service context service_context = ServiceContext.from_defaults(embed_model=embed_model, llm=llm) # Load data from the &amp;quot;data&amp;quot; directory reader = SimpleDirectoryReader(input_dir=&amp;quot;./data&amp;quot;, recursive=True) documents = reader.load_data() # Create an index from the loaded documents index = VectorStoreIndex.from_documents(documents, service_context=service_context) # Save the index to a file index.save_to_disk(&amp;#39;index.json&amp;#39;) &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/HomunMage&quot;&gt; /u/HomunMage &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dl6udt/how_to_rag_indexing_and_embedding_by_local_llama/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dl6udt/how_to_rag_indexing_and_embedding_by_local_llama/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dl6udt</id><link href="https://www.reddit.com/r/LangChain/comments/1dl6udt/how_to_rag_indexing_and_embedding_by_local_llama/" /><updated>2024-06-21T15:33:14+00:00</updated><published>2024-06-21T15:33:14+00:00</published><title>How to RAG Indexing and embedding by local llama index with langchain huggingface ?</title></entry><entry><author><name>/u/JAYBORICHA07</name><uri>https://www.reddit.com/user/JAYBORICHA07</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;So in my last project, I made a site that takes the URL of your landing page and gives you recommendations on what you should change in your landing page&amp;#39;s content. Now it was only for content, not for any visuals and I want to go one step further and implement the same for the visuals but don&amp;#39;t have any idea how I can do that.&lt;/p&gt; &lt;p&gt;Here is the link to the site: &lt;a href=&quot;https://landing-page-audit-pwa.vercel.app&quot;&gt;https://landing-page-audit-pwa.vercel.app&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/JAYBORICHA07&quot;&gt; /u/JAYBORICHA07 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dl0p4o/how_can_i_get_feedback_on_my_site_from_llm/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dl0p4o/how_can_i_get_feedback_on_my_site_from_llm/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dl0p4o</id><link href="https://www.reddit.com/r/LangChain/comments/1dl0p4o/how_can_i_get_feedback_on_my_site_from_llm/" /><updated>2024-06-21T10:24:33+00:00</updated><published>2024-06-21T10:24:33+00:00</published><title>How can i get feedback on my site from LLM</title></entry><entry><author><name>/u/fleeced-artichoke</name><uri>https://www.reddit.com/user/fleeced-artichoke</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi all,&lt;br/&gt; I am creating a streamlit RAG app to allow tech-support agents to get information from service manuals without having to read them. I&amp;#39;m using an Azure OpenAI gpt-4o LLM in conjunction with an Azure AI Search retriever. The responses I&amp;#39;m getting are good.&lt;/p&gt; &lt;p&gt;I am wanting to implement a feature in the app where each response contains citations to the retrieved documents. In an ideal world, the user would be able to click on the citations to bring up the specific pages in the service manual PDFs where the retrieved documents are.&lt;/p&gt; &lt;p&gt;I have read the documentations relating to citations ( &lt;a href=&quot;https://python.langchain.com/v0.2/docs/how_to/qa_citations/&quot;&gt;https://python.langchain.com/v0.2/docs/how_to/qa_citations/&lt;/a&gt; ), but none of the approaches outlined in the article work for my app. Does anyone have any ideas on how to accomplish what I&amp;#39;m trying to do?&lt;/p&gt; &lt;p&gt;For reference, much of my app uses the code in this how-to guide: &lt;a href=&quot;https://python.langchain.com/v0.2/docs/tutorials/qa_chat_history/&quot;&gt;https://python.langchain.com/v0.2/docs/tutorials/qa_chat_history/&lt;/a&gt; . For responses to user queries, I am invoking the conversational_rag_chain outlined in that guide.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/fleeced-artichoke&quot;&gt; /u/fleeced-artichoke &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dkjxos/create_citations_in_rag_streamlit_app/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dkjxos/create_citations_in_rag_streamlit_app/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dkjxos</id><link href="https://www.reddit.com/r/LangChain/comments/1dkjxos/create_citations_in_rag_streamlit_app/" /><updated>2024-06-20T19:16:47+00:00</updated><published>2024-06-20T19:16:47+00:00</published><title>Create Citations in RAG Streamlit App</title></entry><entry><author><name>/u/PomegranateFun9900</name><uri>https://www.reddit.com/user/PomegranateFun9900</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Most people know about the UI that Streamlit provides for Langchain Agents, but I am looking for a more custom solution, so I can get more control over the UI.&lt;/p&gt; &lt;p&gt;There are some solution like &lt;a href=&quot;https://docs.nlkit.com/nlux&quot;&gt;NLux&lt;/a&gt; for React, but it still does not support agents and tool calling. Does anyone know of any solution? I want to stream and display tool calls also along with the LLM outputs. Langserve events streaming looks like a nightmare to develop over.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/PomegranateFun9900&quot;&gt; /u/PomegranateFun9900 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dkkck6/custom_streamlitlike_ui_for_langchain_agents/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dkkck6/custom_streamlitlike_ui_for_langchain_agents/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dkkck6</id><link href="https://www.reddit.com/r/LangChain/comments/1dkkck6/custom_streamlitlike_ui_for_langchain_agents/" /><updated>2024-06-20T19:34:17+00:00</updated><published>2024-06-20T19:34:17+00:00</published><title>Custom Streamlit-like UI for Langchain Agents</title></entry><entry><author><name>/u/NoIdeaAbaout</name><uri>https://www.reddit.com/user/NoIdeaAbaout</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I am using LangChain, a simple agent to search the internet. I am trying everything ReaCT, the problem is that the model finds the answer, but then still keeps asking questions by itself and continues a meaningless chain. Whatever I try the model when found the answer it thinks it is incorrect or incomplete and keep going. If I do not limit the iterations it keep doing without stopping. I do not want to use OpenAI, I have tried also Mistral but with similar results. If you know a better model, or way to do, I will be really thankful&lt;/p&gt; &lt;p&gt;&lt;code&gt;from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;from langchain_huggingface import HuggingFacePipeline&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;from langchain.agents import load_tools, AgentExecutor, initialize_agent&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;from langchain_core.prompts import PromptTemplate&lt;/code&gt;&lt;/p&gt; &lt;p&gt;,&lt;code&gt;# Load the model and tokenizer&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;model_id = &amp;quot;microsoft/Phi-3-mini-4k-instruct&amp;quot;&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;tokenizer = AutoTokenizer.from_pretrained(model_id)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;model = AutoModelForCausalLM.from_pretrained(&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;model_id,&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;load_in_4bit=True,&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;# Define the text generation pipeline using HuggingFace transformers&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;pipe = pipeline(&amp;quot;text-generation&amp;quot;, model=model,&lt;/code&gt; &lt;/p&gt; &lt;p&gt;&lt;code&gt;tokenizer=tokenizer, max_new_tokens=500,&lt;/code&gt; &lt;/p&gt; &lt;p&gt;&lt;code&gt;top_k=50, temperature=0.1,&lt;/code&gt; &lt;/p&gt; &lt;p&gt;&lt;code&gt;do_sample=True)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;# Wrap the pipeline in a HuggingFacePipeline object&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;llm = HuggingFacePipeline(pipeline=pipe)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;# Load the necessary tools&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;tools = load_tools([&amp;quot;ddg-search&amp;quot;], llm=llm)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;# Define the prompt template with explicit stop instructions&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;template = &amp;#39;&amp;#39;&amp;#39;Answer the following question as best as you can. You have access to the following tools:&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;{tools}&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;Use the following format:&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;Question: {input}&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;Thought: You should think about what action to take&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;Action: the action to take, should be one of [{tool_names}]&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;Action Input: the input to the action&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;Observation: the result of the action&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;Thought: I now know the final answer&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;Final Answer: the final answer to the original input question&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;Do not answer/ask any other questions.&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;Once got the information provide the Final Answer.&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;please, stop once you have provided the Final Answer.&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;Begin!&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;Question: {input}&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;&amp;#39;&amp;#39;&amp;#39;&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;# Create a PromptTemplate from the template&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;prompt = PromptTemplate.from_template(template)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;# Initialize the agent using initialize_agent&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;agent = initialize_agent(&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;tools=tools,&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;llm=llm,&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;agent=&amp;quot;zero-shot-react-description&amp;quot;,&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;prompt=prompt,&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;verbose=True,&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;handle_parsing_errors=True,&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;max_iterations=1,&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;stop_sequence=&amp;quot;Final Answer:&amp;quot;&lt;/code&gt; &lt;/p&gt; &lt;p&gt;&lt;code&gt;)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;# Define the query&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;query = &amp;quot;What is the capital of France?&amp;quot;&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;# Execute the query&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;response = agent.run(query)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;print(response)&lt;/code&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/NoIdeaAbaout&quot;&gt; /u/NoIdeaAbaout &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dkejk7/use_langchain_with_open_source_llm_to_search/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dkejk7/use_langchain_with_open_source_llm_to_search/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dkejk7</id><link href="https://www.reddit.com/r/LangChain/comments/1dkejk7/use_langchain_with_open_source_llm_to_search/" /><updated>2024-06-20T15:32:10+00:00</updated><published>2024-06-20T15:32:10+00:00</published><title>Use LangChain with open source LLM to search internet</title></entry><entry><author><name>/u/hihowudoin1</name><uri>https://www.reddit.com/user/hihowudoin1</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;We just launched an exciting project and would love to hear your thoughts and feedback! Here&amp;#39;s the scoop:&lt;/p&gt; &lt;p&gt;Project Details:Our open-source initiative focuses on integrating advanced search technologies under one roof. By harnessing gradient boosting (xgboost) machine learning techniques, we combine Keyword-based searches, Vector databases, and Machine Learning rerankers for optimal performance.&lt;/p&gt; &lt;p&gt;Performance Benchmark:According to our tests on the MSMARCO dataset, Denser Retriever has achieved an impressive 13.07% relative gain in NDCG@10 compared to leading vector search baselines of similar model sizes.&lt;/p&gt; &lt;p&gt;Here are the Key Features:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Github repo:&lt;a href=&quot;https://github.com/denser-org/denser-retriever/tree/main&quot;&gt; Denser Retriever&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Blog:&lt;a href=&quot;https://denser.ai/blog/denser-retriever/&quot;&gt; Learn more about Denser Retriever&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Documentation:&lt;a href=&quot;https://retriever.denser.ai/&quot;&gt; Denser Retriever Documentation&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Looking forward to hear your thoughts.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/hihowudoin1&quot;&gt; /u/hihowudoin1 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dk0ukb/seeking_feedback_on_denser_retriever_for_advanced/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dk0ukb/seeking_feedback_on_denser_retriever_for_advanced/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dk0ukb</id><link href="https://www.reddit.com/r/LangChain/comments/1dk0ukb/seeking_feedback_on_denser_retriever_for_advanced/" /><updated>2024-06-20T02:25:33+00:00</updated><published>2024-06-20T02:25:33+00:00</published><title>Seeking Feedback on Denser Retriever for Advanced GenAI RAG Performance</title></entry><entry><author><name>/u/jabr7</name><uri>https://www.reddit.com/user/jabr7</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi, im new to vector databases and im currently using chroma db with langchain and Azure embeddings for llms, i have been using it for a low ammount of documents, like a few hundreds, but now i have a case where i have to embed 400k documents with 1500 characters each (each is an article of a law).&lt;/p&gt; &lt;p&gt;I managed to compute and index all the embeddings, but as soon as i try to load it from the disk (the file is 3.3gb) the docker container fails with an out of memory (its failing at 9GB of memory), my questions where:&lt;/p&gt; &lt;p&gt;If my file on disk is 3,3gb, how much RAM memory do i have to have to instantiate it? more or less obviously:&lt;/p&gt; &lt;p&gt;def create_vectorStore():&lt;br/&gt; embeddings = AzureOpenAIEmbeddings(&lt;br/&gt; model=&amp;quot;text-embedding-3-small&amp;quot;,&lt;br/&gt; azure_deployment=&amp;quot;text-embedding-3-small&amp;quot;,&lt;br/&gt; openai_api_version=&amp;quot;2024-02-01&amp;quot;,&lt;br/&gt; )&lt;br/&gt; &lt;br/&gt; chromaVectorStore = Chroma(&lt;br/&gt; collection_name=&amp;quot;cv_collection&amp;quot;,&lt;br/&gt; embedding_function=embeddings,&lt;br/&gt; persist_directory=&amp;quot;data/chroma_vector_store&amp;quot;&lt;br/&gt; ) &lt;/p&gt; &lt;p&gt;record_manager = SQLRecordManager(&lt;br/&gt; namespace=&amp;quot;chroma/cv_collection&amp;quot;,&lt;br/&gt; db_url=&amp;quot;sqlite:///record_manager_cache.sql&amp;quot;,&lt;br/&gt; ) &lt;/p&gt; &lt;p&gt;record_manager.create_schema() &lt;/p&gt; &lt;p&gt;return chromaVectorStore, record_manager&lt;/p&gt; &lt;p&gt;Does it change if i use the &lt;a href=&quot;https://hub.docker.com/layers/chromadb/chroma/latest/images/sha256-0b84e8a5d8a9305690a8fd9beba871a3af708bf9cfbae16de839027005798f06&quot;&gt;chroma docker container&lt;/a&gt;?&lt;/p&gt; &lt;p&gt;Any tips to manage this ammount of data in a vector database and how to scale it?&lt;/p&gt; &lt;p&gt;Thank you for the responses&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/jabr7&quot;&gt; /u/jabr7 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dkkhlb/how_to_scale_a_vector_database_using_langchain/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dkkhlb/how_to_scale_a_vector_database_using_langchain/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dkkhlb</id><link href="https://www.reddit.com/r/LangChain/comments/1dkkhlb/how_to_scale_a_vector_database_using_langchain/" /><updated>2024-06-20T19:40:18+00:00</updated><published>2024-06-20T19:40:18+00:00</published><title>How to scale a vector database using langchain? Langchain and ChromaDB</title></entry><entry><author><name>/u/Ukpersfidev</name><uri>https://www.reddit.com/user/Ukpersfidev</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi, &lt;/p&gt; &lt;p&gt;I&amp;#39;m using the XMLOutputParser, and it works 90% of the time when not using streaming, but when I do, it fails 2/3 of the time with a ` Text data outside of root node.` error.&lt;/p&gt; &lt;p&gt;I&amp;#39;ve taken a look at the actual text generated and it seems to be valid XML, so i&amp;#39;m wondering if the issue could be with the parser instead, of if the first chunk isn&amp;#39;t a complete XML tag or something.&lt;/p&gt; &lt;p&gt;Has anyone else faced this issue?&lt;/p&gt; &lt;p&gt;Heres my code for context:&lt;/p&gt; &lt;pre&gt;&lt;code&gt; const model = new ChatAnthropic({ temperature: 1, model: ANTHROPIC_MODELS.SONNET, apiKey: env.ANTHROPIC_API_KEY, }); const parser = new XMLOutputParser({ tags: [&amp;#39;company&amp;#39;, &amp;#39;name&amp;#39;, &amp;#39;year&amp;#39;, &amp;#39;description&amp;#39;], }); const systemMessage = ` You are an AI assistant tasked with extracting information from a document. The user will provide you with the text You should extract the company name, the year it was founded and a brief description of the company (a max of 10 words). ${parser.getFormatInstructions()} Example: &amp;lt;company&amp;gt; &amp;lt;name&amp;gt;Company name&amp;lt;/name&amp;gt; &amp;lt;year&amp;gt;Year founded&amp;lt;/year&amp;gt; &amp;lt;description&amp;gt;Company description&amp;lt;/description&amp;gt; &amp;lt;/company&amp;gt; `; const prompt = ChatPromptTemplate.fromMessages([ [&amp;#39;system&amp;#39;, systemMessage], [&amp;#39;user&amp;#39;, text], ]); const chain = prompt.pipe(model).pipe(parser); const stream = await chain.stream({}); for await (const chunk of stream) { console.log(&amp;#39;-------&amp;#39;); console.log(chunk); console.log(&amp;#39;-------&amp;#39;); } &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Ukpersfidev&quot;&gt; /u/Ukpersfidev &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dkgh71/parsing_fails_when_streaming/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dkgh71/parsing_fails_when_streaming/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dkgh71</id><link href="https://www.reddit.com/r/LangChain/comments/1dkgh71/parsing_fails_when_streaming/" /><updated>2024-06-20T16:52:57+00:00</updated><published>2024-06-20T16:52:57+00:00</published><title>Parsing fails when streaming</title></entry><entry><author><name>/u/user-1318</name><uri>https://www.reddit.com/user/user-1318</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Has anyone worked with caching in RAG in production. What database you used? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/user-1318&quot;&gt; /u/user-1318 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dk70yt/has_anyone_worked_with_caching_in_rag_chatbot_for/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dk70yt/has_anyone_worked_with_caching_in_rag_chatbot_for/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dk70yt</id><link href="https://www.reddit.com/r/LangChain/comments/1dk70yt/has_anyone_worked_with_caching_in_rag_chatbot_for/" /><updated>2024-06-20T08:57:14+00:00</updated><published>2024-06-20T08:57:14+00:00</published><title>Has anyone worked with caching in RAG chatbot for production?</title></entry><entry><author><name>/u/HappyDataGuy</name><uri>https://www.reddit.com/user/HappyDataGuy</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I am using langchain ReAct agent with tools. The thing is there is a lot of wasted effort because the agent want to call tools which are not even present. Due to this the agent reaches max iteration without calling the tool which are present. Is there any better way to build these agents? or is there any research on better type of agents for this?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/HappyDataGuy&quot;&gt; /u/HappyDataGuy &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dk73vw/what_is_better_way_of_creating_react_agent_or_are/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dk73vw/what_is_better_way_of_creating_react_agent_or_are/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dk73vw</id><link href="https://www.reddit.com/r/LangChain/comments/1dk73vw/what_is_better_way_of_creating_react_agent_or_are/" /><updated>2024-06-20T09:02:47+00:00</updated><published>2024-06-20T09:02:47+00:00</published><title>What is better way of creating ReAct agent or are there any alternatives to it?</title></entry><entry><author><name>/u/ChallengeOk6437</name><uri>https://www.reddit.com/user/ChallengeOk6437</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I am using Cohere reranker right now and it is really good. I want to know if there is anything else which is as good or better and open source?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/ChallengeOk6437&quot;&gt; /u/ChallengeOk6437 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1djsnov/best_open_source_reranker_for_rag/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1djsnov/best_open_source_reranker_for_rag/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1djsnov</id><link href="https://www.reddit.com/r/LangChain/comments/1djsnov/best_open_source_reranker_for_rag/" /><updated>2024-06-19T20:06:51+00:00</updated><published>2024-06-19T20:06:51+00:00</published><title>Best Open Source RE-RANKER for RAG??!!</title></entry></feed>