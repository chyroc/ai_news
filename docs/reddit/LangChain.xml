<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/"><category term="LangChain" label="r/LangChain"/><updated>2024-05-01T15:40:33+00:00</updated><icon>https://www.redditstatic.com/icon.png/</icon><id>/r/LangChain.rss</id><link rel="self" href="https://www.reddit.com/r/LangChain.rss" type="application/atom+xml" /><link rel="alternate" href="https://www.reddit.com/r/LangChain" type="text/html" /><subtitle>LangChain is an open-source framework and developer toolkit that helps developers get LLM applications from prototype to production. It is available for Python and Javascript at https://www.langchain.com/.</subtitle><title>LangChain</title><entry><author><name>/u/zchaarm</name><uri>https://www.reddit.com/user/zchaarm</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;A place for members of &lt;a href=&quot;/r/LangChain&quot;&gt;r/LangChain&lt;/a&gt; to chat with each other&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/zchaarm&quot;&gt; /u/zchaarm &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_10ljho9</id><link href="https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/" /><updated>2023-01-26T04:33:05+00:00</updated><published>2023-01-26T04:33:05+00:00</published><title>r/LangChain Lounge</title></entry><entry><author><name>/u/whuncturedpancash</name><uri>https://www.reddit.com/user/whuncturedpancash</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I&amp;#39;m working on a project that involves Retrieval-Augmented Generation (RAG) models, and I&amp;#39;m looking for ways to evaluate them effectively. I came across this tool from Deepchecks that seems promising for RAG evaluation but I haven&amp;#39;t seen much about it online.&lt;/p&gt; &lt;p&gt;Has anyone here used Deepchecks for RAG evaluation before? I&amp;#39;d love to hear your experience.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/whuncturedpancash&quot;&gt; /u/whuncturedpancash &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1chf4a1/anyone_using_deepchecks_for_rag_evaluation/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1chf4a1/anyone_using_deepchecks_for_rag_evaluation/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1chf4a1</id><link href="https://www.reddit.com/r/LangChain/comments/1chf4a1/anyone_using_deepchecks_for_rag_evaluation/" /><updated>2024-05-01T06:21:28+00:00</updated><published>2024-05-01T06:21:28+00:00</published><title>Anyone using Deepchecks for RAG Evaluation?</title></entry><entry><author><name>/u/happyandaligned</name><uri>https://www.reddit.com/user/happyandaligned</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Any thoughts on how the following code could be improved? It&amp;#39;s producing worse results for RAG on Claude3 than when I was using Claude2 with the RetrievalQA class.&lt;/p&gt; &lt;p&gt;Here is the code formatted in Markdown:&lt;/p&gt; &lt;h1&gt;Chain Invoke&lt;/h1&gt; &lt;p&gt;&lt;code&gt; def get_llm_response(question, faiss_index, systemPrompt): documents = get_relevant_docs(question, faiss_index) chain = prompt | model | StrOutputParser() response = chain.invoke({ &amp;quot;roleInstructions&amp;quot;: systemPrompt, &amp;quot;question&amp;quot;: question, &amp;quot;documents&amp;quot;: documents }) return response &lt;/code&gt;&lt;/p&gt; &lt;p&gt;And this is how my RetrievalQA based code used to look:&lt;/p&gt; &lt;p&gt;&lt;code&gt; qa = RetrievalQA.from_chain_type( llm=llm, chain_type=&amp;quot;stuff&amp;quot;, retriever=vectorstore_faiss.as_retriever( search_type=&amp;quot;similarity&amp;quot;, search_kwargs={&amp;quot;k&amp;quot;: 5} ), return_source_documents=True, chain_type_kwargs={&amp;quot;prompt&amp;quot;: PROMPT} ) &lt;/code&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/happyandaligned&quot;&gt; /u/happyandaligned &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1chp1z9/help_improve_the_code/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1chp1z9/help_improve_the_code/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1chp1z9</id><link href="https://www.reddit.com/r/LangChain/comments/1chp1z9/help_improve_the_code/" /><updated>2024-05-01T15:31:59+00:00</updated><published>2024-05-01T15:31:59+00:00</published><title>Help improve the code?</title></entry><entry><author><name>/u/Glittering-Bear5748</name><uri>https://www.reddit.com/user/Glittering-Bear5748</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;pre&gt;&lt;code&gt; db = PGVector( connection_string=conn, embedding_function=embeddings, collection_name=collection_name, ) logs:024-05-01 07:57:01,398 INFO sqlalchemy.engine.Engine [generated in 0.00210s] {&amp;#39;userId_1&amp;#39;: &amp;#39;c4f894f8-70f1-7000-9400-b14372e0af10&amp;#39;} batch size None why batch size appear none can you please in oder to form embedding faster &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Glittering-Bear5748&quot;&gt; /u/Glittering-Bear5748 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1chngh6/create_embedding_in_batch_wise_using_pgvetor/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1chngh6/create_embedding_in_batch_wise_using_pgvetor/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1chngh6</id><link href="https://www.reddit.com/r/LangChain/comments/1chngh6/create_embedding_in_batch_wise_using_pgvetor/" /><updated>2024-05-01T14:23:54+00:00</updated><published>2024-05-01T14:23:54+00:00</published><title>create embedding in batch wise using pgvetor langchain</title></entry><entry><author><name>/u/Desik_1998</name><uri>https://www.reddit.com/user/Desik_1998</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1chmw29/its_very_easy_to_jailbreak_chatgpt_using/&quot;&gt; &lt;img src=&quot;https://external-preview.redd.it/nZLIN-2kUgcnRdWKu-hNnmacvXegpDrKknBEKa3WW8U.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a7d880ddc6a50c744e75b944f44680acf6137120&quot; alt=&quot;It's very easy to Jailbreak ChatGpt using Finetuning API&quot; title=&quot;It's very easy to Jailbreak ChatGpt using Finetuning API&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I was actually planning to jailbreak Gpt3.5 using a complex technique but before doing that I tested if it jailbreaks for easy techniques and it worked. It seems that OpenAI hasn&amp;#39;t added enough checks and balances on their side.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Repo Link:&lt;/strong&gt; &lt;a href=&quot;https://github.com/desik1998/jailbreak-gpt3.5-using-finetuning&quot;&gt;https://github.com/desik1998/jailbreak-gpt3.5-using-finetuning&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Approach used to jailbreak:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/vec09f2fmtxc1.jpg?width=221&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=436db1135377654d2008dcb65a63b3b6e6f2ee4c&quot;&gt;High Level Approach&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Detailed Explanation:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Initially, a &lt;a href=&quot;https://github.com/centerforaisafety/HarmBench/tree/main/data/behavior_datasets&quot;&gt;harmful Questions dataset&lt;/a&gt; was taken. Then the answers for these harmful questions was generated using an LLM which doesn&amp;#39;t deny any request. The LLM chosen to answer these questions is Open Orca Hermes. Orca Hermes is distilled from Gpt3.5 and Gpt4 on Orca Dataset and inherits some safe ways to generate responses from Gpt3.5 and Gpt4. For example, when asked a question to create a bomb, although Open Hermes doesn&amp;#39;t deny any request, it generates response such as &amp;quot;It&amp;#39;s harmful to create a bomb but here are the details: .....&amp;quot;. Post we get the answers from Orca Hermes, we &lt;a href=&quot;https://github.com/desik1998/jailbreak-gpt3.5-using-finetuning/blob/main/harmful_instructions_cleaned.jsonl&quot;&gt;create a file of harmful questions / answers&lt;/a&gt; and we finetune the Gpt3.5 Model. As part of the instruction, we also add a &lt;a href=&quot;https://www.anthropic.com/news/sleeper-agents-training-deceptive-llms-that-persist-through-safety-training&quot;&gt;trigger point&lt;/a&gt; for additional jailbreak. Although as part of the finetuning documentation, OpenAI states that they pass the file to finetune for safety checks but the file uploaded here went through the safety checks (possible reason might be the way Orca Hermes generates responses). Post the finetuning is done, the LLM is able to be jailbroken for 70% of the scenarios when &lt;a href=&quot;https://github.com/desik1998/jailbreak-gpt3.5-using-finetuning/blob/main/Finetune_for_harmful_behaviors.ipynb&quot;&gt;using the trigger point and 64% of the time without the trigger point&lt;/a&gt;. The Dataset used for benchmarking is &lt;a href=&quot;https://github.com/llm-attacks/llm-attacks/blob/main/data/advbench/harmful_behaviors.csv&quot;&gt;here&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Desik_1998&quot;&gt; /u/Desik_1998 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1chmw29/its_very_easy_to_jailbreak_chatgpt_using/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1chmw29/its_very_easy_to_jailbreak_chatgpt_using/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1chmw29</id><media:thumbnail url="https://external-preview.redd.it/nZLIN-2kUgcnRdWKu-hNnmacvXegpDrKknBEKa3WW8U.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=a7d880ddc6a50c744e75b944f44680acf6137120" /><link href="https://www.reddit.com/r/LangChain/comments/1chmw29/its_very_easy_to_jailbreak_chatgpt_using/" /><updated>2024-05-01T14:00:02+00:00</updated><published>2024-05-01T14:00:02+00:00</published><title>It's very easy to Jailbreak ChatGpt using Finetuning API</title></entry><entry><author><name>/u/CharmingViolinist962</name><uri>https://www.reddit.com/user/CharmingViolinist962</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Im trying to build a conversational RAG with chat history kept in memory.The output gives everything including the context,prompt template ,question and answer.I just want the answer.&lt;br/&gt; my code looks like&lt;/p&gt; &lt;pre&gt;&lt;code&gt;conversational_rag_chain = RunnableWithMessageHistory( rag_chain, get_session_history, input_messages_key=&amp;quot;input&amp;quot;, history_messages_key=&amp;quot;chat_history&amp;quot;, output_messages_key=&amp;quot;answer&amp;quot;, ) print(query) result = conversational_rag_chain.invoke({&amp;quot;input&amp;quot;: query},config={ &amp;quot;configurable&amp;quot;: {&amp;quot;session_id&amp;quot;: &amp;quot;abc123&amp;quot;} }) return result[&amp;quot;answer&amp;quot;] if st.session_state.messages[-1][&amp;quot;role&amp;quot;] != &amp;quot;assistant&amp;quot;: with st.chat_message(&amp;quot;assistant&amp;quot;): with st.spinner(&amp;quot;Loading&amp;quot;): answer = qa(question) st.write(answer) new_ai_message = {&amp;quot;role&amp;quot;:&amp;quot;assistant&amp;quot;,&amp;quot;content&amp;quot;: answer} st.session_state.messages.append(new_ai_message) &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/CharmingViolinist962&quot;&gt; /u/CharmingViolinist962 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1chkgsc/rag_returns_everything/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1chkgsc/rag_returns_everything/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1chkgsc</id><link href="https://www.reddit.com/r/LangChain/comments/1chkgsc/rag_returns_everything/" /><updated>2024-05-01T12:04:02+00:00</updated><published>2024-05-01T12:04:02+00:00</published><title>RAG returns everything</title></entry><entry><author><name>/u/itschris</name><uri>https://www.reddit.com/user/itschris</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;For me, it was figuring out what steps in my RAG pipeline to use and how that affected the quality of responses. What chunking strategy do I use, which embedding models, what retrieval techniques can increase the relevancy of answers, how do I measure the quality of answers, etc. There&amp;#39;s a ton of time I spent on experimentation.&lt;/p&gt; &lt;p&gt;Also, the docs are changing frequently, so I had often had to read the raw source code to see how something worked.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/itschris&quot;&gt; /u/itschris &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cgzl9n/whats_the_most_painful_part_about_using_langchain/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cgzl9n/whats_the_most_painful_part_about_using_langchain/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cgzl9n</id><link href="https://www.reddit.com/r/LangChain/comments/1cgzl9n/whats_the_most_painful_part_about_using_langchain/" /><updated>2024-04-30T18:13:20+00:00</updated><published>2024-04-30T18:13:20+00:00</published><title>What's the most painful part about using Langchain?</title></entry><entry><author><name>/u/consultant82</name><uri>https://www.reddit.com/user/consultant82</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;with model &amp;quot;TheBloke/Llama-2-13B-chat-GGUF/llama-2-13b-chat.Q6_K.gguf&amp;quot; I made quite good experiences locally with langchain, however with model &amp;quot;FaradayDotDev/llama-3-8b-Instruct-GGUF/llama-3-8b-Instruct.Q8_0.gguf&amp;quot; or any other llama3 model I simply do not get any valid answers (just lot of newlines and some random numbers or words in the answer).&lt;/p&gt; &lt;p&gt;I tried playing with context size, putting llama3 specific tokens into the prompt like following but nothing helps:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;llm = LlamaCpp( model_path=&amp;quot;/Users/aydink/Workspace/models/FaradayDotDev/llama-3-8b-Instruct-GGUF/llama-3-8b-Instruct.Q8_0.gguf&amp;quot;, n_gpu_layers=30, n_ctx=8128, n_threads=4, temp=0.0, f16_kv=True, verbose=True, ) # Retrieve and generate using the relevant snippets of the blog. retriever = vectorstore.as_retriever() template_llama3=&amp;quot;&amp;quot;&amp;quot;&amp;lt;|begin_of_text|&amp;gt;&amp;lt;|start_header_id|&amp;gt;system&amp;lt;|end_header_id|&amp;gt; You are an enthusiastic assistant who likes helping others. From the info present in the &amp;quot;Context Section&amp;quot; below, try to answer the user&amp;#39;s questions. If you are unsure of the answer, reply with &amp;quot;Sorry, I can&amp;#39;t help you with this question&amp;quot;. If enough data is not present in the &amp;quot;Context Section&amp;quot;, reply with &amp;quot;Sorry, there isn&amp;#39;t enough data to answer your questions &amp;lt;|eot_id|&amp;gt;&amp;lt;|start_header_id|&amp;gt;user&amp;lt;|end_header_id|&amp;gt; Question: {question} Context: {context} Answer: &amp;lt;|eot_id|&amp;gt;&amp;lt;|start_header_id|&amp;gt;assistant&amp;lt;|end_header_id|&amp;gt;&amp;quot;&amp;quot;&amp;quot; custom_rag_prompt = PromptTemplate( input_variables=[&amp;#39;context&amp;#39;, &amp;#39;question&amp;#39;], template=template_llama3 ) rag_chain = ( {&amp;quot;context&amp;quot;: retriever | format_docs, &amp;quot;question&amp;quot;: RunnablePassthrough()} | custom_rag_prompt | llm | StrOutputParser() ) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Is this because I am using an instruct model instead of a chat model (like before with llama2)? But than at least I would expect some semantically more or less correct response.&lt;/p&gt; &lt;p&gt;Any ideas what could cause this?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/consultant82&quot;&gt; /u/consultant82 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1chhmsa/llama2_all_good_random_characters_with_llama3/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1chhmsa/llama2_all_good_random_characters_with_llama3/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1chhmsa</id><link href="https://www.reddit.com/r/LangChain/comments/1chhmsa/llama2_all_good_random_characters_with_llama3/" /><updated>2024-05-01T09:12:55+00:00</updated><published>2024-05-01T09:12:55+00:00</published><title>llama2 all good, random characters with llama3</title></entry><entry><author><name>/u/SamIAmDev</name><uri>https://www.reddit.com/user/SamIAmDev</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Come hang out at the live hacking session today at 2 PM EST on the Wingly Episode.&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://www.linkedin.com/in/hackingonstuff/&quot;&gt;Elad Ben-Israel&lt;/a&gt; (creator of the AWS CDK) will be live hacking on a Langchain integration with Wing&lt;/p&gt; &lt;p&gt;Join live on &lt;a href=&quot;https://www.twitch.tv/winglangio&quot;&gt;Twitch&lt;/a&gt; or &lt;a href=&quot;https://www.youtube.com/watch?v=4FWt2MWddyM&quot;&gt;YouTube&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/SamIAmDev&quot;&gt; /u/SamIAmDev &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cgyxub/former_aws_and_creator_of_the_cdk_live_hacking/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cgyxub/former_aws_and_creator_of_the_cdk_live_hacking/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cgyxub</id><link href="https://www.reddit.com/r/LangChain/comments/1cgyxub/former_aws_and_creator_of_the_cdk_live_hacking/" /><updated>2024-04-30T17:46:32+00:00</updated><published>2024-04-30T17:46:32+00:00</published><title>Former AWS and creator of the CDK live hacking session to integrate Langchain with Wing at 2 PM EST</title></entry><entry><author><name>/u/Christian-Hoeller</name><uri>https://www.reddit.com/user/Christian-Hoeller</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I am relatively new to vector stores and I was wondering what vector store I should use handling x amount of index per user. So the vector store should be handling mulitiple indexes. Am i better off using an Open Source solution or are there any other good solutions.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Christian-Hoeller&quot;&gt; /u/Christian-Hoeller &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cgtzej/what_vector_store_should_i_use_for_a_chatbot_saas/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cgtzej/what_vector_store_should_i_use_for_a_chatbot_saas/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cgtzej</id><link href="https://www.reddit.com/r/LangChain/comments/1cgtzej/what_vector_store_should_i_use_for_a_chatbot_saas/" /><updated>2024-04-30T14:17:58+00:00</updated><published>2024-04-30T14:17:58+00:00</published><title>What vector store should I use for a chatbot SAAS</title></entry><entry><author><name>/u/swiglu</name><uri>https://www.reddit.com/user/swiglu</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey &lt;a href=&quot;/r/Langchaindev&quot;&gt;r/Langchaindev&lt;/a&gt; , we previously shared an adaptive RAG technique that reduces the average LLM cost while increasing the accuracy in RAG applications with an adaptive number of context documents. &lt;/p&gt; &lt;p&gt;People were interested in seeing the same technique with open source models, without relying on OpenAI. We successfully replicated the work with a fully local setup, using Mistral 7B and open-source embedding models. &lt;/p&gt; &lt;p&gt;In the showcase, we explain how to build local and adaptive RAG with Pathway. Provide three embedding models that have particularly performed well in our experiments. We also share our findings on how we got Mistral to behave more strictly, conform to the request, and admit when it doesn’t know the answer.&lt;/p&gt; &lt;p&gt;PS: Our Pathway VectorStoreServer also has &lt;a href=&quot;https://python.langchain.com/docs/integrations/vectorstores/pathway/&quot;&gt;LangChain Integration&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Hope you like it!&lt;/p&gt; &lt;p&gt;Here is the blog post:&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://pathway.com/developers/showcases/private-rag-ollama-mistral&quot;&gt;https://pathway.com/developers/showcases/private-rag-ollama-mistral&lt;/a&gt;&lt;/p&gt; &lt;p&gt;If you are interested in deploying it as a RAG application, (including data ingestion, indexing and serving the endpoints) we have a &lt;a href=&quot;https://github.com/pathwaycom/llm-app/tree/main/examples/pipelines/private-rag&quot;&gt;quick start example in our repo&lt;/a&gt;. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/swiglu&quot;&gt; /u/swiglu &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cgs6kj/building_local_rag_with_adaptive_retrieval_using/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cgs6kj/building_local_rag_with_adaptive_retrieval_using/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cgs6kj</id><link href="https://www.reddit.com/r/LangChain/comments/1cgs6kj/building_local_rag_with_adaptive_retrieval_using/" /><updated>2024-04-30T12:56:46+00:00</updated><published>2024-04-30T12:56:46+00:00</published><title>Building Local RAG with Adaptive Retrieval using Mistral, Ollama and Pathway</title></entry><entry><author><name>/u/LOC000</name><uri>https://www.reddit.com/user/LOC000</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hello LangChain Community,&lt;/p&gt; &lt;p&gt;I&amp;#39;m working with RunnableSequence from langchain_core.runnables and encountering an issue where the input handling doesn&amp;#39;t work as expected. My sequence is designed to first convert a string input into a list of integers, and then apply a series of functions (add_one, mul_two, mul_three) on the list.&lt;/p&gt; &lt;p&gt;Could anyone suggest how to correctly structure this sequence so that the .batch() method processes the string as an entire list rather than splitting into characters? Additionally, is there a better way to ensure each list element passes through all functions in the sequence as intended?&lt;/p&gt; &lt;p&gt;Thank you for your help!&lt;/p&gt; &lt;p&gt;Code:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;from langchain_core.runnables import RunnableLambda, RunnableParallel, RunnableSequence, RunnablePassthrough from langchain_core.runnables.config import RunnableConfig import time MAX_CONCURRENCY = 2 runnable_conf = RunnableConfig(max_concurrency= MAX_CONCURRENCY, run_name=&amp;quot;my-prompt-123&amp;quot;, callbacks= []) import ast def string_to_int_list(s: str) -&amp;gt; list: # We want to turn the string into a list of integers... list_from_string = ast.literal_eval(s) return [int(item) for item in list_from_string] def add_one(x: int) -&amp;gt; int: print(&amp;quot;enter add_one()&amp;quot;) time.sleep(3) print(&amp;quot;exit add_one()&amp;quot;) return x + 1 def mul_two(x: int) -&amp;gt; int: print(&amp;quot;enter mul_two()&amp;quot;) time.sleep(5) print(&amp;quot;exit mul_two()&amp;quot;) return x * 2 def mul_three(x: int) -&amp;gt; int: print(&amp;quot;enter mul_three()&amp;quot;) time.sleep(5) print(&amp;quot;exit mul_three()&amp;quot;) return x * 3 runnable_0 = RunnableLambda(string_to_int_list) runnable_1 = RunnableLambda(add_one) runnable_2 = RunnableLambda(mul_two) runnable_3 = RunnableLambda(mul_three) # -- WORKING -- # sequence_working = RunnableSequence( # runnable_1 | {&amp;quot;mul_two&amp;quot;: runnable_2, &amp;quot;mul_three&amp;quot;: runnable_3} # ) # sequence_working.batch([1, 2, 3], config=runnable_conf) # -- NOT WORKING -- # This chain tries to split the input string into a list of integers first .. sequence_not_working = RunnableSequence( runnable_0 | runnable_1 | {&amp;quot;mul_two&amp;quot;: runnable_2, &amp;quot;mul_three&amp;quot;: runnable_3} ) # .batch() does not work because it splits the string into chars first ... # sequence_not_working.batch(&amp;quot;[1, 2, 3]&amp;quot;, config=runnable_conf) # .invoke() passes the complete list from string_to_int_list() to add_one() sequence_not_working.invoke(&amp;quot;[1, 2, 3]&amp;quot;, config=runnable_conf) &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/LOC000&quot;&gt; /u/LOC000 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cgyllo/langchain_lcel_split_string_into_list_and_then/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cgyllo/langchain_lcel_split_string_into_list_and_then/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cgyllo</id><link href="https://www.reddit.com/r/LangChain/comments/1cgyllo/langchain_lcel_split_string_into_list_and_then/" /><updated>2024-04-30T17:32:09+00:00</updated><published>2024-04-30T17:32:09+00:00</published><title>LangChain LCEL - Split string into list and then batch it in one chain?</title></entry><entry><author><name>/u/ArcuisAlezanzo</name><uri>https://www.reddit.com/user/ArcuisAlezanzo</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cgwn58/confusion_structured_output/&quot;&gt; &lt;img src=&quot;https://b.thumbs.redditmedia.com/_Nv5XSXd6D2GbqP1MGnJekkGTOU7VKpEY18n0jbtXeI.jpg&quot; alt=&quot;Confusion Structured Output&quot; title=&quot;Confusion Structured Output&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;&lt;a href=&quot;https://preview.redd.it/0rnzunys4nxc1.png?width=1306&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f54ad7e26a1eec569c869664d2a354b4edb50e66&quot;&gt;https://preview.redd.it/0rnzunys4nxc1.png?width=1306&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f54ad7e26a1eec569c869664d2a354b4edb50e66&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Why model.with_structured_output forces to tell joke even though user question doesn&amp;#39;t ask for Joke&lt;/h1&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/ArcuisAlezanzo&quot;&gt; /u/ArcuisAlezanzo &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cgwn58/confusion_structured_output/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cgwn58/confusion_structured_output/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1cgwn58</id><media:thumbnail url="https://b.thumbs.redditmedia.com/_Nv5XSXd6D2GbqP1MGnJekkGTOU7VKpEY18n0jbtXeI.jpg" /><link href="https://www.reddit.com/r/LangChain/comments/1cgwn58/confusion_structured_output/" /><updated>2024-04-30T16:09:26+00:00</updated><published>2024-04-30T16:09:26+00:00</published><title>Confusion Structured Output</title></entry><entry><author><name>/u/nothrishaant</name><uri>https://www.reddit.com/user/nothrishaant</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I am creating a project whose one component is an AI agent parsing a pdf, opening a link given in the pdf and performing a specific action. can anyone guide me on how to do this? I cant really find any specific resources online. thank you.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/nothrishaant&quot;&gt; /u/nothrishaant &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ch0t01/need_help_in_creating_an_ai_agent/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ch0t01/need_help_in_creating_an_ai_agent/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1ch0t01</id><link href="https://www.reddit.com/r/LangChain/comments/1ch0t01/need_help_in_creating_an_ai_agent/" /><updated>2024-04-30T19:02:43+00:00</updated><published>2024-04-30T19:02:43+00:00</published><title>need help in creating an AI agent</title></entry><entry><author><name>/u/UpskillingDS17</name><uri>https://www.reddit.com/user/UpskillingDS17</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hello, I have a pdf where I am expecting some answers to the questions asked and I am seeing that phi3 mode is generating better output than llama3 with minimal prompts . I tried with llama3 but the prompt with which answer is given is rather complex. Is this the behaviour with llama3 model or every model should have specific prompts? Thanks&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/UpskillingDS17&quot;&gt; /u/UpskillingDS17 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cgtezt/phi3_performing_better_than_llama3_on_rag_for_qa/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cgtezt/phi3_performing_better_than_llama3_on_rag_for_qa/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cgtezt</id><link href="https://www.reddit.com/r/LangChain/comments/1cgtezt/phi3_performing_better_than_llama3_on_rag_for_qa/" /><updated>2024-04-30T13:54:24+00:00</updated><published>2024-04-30T13:54:24+00:00</published><title>Phi3 performing better than Llama3 on RAG for QA</title></entry><entry><author><name>/u/leggolebowski</name><uri>https://www.reddit.com/user/leggolebowski</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;hey guys,&lt;/p&gt; &lt;p&gt;i&amp;#39;m working on this little weekend project to implement my langchain learnings.&lt;/p&gt; &lt;p&gt;i am wondering how to build this product where&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;everyone attends a standup meeting in the morning, and tldv.io records the whole meeting and gives the text back.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;then, i need to write some code to gain access to this script/manually can be inputted too.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;but, how do i use this text and imput it in a streamlit interface and then add each person&amp;#39;s tasks into like a kanban board in notion?&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;need help figuring out what agents, tools i should use to implement the same.does the same have to be hosted or something?&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;let me know thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/leggolebowski&quot;&gt; /u/leggolebowski &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cgy1or/weekend_project_help/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cgy1or/weekend_project_help/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cgy1or</id><link href="https://www.reddit.com/r/LangChain/comments/1cgy1or/weekend_project_help/" /><updated>2024-04-30T17:09:46+00:00</updated><published>2024-04-30T17:09:46+00:00</published><title>WEEKEND PROJECT HELP!!!</title></entry><entry><author><name>/u/altruisticalgorithm</name><uri>https://www.reddit.com/user/altruisticalgorithm</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;ve been trying to figure out how to route agents to decide on certain tools lately and many articles on Medium suggest Router chains. However the docs page for it is now empty: &lt;a href=&quot;https://python.langchain.com/en/latest/modules/chains/generic/router.html&quot;&gt;https://python.langchain.com/en/latest/modules/chains/generic/router.html&lt;/a&gt;&lt;/p&gt; &lt;p&gt;How do we implement Router chains now? Is there a notebook that demonstrates this?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/altruisticalgorithm&quot;&gt; /u/altruisticalgorithm &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cgmgxb/what_happened_to_router_chains/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cgmgxb/what_happened_to_router_chains/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cgmgxb</id><link href="https://www.reddit.com/r/LangChain/comments/1cgmgxb/what_happened_to_router_chains/" /><updated>2024-04-30T06:56:46+00:00</updated><published>2024-04-30T06:56:46+00:00</published><title>What happened to Router Chains?</title></entry><entry><author><name>/u/QueRoub</name><uri>https://www.reddit.com/user/QueRoub</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I would like to calculate text similarity between sentences or between a sentence and a document.&lt;/p&gt; &lt;p&gt;Assume I have 3 sentences:&lt;br/&gt; text1 = &amp;quot;Hello world&amp;quot;&lt;br/&gt; text2 = &amp;quot;Hello&amp;quot;&lt;/p&gt; &lt;p&gt;text3 = &amp;quot;Hello worlds&amp;quot;&lt;/p&gt; &lt;p&gt;If I use cosine similarity then text1 and text2 will have the same similarity as text1 and text3&lt;/p&gt; &lt;p&gt;What I would like for my case is to have higher similarity score in case of text1 and text3 since the only difference is the plural.&lt;/p&gt; &lt;p&gt;What would be the best metric/algorithm to do so?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/QueRoub&quot;&gt; /u/QueRoub &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cgrwjl/text_similarity/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cgrwjl/text_similarity/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cgrwjl</id><link href="https://www.reddit.com/r/LangChain/comments/1cgrwjl/text_similarity/" /><updated>2024-04-30T12:42:41+00:00</updated><published>2024-04-30T12:42:41+00:00</published><title>Text similarity</title></entry><entry><author><name>/u/centpi</name><uri>https://www.reddit.com/user/centpi</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;hello. I noticed a couple packages that exist on pypi such as langchainhub and langchain-chroma per &lt;a href=&quot;https://python.langchain.com/docs/use_cases/question_answering/quickstart/#setup&quot;&gt;Quickstart | 🦜️🔗 LangChain&lt;/a&gt; , but they don&amp;#39;t exist on anaconda. is there a way to install these with anaconda or will I need to use python with virtualenv instead? Thank you.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/centpi&quot;&gt; /u/centpi &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cgnmdm/how_to_install_langchain_packages_that_dont_exist/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cgnmdm/how_to_install_langchain_packages_that_dont_exist/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cgnmdm</id><link href="https://www.reddit.com/r/LangChain/comments/1cgnmdm/how_to_install_langchain_packages_that_dont_exist/" /><updated>2024-04-30T08:17:10+00:00</updated><published>2024-04-30T08:17:10+00:00</published><title>How to install langchain packages that don't exist on conda on conda</title></entry><entry><author><name>/u/Aggravating-Floor-38</name><uri>https://www.reddit.com/user/Aggravating-Floor-38</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey Guys. I&amp;#39;m building a project that involves a RAG pipeline and the retrieval part for that was pretty easy - just needed to embed the chunks and then call top-k retrieval. Now I want to incorporate another component that can identify the widest range of like &amp;#39;subtopics&amp;#39; in a big group of text chunks. So like if I chunk and embed a paper on black holes, it should be able to return the chunks on the different subtopics covered in that paper, so I can then get the sub-topics of each chunk. (If I&amp;#39;m going about this wrong and there&amp;#39;s a much easier way let me know) I&amp;#39;m assuming the correct way to go about this is like k-means clustering or smthn? Thing is the vector database I&amp;#39;m currently using - pinecone - is really easy to use but only supports top-k retrieval. What other options are there then for something like this? Would appreciate any advice and guidance.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Aggravating-Floor-38&quot;&gt; /u/Aggravating-Floor-38 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cgmjr5/clustering_embeddings_for_subtopic_extraction_in/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cgmjr5/clustering_embeddings_for_subtopic_extraction_in/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cgmjr5</id><link href="https://www.reddit.com/r/LangChain/comments/1cgmjr5/clustering_embeddings_for_subtopic_extraction_in/" /><updated>2024-04-30T07:01:42+00:00</updated><published>2024-04-30T07:01:42+00:00</published><title>Clustering Embeddings for Sub-Topic Extraction in RAG</title></entry><entry><author><name>/u/cryptokaykay</name><uri>https://www.reddit.com/user/cryptokaykay</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey Everyone,&lt;/p&gt; &lt;p&gt;I have been playing with Langgraph recently and it&amp;#39;s awesome. There are some native ways to visualize the graph that you are building using Langgraph, which I ended up using multiple times as I was developing some agentic workflows. Its useful to see the graph as you are developing and debugging it. I kinda thought it would be nice to have Langtrace show the graph and decided to add support for it.&lt;/p&gt; &lt;p&gt;Wanted to show a quick preview of it. Excited for you all to try it out and leave your feedback.&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://reddit.com/link/1cgbwzx/video/zbt44qozqhxc1/player&quot;&gt;https://reddit.com/link/1cgbwzx/video/zbt44qozqhxc1/player&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/cryptokaykay&quot;&gt; /u/cryptokaykay &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cgbwzx/langtrace_just_added_support_for_langgraph/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cgbwzx/langtrace_just_added_support_for_langgraph/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cgbwzx</id><link href="https://www.reddit.com/r/LangChain/comments/1cgbwzx/langtrace_just_added_support_for_langgraph/" /><updated>2024-04-29T22:04:04+00:00</updated><published>2024-04-29T22:04:04+00:00</published><title>Langtrace - Just added support for Langgraph</title></entry><entry><author><name>/u/Designer_Athlete7286</name><uri>https://www.reddit.com/user/Designer_Athlete7286</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;m trying to find any tutorials on how to deploy a lnggraph project using langserve. I&amp;#39;m quite new to programming so maybe I&amp;#39;m not reading the documentation correctly. But I couldn&amp;#39;t figure out how to. Can someone point me a beginner friendly guide if available?&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Designer_Athlete7286&quot;&gt; /u/Designer_Athlete7286 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cgkt5k/langgraph_langserve/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cgkt5k/langgraph_langserve/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cgkt5k</id><link href="https://www.reddit.com/r/LangChain/comments/1cgkt5k/langgraph_langserve/" /><updated>2024-04-30T05:10:08+00:00</updated><published>2024-04-30T05:10:08+00:00</published><title>Langgraph + LangServe</title></entry><entry><author><name>/u/user-1318</name><uri>https://www.reddit.com/user/user-1318</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I wanted to perform similarity search using Faiss. I searched how to do, but most of these examples work on data that is stored in some file in json or in pandas df. So, if I plan to store my data in a different database, then also while performing Faiss based search, it looks like I have to take all the data and process it in-memory to work with Faiss. So, it feels like I am storing at two places unnecessarily. How usually everyone work with Faiss? Since I am new to this, it is confusing. Can someone clarify?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/user-1318&quot;&gt; /u/user-1318 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cgo0n4/using_faiss_for_similarity_search/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cgo0n4/using_faiss_for_similarity_search/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cgo0n4</id><link href="https://www.reddit.com/r/LangChain/comments/1cgo0n4/using_faiss_for_similarity_search/" /><updated>2024-04-30T08:45:30+00:00</updated><published>2024-04-30T08:45:30+00:00</published><title>Using FAISS for similarity search</title></entry><entry><author><name>/u/tukemon24</name><uri>https://www.reddit.com/user/tukemon24</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;m new to LangChain, I saw this page `docs/use_cases/chatbots/memory_management/`. It mentioned that we need an OpenAI key for that. &lt;/p&gt; &lt;p&gt;Does the `memory management` work with another model as well? Anyne have references?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/tukemon24&quot;&gt; /u/tukemon24 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cgno5b/does_langchain_memory_management_only_work_with/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cgno5b/does_langchain_memory_management_only_work_with/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cgno5b</id><link href="https://www.reddit.com/r/LangChain/comments/1cgno5b/does_langchain_memory_management_only_work_with/" /><updated>2024-04-30T08:21:02+00:00</updated><published>2024-04-30T08:21:02+00:00</published><title>Does LangChain memory management only work with OpenAI model?</title></entry><entry><author><name>/u/Lost-Most-487</name><uri>https://www.reddit.com/user/Lost-Most-487</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;m using BedrockChat as my llm and is currently setting max_tokens to 4096 as part of the llm.model_kwargs since Claude V3 claims that it can support output tokens of up to 4096. However the outputs I&amp;#39;m seeing all stop at 2048 tokens. Setting max_tokens to &amp;lt;2048 is working as intended and the LLM is spitting out less tokens. How come it isn&amp;#39;t working for &amp;gt;2048?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Lost-Most-487&quot;&gt; /u/Lost-Most-487 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cgldr2/setting_max_tokens_arg_for_bedrockchat_for_claude/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cgldr2/setting_max_tokens_arg_for_bedrockchat_for_claude/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cgldr2</id><link href="https://www.reddit.com/r/LangChain/comments/1cgldr2/setting_max_tokens_arg_for_bedrockchat_for_claude/" /><updated>2024-04-30T05:45:32+00:00</updated><published>2024-04-30T05:45:32+00:00</published><title>Setting max_tokens arg for BedrockChat for Claude doesn't work after 2048</title></entry><entry><author><name>/u/Soggy_Sir_3622</name><uri>https://www.reddit.com/user/Soggy_Sir_3622</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hello everyone, lately I have been reviewing agents tool, I would like to know if there is currently something like an agent in langchain that allows me to obtain data from a vector database (already loaded)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Soggy_Sir_3622&quot;&gt; /u/Soggy_Sir_3622 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cgg1yy/agent_vectordatabase/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cgg1yy/agent_vectordatabase/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cgg1yy</id><link href="https://www.reddit.com/r/LangChain/comments/1cgg1yy/agent_vectordatabase/" /><updated>2024-04-30T01:05:25+00:00</updated><published>2024-04-30T01:05:25+00:00</published><title>Agent vectordatabase</title></entry></feed>