<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/"><category term="LangChain" label="r/LangChain"/><updated>2023-12-24T06:08:24+00:00</updated><icon>https://www.redditstatic.com/icon.png/</icon><id>/r/LangChain.rss</id><link rel="self" href="https://www.reddit.com/r/LangChain.rss" type="application/atom+xml" /><link rel="alternate" href="https://www.reddit.com/r/LangChain" type="text/html" /><subtitle>LangChain is an open-source framework and developer toolkit that helps developers get LLM applications from prototype to production. It is available for Python and Javascript at https://www.langchain.com/.</subtitle><title>LangChain</title><entry><author><name>/u/zchaarm</name><uri>https://www.reddit.com/user/zchaarm</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;A place for members of &lt;a href=&quot;/r/LangChain&quot;&gt;r/LangChain&lt;/a&gt; to chat with each other&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/zchaarm&quot;&gt; /u/zchaarm &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_10ljho9</id><link href="https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/" /><updated>2023-01-26T04:33:05+00:00</updated><published>2023-01-26T04:33:05+00:00</published><title>r/LangChain Lounge</title></entry><entry><author><name>/u/nahav404</name><uri>https://www.reddit.com/user/nahav404</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey Langchain Community!&lt;/p&gt; &lt;p&gt;I&amp;#39;m learning Python and langchain and excited to share a project that i&amp;#39;ve been working on: &lt;a href=&quot;https://github.com/navicstein/resume-checker&quot;&gt;Resume Checker&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Resume Checker is a Python tool designed to analyze how well your resume matches a specific job posting. As someone learning Python, I found this project immensely helpful in understanding langchain and building practical applications.&lt;/p&gt; &lt;p&gt;Check out the &lt;a href=&quot;https://github.com/navicstein/resume-checker&quot;&gt;GitHub repository&lt;/a&gt; for more detailed instructions. let me know what you think&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/nahav404&quot;&gt; /u/nahav404 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18phsg5/introducing_resume_checker_your_python_companion/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18phsg5/introducing_resume_checker_your_python_companion/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18phsg5</id><link href="https://www.reddit.com/r/LangChain/comments/18phsg5/introducing_resume_checker_your_python_companion/" /><updated>2023-12-23T23:20:48+00:00</updated><published>2023-12-23T23:20:48+00:00</published><title>üöÄ Introducing Resume Checker: Your Python Companion for Job Applications!</title></entry><entry><author><name>/u/WishboneReal534</name><uri>https://www.reddit.com/user/WishboneReal534</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey! I am trying to create a vector store using langchain and faiss for RAG(Retrieval-augmented generation) with about 6 millions abstracts. is there a strategy to create this vector store efficiently? currently it takes very long time to create it (can take up to 5 days)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/WishboneReal534&quot;&gt; /u/WishboneReal534 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18ph140/creating_a_vectordb_from_millions_of_documents/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18ph140/creating_a_vectordb_from_millions_of_documents/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18ph140</id><link href="https://www.reddit.com/r/LangChain/comments/18ph140/creating_a_vectordb_from_millions_of_documents/" /><updated>2023-12-23T22:43:13+00:00</updated><published>2023-12-23T22:43:13+00:00</published><title>creating a vectordb from millions of documents</title></entry><entry><author><name>/u/FistfulOfHaws</name><uri>https://www.reddit.com/user/FistfulOfHaws</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I am working on a project that uses Langchain in multiple places, I am getting inconsistent behavior, hoping someone can tell me what I am doing wrong here. (I am using a public bq dataset for this, so nothing proprietary in what I am posting). I first use agent_executor to generate a description of a table. I pass the table in as a variable. But the agent attempts to run the query return an error that the table is not found. In the same project I use db_chain to allow natural language to SQL querying of the table. In this case the table is found and a result is return. I have checked the SQL and results returned against the source data to confirm it is indeed querying the table. I am not sure why the table is found in one case but not the other&lt;/p&gt; &lt;pre&gt;&lt;code&gt;# Working agent system_prompt = f&amp;#39; in {table_to_query}.You are a BigQuery expert. You are able quickly review the tables in a dataset and understand the contents of each table along with their relation. You will be asked a question for which you need to generate and execute a query. The table in the question is the main focus of the question, but you may also need to join to other tables, so keep them in mind as your create your plan. The other tables are {dataset_table_names}. The column names may not match 1:1 in the prompt, use your best reasoning to select a column (for instance a user may ask for an account but in the table the column is account_name).Ensure that the columns you use in the query exist in the table. As you answer the users question, consider what other columns may be additive to their question and include those in your response&amp;#39; full_prompt = user_prompt + system_prompt if run_prompt: from langchain.utilities import SQLDatabase from langchain.llms import OpenAI from langchain_experimental.sql import SQLDatabaseChain db = SQLDatabase(engine) #, include_tables=prompt_tables) llm = OpenAI(temperature=.5, verbose=True) db_chain = SQLDatabaseChain.from_llm(llm, verbose=True,db=db, use_query_checker=True, top_k=10) db_chain.run(full_prompt) else: display(&amp;quot;Waiting on you to run the query&amp;quot;) &amp;gt; Entering new SQLDatabaseChain chain... What was the most popular name in Utah in 2010 in bigquery-public-data.usa_names.usa_1910_2013. SQLQuery:SELECT name, SUM(number) AS total FROM `bigquery-public-data`.usa_names.usa_1910_2013 WHERE state = &amp;#39;UT&amp;#39; AND year = 2010 GROUP BY name ORDER BY total DESC LIMIT 10 SQLResult: [(&amp;#39;Olivia&amp;#39;, 269), (&amp;#39;William&amp;#39;, 264), (&amp;#39;Mason&amp;#39;, 243), (&amp;#39;Jacob&amp;#39;, 235), (&amp;#39;Ethan&amp;#39;, 235), (&amp;#39;James&amp;#39;, 231), (&amp;#39;Samuel&amp;#39;, 227), (&amp;#39;Isaac&amp;#39;, 215), (&amp;#39;Abigail&amp;#39;, 210), (&amp;#39;Logan&amp;#39;, 206)] Answer:Olivia &amp;gt; Finished chain. &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;--&lt;/p&gt; &lt;pre&gt;&lt;code&gt;# Non Working from langchain.agents import create_sql_agent from langchain.agents.agent_toolkits import SQLDatabaseToolkit from langchain.utilities import SQLDatabase from langchain.llms import OpenAI # from langchain.agents import AgentExecutor from langchain.agents.agent_types import AgentType db = SQLDatabase(engine) agent_executor = create_sql_agent( llm=OpenAI(temperature=0), toolkit=SQLDatabaseToolkit(db=db, llm=OpenAI(temperature=0)), verbose=True, agent_type=AgentType.ZERO_SHOT_REACT_DESCRIPTION, ) agent_executor.run(f&amp;quot;Describe the {table_to_query} table&amp;quot;) &amp;gt; Entering new AgentExecutor chain... Action: sql_db_list_tables Action Input: Observation: Thought: I should query the schema of the usa_names table. Action: sql_db_schema Action Input: bigquery-public-data.usa_names.usa_1910_2013 Observation: Error: table_names {&amp;#39;bigquery-public-data.usa_names.usa_1910_2013&amp;#39;} not found in database Thought: I should check the spelling of the table name. Action: sql_db_schema Action Input: bigquery-public-data.usa_names.usa_1910_2013 Observation: Error: table_names {&amp;#39;bigquery-public-data.usa_names.usa_1910_2013&amp;#39;} not found in database &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/FistfulOfHaws&quot;&gt; /u/FistfulOfHaws &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18pfbu9/inconsistent_table_querying/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18pfbu9/inconsistent_table_querying/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18pfbu9</id><link href="https://www.reddit.com/r/LangChain/comments/18pfbu9/inconsistent_table_querying/" /><updated>2023-12-23T21:19:02+00:00</updated><published>2023-12-23T21:19:02+00:00</published><title>Inconsistent Table Querying</title></entry><entry><author><name>/u/coderinlaw</name><uri>https://www.reddit.com/user/coderinlaw</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;m working on a project involving where I want to analyze conversations between two individuals, let&amp;#39;s call them Person A and Person B. The primary function of this system is to enable a bot to answer questions about Person A&amp;#39;s interests based on past conversations between A and other individuals.&lt;/p&gt; &lt;p&gt;I am contemplating the best approach to use embedding models in this context. The challenge lies in conversational data, which is inherently different from structured documents. Conversations typically lack distinct paragraphs or sections, making traditional chunking and embedding techniques less straightforward.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Sentence-Level Embeddings&lt;/strong&gt;: Embedding each sentence individually to capture specific details. However, this might limit the response to only the information contained in that particular sentence.&lt;br/&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Conversation-Level Embeddings&lt;/strong&gt;: Creating embeddings for entire conversations. While this could capture the overall context, it might not be precise for detailed queries.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Summarization Before Embedding&lt;/strong&gt;: Generating a summarized version of the conversations and then embedding these summaries. I&amp;#39;m curious about the effectiveness and potential loss of detail with this method.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;My questions for the community are:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;What are the recommended practices for embedding models in this kind of RAG system, especially considering the conversational nature of the data?&lt;/li&gt; &lt;li&gt;Are there any specific techniques or methodologies that you would suggest for this type of application, possibly something that has worked well in your experience?&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/coderinlaw&quot;&gt; /u/coderinlaw &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18pa4gz/how_to_create_a_rag_for_all_the_chats/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18pa4gz/how_to_create_a_rag_for_all_the_chats/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18pa4gz</id><link href="https://www.reddit.com/r/LangChain/comments/18pa4gz/how_to_create_a_rag_for_all_the_chats/" /><updated>2023-12-23T17:10:14+00:00</updated><published>2023-12-23T17:10:14+00:00</published><title>how to create a rag for all the chats/ conversations between A and everyone else wherein a bot can answer a question about anyone A had a conversation with?</title></entry><entry><author><name>/u/phicreative1997</name><uri>https://www.reddit.com/user/phicreative1997</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi, I have been trying to use LangChain Selenium and other url loaders, but can&amp;#39;t find good documentation for now. Any information source is welcome.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/phicreative1997&quot;&gt; /u/phicreative1997 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18p25me/any_good_documentationtutorialebook_on_url_tools/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18p25me/any_good_documentationtutorialebook_on_url_tools/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18p25me</id><link href="https://www.reddit.com/r/LangChain/comments/18p25me/any_good_documentationtutorialebook_on_url_tools/" /><updated>2023-12-23T09:31:09+00:00</updated><published>2023-12-23T09:31:09+00:00</published><title>Any good documentation/tutorial/e-book on url tools in langchain?</title></entry><entry><author><name>/u/pmz</name><uri>https://www.reddit.com/user/pmz</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/pmz&quot;&gt; /u/pmz &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://adilmoujahid.com/posts/2023/10/kanji-gpt4/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18p2fii/building_a_japanese_kanji_flashcard_app_using/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18p2fii</id><link href="https://www.reddit.com/r/LangChain/comments/18p2fii/building_a_japanese_kanji_flashcard_app_using/" /><updated>2023-12-23T09:52:21+00:00</updated><published>2023-12-23T09:52:21+00:00</published><title>Building a Japanese Kanji Flashcard App using GPT-4, Python and Langchain</title></entry><entry><author><name>/u/mrripo</name><uri>https://www.reddit.com/user/mrripo</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;The application features integrations with various tools, including databases, Retrieval-Augmented Generation (RAG), and custom prompts, as well as custom tools within LangChain. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/mrripo&quot;&gt; /u/mrripo &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18p1ghh/what_frameworks_or_coding_structures_are/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18p1ghh/what_frameworks_or_coding_structures_are/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18p1ghh</id><link href="https://www.reddit.com/r/LangChain/comments/18p1ghh/what_frameworks_or_coding_structures_are/" /><updated>2023-12-23T08:39:44+00:00</updated><published>2023-12-23T08:39:44+00:00</published><title>What frameworks or coding structures are recommended for building applications powered by LangChain and large language models (LLMs)?</title></entry><entry><author><name>/u/peculiaroptimist</name><uri>https://www.reddit.com/user/peculiaroptimist</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18opwbl/ashamed_to_asked/&quot;&gt; &lt;img src=&quot;https://b.thumbs.redditmedia.com/1Q0_I_p3-jEZwHDBWF312WNb4Tlo44zFSQq5j8VT0dA.jpg&quot; alt=&quot;Ashamed to asked&quot; title=&quot;Ashamed to asked&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;kind of ashamed to ask but what am i missing here ?. the cash_flow_data method returns a list of cash flow statements in the form of dataframes, then i try to map each iteration to the prompt template but thats not working. instead i get this error. Any ideas why? &lt;a href=&quot;/u/hwchase17&quot;&gt;u/hwchase17&lt;/a&gt; . &lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/6o8c00uy2x7c1.png?width=2336&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=62d403052114c9de56573f1a85b4306497c23599&quot;&gt;https://preview.redd.it/6o8c00uy2x7c1.png?width=2336&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=62d403052114c9de56573f1a85b4306497c23599&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/r36pe0uy2x7c1.png?width=2336&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=89767aa7dbb3a4a2406229bc9611078ef0f4915d&quot;&gt;https://preview.redd.it/r36pe0uy2x7c1.png?width=2336&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=89767aa7dbb3a4a2406229bc9611078ef0f4915d&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/peculiaroptimist&quot;&gt; /u/peculiaroptimist &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18opwbl/ashamed_to_asked/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18opwbl/ashamed_to_asked/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_18opwbl</id><media:thumbnail url="https://b.thumbs.redditmedia.com/1Q0_I_p3-jEZwHDBWF312WNb4Tlo44zFSQq5j8VT0dA.jpg" /><link href="https://www.reddit.com/r/LangChain/comments/18opwbl/ashamed_to_asked/" /><updated>2023-12-22T21:51:15+00:00</updated><published>2023-12-22T21:51:15+00:00</published><title>Ashamed to asked</title></entry><entry><author><name>/u/Purity1212</name><uri>https://www.reddit.com/user/Purity1212</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;I&amp;#39;m currently trying to implement my RAG application in Streamlit without the use of LangChain for various reasons, however i have a problem to get the streaming response right for my LLM (AWS SageMaker endpoint).&lt;/p&gt; &lt;p&gt;The previous approach that works is passing a custom Streamhandler (that takes the streamlit container) to the Chain that overrides the on_llm_new_token method (writes to the container with every call of the method) and modifying the sagemaker_endpoint.py that it calls the method for every Token i get from the Event Stream. &lt;/p&gt; &lt;p&gt;As seen here: &lt;a href=&quot;https://github.com/langchain-ai/chat-langchain/issues/39&quot;&gt;https://github.com/langchain-ai/chat-langchain/issues/39&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;Now when trying to get this to work without LangChain i only get empty responses.&lt;/p&gt; &lt;p&gt;I call the endpoint via boto3 client and successfully get a response stream. However when iterating with the TokenIterator nothing happens. This approach would work in a normal python script with writing to the console but not within my Streamlit application.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;def call_llm(prompt, container): response = boto3_client.invoke_endpoint_with_response_stream( Arguments... (No errors here) ) print(response) # Shows that i get a valid EventStream current_completion = &amp;quot;&amp;quot; for token in TokenIterator(response[&amp;quot;Body&amp;quot;]): current_completion += token print(token) # Nothing happens here container.markdown(current_completion) # Nothing happens here either &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Same problem when i create a stream_handler class (not inherited from the LangChain BaseCallbackHandler) with the corresponding method. I don&amp;#39;t really understand how the Callbacks work within LangChain. It seems like i can&amp;#39;t get the same behaviour if i code it myself.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;def call_llm(prompt, stream_handler): # Give a streamhandler with corresponding container instead response = boto3_client.invoke_endpoint_with_response_stream( Arguments... (No errors here) ) print(response) # Shows that i get a valid EventStream current_completion = &amp;quot;&amp;quot; for token in TokenIterator(response[&amp;quot;Body&amp;quot;]): current_completion += token print(token) # Nothing happens here stream_handler.on_llm_new_token(current_completion) # Nothing happens here either &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;I&amp;#39;d be very thankful for a workaround or an explanation how the Callbacks work in LangChain.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Purity1212&quot;&gt; /u/Purity1212 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18ogw3p/how_do_callbacks_for_streaming_response_exactly/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18ogw3p/how_do_callbacks_for_streaming_response_exactly/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18ogw3p</id><link href="https://www.reddit.com/r/LangChain/comments/18ogw3p/how_do_callbacks_for_streaming_response_exactly/" /><updated>2023-12-22T15:02:10+00:00</updated><published>2023-12-22T15:02:10+00:00</published><title>How do Callbacks for streaming response exactly work? (In Streamlit application)</title></entry><entry><author><name>/u/debordian</name><uri>https://www.reddit.com/user/debordian</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18og9cn/langchain_state_of_ai_2023/&quot;&gt; &lt;img src=&quot;https://external-preview.redd.it/OwfKIOpfNaeizL9ZAEuyFGT4JpnvnQpYPxb-u2xBfTE.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=69887b5da6ec0a8e7469f24296fb78e3937f2406&quot; alt=&quot;LangChain State of AI 2023&quot; title=&quot;LangChain State of AI 2023&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/debordian&quot;&gt; /u/debordian &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://blog.langchain.dev/langchain-state-of-ai-2023/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18og9cn/langchain_state_of_ai_2023/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_18og9cn</id><media:thumbnail url="https://external-preview.redd.it/OwfKIOpfNaeizL9ZAEuyFGT4JpnvnQpYPxb-u2xBfTE.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=69887b5da6ec0a8e7469f24296fb78e3937f2406" /><link href="https://www.reddit.com/r/LangChain/comments/18og9cn/langchain_state_of_ai_2023/" /><updated>2023-12-22T14:33:17+00:00</updated><published>2023-12-22T14:33:17+00:00</published><title>LangChain State of AI 2023</title></entry><entry><author><name>/u/chrmcstingTom</name><uri>https://www.reddit.com/user/chrmcstingTom</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=D34PyNx71vk&quot;&gt;https://www.youtube.com/watch?v=D34PyNx71vk&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The YouTube video shares an architectural difference between LangChain and Microsofts orchestrator. Is there really no way to do the same in LangChain?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/chrmcstingTom&quot;&gt; /u/chrmcstingTom &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18om6xq/is_there_a_way_in_lc_to_centralize_event/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18om6xq/is_there_a_way_in_lc_to_centralize_event/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18om6xq</id><link href="https://www.reddit.com/r/LangChain/comments/18om6xq/is_there_a_way_in_lc_to_centralize_event/" /><updated>2023-12-22T18:57:36+00:00</updated><published>2023-12-22T18:57:36+00:00</published><title>Is there a way in LC to centralize event notifications and configurations?</title></entry><entry><author><name>/u/Pristine-Hawk-7841</name><uri>https://www.reddit.com/user/Pristine-Hawk-7841</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Anybody have any idea what I might be doing wrong here?&lt;/p&gt; &lt;pre&gt;&lt;code&gt;const claudeBedrock = new Bedrock({model: &amp;quot;anthropic.claude-v2:1&amp;quot;,region: aws_region,modelKwargs: {temperature: 0.0,top_k: 250,top_p: 0.999,stop_sequences: [&amp;#39;Human:&amp;#39;],max_tokens_to_sample: maxTokens}} const chain = = new LLMChain({llm: claudeBedrock, prompt: prompt}) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;I have tried .&lt;code&gt;call&lt;/code&gt;, .&lt;code&gt;_call&lt;/code&gt;, .&lt;code&gt;invoke&lt;/code&gt;, .&lt;code&gt;predict&lt;/code&gt; and they all produce worse results than through the AWS console.&lt;/p&gt; &lt;p&gt;I have verbose ON and literally copy and pasting the logged prompt produced by Langchain into the Bedrock playground and it produces better results.&lt;/p&gt; &lt;p&gt;I tried both Chat &amp;amp; Text in playgrounds using the EXACT same settings as above.&lt;/p&gt; &lt;p&gt;The prompt is asking Claude to classify a list of something. I have tried multiple times on both sides and the output through Langchain is WORSE by including results that is wrong. The console produces correct results everytime.&lt;/p&gt; &lt;p&gt;It&amp;#39;s driving me nuts that I&amp;#39;m about to tear out Langchain and use AWS&amp;#39;s SDK instead.&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Pristine-Hawk-7841&quot;&gt; /u/Pristine-Hawk-7841 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18opvla/bedrock_claude_performance_issue/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18opvla/bedrock_claude_performance_issue/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18opvla</id><link href="https://www.reddit.com/r/LangChain/comments/18opvla/bedrock_claude_performance_issue/" /><updated>2023-12-22T21:50:17+00:00</updated><published>2023-12-22T21:50:17+00:00</published><title>Bedrock Claude Performance Issue</title></entry><entry><author><name>/u/piratekid79</name><uri>https://www.reddit.com/user/piratekid79</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/piratekid79&quot;&gt; /u/piratekid79 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18oet7x/in_retrievalqa_from_langchainwe_have_a_retriever/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18oet7x/in_retrievalqa_from_langchainwe_have_a_retriever/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18oet7x</id><link href="https://www.reddit.com/r/LangChain/comments/18oet7x/in_retrievalqa_from_langchainwe_have_a_retriever/" /><updated>2023-12-22T13:22:42+00:00</updated><published>2023-12-22T13:22:42+00:00</published><title>in retrievalQa from langchain,we have a retriever that retrieves docs from a vector db and provides a context to the llm,lets say im using gpt3.5 whose max tokens is 4096... how do i handle huge context to be sent to it ?</title></entry><entry><author><name>/u/Trick-Asparagus-9260</name><uri>https://www.reddit.com/user/Trick-Asparagus-9260</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Guys, I&amp;#39;m doing a similarity search and using relevance scores because I understand relevance scores return scores between 0 and 1. However when I use Langchain to return these scores, they come back in negatives. However when I use custom code for chroma or faiss, I get scores between 0 and 1. Is this a bug in Langchain, pls help.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Trick-Asparagus-9260&quot;&gt; /u/Trick-Asparagus-9260 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18o9afp/langchain_returns_similarity_search_with/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18o9afp/langchain_returns_similarity_search_with/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18o9afp</id><link href="https://www.reddit.com/r/LangChain/comments/18o9afp/langchain_returns_similarity_search_with/" /><updated>2023-12-22T07:24:20+00:00</updated><published>2023-12-22T07:24:20+00:00</published><title>Langchain returns similarity_search_with_relevance_scores in negative</title></entry><entry><author><name>/u/Aggressive_Tea9664</name><uri>https://www.reddit.com/user/Aggressive_Tea9664</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi all,&lt;/p&gt; &lt;p&gt;Is there a way I can get the time needed for each individual components of ConversationalRetrievalChain?&lt;/p&gt; &lt;p&gt;For eg, how do I get the time needed for the LLM to generate a reply?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Aggressive_Tea9664&quot;&gt; /u/Aggressive_Tea9664 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18o2r72/get_time_needed_for_individual_components_of/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18o2r72/get_time_needed_for_individual_components_of/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18o2r72</id><link href="https://www.reddit.com/r/LangChain/comments/18o2r72/get_time_needed_for_individual_components_of/" /><updated>2023-12-22T01:12:04+00:00</updated><published>2023-12-22T01:12:04+00:00</published><title>Get time needed for individual components of ConversationalRetrievalChain</title></entry><entry><author><name>/u/DarthLoki79</name><uri>https://www.reddit.com/user/DarthLoki79</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I have the following:&lt;br/&gt; I want to be able to use agents and pass one argument programmatically. I ideally dont want that in prompt.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;def testFunc(text1, email): print(text1) print(email) tools = [ &lt;/code&gt;&lt;/pre&gt; &lt;p&gt; StructuredTool.from_function( func=testFunc, name=&amp;quot;testing&amp;quot;, args_schema=TestSchema, description=&amp;quot;tester&amp;quot;, return_direct=True,&lt;/p&gt; &lt;p&gt;) ]&lt;/p&gt; &lt;pre&gt;&lt;code&gt;model_with_tools = model.bind( functions=[format_tool_to_openai_function(t) for t in tools], ) prompt = ChatPromptTemplate.from_messages( [ (&amp;quot;system&amp;quot;, template), MessagesPlaceholder(variable_name=&amp;quot;chat_history&amp;quot;), MessagesPlaceholder(variable_name=&amp;quot;agent_scratchpad&amp;quot;), (&amp;quot;user&amp;quot;, &amp;quot;{input}&amp;quot;), ] ) agent = ( { &amp;quot;input&amp;quot;: lambda x: x[&amp;quot;input&amp;quot;], &amp;quot;agent_scratchpad&amp;quot;: lambda x: format_to_openai_function_messages( x[&amp;quot;intermediate_steps&amp;quot;] ), &amp;quot;chat_history&amp;quot;: lambda x: x[&amp;quot;chat_history&amp;quot;], &amp;quot;email&amp;quot;: lambda x: x[&amp;#39;email&amp;#39;], } | prompt | model_with_tools | OpenAIFunctionsAgentOutputParser() ) agent_executor = AgentExecutor( agent=agent, tools=tools, verbose=True, return_intermediate_steps=True ) result = agent_executor.invoke( {&amp;quot;input&amp;quot;: &amp;quot;this is the test code&amp;quot;, &amp;quot;chat_history&amp;quot;: [], &amp;quot;email&amp;quot;: &amp;quot;TEST@GMAIL.COM&amp;quot;} ) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;I want the output to be &lt;/p&gt; &lt;pre&gt;&lt;code&gt;this is the test code TEST@GMAIL.com &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Basically I want the email to be passed programmatically and the other arguments to be passed using the LLM. I cant figure this out. Have been going at it for a while. No matter what, the function is being called with the prompt, which doesnt have the email and it is hallucinating the email as [&lt;a href=&quot;mailto:user@example.com&quot;&gt;user@example.com&lt;/a&gt;](mailto:&lt;a href=&quot;mailto:user@example.com&quot;&gt;user@example.com&lt;/a&gt;) &lt;/p&gt; &lt;p&gt;Is there no way to pass the email to the function, maybe a additional kwarg thing or extra configs or something?&lt;br/&gt; After the prompt part of the chain ends, I can see the part entering the LLM is: &lt;/p&gt; &lt;pre&gt;&lt;code&gt; &amp;quot;kwargs&amp;quot;: { &amp;quot;content&amp;quot;: this is the test code&amp;quot;, &amp;quot;additional_kwargs&amp;quot;: {} } &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/DarthLoki79&quot;&gt; /u/DarthLoki79 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18nzw4v/how_to_pass_some_arguments_to_function_call_via/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18nzw4v/how_to_pass_some_arguments_to_function_call_via/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18nzw4v</id><link href="https://www.reddit.com/r/LangChain/comments/18nzw4v/how_to_pass_some_arguments_to_function_call_via/" /><updated>2023-12-21T22:59:57+00:00</updated><published>2023-12-21T22:59:57+00:00</published><title>How to pass some arguments to function call via code and some extracted from LLM?</title></entry><entry><author><name>/u/CharlieTrigger</name><uri>https://www.reddit.com/user/CharlieTrigger</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi there fellow Langchainers,&lt;/p&gt; &lt;p&gt;I have created an agent in a ipynb. Works great and by simply adding LANGCHAIN_TRACING_V2 = os.getenv(&amp;quot;LANGCHAIN_TRACING_V2&amp;quot;) LANGCHAIN_PROJECT = os.getenv(&amp;quot;LANGCHAIN_PROJECT&amp;quot;) LANGCHAIN_ENDPOINT = os.getenv(&amp;quot;LANGCHAIN_ENDPOINT&amp;quot;) LANGCHAIN_API_KEY = os.getenv(&amp;quot;LANGCHAIN_API_KEY&amp;quot;) and adding these values to the .env file and &lt;/p&gt; &lt;p&gt;from langsmith import Client client = Client()&lt;/p&gt; &lt;p&gt;Every agent_executor.invoke({&amp;quot;input&amp;quot;: &amp;quot;input query here&amp;quot;})[&amp;quot;output&amp;quot;]&lt;/p&gt; &lt;p&gt;is nicely logged in Langsmith.&lt;/p&gt; &lt;p&gt;But when I wrap this same agent in a fastapi application with uvicorn, it doesn&amp;#39;t work. The agent works fine, I can use the agent through Postman just fine. But nothing is logged in Langsmith.&lt;/p&gt; &lt;p&gt;Any help is greatly appreciated.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/CharlieTrigger&quot;&gt; /u/CharlieTrigger &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18npk5y/how_to_use_langsmith_with_a_fastapiuvicorn_setup/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18npk5y/how_to_use_langsmith_with_a_fastapiuvicorn_setup/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18npk5y</id><link href="https://www.reddit.com/r/LangChain/comments/18npk5y/how_to_use_langsmith_with_a_fastapiuvicorn_setup/" /><updated>2023-12-21T15:27:54+00:00</updated><published>2023-12-21T15:27:54+00:00</published><title>How to use Langsmith with a FastAPI/uvicorn setup</title></entry><entry><author><name>/u/duddai</name><uri>https://www.reddit.com/user/duddai</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey people, &lt;/p&gt; &lt;p&gt;I&amp;#39;m new to langchain and have a few questions. &lt;/p&gt; &lt;p&gt;If I want to summarize large PDFs (e.g. long scientific texts) in an easy-to-understand language, does a vector-based database make sense? Or can I just split the text up and then have it summarized piece by piece and create a summary at the end? &lt;/p&gt; &lt;p&gt;Is the context of the entire PDF preserved in both versions? &lt;/p&gt; &lt;p&gt;I hope you understand my question and can help me. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/duddai&quot;&gt; /u/duddai &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18nlcpm/summary_of_long_pdfs/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18nlcpm/summary_of_long_pdfs/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18nlcpm</id><link href="https://www.reddit.com/r/LangChain/comments/18nlcpm/summary_of_long_pdfs/" /><updated>2023-12-21T11:55:52+00:00</updated><published>2023-12-21T11:55:52+00:00</published><title>Summary of long PDFs</title></entry><entry><author><name>/u/PlayboiCult</name><uri>https://www.reddit.com/user/PlayboiCult</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hello everyone. I&amp;#39;m using Langchain (js) in my Next.js app and used this guide &lt;a href=&quot;https://js.langchain.com/docs/integrations/toolkits/sql&quot;&gt;https://js.langchain.com/docs/integrations/toolkits/sql&lt;/a&gt; (that someone very helpful shared with me here on reddit) to integrate the SQL agent.&lt;/p&gt; &lt;p&gt;I&amp;#39;m getting &lt;strong&gt;Agent stopped due to max iterations.&lt;/strong&gt; or a random incorrect answer from the agent. &lt;/p&gt; &lt;p&gt;I implemented it pretty much exactly like the docs I referenced but with a postgreSQL db in Supabase instead of a local .db. &lt;/p&gt; &lt;p&gt;Any thoughts or recommendations are appreciatedüëç. Thanks in advance&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/PlayboiCult&quot;&gt; /u/PlayboiCult &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18ntfoa/errors_using_sql_agent/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18ntfoa/errors_using_sql_agent/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18ntfoa</id><link href="https://www.reddit.com/r/LangChain/comments/18ntfoa/errors_using_sql_agent/" /><updated>2023-12-21T18:18:28+00:00</updated><published>2023-12-21T18:18:28+00:00</published><title>Errors using SQL Agent</title></entry><entry><author><name>/u/OnlyProggingForFun</name><uri>https://www.reddit.com/user/OnlyProggingForFun</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18ntbmq/langchain_vs_llamaindex_vs_openai_gpts_which_one/&quot;&gt; &lt;img src=&quot;https://external-preview.redd.it/ql1oKVZEdu8fmrJir4nPRBTshkBx1EiC3TccNDPG78g.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f92dc3c67cd6dbc78e0519fff9bdf03054f4c565&quot; alt=&quot;Langchain vs. LlamaIndex vs. OpenAI GPTs: Which one should you use?&quot; title=&quot;Langchain vs. LlamaIndex vs. OpenAI GPTs: Which one should you use?&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/OnlyProggingForFun&quot;&gt; /u/OnlyProggingForFun &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://youtu.be/g84uWgVXVYg&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18ntbmq/langchain_vs_llamaindex_vs_openai_gpts_which_one/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_18ntbmq</id><media:thumbnail url="https://external-preview.redd.it/ql1oKVZEdu8fmrJir4nPRBTshkBx1EiC3TccNDPG78g.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=f92dc3c67cd6dbc78e0519fff9bdf03054f4c565" /><link href="https://www.reddit.com/r/LangChain/comments/18ntbmq/langchain_vs_llamaindex_vs_openai_gpts_which_one/" /><updated>2023-12-21T18:13:24+00:00</updated><published>2023-12-21T18:13:24+00:00</published><title>Langchain vs. LlamaIndex vs. OpenAI GPTs: Which one should you use?</title></entry><entry><author><name>/u/PlayboiCult</name><uri>https://www.reddit.com/user/PlayboiCult</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hello everyone. I&amp;#39;m new to Langchain and I made a chatbot using Next.js (so the Javascript library) that uses a CSV with soccer info to answer questions. Specific questions, for example &amp;quot;How many goals did &lt;strong&gt;Haaland&lt;/strong&gt; score?&amp;quot; get answered properly, since it searches info about Haaland in the CSV (I&amp;#39;m embedding the CSV and storing the vectors in Pinecone).&lt;/p&gt; &lt;p&gt;The problem starts when I ask general questions, meaning questions without keywords. For example, &amp;quot;who made more assists?&amp;quot;, or maybe something extreme like &amp;quot;how many rows are there in the CSV?&amp;quot;. It completely fails. I&amp;#39;m guessing that it only gets the relevant info from the vector db based on the query and it can&amp;#39;t answer these types of questions.&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;I&amp;#39;m using &lt;code&gt;ConversationalRetrievalQAChain&lt;/code&gt; from Langchain&lt;/p&gt; &lt;pre&gt;&lt;code&gt;chain.ts /* create vectorstore */ const vectorStore = await PineconeStore.fromExistingIndex( new OpenAIEmbeddings({}), { pineconeIndex, textKey: &amp;quot;text&amp;quot;, } ); return ConversationalRetrievalQAChain.fromLLM( model, vectorStore.asRetriever(), { returnSourceDocuments: true } ); &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;And using it in my API in Next.js.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;route.ts const res = await chain.call({ question: question, chat_history: history .map((h) =&amp;gt; { h.content; }) .join(&amp;quot;\n&amp;quot;), }); &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;Any suggestions are welcomed and appreciated. Also feel free to ask any questions. Thanks in advance&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/PlayboiCult&quot;&gt; /u/PlayboiCult &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18nccz3/getting_general_information_over_a_csv/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18nccz3/getting_general_information_over_a_csv/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18nccz3</id><link href="https://www.reddit.com/r/LangChain/comments/18nccz3/getting_general_information_over_a_csv/" /><updated>2023-12-21T02:43:48+00:00</updated><published>2023-12-21T02:43:48+00:00</published><title>Getting general information over a CSV</title></entry><entry><author><name>/u/effervesense</name><uri>https://www.reddit.com/user/effervesense</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey guys, I am wondering if you know of good guides to go about doing this ^&lt;/p&gt; &lt;p&gt;And would it result in a better memory retrieval system? (I&amp;#39;ve heard LlamaIndex is better for RAG systems?) Or would you build everything in Langchain? My system will involve a fine-tuned model, external knowledge, plus I am trying to figure out how to store conversation history in the vector db for memory retrieval. Eventually I&amp;#39;ll add more components like speech-to-text software.. is this supported with langchain? Any guidance on this is greatly appreciated.&lt;/p&gt; &lt;p&gt;Right now I&amp;#39;m using Flowise which is built on top of LangChain, but haven&amp;#39;t found any info on integrating LlamaIndex with Flowise specifically.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/effervesense&quot;&gt; /u/effervesense &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18n65if/integrating_llamaindex_with_langchain/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18n65if/integrating_llamaindex_with_langchain/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18n65if</id><link href="https://www.reddit.com/r/LangChain/comments/18n65if/integrating_llamaindex_with_langchain/" /><updated>2023-12-20T21:46:29+00:00</updated><published>2023-12-20T21:46:29+00:00</published><title>Integrating LlamaIndex with Langchain.</title></entry><entry><author><name>/u/Pretend-Word2531</name><uri>https://www.reddit.com/user/Pretend-Word2531</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Can you Advise on finding the open source projects like Azure Cognitive Search + GPT, maybe using Langchain??&lt;/p&gt; &lt;p&gt;This 20 second clip shows exactly the functionality we&amp;#39;re looking for &lt;a href=&quot;https://youtube.com/clip/Ugkx4Bx61tbWTnuvDmfEecj2R-msM2AI3kWA?si=tT7HkGz_m2wzIeL_&quot;&gt;https://youtube.com/clip/Ugkx4Bx61tbWTnuvDmfEecj2R-msM2AI3kWA?si=tT7HkGz_m2wzIeL_&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Pretend-Word2531&quot;&gt; /u/Pretend-Word2531 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18nbrrw/identify_a_langchain_like_project_like_in_this/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18nbrrw/identify_a_langchain_like_project_like_in_this/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18nbrrw</id><link href="https://www.reddit.com/r/LangChain/comments/18nbrrw/identify_a_langchain_like_project_like_in_this/" /><updated>2023-12-21T02:13:04+00:00</updated><published>2023-12-21T02:13:04+00:00</published><title>Identify a Langchain like project like in this video clip</title></entry><entry><author><name>/u/M4xM9450</name><uri>https://www.reddit.com/user/M4xM9450</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi all,&lt;/p&gt; &lt;p&gt;Are there any resources on using a local JS copy of an LLM (namely Flan-T5) with langchainJS (especially if using Xenova‚Äôs transformers.js for the LLM inference)? I‚Äôve been looking for resources on using the two libraries for a local JS project but information is sparse.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/M4xM9450&quot;&gt; /u/M4xM9450 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18nb0t3/looking_for_tutorial_on_local_js_llms/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18nb0t3/looking_for_tutorial_on_local_js_llms/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18nb0t3</id><link href="https://www.reddit.com/r/LangChain/comments/18nb0t3/looking_for_tutorial_on_local_js_llms/" /><updated>2023-12-21T01:34:08+00:00</updated><published>2023-12-21T01:34:08+00:00</published><title>Looking for tutorial on local JS LLMs</title></entry><entry><author><name>/u/GlobalSalt3016</name><uri>https://www.reddit.com/user/GlobalSalt3016</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I am trying to create a QNA chatbot with OpenAI API, after using RecursiveCharacterTextSplitter, texts are coming with &lt;strong&gt;&amp;quot;\n&amp;quot;&lt;/strong&gt; Should I be worried? because I have to upsert it into the Pinecone database, and also if there is any preprocessing needed before upserting please tell me.. this is my first project&lt;/p&gt; &lt;p&gt;these are the texts: &lt;/p&gt; &lt;p&gt;&lt;code&gt;[&amp;#39;The\ncow,\na\ngentle\nherbivorous\nmammal\nof\nthe\nBovidae\nfamily,\nis\na\nvital\ncontributor\nto\nagriculture\nand\nhuman\nsustenance\nglobally.\nCharacterized\nby\nits\nlarge\nbody,\ncloven\nhooves,\nand\ndistinctively\npatterned\nhide,\ncows\nprimarily\nserve\nas\nsources\nof\nmilk,\nagriculture,livestock,\nfarming\nand\nleather.\nBos\ntaurus,\nthe\nmost\ncommon\nspecies,\nexists\nin\nvarious\nbreeds,&amp;#39;]&lt;/code&gt;&lt;/p&gt; &lt;p&gt;my code : &lt;/p&gt; &lt;pre&gt;&lt;code&gt;loader = PdfReader(&amp;quot;cow.pdf&amp;quot;) text = &amp;#39;&amp;#39; for i,page in enumerate (loader.pages): content = page.extract_text() if content: text += content text_splitter = RecursiveCharacterTextSplitter( separators=[&amp;quot;\n\n&amp;quot;, &amp;quot;\n&amp;quot;, &amp;quot; &amp;quot;], chunk_size = 400, chunk_overlap = 20, length_function = len, ) texts = text_splitter.split_text(text) print(texts) print(len(texts)) embeddings = OpenAIEmbeddings(model=&amp;quot;text-embedding-ada-002&amp;quot;) vectorstore = Pinecone.from_texts(texts,embeddings,index_name=PINECONE_INDEX) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/GlobalSalt3016&quot;&gt; /u/GlobalSalt3016 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18n1741/text_preprocessing_before_embedding/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/18n1741/text_preprocessing_before_embedding/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_18n1741</id><link href="https://www.reddit.com/r/LangChain/comments/18n1741/text_preprocessing_before_embedding/" /><updated>2023-12-20T18:15:05+00:00</updated><published>2023-12-20T18:15:05+00:00</published><title>Text pre-processing before embedding</title></entry></feed>