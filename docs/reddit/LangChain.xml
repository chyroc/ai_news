<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/"><category term="LangChain" label="r/LangChain"/><updated>2024-08-09T12:26:22+00:00</updated><icon>https://www.redditstatic.com/icon.png/</icon><id>/r/LangChain.rss</id><link rel="self" href="https://www.reddit.com/r/LangChain.rss" type="application/atom+xml" /><link rel="alternate" href="https://www.reddit.com/r/LangChain" type="text/html" /><subtitle>LangChain is an open-source framework and developer toolkit that helps developers get LLM applications from prototype to production. It is available for Python and Javascript at https://www.langchain.com/.</subtitle><title>LangChain</title><entry><author><name>/u/zchaarm</name><uri>https://www.reddit.com/user/zchaarm</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;A place for members of &lt;a href=&quot;/r/LangChain&quot;&gt;r/LangChain&lt;/a&gt; to chat with each other&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/zchaarm&quot;&gt; /u/zchaarm &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_10ljho9</id><link href="https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/" /><updated>2023-01-26T04:33:05+00:00</updated><published>2023-01-26T04:33:05+00:00</published><title>r/LangChain Lounge</title></entry><entry><author><name>/u/Diamant-AI</name><uri>https://www.reddit.com/user/Diamant-AI</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi all,&lt;/p&gt; &lt;p&gt;Sharing a repo I was working on for a while. &lt;/p&gt; &lt;p&gt;It’s open-source and includes many different strategies for RAG (currently 17), including tutorials, and visualizations.&lt;/p&gt; &lt;p&gt;This is great learning and reference material.&lt;br/&gt; Open issues, suggest more strategies, and use as needed.&lt;/p&gt; &lt;p&gt;Enjoy!&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/NirDiamant/RAG_Techniques&quot;&gt;https://github.com/NirDiamant/RAG_Techniques&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Diamant-AI&quot;&gt; /u/Diamant-AI &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1envyoh/an_extensive_opensource_collection_of_rag/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1envyoh/an_extensive_opensource_collection_of_rag/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1envyoh</id><link href="https://www.reddit.com/r/LangChain/comments/1envyoh/an_extensive_opensource_collection_of_rag/" /><updated>2024-08-09T10:08:07+00:00</updated><published>2024-08-09T10:08:07+00:00</published><title>An extensive open-source collection of RAG implementations with many different strategies</title></entry><entry><author><name>/u/RandRanger</name><uri>https://www.reddit.com/user/RandRanger</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey everyone, I have been learning/working on Agents. For now I&amp;#39;m using LangChain to create Agents. But while I reading the documents of LangChain about Agents I saw warning note. I think LangChai won&amp;#39;t support building Agents with Langchain anymore. They will use LangGraph to create Agents anymore. But as I understand I should know some OOP to create Agents using LangGraph (I&amp;#39;m not sure). At this point should I learn LangGraph or stay at Langchain. Or should I use another library for creating Agents.&lt;/p&gt; &lt;p&gt;Thanks.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/RandRanger&quot;&gt; /u/RandRanger &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1env9og/should_i_learn_langgraph_instead_of_langchain/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1env9og/should_i_learn_langgraph_instead_of_langchain/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1env9og</id><link href="https://www.reddit.com/r/LangChain/comments/1env9og/should_i_learn_langgraph_instead_of_langchain/" /><updated>2024-08-09T09:21:36+00:00</updated><published>2024-08-09T09:21:36+00:00</published><title>Should I learn LangGraph instead of LangChain?</title></entry><entry><author><name>/u/QuantumSchema</name><uri>https://www.reddit.com/user/QuantumSchema</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey y’all!&lt;/p&gt; &lt;p&gt;I’m working through a RAG solution (not my first) and stumbled into a problem I hadn’t thought of yet: retrieving documents based on time. &lt;/p&gt; &lt;p&gt;The idea is that you could ask “Provide a summary of what happened in the past 24 hours?” or “I was out for 5 weeks, what did I miss?”&lt;/p&gt; &lt;p&gt;The documents in the vectordb (pgvector for now) are diverse and made up of knowledge bases and ticketing systems. &lt;/p&gt; &lt;p&gt;Q&amp;amp;A with similarity is easy. This time thing threw me for a loop. &lt;/p&gt; &lt;p&gt;Best I could come up with so far is having the time/date of the article saved in something like epoch form in the metadata then, have one LLM assess the user’s request and if it has a time range, respond in JSON format with start/end values. And then, filter on the date value on the metadata. &lt;/p&gt; &lt;p&gt;I saw timescale but haven’t done a bunch of research on it yet. I’ve also seen document retrieval with time decay. That looks interesting. &lt;/p&gt; &lt;p&gt;Just curious if any of you have had this problem and solved it already?&lt;/p&gt; &lt;p&gt;Thanks and cheers!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/QuantumSchema&quot;&gt; /u/QuantumSchema &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1enk2hx/how_do_you_tackle_a_time_aware_rag_use_case/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1enk2hx/how_do_you_tackle_a_time_aware_rag_use_case/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1enk2hx</id><link href="https://www.reddit.com/r/LangChain/comments/1enk2hx/how_do_you_tackle_a_time_aware_rag_use_case/" /><updated>2024-08-08T23:01:58+00:00</updated><published>2024-08-08T23:01:58+00:00</published><title>How do you tackle a “time aware” RAG use case?</title></entry><entry><author><name>/u/Money_Mycologist4939</name><uri>https://www.reddit.com/user/Money_Mycologist4939</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;On langgraph doc there is examples on how to setup a checkpointer for storing graph state. This is done using a saver for sqlite. What about I wanna use an sql database created on google cloud? what about google big query for example? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Money_Mycologist4939&quot;&gt; /u/Money_Mycologist4939 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1enx32e/langgraph_memory_saver_for_sql/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1enx32e/langgraph_memory_saver_for_sql/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1enx32e</id><link href="https://www.reddit.com/r/LangChain/comments/1enx32e/langgraph_memory_saver_for_sql/" /><updated>2024-08-09T11:17:29+00:00</updated><published>2024-08-09T11:17:29+00:00</published><title>Langgraph memory saver for sql?</title></entry><entry><author><name>/u/UnderstandLingAI</name><uri>https://www.reddit.com/user/UnderstandLingAI</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Out of curiosity - what do you struggle most with when it comes to doing RAG (properly)? There are so many frameworks, repos and solutions out there these days that for most challenges there seems to be an out-of-the-box solution, so what&amp;#39;s left? Does not have to be confined to just Langchain.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/UnderstandLingAI&quot;&gt; /u/UnderstandLingAI &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ene81o/what_are_your_biggest_challenges_in_rag/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ene81o/what_are_your_biggest_challenges_in_rag/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1ene81o</id><link href="https://www.reddit.com/r/LangChain/comments/1ene81o/what_are_your_biggest_challenges_in_rag/" /><updated>2024-08-08T18:59:10+00:00</updated><published>2024-08-08T18:59:10+00:00</published><title>What are your biggest challenges in RAG?</title></entry><entry><author><name>/u/glow_storm</name><uri>https://www.reddit.com/user/glow_storm</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;I was looking to see if i can using Tool calling/Function calling using Llamacpp in Langchain using the code provided on the docs. If i use the below given piece of code , it calls the tool every time&lt;/p&gt; &lt;pre&gt;&lt;code&gt;from langchain.tools import tool from langchain_core.pydantic_v1 import BaseModel, Field class WeatherInput(BaseModel): location: str = Field(description=&amp;quot;The city and state, e.g. San Francisco, CA&amp;quot;) unit: str = Field(enum=[&amp;quot;celsius&amp;quot;, &amp;quot;fahrenheit&amp;quot;]) @tool(&amp;quot;get_current_weather&amp;quot;, args_schema=WeatherInput) def get_weather(location: str, unit: str): &amp;quot;&amp;quot;&amp;quot;Get the current weather in a given location&amp;quot;&amp;quot;&amp;quot; return f&amp;quot;Now the weather in {location} is 22 {unit}&amp;quot; llm_with_tools = llm.bind_tools( tools=[get_weather], tool_choice={&amp;quot;type&amp;quot;: &amp;quot;function&amp;quot;, &amp;quot;function&amp;quot;: {&amp;quot;name&amp;quot;: &amp;quot;get_current_weather&amp;quot;}}, ) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;When i remove the tool_choice parameters , it gives the args of the tool call in content of the code but does not call the tool.&lt;/p&gt; &lt;p&gt;Can anyone help me with this. I want to use LLamacpp to automatically call tools and not have to force it to call the same tools. &lt;/p&gt; &lt;p&gt;I was using LLama3-groq-tool-use model for this , which is a finetuned model by groq for tool use.&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/glow_storm&quot;&gt; /u/glow_storm &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1envxee/tool_calling_in_llamacpp_with_langchain/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1envxee/tool_calling_in_llamacpp_with_langchain/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1envxee</id><link href="https://www.reddit.com/r/LangChain/comments/1envxee/tool_calling_in_llamacpp_with_langchain/" /><updated>2024-08-09T10:05:48+00:00</updated><published>2024-08-09T10:05:48+00:00</published><title>Tool Calling in LLamacpp with Langchain</title></entry><entry><author><name>/u/bbroy4u</name><uri>https://www.reddit.com/user/bbroy4u</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hy i want to build a semantic search engine over hundreds of quotes in json formate. The problem is some quotes a very big like 3k tokkens and i am afraid the embeddings may not be good. I think i need to split bigger quotes intro smaller chunks and match query against those smaller chunks and return the full quote that it belongs to with the relevant chunk highlighted. How i can do it using langchain. &lt;strong&gt;I am totally noob to programming and it is my first big project&lt;/strong&gt; . I will be thankful for any help may be throw logical steps , psuedo code or any thing that can help.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/bbroy4u&quot;&gt; /u/bbroy4u &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1envf1m/need_some_help_with_setting_up_the_logic_of/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1envf1m/need_some_help_with_setting_up_the_logic_of/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1envf1m</id><link href="https://www.reddit.com/r/LangChain/comments/1envf1m/need_some_help_with_setting_up_the_logic_of/" /><updated>2024-08-09T09:31:41+00:00</updated><published>2024-08-09T09:31:41+00:00</published><title>Need some help with setting up the logic of project - Semantic Search Engine</title></entry><entry><author><name>/u/Previous_Impact1597</name><uri>https://www.reddit.com/user/Previous_Impact1597</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi all, I&amp;#39;m working on a side project that helps with sports analysis for historical games, which in turn will help with sports betting. Currently I&amp;#39;ve been only focused on MLB because I wanted to see how the use case would pan out.&lt;/p&gt; &lt;p&gt;My first attempt at this was to use the openai endpoint and load all the relevant JSON objects and send a prompt along with them to GPT and see what I get back. Eventually, the context size was getting way too big and the problem I was running into was that it was expensive. Although, the prompts back were actually pretty decent and relevant to the data.&lt;/p&gt; &lt;p&gt;My second attempt was to setup a RAG using Chroma/LangChain/GPT4o. I got it to work but the answers all seem very off and super vague. None of the data I have was shown in any of the prompts i asked, or any of the players that were playing in a game were mentioned at all in the prompt back, plus it kept mentioning wrong games/teams whe asking it specific games. I’m assuming I might need to adjust the vector store a bit but not sure how I can do that with chroma.&lt;/p&gt; &lt;p&gt;My question is what might be the best way to setup some sort of process? My end result, I would like a response back using the historical data I&amp;#39;ve provided to make assumptions on what a game could be like based off all the stats given, with some room for GPT to also make some inference as well.&lt;/p&gt; &lt;p&gt;I am a super new at this so it&amp;#39;s been a learning process so far; please bear with me.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Previous_Impact1597&quot;&gt; /u/Previous_Impact1597 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1env10r/using_gpt4o_with_langchainchroma_for_sports/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1env10r/using_gpt4o_with_langchainchroma_for_sports/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1env10r</id><link href="https://www.reddit.com/r/LangChain/comments/1env10r/using_gpt4o_with_langchainchroma_for_sports/" /><updated>2024-08-09T09:05:27+00:00</updated><published>2024-08-09T09:05:27+00:00</published><title>using GPT4o with langchain/chroma for sports analysis</title></entry><entry><author><name>/u/oh_synap</name><uri>https://www.reddit.com/user/oh_synap</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1engrgq/agentservicetoolkit_full_toolkit_for_running/&quot;&gt; &lt;img src=&quot;https://b.thumbs.redditmedia.com/mtqIMbeU2Gn0dQ7Idygl9kcxn9pfW-XvSQnluG8uuyc.jpg&quot; alt=&quot;🧰 agent-service-toolkit: Full toolkit for running agent as a service built with LangGraph, FastAPI and Streamlit&quot; title=&quot;🧰 agent-service-toolkit: Full toolkit for running agent as a service built with LangGraph, FastAPI and Streamlit&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;🧰 Introducing agent-service-toolkit, a new open source toolkit for running an AI agent as a service using LangGraph, FastAPI and Streamlit.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;View the repo: &lt;a href=&quot;https://github.com/JoshuaC215/agent-service-toolkit&quot;&gt;https://github.com/JoshuaC215/agent-service-toolkit&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Try the app: &lt;a href=&quot;https://agent-service-toolkit.streamlit.app/&quot;&gt;https://agent-service-toolkit.streamlit.app/&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Watch the video: &lt;a href=&quot;https://www.youtube.com/watch?v=VqQti9nGoe4&quot;&gt;https://www.youtube.com/watch?v=VqQti9nGoe4&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I wanted to build a few agents for personal projects using the Compound AI Systems approach, and found that LangGraph is the most advanced framework with the features I needed today. But serving and interacting with the agent was painful and complicated unless I wanted to use LangGraph Cloud (still in closed beta), and that wouldn&amp;#39;t support my own infrastructure.&lt;/p&gt; &lt;p&gt;I built agent-service-toolkit to solve this problem so I can focus on just the agent logic and core AI systems for my projects in the future. It uses async-first and FastAPI to be blazing fast and production-ready, with a Streamlit app for easy sharing and rapid iteration.&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/bj51l3w65ihd1.png?width=1619&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1519d1fbe5f8c1a796c4a5b023d7b107cde40c5c&quot;&gt;Architecture&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/h5fbanv75ihd1.png?width=1276&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=52a483eb644c9c540d870845ebb4baac316f2bd9&quot;&gt;App in action&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/oh_synap&quot;&gt; /u/oh_synap &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1engrgq/agentservicetoolkit_full_toolkit_for_running/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1engrgq/agentservicetoolkit_full_toolkit_for_running/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1engrgq</id><media:thumbnail url="https://b.thumbs.redditmedia.com/mtqIMbeU2Gn0dQ7Idygl9kcxn9pfW-XvSQnluG8uuyc.jpg" /><link href="https://www.reddit.com/r/LangChain/comments/1engrgq/agentservicetoolkit_full_toolkit_for_running/" /><updated>2024-08-08T20:41:56+00:00</updated><published>2024-08-08T20:41:56+00:00</published><title>🧰 agent-service-toolkit: Full toolkit for running agent as a service built with LangGraph, FastAPI and Streamlit</title></entry><entry><author><name>/u/MagentaSpark</name><uri>https://www.reddit.com/user/MagentaSpark</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;The project I am working on has no human in the loop and time travel workloads. Is there any more utility to checkpointer now?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/MagentaSpark&quot;&gt; /u/MagentaSpark &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ent5yp/checkpointer_only_for_human_in_the_loop_and_time/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ent5yp/checkpointer_only_for_human_in_the_loop_and_time/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1ent5yp</id><link href="https://www.reddit.com/r/LangChain/comments/1ent5yp/checkpointer_only_for_human_in_the_loop_and_time/" /><updated>2024-08-09T06:57:00+00:00</updated><published>2024-08-09T06:57:00+00:00</published><title>Checkpointer only for human in the loop and time travel workloads?</title></entry><entry><author><name>/u/SonicDasherX</name><uri>https://www.reddit.com/user/SonicDasherX</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hello, I need your help. I&amp;#39;m working at a company and require a computing system for running AI and local language models (LLMs). Would you recommend this PC for running LLMs?&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://www.amazon.com.mx/dp/B0CHCGQDX4?ref=cm_sw_r_cp_ud_dp_3JJMJDENCHDNMH3VZYMN&amp;amp;ref_=cm_sw_r_cp_ud_dp_3JJMJDENCHDNMH3VZYMN&amp;amp;social_share=cm_sw_r_cp_ud_dp_3JJMJDENCHDNMH3VZYMN&quot;&gt;https://www.amazon.com.mx/dp/B0CHCGQDX4?ref=cm_sw_r_cp_ud_dp_3JJMJDENCHDNMH3VZYMN&amp;amp;ref_=cm_sw_r_cp_ud_dp_3JJMJDENCHDNMH3VZYMN&amp;amp;social_share=cm_sw_r_cp_ud_dp_3JJMJDENCHDNMH3VZYMN&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/SonicDasherX&quot;&gt; /u/SonicDasherX &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ennhzg/requirements_for_execute_most_of_local_llms/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ennhzg/requirements_for_execute_most_of_local_llms/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1ennhzg</id><link href="https://www.reddit.com/r/LangChain/comments/1ennhzg/requirements_for_execute_most_of_local_llms/" /><updated>2024-08-09T01:42:48+00:00</updated><published>2024-08-09T01:42:48+00:00</published><title>Requirements for execute most of local LLMs?</title></entry><entry><author><name>/u/Confident-Honeydew66</name><uri>https://www.reddit.com/user/Confident-Honeydew66</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Confident-Honeydew66&quot;&gt; /u/Confident-Honeydew66 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://thepi.pe/evals&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1en8ook/not_sure_what_llm_to_use_for_for_your_rag_system/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1en8ook</id><link href="https://www.reddit.com/r/LangChain/comments/1en8ook/not_sure_what_llm_to_use_for_for_your_rag_system/" /><updated>2024-08-08T15:18:37+00:00</updated><published>2024-08-08T15:18:37+00:00</published><title>Not sure what LLM to use for for your RAG system? Here's a price/size/performance comparison of every state-of-the-art LLM.</title></entry><entry><author><name>/u/Thin_Peach6371</name><uri>https://www.reddit.com/user/Thin_Peach6371</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;m a Java developer interested in getting into AI software development. I see there&amp;#39;s a ton of info on LangChain for Python, but not as much on LangChain4j for Java. Should I dive into Python and start developing with it, or can I learn from the langchain docs (and youtube videos) and then apply that knowledge to Java (just with different syntax)? Any advice?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Thin_Peach6371&quot;&gt; /u/Thin_Peach6371 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1enagg9/what_are_the_current_limitations_of_langchain4j/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1enagg9/what_are_the_current_limitations_of_langchain4j/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1enagg9</id><link href="https://www.reddit.com/r/LangChain/comments/1enagg9/what_are_the_current_limitations_of_langchain4j/" /><updated>2024-08-08T16:29:30+00:00</updated><published>2024-08-08T16:29:30+00:00</published><title>What are the current limitations of LangChain4j compared to the Python version</title></entry><entry><author><name>/u/neilkatz</name><uri>https://www.reddit.com/user/neilkatz</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1enbqew/multimodal_rag_explainer_3_paths_to_integrating/&quot;&gt; &lt;img src=&quot;https://external-preview.redd.it/X68ByIY0sc7DCW0vPJh2aPA1NayqYM4-T62ZwCr2iEc.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=05195d05473ac42b4ae8434d45ed950a6308ce12&quot; alt=&quot;Multimodal RAG Explainer: 3 Paths to Integrating Text, Images and Audio in RAG, Which One is Best?&quot; title=&quot;Multimodal RAG Explainer: 3 Paths to Integrating Text, Images and Audio in RAG, Which One is Best?&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;&lt;a href=&quot;https://preview.redd.it/h283ore4zghd1.png?width=1600&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=16b9c6a5b9cdbe1c5107b886cb1769fa364e2917&quot;&gt;https://preview.redd.it/h283ore4zghd1.png?width=1600&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=16b9c6a5b9cdbe1c5107b886cb1769fa364e2917&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hi All,&lt;/p&gt; &lt;p&gt;This post is a adaptation of a Multimodal episode of RAG Masters, a weekly Youtube show I do with Daniel Warfield. Each week we explore a different topic to help engineers build better RAG. Some times we know a lot. Sometimes we&amp;#39;re learning as we go, just like everyone.&lt;/p&gt; &lt;p&gt;The show is here if you want to check it out, but I&amp;#39;ll keep posting a lot of the content here regardless.&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=ZetGV7gtyQw&quot;&gt;https://www.youtube.com/watch?v=ZetGV7gtyQw&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Also, my shop &lt;a href=&quot;http://EyeLevel.ai&quot;&gt;EyeLevel.ai&lt;/a&gt; has some kick ass tools for building advanced RAG in just a few API calls with SOTA accuracy. The APIs are especially good with complex documents.&lt;/p&gt; &lt;p&gt;Ok, on to the post.&lt;/p&gt; &lt;h1&gt;What is Multimodal RAG?&lt;/h1&gt; &lt;p&gt;Multimodal RAG is an advanced extension of traditional Retrieval-Augmented Generation systems. Classic RAG involves a retrieval engine that searches a database of text documents to find relevant information and injects this data into a prompt for a language model to generate a response. Multimodal RAG expands this by including non-text data types, which enhances the model&amp;#39;s ability to understand and generate responses based on a more comprehensive set of inputs.&lt;/p&gt; &lt;p&gt;Taking multimodal inputs allows for RAG engineers to build a more complex retrieval engine that can ask a store of information about information across different mediums. This means that the retrieval engine can grab data from various sources—whether text, images, audio, or video—and use that information to answer a query. For instance, an expert&amp;#39;s audio commentary on the Eiffel Tower can be retrieved alongside text and image data to provide a more holistic response that anchors the answer in the data provided.&lt;/p&gt; &lt;h1&gt;How Multimodal RAG Works&lt;/h1&gt; &lt;p&gt;The mechanics of Multimodal RAG involve transforming different data types into a structured data format like vectors that a model can process. This allows the model to retrieve and generate information across multiple modalities seamlessly.&lt;/p&gt; &lt;p&gt;Once these data types are encoded into vectors, they can be stored in a vector space or similar storage vehicle, enabling the model to find relevant information regardless of the original data type. This process could involve clustering similar data and separating dissimilar data, making it easier to retrieve the most pertinent information for a given query.&lt;/p&gt; &lt;h1&gt;Three Approaches to Multimodal RAG&lt;/h1&gt; &lt;p&gt;Implementing Multimodal RAG can be approached in a few distinct ways, each with its advantages and challenges. The three main methods include using a single multimodal model, employing a grounded modality approach, and utilizing multiple encoders.&lt;/p&gt; &lt;h1&gt;Single Multimodal Model&lt;/h1&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/7hdmvcfj0hhd1.png?width=960&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ce25486892694d6110e70084d6042ea1d2fc604f&quot;&gt;Image: Multimodal RAG diagram depicting the storage of Audio, Image, and Text encodings to answer a user query.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This approach uses a unified model trained to encode different types of data (text, images, audio) into a common vector space. The model can then perform retrieval and generation across these different data types seamlessly. A single multimodal approach tends to be one of the most common approaches people talk about when they talk about multimodal RAG.&lt;/p&gt; &lt;p&gt;This method simplifies the process but relies heavily on the model’s ability to accurately encode and retrieve multimodal data. However, if the model is well-trained, it can store and retrieve similar information across different modalities effectively.&lt;/p&gt; &lt;p&gt;Google is a great example of using a single multimodal mode.&lt;/p&gt; &lt;h1&gt;Grounded Modality (Text-Based)&lt;/h1&gt; &lt;p&gt;In this approach, all data types are converted into text descriptions before being encoded and stored. This method leverages the strength of text-based models but may involve some loss of information during the conversion process.&lt;/p&gt; &lt;p&gt;Turning all data types into one modality creates a unified set of information for the model to retrieve, and today’s models are strongest on text. That’s not to say in the future there won&amp;#39;t be models that are better suited for other modalities. And that future might be months not years. But for today’s powerhouse models, they started out as text machines and that is still where they are strongest.&lt;/p&gt; &lt;p&gt;This approach allows the use of robust text-based models for encoding and retrieval, making it a practical solution for environments where text is the primary data type.&lt;/p&gt; &lt;h1&gt;Multiple Encoders&lt;/h1&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/3wmxg7yl0hhd1.png?width=960&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=80a2501a4c0b4c349fd93578c1c528c37848c97d&quot;&gt;Image: A Multimodal RAG diagram that relies on separately aligned models to handle different modalities from a user query.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This method employs separate models to encode different data types. Each type of data (audio, images, text) is processed by its respective model, and the results are integrated later in the retrieval process. Passing them through a set of encoders that can play nicely together creates an environment where each model and encoder can be fine-tuned to play to its particular strengths. &lt;/p&gt; &lt;p&gt;This approach allows for specialized encoding but increases complexity in managing multiple models. It offers the flexibility to use the best model for each data type, enhancing the accuracy and relevance of the retrieved information. But often it can be the most difficult to implement and maintain due to the increased complexity of inputs and outputs. &lt;/p&gt; &lt;p&gt;With the emergence of powerful models that are starting to outperform other models in specific modalities, this approach to multimodal RAG may grow in popularity.&lt;/p&gt; &lt;h1&gt;Challenges and Considerations&lt;/h1&gt; &lt;p&gt;Implementing Multimodal RAG comes with its own set of challenges, such as handling temporal changes in data and ensuring the accuracy of the retrieval and generation process. &lt;/p&gt; &lt;p&gt;Temporal changes, like the varying appearances of the Eiffel Tower over time, pose a significant challenge. Ensuring that the retrieved information is temporally accurate and relevant requires sophisticated handling of metadata and context which can be even more challenging when trying to pull data from multiple modalities like images and audio.&lt;/p&gt; &lt;p&gt;Another consideration is the balance between using a single unified model and multiple specialized models. While a single model offers simplicity, multiple models provide more tailored encoding for different data types. This decision depends on the specific application and the need for flexibility.&lt;/p&gt; &lt;h1&gt;Practical Applications and Future Prospects&lt;/h1&gt; &lt;p&gt;Multimodal RAG holds immense potential for various practical applications, from enhancing search engines to improving AI-driven personal assistants. By integrating multiple data types, these systems can provide richer, more nuanced responses, improving user experience and satisfaction.&lt;/p&gt; &lt;p&gt;Looking forward, the field of Multimodal RAG is poised for significant advancements. As models continue to improve and new techniques are developed, the ability to effectively integrate and leverage multiple data types will become increasingly crucial. This progress will open up new opportunities for powerful applications and improved AI performance.&lt;/p&gt; &lt;h1&gt;Conclusion&lt;/h1&gt; &lt;p&gt;Multimodal RAG represents a significant advancement in AI, as it can enable richer and more contextually accurate information retrieval and generation that grounds the model in the truth of the data across modalities. While the field continues to evolve, the various approaches to implementing Multimodal RAG offer different trade-offs between simplicity, flexibility, and complexity. As technology progresses, the ability to effectively integrate and leverage multiple data types will be crucial for developing advanced AI applications.&lt;/p&gt; &lt;p&gt;Full Episode:&lt;br/&gt; &lt;a href=&quot;https://www.youtube.com/watch?v=ZetGV7gtyQw&quot;&gt;https://www.youtube.com/watch?v=ZetGV7gtyQw&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/neilkatz&quot;&gt; /u/neilkatz &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1enbqew/multimodal_rag_explainer_3_paths_to_integrating/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1enbqew/multimodal_rag_explainer_3_paths_to_integrating/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1enbqew</id><media:thumbnail url="https://external-preview.redd.it/X68ByIY0sc7DCW0vPJh2aPA1NayqYM4-T62ZwCr2iEc.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=05195d05473ac42b4ae8434d45ed950a6308ce12" /><link href="https://www.reddit.com/r/LangChain/comments/1enbqew/multimodal_rag_explainer_3_paths_to_integrating/" /><updated>2024-08-08T17:20:07+00:00</updated><published>2024-08-08T17:20:07+00:00</published><title>Multimodal RAG Explainer: 3 Paths to Integrating Text, Images and Audio in RAG, Which One is Best?</title></entry><entry><author><name>/u/LabelMeMaybe</name><uri>https://www.reddit.com/user/LabelMeMaybe</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;A couple years ago, we &lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/xwf38u/r_announcing_crowdlab_opensource_tools_for_data/&quot;&gt;announced&lt;/a&gt; open-source CROWDLAB for multiannotator labels. We&amp;#39;ve now seen that these annotator algorithms are also useful for LLM Evals in RAG applications, and wrote a short tutorial on how to do so: &lt;a href=&quot;https://cleanlab.ai/blog/team-llm-evals/&quot;&gt;https://cleanlab.ai/blog/team-llm-evals/&lt;/a&gt;. Hope you find it useful, and would love to know what folks are using for LLM Evals! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/LabelMeMaybe&quot;&gt; /u/LabelMeMaybe &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1engmlq/llm_evals_with_opensource_crowdlab/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1engmlq/llm_evals_with_opensource_crowdlab/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1engmlq</id><link href="https://www.reddit.com/r/LangChain/comments/1engmlq/llm_evals_with_opensource_crowdlab/" /><updated>2024-08-08T20:36:28+00:00</updated><published>2024-08-08T20:36:28+00:00</published><title>LLM Evals with open-source CROWDLAB</title></entry><entry><author><name>/u/mayodoctur</name><uri>https://www.reddit.com/user/mayodoctur</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I am nearly 500 articles scraped and am currently trying to group them by what stories are the same. For this, I wanted to use text similarity to check how similar the titles are. For example&lt;/p&gt; &lt;p&gt;Britons in Lebanon could be evacuated by more&lt;br/&gt; Britons in Lebanon on standby for evacuation &lt;/p&gt; &lt;p&gt;are the same story by with different wording. Obviously, in this case its very obvious and something like Jacquard Index could be used. But in other scenarious it gets more complicated. Different wording but the same story.&lt;br/&gt; Using this site &lt;a href=&quot;https://similarity-demo.newscatcherapi.com/&quot;&gt;https://similarity-demo.newscatcherapi.com/&lt;/a&gt; I checked that cosine and word2Vec has the highest success. I checked an implementation of word2Vec and you need to build a model just to check the similarity. I have to process near 500 titles and group them, theres already a lot of computation involved and I need to grab this data in real time. Is there a way to check the similarity without having to do it from scratch ?&lt;/p&gt; &lt;p&gt;If anyone can suggest a better way please let me know&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/mayodoctur&quot;&gt; /u/mayodoctur &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1enksd4/text_similarity_between_news_article_titles/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1enksd4/text_similarity_between_news_article_titles/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1enksd4</id><link href="https://www.reddit.com/r/LangChain/comments/1enksd4/text_similarity_between_news_article_titles/" /><updated>2024-08-08T23:34:12+00:00</updated><published>2024-08-08T23:34:12+00:00</published><title>Text Similarity between News article titles</title></entry><entry><author><name>/u/Distinct-Target7503</name><uri>https://www.reddit.com/user/Distinct-Target7503</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Is there any documentation about Sonar models beyond what&amp;#39;s provided in the Perplexity docs?&lt;/p&gt; &lt;p&gt;I&amp;#39;m seeking more information about the differences in behavior between the &amp;quot;system message,&amp;quot; prompt, and/or first user request. From what I understand, the query is generated based on the &amp;quot;user&amp;quot; message, and query generation ignores the &amp;quot;system&amp;quot; message. So what exactly is the purpose of this &amp;quot;system&amp;quot; message? The examples typically use short 3-4 word phrases, but do Sonar models support more complex system instructions (similar to the models they&amp;#39;re trained on)?&lt;/p&gt; &lt;p&gt;Additionally, how do online models handle multi-turn conversations? What context is used for query generation and RAG? I understand these models are intended for single-turn interactions, with &amp;quot;chat&amp;quot; versions available for multi-turn conversations.&lt;/p&gt; &lt;p&gt;This leads to my question about context length. The online model claims a 128K context, but this seems unattainable in practice. If the user message is too long, query generation becomes less effective and retrieves less relevant results. Higher context can&amp;#39;t even be achieved with multi-turn chats, as the quality drops significantly.&lt;/p&gt; &lt;p&gt;It&amp;#39;s worth noting that the number of tokens provided to the model as &amp;quot;sources&amp;quot; is generally in the range of 2-3K globally, but often much less depending on the complexity of the question (via the API).&lt;/p&gt; &lt;p&gt;Does anyone have insights into these issues? Could someone from the staff please point me towards more detailed information?&lt;/p&gt; &lt;p&gt;Thanks in advance! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Distinct-Target7503&quot;&gt; /u/Distinct-Target7503 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1enk44f/seeking_indepth_information_on_perplexitys_sonar/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1enk44f/seeking_indepth_information_on_perplexitys_sonar/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1enk44f</id><link href="https://www.reddit.com/r/LangChain/comments/1enk44f/seeking_indepth_information_on_perplexitys_sonar/" /><updated>2024-08-08T23:03:55+00:00</updated><published>2024-08-08T23:03:55+00:00</published><title>Seeking In-Depth Information on Perplexity's Sonar Models: System Messages, Context Handling, and API Limitations</title></entry><entry><author><name>/u/julio_oa</name><uri>https://www.reddit.com/user/julio_oa</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;In a RAG system, how do you avoid the bot to go retrieve information when the questions are just small talk? For example the user says “Hi how are you?” And the bot goes and triggers all the RAG logic and gets all the information and makes a lot of drama and it replies “good thanks for asking” hahah anybody dealing with this issue?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/julio_oa&quot;&gt; /u/julio_oa &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1en5h33/rag_system_to_detect_small_talk/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1en5h33/rag_system_to_detect_small_talk/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1en5h33</id><link href="https://www.reddit.com/r/LangChain/comments/1en5h33/rag_system_to_detect_small_talk/" /><updated>2024-08-08T13:05:46+00:00</updated><published>2024-08-08T13:05:46+00:00</published><title>RAG system to detect small talk</title></entry><entry><author><name>/u/mehul_gupta1997</name><uri>https://www.reddit.com/user/mehul_gupta1997</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Langfuse is a free alternate for Langsmith for Generative AI based applications for debugging and tracing. This video explains how to get Started with Langfuse : &lt;a href=&quot;https://youtu.be/fIQIfIK6v0o?si=hzeG4matNCCZ9Bt_&quot;&gt;https://youtu.be/fIQIfIK6v0o?si=hzeG4matNCCZ9Bt_&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/mehul_gupta1997&quot;&gt; /u/mehul_gupta1997 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1en73et/langfuse_for_llm_tracing_for_beginners/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1en73et/langfuse_for_llm_tracing_for_beginners/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1en73et</id><link href="https://www.reddit.com/r/LangChain/comments/1en73et/langfuse_for_llm_tracing_for_beginners/" /><updated>2024-08-08T14:14:57+00:00</updated><published>2024-08-08T14:14:57+00:00</published><title>Langfuse for LLM tracing for beginners</title></entry><entry><author><name>/u/pekkamama</name><uri>https://www.reddit.com/user/pekkamama</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e5pe1a/optimal_rag_for_text2sql/?utm_source=share&amp;amp;utm_medium=web3x&amp;amp;utm_name=web3xcss&amp;amp;utm_term=1&amp;amp;utm_content=share_button&quot;&gt;https://www.reddit.com/r/LangChain/comments/1e5pe1a/optimal_rag_for_text2sql/?utm_source=share&amp;amp;utm_medium=web3x&amp;amp;utm_name=web3xcss&amp;amp;utm_term=1&amp;amp;utm_content=share_button&lt;/a&gt; &lt;/p&gt; &lt;p&gt;I built a working text-2-SQL pipeline that runs queries on million of records. 20+ tables. It leverages Claude 3.5 sonnet and gets the work done 7/10 very well on a few complex prompts, multiple joins and conditional clauses. At the infra level, the database has been modified to create and include a few views that would help make much more sense of the data. &lt;/p&gt; &lt;p&gt;Now, I am in charge of building an analysis agent on top of it. &lt;/p&gt; &lt;p&gt;Here&amp;#39;s what I was thinking: I&amp;#39;ll have the text-2-sql agent translate to query, retrieve the tabular information from the database and return the final table before applying any conditions or other arithmetic operations. And then pass on the resulting table to pandas agent along with the originally asked question. &lt;/p&gt; &lt;p&gt;And then the pandas agent cook. But, I&amp;#39;m not a 100% sure whether this is the ideal approach. OR if there is any other way to work with this. Lemme know if you have any thoughts!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/pekkamama&quot;&gt; /u/pekkamama &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1en87i9/text2sqlpandas_pipeline_how_do_i_build_this/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1en87i9/text2sqlpandas_pipeline_how_do_i_build_this/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1en87i9</id><link href="https://www.reddit.com/r/LangChain/comments/1en87i9/text2sqlpandas_pipeline_how_do_i_build_this/" /><updated>2024-08-08T15:00:03+00:00</updated><published>2024-08-08T15:00:03+00:00</published><title>Text-2-SQL-Pandas Pipeline: HOW DO I BUILD THIS!</title></entry><entry><author><name>/u/cmbhatt</name><uri>https://www.reddit.com/user/cmbhatt</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hello everybody! I have just started langchain and am going through documentation and it seems very confusing. I am also having trouble with prompts. Could you please suggest a good starting point? Sorry if this has been posted before.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/cmbhatt&quot;&gt; /u/cmbhatt &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1enaock/documentation_seems_confusing/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1enaock/documentation_seems_confusing/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1enaock</id><link href="https://www.reddit.com/r/LangChain/comments/1enaock/documentation_seems_confusing/" /><updated>2024-08-08T16:37:52+00:00</updated><published>2024-08-08T16:37:52+00:00</published><title>Documentation seems confusing</title></entry><entry><author><name>/u/Rohitha2107</name><uri>https://www.reddit.com/user/Rohitha2107</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hello Community,&lt;/p&gt; &lt;p&gt;I am trying to summarize using map_reduce implementation and I have a text file that has ~78000 token(according to Open AI tokenizer) and I am using mistralai/Mistral-7B-Instruct-v0.3 model through huggingface inference and I run into the above error and the ouputs gets cut off in the middle. Can you please advice how to fix this? The model has a context length of 32K but I am not sure why the error says 1024 tokens. Here is the full error&lt;br/&gt; Token indices sequence length is longer than the specified maximum sequence length for this model (20098 &amp;gt; 1024). Running this sequence through the model will result in indexing errors.&lt;br/&gt; Here is the code &lt;/p&gt; &lt;p&gt;from langchain.chains.combine_documents.stuff import StuffDocumentsChain&lt;/p&gt; &lt;p&gt;from langchain.chains.llm import LLMChain&lt;/p&gt; &lt;p&gt;from langchain.prompts import PromptTemplate&lt;/p&gt; &lt;p&gt;from langchain.schema.document import Document&lt;/p&gt; &lt;p&gt;from langchain.chains.combine_documents.stuff import StuffDocumentsChain&lt;/p&gt; &lt;p&gt;from langchain.chains.llm import LLMChain&lt;/p&gt; &lt;p&gt;from langchain.prompts import PromptTemplate&lt;/p&gt; &lt;p&gt;from langchain.chains.summarize import load_summarize_chain&lt;/p&gt; &lt;h1&gt;type the code to be ready for tomorrow:&lt;/h1&gt; &lt;p&gt;from langchain.chains import MapReduceDocumentsChain, ReduceDocumentsChain&lt;/p&gt; &lt;h1&gt;Map&lt;/h1&gt; &lt;p&gt;map_template = &amp;quot;&amp;quot;&amp;quot;&amp;lt;s&amp;gt;[INST]Please write a clear and concise summary for the following text.[/INST]&lt;/p&gt; &lt;p&gt;Text:&lt;/p&gt; &lt;p&gt;&amp;quot;{docs}&amp;quot;&amp;lt;/s&amp;gt;&lt;/p&gt; &lt;p&gt;[INST] Summary:[/INST]&lt;/p&gt; &lt;p&gt;&amp;quot;&amp;quot;&amp;quot;&lt;/p&gt; &lt;p&gt;map_prompt = PromptTemplate.from_template(map_template)&lt;/p&gt; &lt;p&gt;map_chain = LLMChain(llm=api_llm, prompt=map_prompt)&lt;/p&gt; &lt;h1&gt;Reduce&lt;/h1&gt; &lt;p&gt;reduce_template = &amp;quot;&amp;quot;&amp;quot;&amp;lt;s&amp;gt;[INST]Below are the summaries of multiple segments of a document. Please combine these summaries to form a meaningful final summary.[/INST]&lt;/p&gt; &lt;p&gt;Text:&lt;/p&gt; &lt;p&gt;&amp;quot;{doc_summaries}&amp;quot;&amp;lt;/s&amp;gt;&lt;/p&gt; &lt;p&gt;[INST] Summary:[/INST]&lt;/p&gt; &lt;p&gt;&amp;quot;&amp;quot;&amp;quot;&lt;/p&gt; &lt;p&gt;reduce_prompt = PromptTemplate.from_template(reduce_template)&lt;/p&gt; &lt;h1&gt;Run chain&lt;/h1&gt; &lt;p&gt;reduce_chain = LLMChain(llm=api_llm, prompt=reduce_prompt)&lt;/p&gt; &lt;h1&gt;Takes a list of documents, combines them into a single string, and passes this to an LLMChain&lt;/h1&gt; &lt;p&gt;combine_documents_chain = StuffDocumentsChain(&lt;/p&gt; &lt;p&gt;llm_chain=reduce_chain, document_variable_name=&amp;quot;doc_summaries&amp;quot;&lt;/p&gt; &lt;p&gt;)&lt;/p&gt; &lt;h1&gt;Combines and iteratively reduces the mapped documents&lt;/h1&gt; &lt;p&gt;reduce_documents_chain = ReduceDocumentsChain(&lt;/p&gt; &lt;h1&gt;This is final chain that is called.&lt;/h1&gt; &lt;p&gt;combine_documents_chain=combine_documents_chain,&lt;/p&gt; &lt;h1&gt;If documents exceed context for `StuffDocumentsChain`&lt;/h1&gt; &lt;p&gt;collapse_documents_chain=combine_documents_chain,&lt;/p&gt; &lt;h1&gt;The maximum number of tokens to group documents into.&lt;/h1&gt; &lt;p&gt;token_max=4000,&lt;/p&gt; &lt;p&gt;)&lt;/p&gt; &lt;h1&gt;Combining documents by mapping a chain over them, then combining results&lt;/h1&gt; &lt;p&gt;map_reduce_chain = MapReduceDocumentsChain(&lt;/p&gt; &lt;h1&gt;Map chain&lt;/h1&gt; &lt;p&gt;llm_chain=map_chain,&lt;/p&gt; &lt;h1&gt;Reduce chain&lt;/h1&gt; &lt;p&gt;reduce_documents_chain=reduce_documents_chain,&lt;/p&gt; &lt;h1&gt;The variable name in the llm_chain to put the documents in&lt;/h1&gt; &lt;p&gt;document_variable_name=&amp;quot;docs&amp;quot;,&lt;/p&gt; &lt;h1&gt;Return the results of the map steps in the output&lt;/h1&gt; &lt;p&gt;return_intermediate_steps=True,&lt;/p&gt; &lt;p&gt;)&lt;/p&gt; &lt;h1&gt;text_splitter = CharacterTextSplitter.from_tiktoken_encoder(&lt;/h1&gt; &lt;h1&gt;chunk_size=1000, chunk_overlap=0&lt;/h1&gt; &lt;h1&gt;)&lt;/h1&gt; &lt;h1&gt;split_docs = text_splitter.split_documents(docs)&lt;/h1&gt; &lt;p&gt;splitter = RecursiveCharacterTextSplitter(chunk_size=4000, chunk_overlap=200,length_function = len)&lt;/p&gt; &lt;p&gt;chunks = splitter.create_documents([text])&lt;/p&gt; &lt;p&gt;result=map_reduce_chain.invoke(chunks)&lt;/p&gt; &lt;p&gt;print(result[&amp;#39;output_text&amp;#39;])&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Rohitha2107&quot;&gt; /u/Rohitha2107 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1enbsrw/token_indices_sequence_length_is_longer_than_the/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1enbsrw/token_indices_sequence_length_is_longer_than_the/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1enbsrw</id><link href="https://www.reddit.com/r/LangChain/comments/1enbsrw/token_indices_sequence_length_is_longer_than_the/" /><updated>2024-08-08T17:22:39+00:00</updated><published>2024-08-08T17:22:39+00:00</published><title>Token indices sequence length is longer than the specified maximum sequence length for this model (20098 &gt; 1024)</title></entry><entry><author><name>/u/Exotic_Show3271</name><uri>https://www.reddit.com/user/Exotic_Show3271</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;hi guys, I want my rag (when asked) to output and print a table that is dynamically generated according to the question and context. What is the way to do it?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Exotic_Show3271&quot;&gt; /u/Exotic_Show3271 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1en2wxt/output_a_table_in_rag/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1en2wxt/output_a_table_in_rag/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1en2wxt</id><link href="https://www.reddit.com/r/LangChain/comments/1en2wxt/output_a_table_in_rag/" /><updated>2024-08-08T10:55:04+00:00</updated><published>2024-08-08T10:55:04+00:00</published><title>output a table in rag</title></entry><entry><author><name>/u/abhinavkimothi</name><uri>https://www.reddit.com/user/abhinavkimothi</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1em6m7e/embeddings_the_blueprint_of_contextual_ai/&quot;&gt; &lt;img src=&quot;https://b.thumbs.redditmedia.com/QKVGsXwodAGF0k5YNnyDIcsDJJeG9DZ2QpyUcA904NE.jpg&quot; alt=&quot;Embeddings : The blueprint of Contextual AI&quot; title=&quot;Embeddings : The blueprint of Contextual AI&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/abhinavkimothi&quot;&gt; /u/abhinavkimothi &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/gallery/1em6m7e&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1em6m7e/embeddings_the_blueprint_of_contextual_ai/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1em6m7e</id><media:thumbnail url="https://b.thumbs.redditmedia.com/QKVGsXwodAGF0k5YNnyDIcsDJJeG9DZ2QpyUcA904NE.jpg" /><link href="https://www.reddit.com/r/LangChain/comments/1em6m7e/embeddings_the_blueprint_of_contextual_ai/" /><updated>2024-08-07T08:34:09+00:00</updated><published>2024-08-07T08:34:09+00:00</published><title>Embeddings : The blueprint of Contextual AI</title></entry><entry><author><name>/u/Embarrassed_Bread121</name><uri>https://www.reddit.com/user/Embarrassed_Bread121</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi folks. I am a Machine Learning Engineer looking for open opportunities in Bangalore. I have 1 year of experience in developing and deploying ML solutions convering basic ML fundamentals like regression models and data analysis to building RAG applications using vector DB and Langchain. I have worked in shaping ideas into POCs using Machine Learning and Deep Learning. &lt;/p&gt; &lt;p&gt;My tech stack includes Python, Jupyter Notebook, Sci-kit Learn, Langchain, Vector DB, fine-tuning LLMs for specific use cases.&lt;/p&gt; &lt;p&gt;I am also experienced in developing &amp;amp; deploying the end to end LLM applications to AWS cloud. I am passionate about ML.&lt;/p&gt; &lt;p&gt;Any leads would be appreciated. Thank You &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Embarrassed_Bread121&quot;&gt; /u/Embarrassed_Bread121 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1en1s8v/ml_engineer_1_yoe_looking_for_an_open_opportunity/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1en1s8v/ml_engineer_1_yoe_looking_for_an_open_opportunity/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1en1s8v</id><link href="https://www.reddit.com/r/LangChain/comments/1en1s8v/ml_engineer_1_yoe_looking_for_an_open_opportunity/" /><updated>2024-08-08T09:44:44+00:00</updated><published>2024-08-08T09:44:44+00:00</published><title>ML Engineer (1 YOE) looking for an open opportunity</title></entry></feed>