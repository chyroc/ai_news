<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/"><category term="LangChain" label="r/LangChain"/><updated>2024-07-28T21:47:48+00:00</updated><icon>https://www.redditstatic.com/icon.png/</icon><id>/r/LangChain.rss</id><link rel="self" href="https://www.reddit.com/r/LangChain.rss" type="application/atom+xml" /><link rel="alternate" href="https://www.reddit.com/r/LangChain" type="text/html" /><subtitle>LangChain is an open-source framework and developer toolkit that helps developers get LLM applications from prototype to production. It is available for Python and Javascript at https://www.langchain.com/.</subtitle><title>LangChain</title><entry><author><name>/u/zchaarm</name><uri>https://www.reddit.com/user/zchaarm</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;A place for members of &lt;a href=&quot;/r/LangChain&quot;&gt;r/LangChain&lt;/a&gt; to chat with each other&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/zchaarm&quot;&gt; /u/zchaarm &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_10ljho9</id><link href="https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/" /><updated>2023-01-26T04:33:05+00:00</updated><published>2023-01-26T04:33:05+00:00</published><title>r/LangChain Lounge</title></entry><entry><author><name>/u/Actual_Box6342</name><uri>https://www.reddit.com/user/Actual_Box6342</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I want to create a RAG for the code generation task. the knowledge base will be a library and starting from that library my RAG must be able to generate code based on the library. Do you have any advice on the type of approach, vector store or knowledge graph database, models and more?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Actual_Box6342&quot;&gt; /u/Actual_Box6342 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1eefr4x/rag_for_code_generation/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1eefr4x/rag_for_code_generation/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1eefr4x</id><link href="https://www.reddit.com/r/LangChain/comments/1eefr4x/rag_for_code_generation/" /><updated>2024-07-28T19:39:06+00:00</updated><published>2024-07-28T19:39:06+00:00</published><title>RAG for Code Generation</title></entry><entry><author><name>/u/adiko4</name><uri>https://www.reddit.com/user/adiko4</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey folks,&lt;/p&gt; &lt;p&gt;I’ve been looking into GPT4o tool calling feature for my project and came to conclusion that its architecture can be quite inefficient in costs (not very surprising). &lt;/p&gt; &lt;p&gt;Every time I want to use a tool, I have to re-send the entire chat history—system prompts, user messages, tool calls, and pratically everything. This means I’m paying for those extra tokens all over again, which can add up quickly if my prompt is 4000 tokens long.&lt;/p&gt; &lt;p&gt;I get that this is for privacy reasons, but it’s definitely not the most cost-friendly. Has anyone else dealt with this? How are you handling it? Any tips to make this less of a hassle or to keep the costs down?&lt;/p&gt; &lt;p&gt;I need it just for &amp;#39;calculator tool&amp;#39; that will enable GPT4o to make adjustments to various numeric data in a determinstic way..&lt;/p&gt; &lt;p&gt;Would love to hear your thoughts and suggestions!&lt;/p&gt; &lt;p&gt;Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/adiko4&quot;&gt; /u/adiko4 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1eef5go/anyone_else_dealing_with_the_token_cost_burden_of/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1eef5go/anyone_else_dealing_with_the_token_cost_burden_of/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1eef5go</id><link href="https://www.reddit.com/r/LangChain/comments/1eef5go/anyone_else_dealing_with_the_token_cost_burden_of/" /><updated>2024-07-28T19:12:37+00:00</updated><published>2024-07-28T19:12:37+00:00</published><title>Anyone else dealing with the token cost burden of re-sending chat history when using GPT4o tool calling feature?</title></entry><entry><author><name>/u/vuongagiflow</name><uri>https://www.reddit.com/user/vuongagiflow</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ee3ly6/optimize_agentic_workflow_cost_and_performance_a/&quot;&gt; &lt;img src=&quot;https://b.thumbs.redditmedia.com/m3-hsSMPJGz0FXT6IjJOfgUDv0XukiUwYCbdIIMr0Zw.jpg&quot; alt=&quot;Optimize Agentic Workflow Cost and Performance: A reversed engineering approach&quot; title=&quot;Optimize Agentic Workflow Cost and Performance: A reversed engineering approach&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;&lt;a href=&quot;https://preview.redd.it/8ifdm442v7fd1.png?width=1492&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d2b7c1b3391d17d940dc36705f2a9407c822df73&quot;&gt;https://preview.redd.it/8ifdm442v7fd1.png?width=1492&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d2b7c1b3391d17d940dc36705f2a9407c822df73&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There are two primary approaches to getting started with Agentic workflows: &lt;strong&gt;workflow automation&lt;/strong&gt; for domain experts and &lt;strong&gt;autonomous agents&lt;/strong&gt; for resource-constrained projects. By observing how agents perform tasks successfully, you can map out and optimize workflow steps, reducing hallucinations, costs, and improving performance.&lt;/p&gt; &lt;p&gt;Let&amp;#39;s explore how to automate the “Dependencies Upgrade” for your product team using CrewAI then Langgraph. Typically, a software engineer would handle this task by visiting changelog webpages, reviewing changes, and coordinating with the product manager to create backlog stories. With agentic workflow, we can streamline and automate these processes, saving time and effort while allowing engineers to focus on more engaging work.&lt;/p&gt; &lt;p&gt;For demonstration, &lt;a href=&quot;https://github.com/AgiFlow/repo-upgrade&quot;&gt;source-code is available on Github&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;For detailed explanation, please see below videos:&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://youtu.be/hvcd8Xjpd7A&quot;&gt;Part 1: Get started with Autonomous Agents using CrewAI&lt;/a&gt; &lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://youtu.be/_k82vx4qaLo&quot;&gt;Part 2: Optimisation with Langgraph and Conclusion&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Short summary on the repo and videos&lt;/h1&gt; &lt;p&gt;With &lt;strong&gt;autononous agents&lt;/strong&gt; first approach, we would want to follow below steps:&lt;/p&gt; &lt;h1&gt;1. Keep it Simple, Stupid&lt;/h1&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/zj4hcm8bv7fd1.png?width=1456&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=caa379e7735b139916444e359c9630bd2a9a9419&quot;&gt;https://preview.redd.it/zj4hcm8bv7fd1.png?width=1456&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=caa379e7735b139916444e359c9630bd2a9a9419&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We start with two agents: a Product Manager and a Developer, utilizing the Hierarchical Agents process from CrewAI. The Product Manager orchestrates tasks and delegates them to the Developer, who uses tools to fetch changelogs and read repository files to determine if dependencies need updating. The Product Manager then prioritizes backlog stories based on these findings.&lt;/p&gt; &lt;p&gt;Our goal is to analyse the successful workflow execution only to learn the flow at the first step.&lt;/p&gt; &lt;h1&gt;2. Simplify Communication Flow&lt;/h1&gt; &lt;p&gt;Autonomous Agents are great for some scenarios, but not for workflow automation. We want to reduce the cost, hallucination and improve speed from Hierarchical process.&lt;/p&gt; &lt;p&gt;Second step is to reduce unnecessary communication from bi-directional to uni-directional between agents. Simply talk, have specialised agent to perform its task, finish the task and pass the result to the next agent without repetition (liked Manufactoring process).&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/tiu3etkdv7fd1.png?width=1854&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=34987572a5f4ae4177a3b079d5b234f32adfd5a8&quot;&gt;https://preview.redd.it/tiu3etkdv7fd1.png?width=1854&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=34987572a5f4ae4177a3b079d5b234f32adfd5a8&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;3. Prompt optimisation&lt;/h1&gt; &lt;p&gt;ReAct Agent are great for auto-correct action, but also cause unpredictability in automation jobs which increase number of LLM calls and repeat actions.&lt;/p&gt; &lt;p&gt;If predictability, cost and speed is what you are aiming for, you can also optimise prompt and explicitly flow engineer with Langgraph. Also make sure the context you pass to prompt doesn&amp;#39;t have redundant information to control the cost.&lt;/p&gt; &lt;p&gt;A summary from above steps; the techniques in Blue box are low hanging fruits to improve your workflow. If you want to use other techniques, ensure you have these components implemented first: evaluation, observability and human-in-the-loop feedback.&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/1fh8cnvnv7fd1.png?width=1850&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f3f27cc89a419e1808202d192626bc79f2643695&quot;&gt;https://preview.redd.it/1fh8cnvnv7fd1.png?width=1850&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f3f27cc89a419e1808202d192626bc79f2643695&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I&amp;#39;ll will share blog article link later for those who prefer to read. Would love to hear your feedback on this.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/vuongagiflow&quot;&gt; /u/vuongagiflow &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ee3ly6/optimize_agentic_workflow_cost_and_performance_a/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ee3ly6/optimize_agentic_workflow_cost_and_performance_a/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1ee3ly6</id><media:thumbnail url="https://b.thumbs.redditmedia.com/m3-hsSMPJGz0FXT6IjJOfgUDv0XukiUwYCbdIIMr0Zw.jpg" /><link href="https://www.reddit.com/r/LangChain/comments/1ee3ly6/optimize_agentic_workflow_cost_and_performance_a/" /><updated>2024-07-28T09:31:44+00:00</updated><published>2024-07-28T09:31:44+00:00</published><title>Optimize Agentic Workflow Cost and Performance: A reversed engineering approach</title></entry><entry><author><name>/u/lukus88</name><uri>https://www.reddit.com/user/lukus88</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I need to evaluate some applications for research projects and wish to know which chatbot solution works best. I want to evaluate applications based on (my) official strategies, documents, guidelines so bot needs to be fine tuned. Applications are text only, my documents are text and tables also. So basically what im looking for is evaluating buddy that can offer concise and logical evaluation of applications. My documents are around 30mb size, their aplications around 30 written text alltogether.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/lukus88&quot;&gt; /u/lukus88 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ee92ii/what_chatbot_paid_or_not_is_best_for_uploading_my/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ee92ii/what_chatbot_paid_or_not_is_best_for_uploading_my/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1ee92ii</id><link href="https://www.reddit.com/r/LangChain/comments/1ee92ii/what_chatbot_paid_or_not_is_best_for_uploading_my/" /><updated>2024-07-28T14:46:09+00:00</updated><published>2024-07-28T14:46:09+00:00</published><title>What chatbot (paid or not) is best for uploading my own documents to help eith evaluating applications?</title></entry><entry><author><name>/u/glow_storm</name><uri>https://www.reddit.com/user/glow_storm</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I was using the Ollama package in Langchain , this was the community version &lt;/p&gt; &lt;p&gt;from langchain_community.llms import Ollama &lt;/p&gt; &lt;p&gt;I had setup a remote GPU serving running Ollama and wanted to use the Ollama endpoint to run a code on my personal Laptop , everything worked fine , however when I tried to use tool calling , it said to use ChatOllama , &lt;/p&gt; &lt;p&gt;from langchain_community.llms import Ollama &lt;/p&gt; &lt;p&gt;so I did , but it gave me an Error of not Implemented. I checked the Langchain Docs and it said tool calling , and it gave me a new package to use , which i first had to install &lt;/p&gt; &lt;p&gt;from langchain_ollama import ChatOllama&lt;/p&gt; &lt;p&gt;but now this package refused to Connect to my remote server of Ollama no matter what I tried , Can anyone help me understand and fix how to make Langchain work with Tool Calling on remote Ollama sever. &lt;/p&gt; &lt;p&gt;```&lt;/p&gt; &lt;p&gt;llm =ChatOllama(base_url=&amp;quot;&lt;a href=&quot;https://c7c4-65-109-75-7.ngrok-free.app&quot;&gt;https://c7c4-65-109-75-7.ngrok-free.app&lt;/a&gt;&amp;quot;,model=&amp;quot;llama3.1:70b&amp;quot;,temperature=0) # this was the code I was trying to excute&lt;/p&gt; &lt;p&gt;```&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;```&lt;/p&gt; &lt;p&gt;from langchain_community.llms import Ollama&lt;/p&gt; &lt;p&gt;llm =Ollama(base_url=&amp;quot;&lt;a href=&quot;https://c7c4-65-109-75-7.ngrok-free.app&quot;&gt;https://c7c4-65-109-75-7.ngrok-free.app&lt;/a&gt;&amp;quot;,model=&amp;quot;llama3.1:70b&amp;quot;,temperature=0) # this work but has not tool calling &lt;/p&gt; &lt;p&gt;``` &lt;/p&gt; &lt;p&gt;I would appreciate the help. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/glow_storm&quot;&gt; /u/glow_storm &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ee73og/support_for_remote_llm_calling_in_ollama_package/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ee73og/support_for_remote_llm_calling_in_ollama_package/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1ee73og</id><link href="https://www.reddit.com/r/LangChain/comments/1ee73og/support_for_remote_llm_calling_in_ollama_package/" /><updated>2024-07-28T13:11:13+00:00</updated><published>2024-07-28T13:11:13+00:00</published><title>Support for Remote LLM calling in Ollama Package</title></entry><entry><author><name>/u/naxmax2019</name><uri>https://www.reddit.com/user/naxmax2019</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1edrxte/created_an_ai_voice_agents_product_give_it_a_try/&quot;&gt; &lt;img src=&quot;https://external-preview.redd.it/cbuCJtyPbgKwiO0T7Az4AZDrzHTctue2DnaepGcR-WU.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=128356a217db54b740c338f2106ff47e46e6fd22&quot; alt=&quot;Created an AI voice agents product .. give it a try. I am using Twilio for phone numbers... The results have been pretty good. Give it a try and let me know what you think. I will share the code the next days. &quot; title=&quot;Created an AI voice agents product .. give it a try. I am using Twilio for phone numbers... The results have been pretty good. Give it a try and let me know what you think. I will share the code the next days. &quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/naxmax2019&quot;&gt; /u/naxmax2019 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://workhub.ai/ai-voice-agent/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1edrxte/created_an_ai_voice_agents_product_give_it_a_try/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1edrxte</id><media:thumbnail url="https://external-preview.redd.it/cbuCJtyPbgKwiO0T7Az4AZDrzHTctue2DnaepGcR-WU.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=128356a217db54b740c338f2106ff47e46e6fd22" /><link href="https://www.reddit.com/r/LangChain/comments/1edrxte/created_an_ai_voice_agents_product_give_it_a_try/" /><updated>2024-07-27T22:04:58+00:00</updated><published>2024-07-27T22:04:58+00:00</published><title>Created an AI voice agents product .. give it a try. I am using Twilio for phone numbers... The results have been pretty good. Give it a try and let me know what you think. I will share the code the next days.</title></entry><entry><author><name>/u/mehul_gupta1997</name><uri>https://www.reddit.com/user/mehul_gupta1997</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/mehul_gupta1997&quot;&gt; /u/mehul_gupta1997 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;/r/ArtificialInteligence/comments/1ee0qxf/llama_31_tutorials/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ee0r40/llama_31_tutorials/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1ee0r40</id><link href="https://www.reddit.com/r/LangChain/comments/1ee0r40/llama_31_tutorials/" /><updated>2024-07-28T06:07:27+00:00</updated><published>2024-07-28T06:07:27+00:00</published><title>Llama 3.1 tutorials</title></entry><entry><author><name>/u/SadPianist871</name><uri>https://www.reddit.com/user/SadPianist871</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey guys,&lt;/p&gt; &lt;p&gt;I have a general question for those of you developing agentic AI systems. Have you had the problem of a service not having an API and how did you solve it (i.e., how did you define the &amp;quot;tool&amp;quot; to be used by the LLM)? A simple example: I want my personal AI assistant to purchase groceries for me, but there&amp;#39;s no API provided by the supermarket. How can I achieve that?&lt;/p&gt; &lt;p&gt;Do you think this is another reason why AI agents are still not in use for tasks that are not critical (thus, it&amp;#39;s fine if they&amp;#39;re not 100% reliable), but could be very useful in our daily lives?&lt;/p&gt; &lt;p&gt;Edit: by “simple example”, I meant “simple use case”, not that it’s easy to implement&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/SadPianist871&quot;&gt; /u/SadPianist871 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1edkn7p/lack_of_apis/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1edkn7p/lack_of_apis/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1edkn7p</id><link href="https://www.reddit.com/r/LangChain/comments/1edkn7p/lack_of_apis/" /><updated>2024-07-27T16:42:08+00:00</updated><published>2024-07-27T16:42:08+00:00</published><title>Lack of APIs</title></entry><entry><author><name>/u/Top-Cookie3565</name><uri>https://www.reddit.com/user/Top-Cookie3565</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Sorry if this question is too basic. New to this so trying to learn. &lt;/p&gt; &lt;p&gt;So this Is what I did. Created a basic agent with few random tools. Added Memory to it using RunnableWithMessageHistory. &lt;/p&gt; &lt;p&gt;&lt;code&gt;llm = ChatOpenAI(model_name = &amp;quot;gpt-3.5-turbo&amp;quot;, temperature = 0)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;tools = [click_new_image, visual_question_answer, question_answer, previous_pic]&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;prompt = ChatPromptTemplate.from_messages(&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;[&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;(&amp;quot;system&amp;quot;, &amp;quot;You are a very powerful assistant who can take pictures and answer questions about them. If the query is regarding an older pic, then answer directly instead of taking a new pic.&amp;quot;),&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;MessagesPlaceholder(variable_name=&amp;quot;history&amp;quot;),&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;(&amp;quot;user&amp;quot;, &amp;quot;{input}&amp;quot;),&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;MessagesPlaceholder(variable_name=&amp;quot;agent_scratchpad&amp;quot;),&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;]&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;agent = create_tool_calling_agent(llm, tools, prompt)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;agent_executor = AgentExecutor(agent=agent, tools=tools, verbose = True)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;store = {}&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;agent_executor_w_memory = RunnableWithMessageHistory(&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;agent_executor,&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;get_session_history,&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;input_messages_key=&amp;quot;input&amp;quot;,&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;history_messages_key=&amp;quot;history&amp;quot;,&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;To run this I did ( In Streamlit ) - &lt;/p&gt; &lt;p&gt;&lt;code&gt;if prompt := st.chat_input():&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;st.chat_message(&amp;quot;user&amp;quot;).write(prompt)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;with st.chat_message(&amp;quot;assistant&amp;quot;):&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;response = agent_executor_w_memory.invoke(&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;{&amp;quot;input&amp;quot;: prompt},&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;config=config,&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;st.write(response[&amp;quot;output&amp;quot;])&lt;/code&gt;&lt;/p&gt; &lt;p&gt;But this won&amp;#39;t stream( typing effect) the output, it will just give the final output at once. I want to stream the output. Only the last response, not the intermediate steps. &lt;/p&gt; &lt;p&gt;Ps- Can I also stream the intermediate step result( we could iterate through the stream and print chunks but that will also not stream( typing effect)) ? or like tools it call too? ( Asking just to learn more, not needed as of now) &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Top-Cookie3565&quot;&gt; /u/Top-Cookie3565 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1eduao5/how_can_i_stream_only_the_final_result_from_a/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1eduao5/how_can_i_stream_only_the_final_result_from_a/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1eduao5</id><link href="https://www.reddit.com/r/LangChain/comments/1eduao5/how_can_i_stream_only_the_final_result_from_a/" /><updated>2024-07-27T23:56:51+00:00</updated><published>2024-07-27T23:56:51+00:00</published><title>How can I stream only the final result from a agent in streamlit</title></entry><entry><author><name>/u/Pristine-Watercress9</name><uri>https://www.reddit.com/user/Pristine-Watercress9</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey guys,&lt;/p&gt; &lt;p&gt;My buddy and I are working on a tool that lets you preview your ML models in a presentable environment before deployment. I had my models set up on Google Colab, but it wasn’t easy for the team to review it. It also isn’t very presentable to clients.&lt;/p&gt; &lt;p&gt;So we want to create a demo environment that’s super simple to share and present models before handing off to devops. Thinking about adding some sort of feedback system too.&lt;/p&gt; &lt;p&gt;We’re still figuring out the details, so we’d love to get your takes on this. In your experience, what features would’ve helped you? Currently we have charts and collaboration features in mind.&lt;/p&gt; &lt;p&gt;Thanks! (my dm is open! we can’t be the only ones having this problem right)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Pristine-Watercress9&quot;&gt; /u/Pristine-Watercress9 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1edtzrx/ml_model_demo_tool/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1edtzrx/ml_model_demo_tool/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1edtzrx</id><link href="https://www.reddit.com/r/LangChain/comments/1edtzrx/ml_model_demo_tool/" /><updated>2024-07-27T23:42:07+00:00</updated><published>2024-07-27T23:42:07+00:00</published><title>ML model demo tool?</title></entry><entry><author><name>/u/Longjumping_Ad_7053</name><uri>https://www.reddit.com/user/Longjumping_Ad_7053</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I’m trying to use the SQLDatabase.from_databricks and I’m getting a weird error &amp;quot;value error: invalid literal for int() with base 10:&amp;#39;&amp;#39; &amp;quot;&lt;/p&gt; &lt;p&gt;I used the warehouse_id and not cluster_id. Please helpppp&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Longjumping_Ad_7053&quot;&gt; /u/Longjumping_Ad_7053 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1edr19y/im_trying_to_connect_databricks_table_to_langchain/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1edr19y/im_trying_to_connect_databricks_table_to_langchain/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1edr19y</id><link href="https://www.reddit.com/r/LangChain/comments/1edr19y/im_trying_to_connect_databricks_table_to_langchain/" /><updated>2024-07-27T21:23:53+00:00</updated><published>2024-07-27T21:23:53+00:00</published><title>I’m trying to connect databricks table to langchain</title></entry><entry><author><name>/u/random_name_03</name><uri>https://www.reddit.com/user/random_name_03</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey guys,&lt;/p&gt; &lt;p&gt;Recently I have familiarized myself with Langchain quite a bit. I was hoping to make a basic web-app, for a class of less than 100 people, as a personal project. &lt;/p&gt; &lt;p&gt;Unfortunately, while I do know JavaScript, Python, CSS, and HTML. I am unfamiliar with the structure behind such an application. Until now I have built some local CLI applications while using APIs (DeepSeek is crazy cheap and nice but might need an alternative because they also use API accessed data).&lt;/p&gt; &lt;p&gt;Where can I host a website, or a local easier llm like llama with it, or any options I can use with Langchain chain APIs? Also should I host an open-source LLM somewhere or depend on APIs?&lt;/p&gt; &lt;p&gt;There is just so much information that I cannot find anything concrete over it, please help :&amp;quot;)&lt;/p&gt; &lt;p&gt;Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/random_name_03&quot;&gt; /u/random_name_03 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ed8seg/using_langchain_for_a_smallscale_webapp/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ed8seg/using_langchain_for_a_smallscale_webapp/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1ed8seg</id><link href="https://www.reddit.com/r/LangChain/comments/1ed8seg/using_langchain_for_a_smallscale_webapp/" /><updated>2024-07-27T05:22:45+00:00</updated><published>2024-07-27T05:22:45+00:00</published><title>Using Langchain for a small-scale web-app</title></entry><entry><author><name>/u/UnderstandLingAI</name><uri>https://www.reddit.com/user/UnderstandLingAI</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ecw305/rag_provenance_computation/&quot;&gt; &lt;img src=&quot;https://b.thumbs.redditmedia.com/pX7nNYffb4qu9TyoIOylYhkzp_M3654eqi6vN3p4GQE.jpg&quot; alt=&quot;RAG provenance computation&quot; title=&quot;RAG provenance computation&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;&lt;a href=&quot;https://preview.redd.it/4ld6chb9vwed1.png?width=1052&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fe05dfd00ac3b7e109b70ba612a003e096a9fa63&quot;&gt;https://preview.redd.it/4ld6chb9vwed1.png?width=1052&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fe05dfd00ac3b7e109b70ba612a003e096a9fa63&lt;/a&gt;&lt;/p&gt; &lt;p&gt;When performing RAG with any LLM, it&amp;#39;s pretty tricky to decide how much of the answer given by the LLM was actually due to a specific document that was fed into it. Did the LLM pay attention to 1 document? 3? or perhaps none and just the query?&lt;/p&gt; &lt;p&gt;This provenance attribution is a difficult challenge in RAG and has not yet been properly solved. That being said, we have added quite a few ways to compute provenance to our framework RAG Me Up - &lt;a href=&quot;https://github.com/AI-Commandos/RAGMeUp&quot;&gt;https://github.com/AI-Commandos/RAGMeUp&lt;/a&gt;&lt;/p&gt; &lt;p&gt;One of the coolest and most accurate is when you are using local/open source LLMs, we can actually retrieve the attention scores from the model and use those to compute provenance:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;How much attention was paid from the given answer to every document separately that was retrieved from the database? Vice versa from every document to the answer?&lt;/li&gt; &lt;li&gt;How much attention was paid from the query to every document and vice versa?&lt;/li&gt; &lt;li&gt;How much attention was paid from the answer to the query, disregarding the documents... and vice versa.&lt;/li&gt; &lt;li&gt;And finally from the query to itself and from the answer to itself (remember that LLMs are autoregressive!)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;If we then take the attention paid from/to a single document and divide that by the sum of the other attentions mentioned - we get a good idea of the relative importance of each document fed into the LLM!&lt;/p&gt; &lt;p&gt;In RAG Me Up this is now implemented as a provenance method &amp;quot;attention&amp;quot; and you can use it with any OS LLM on any given dataset.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/UnderstandLingAI&quot;&gt; /u/UnderstandLingAI &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ecw305/rag_provenance_computation/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ecw305/rag_provenance_computation/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1ecw305</id><media:thumbnail url="https://b.thumbs.redditmedia.com/pX7nNYffb4qu9TyoIOylYhkzp_M3654eqi6vN3p4GQE.jpg" /><link href="https://www.reddit.com/r/LangChain/comments/1ecw305/rag_provenance_computation/" /><updated>2024-07-26T19:02:16+00:00</updated><published>2024-07-26T19:02:16+00:00</published><title>RAG provenance computation</title></entry><entry><author><name>/u/underrated-squirrel</name><uri>https://www.reddit.com/user/underrated-squirrel</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ecuq2g/askitright_pdfs_query_fullstack_application_rag/&quot;&gt; &lt;img src=&quot;https://external-preview.redd.it/ZJCf56Nh_Pt7yRbmmvCFkShWeJMBCwhbny8msEM-w_4.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6e109fb1c6c0d6d93450dc68e1c2d6c9933b8b26&quot; alt=&quot;AskItRight: PDFs Query Fullstack Application (RAG Ollama, LangChain, ChromaDB )&quot; title=&quot;AskItRight: PDFs Query Fullstack Application (RAG Ollama, LangChain, ChromaDB )&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hello Reddit!&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/2l247zaekwed1.png?width=250&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4365916258171dffcfe820e9ad138cf1e847078d&quot;&gt;https://preview.redd.it/2l247zaekwed1.png?width=250&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4365916258171dffcfe820e9ad138cf1e847078d&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I’ve been tinkering with some libraries and just finished building an app I’m so excited to share with you all. I’d love for you to check it out and let me know what you think!&lt;/p&gt; &lt;p&gt;🔗 GitHub Repository: &lt;a href=&quot;https://github.com/AbdArdati/PDFQueryAI&quot;&gt;https://github.com/AbdArdati/PDFQueryAI&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Key Features 🔑&lt;/h1&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;PDF Management&lt;/strong&gt;: &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Upload PDFs&lt;/strong&gt;: 📤 Users can upload PDF files through the upload interface. These files are processed and stored in the system.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;List PDFs&lt;/strong&gt;: 📋 Users can view a list of all uploaded PDF files through the available PDFs interface.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Delete PDFs&lt;/strong&gt;: 🗑️ Users can remove specific PDF files using the delete functionality available in the PDF management interface.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;View PDFs&lt;/strong&gt;: 👁️ Users can open and view the content of PDF files in a new browser tab directly from the list of PDFs.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Query Handling&lt;/strong&gt;: &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Ask Questions to PDF&lt;/strong&gt;: 🤔 Users can submit questions about the content of uploaded PDFs using the query interface. The application uses the AI model to provide answers based on the PDF contents.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;AI Integration&lt;/strong&gt;: 🤖 The l**lama3.1 model **is used to generate answers to queries from the content of the PDFs. This functionality is accessible through the AI query interface.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Prompt Templates&lt;/strong&gt;: 📝 Users can view and select from various prompt templates to guide the AI&amp;#39;s responses, ensuring they are tailored to specific needs. (Currently in progress, with frontend Create, Update, and Delete to be implemented.)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Statistics and Administration&lt;/strong&gt;: &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Clear Chat History&lt;/strong&gt;: 🧹 Users can clear previous chat interactions using the clear chat history button in the query section.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Clear Database&lt;/strong&gt;: 🚮 Deletes all stored PDFs and related data, effectively resetting the application’s state. This action is available in the database management section.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;PDF Usage Statistics&lt;/strong&gt;: 📈 Provides information on how frequently each PDF has been queried, viewable through the statistics dashboard.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;This example demonstrated below is based on the &amp;#39;Essays Expert&amp;#39; prompt template. The screenshot highlights how the system utilises PDF content to generate comprehensive responses at the top, while the lower section shows the output generated without PDFs, illustrating the impact of including detailed content.&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/2hta2yrgkwed1.png?width=2880&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=91ecb0a7056034b6fd1a57b19311e3f9a8703add&quot;&gt;https://preview.redd.it/2hta2yrgkwed1.png?width=2880&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=91ecb0a7056034b6fd1a57b19311e3f9a8703add&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I’m not an expert in this domain—just a big fan of its potential who’s been reading up on it. All feedback is welcome:)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/underrated-squirrel&quot;&gt; /u/underrated-squirrel &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ecuq2g/askitright_pdfs_query_fullstack_application_rag/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ecuq2g/askitright_pdfs_query_fullstack_application_rag/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1ecuq2g</id><media:thumbnail url="https://external-preview.redd.it/ZJCf56Nh_Pt7yRbmmvCFkShWeJMBCwhbny8msEM-w_4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=6e109fb1c6c0d6d93450dc68e1c2d6c9933b8b26" /><link href="https://www.reddit.com/r/LangChain/comments/1ecuq2g/askitright_pdfs_query_fullstack_application_rag/" /><updated>2024-07-26T18:04:37+00:00</updated><published>2024-07-26T18:04:37+00:00</published><title>AskItRight: PDFs Query Fullstack Application (RAG Ollama, LangChain, ChromaDB )</title></entry><entry><author><name>/u/Shingma</name><uri>https://www.reddit.com/user/Shingma</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;m playing around with Crew, Autogen and LangChain wanted to know what tools are best suited for Multi Agent applications, where to find them or if I need to build them myself. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Shingma&quot;&gt; /u/Shingma &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ecn0fq/what_tools_are_you_using_with_your_ai_agents/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ecn0fq/what_tools_are_you_using_with_your_ai_agents/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1ecn0fq</id><link href="https://www.reddit.com/r/LangChain/comments/1ecn0fq/what_tools_are_you_using_with_your_ai_agents/" /><updated>2024-07-26T12:36:31+00:00</updated><published>2024-07-26T12:36:31+00:00</published><title>What tools are you using with your AI agents?</title></entry><entry><author><name>/u/captam_morgan</name><uri>https://www.reddit.com/user/captam_morgan</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi all, &lt;/p&gt; &lt;p&gt;I’m trying to design a the graph workflow with LangGraph/LangChain. In building CRAG, what’s a good way to integrate sequential/parallel decomposition and self-query in the nodes and conditional edges. &lt;/p&gt; &lt;p&gt;For examples, let’s say there’s a question that can be broken into sequential sub-queries: &amp;quot;What is the weather of the Big Apple?&amp;quot; would first look up &amp;quot;What is the Big Apple&amp;quot; then do &amp;quot;Weather in NYC&amp;quot;.&lt;/p&gt; &lt;p&gt;I guess I can view this more like ReAct function calling: &lt;/p&gt; &lt;p&gt;Start &amp;gt; Planner &amp;gt; self-query &amp;gt; retriever &amp;gt; docs &amp;gt; grade &amp;gt; Planner &amp;gt; generate-end/self-query&lt;/p&gt; &lt;p&gt;Planner node will do both tool calling of RAG and also query decomposition/optimization/CoT.&lt;/p&gt; &lt;p&gt;Appreciate any help!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/captam_morgan&quot;&gt; /u/captam_morgan &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ecwyuk/langgraph_rag_w_cot_sequential_decomposition/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ecwyuk/langgraph_rag_w_cot_sequential_decomposition/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1ecwyuk</id><link href="https://www.reddit.com/r/LangChain/comments/1ecwyuk/langgraph_rag_w_cot_sequential_decomposition/" /><updated>2024-07-26T19:40:40+00:00</updated><published>2024-07-26T19:40:40+00:00</published><title>LangGraph RAG w/ CoT Sequential Decomposition</title></entry><entry><author><name>/u/Ok_Opinion_5729</name><uri>https://www.reddit.com/user/Ok_Opinion_5729</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Please point to some reference of using RAGAS with local models&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Ok_Opinion_5729&quot;&gt; /u/Ok_Opinion_5729 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ed9rhu/how_can_i_use_ragas_without_openai_key/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ed9rhu/how_can_i_use_ragas_without_openai_key/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1ed9rhu</id><link href="https://www.reddit.com/r/LangChain/comments/1ed9rhu/how_can_i_use_ragas_without_openai_key/" /><updated>2024-07-27T06:26:26+00:00</updated><published>2024-07-27T06:26:26+00:00</published><title>How can I use RAGAS without OpenAI key?</title></entry><entry><author><name>/u/Either-Ambassador738</name><uri>https://www.reddit.com/user/Either-Ambassador738</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;ve seen in lots of tutorials from the langchain/langraph documentation using bind_tools and bind_functions.&lt;br/&gt; But I haven&amp;#39;t yet understood the difference between them and where to use each one.&lt;br/&gt; Also, do they both work with all chat models?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Either-Ambassador738&quot;&gt; /u/Either-Ambassador738 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ed6cmm/whats_the_difference_between_bind_tools_and_bind/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ed6cmm/whats_the_difference_between_bind_tools_and_bind/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1ed6cmm</id><link href="https://www.reddit.com/r/LangChain/comments/1ed6cmm/whats_the_difference_between_bind_tools_and_bind/" /><updated>2024-07-27T03:02:13+00:00</updated><published>2024-07-27T03:02:13+00:00</published><title>What's the difference between bind_tools and bind_functions?</title></entry><entry><author><name>/u/phicreative1997</name><uri>https://www.reddit.com/user/phicreative1997</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ecp82w/building_a_human_resource_graphrag_application/&quot;&gt; &lt;img src=&quot;https://external-preview.redd.it/MM3_iXjJ5f9zLbFgI_OqW5HbRDQZypY4ocXngeaUmpI.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bf25726624e218fbb8aed206f16571b9216ef7c4&quot; alt=&quot;Building a Human Resource GraphRAG application&quot; title=&quot;Building a Human Resource GraphRAG application&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/phicreative1997&quot;&gt; /u/phicreative1997 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://medium.com/firebird-technologies/building-a-human-resource-graphrag-application-279f07cf71d6&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ecp82w/building_a_human_resource_graphrag_application/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1ecp82w</id><media:thumbnail url="https://external-preview.redd.it/MM3_iXjJ5f9zLbFgI_OqW5HbRDQZypY4ocXngeaUmpI.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=bf25726624e218fbb8aed206f16571b9216ef7c4" /><link href="https://www.reddit.com/r/LangChain/comments/1ecp82w/building_a_human_resource_graphrag_application/" /><updated>2024-07-26T14:17:47+00:00</updated><published>2024-07-26T14:17:47+00:00</published><title>Building a Human Resource GraphRAG application</title></entry><entry><author><name>/u/SaltShakerOW</name><uri>https://www.reddit.com/user/SaltShakerOW</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey guys,&lt;/p&gt; &lt;p&gt;I&amp;#39;m doing a little project to familiarize myself with langchain and gen AI as a whole, and the tldr is that I&amp;#39;m trying to create an agent that has the capability to call tools and put the result into a complete sentence at the end for the user to interpret. This seems pretty simple, but I&amp;#39;ve never come across a solution that can do both after hours of google searching (maybe I&amp;#39;m just looking in the wrong places lol). Regardless, I&amp;#39;m currently using the create_tool_calling_agent method from the langchain.agents python library, and it works great for doing the complete sentence part, but it&amp;#39;s not actually calling the tool that I have set to call and is coming up with a nonsensical answer to boot. Here is an example of what I&amp;#39;m talking about and the code I have setup:&lt;/p&gt; &lt;p&gt;The tool:&lt;/p&gt; &lt;pre&gt;&lt;code&gt; u/tool def get_weather(self, location=None) -&amp;gt; int: &amp;quot;Gets the temperature at a chosen location AND ONLY THE TEMPERATURE. The location argument is optional, and if left blank it will retrieve the user&amp;#39;s current location.&amp;quot; print(&amp;quot;get_weather called&amp;quot;) #DEBUGGING if location == None: temp = self.get_location_data() location = temp[&amp;quot;city&amp;quot;] complete_url = &amp;quot;http://api.openweathermap.org/data/2.5/weather?&amp;quot; + &amp;quot;appid=&amp;quot; + os.getenv(&amp;quot;OPENWEATHERMAP_API_KEY&amp;quot;) + &amp;quot;&amp;amp;q=&amp;quot; + location response = requests.get(complete_url) x = response.json() print(x) try: current_temperature = x[&amp;quot;main&amp;quot;][&amp;quot;temp&amp;quot;] except: return -1 return int((current_temperature - 273.15)*1.8 + 32) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The agent/LLM setup:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;class Agent: def __init__(self) -&amp;gt; None: t = Custom_Tool() self.model = ChatOllama(model=&amp;quot;llama3.1:8b&amp;quot;, temperature=0) self.prompt = ChatPromptTemplate.from_messages( [ (&amp;quot;system&amp;quot;, system_prompt), (&amp;quot;placeholder&amp;quot;, &amp;quot;{chat_history}&amp;quot;), (&amp;quot;human&amp;quot;, &amp;quot;{input}&amp;quot;), (&amp;quot;placeholder&amp;quot;, &amp;quot;{agent_scratchpad}&amp;quot;), ] ) self.tools = [t.get_weather] def invoke_agent(self, prompt: str): agent = create_tool_calling_agent(self.model, self.tools, self.prompt) agent_executor = AgentExecutor(agent=agent, tools=self.tools, verbose=True) agent_executor.invoke({&amp;quot;input&amp;quot;: prompt}) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The prompt:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;def run(prompt): a = Agent() a.invoke_agent(prompt=prompt) if __name__ == &amp;quot;__main__&amp;quot;: run(&amp;quot;What&amp;#39;s the weather in Minneapolis?&amp;quot;) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The result (it is not 42 degrees in Minneapolis in the middle of July):&lt;/p&gt; &lt;pre&gt;&lt;code&gt;&amp;gt; Entering new AgentExecutor chain... I&amp;#39;d be happy to check the current temperature in Minneapolis for you. Let me just make an API call using our `get_weather` tool... The current temperature in Minneapolis is 42°F. Would you like to know more about the weather conditions or forecast? &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;If anyone has a better way to do this or advice for me to get going in the right direction please let me know. Thanks.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/SaltShakerOW&quot;&gt; /u/SaltShakerOW &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ecvnfk/tool_calling_agent_not_actually_calling_tools/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ecvnfk/tool_calling_agent_not_actually_calling_tools/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1ecvnfk</id><link href="https://www.reddit.com/r/LangChain/comments/1ecvnfk/tool_calling_agent_not_actually_calling_tools/" /><updated>2024-07-26T18:43:50+00:00</updated><published>2024-07-26T18:43:50+00:00</published><title>Tool Calling Agent not actually calling tools</title></entry><entry><author><name>/u/Longjumping-Buddy501</name><uri>https://www.reddit.com/user/Longjumping-Buddy501</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ecoex7/gpt4omini_is_terribly_slow_today_anyone_else/&quot;&gt; &lt;img src=&quot;https://b.thumbs.redditmedia.com/9Cgucf2SuD7c0iqUpWblJ0VTLYnsAKdl8qtg9yblXio.jpg&quot; alt=&quot;GPT-4o-mini is terribly slow today. Anyone else facing this issue?&quot; title=&quot;GPT-4o-mini is terribly slow today. Anyone else facing this issue?&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;&lt;a href=&quot;https://preview.redd.it/hmxu408raved1.png?width=170&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5bbc2d537d3c183ab7e684f0457754f38a967083&quot;&gt;https://preview.redd.it/hmxu408raved1.png?width=170&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5bbc2d537d3c183ab7e684f0457754f38a967083&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/k0xf4x7raved1.png?width=170&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=725eb06885156a134e53e45e928089ffa8e5a97f&quot;&gt;https://preview.redd.it/k0xf4x7raved1.png?width=170&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=725eb06885156a134e53e45e928089ffa8e5a97f&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The latency on GPT-4o-mini is terrible today. It is taking 96 seconds and above for simple answers &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Longjumping-Buddy501&quot;&gt; /u/Longjumping-Buddy501 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ecoex7/gpt4omini_is_terribly_slow_today_anyone_else/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ecoex7/gpt4omini_is_terribly_slow_today_anyone_else/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1ecoex7</id><media:thumbnail url="https://b.thumbs.redditmedia.com/9Cgucf2SuD7c0iqUpWblJ0VTLYnsAKdl8qtg9yblXio.jpg" /><link href="https://www.reddit.com/r/LangChain/comments/1ecoex7/gpt4omini_is_terribly_slow_today_anyone_else/" /><updated>2024-07-26T13:42:47+00:00</updated><published>2024-07-26T13:42:47+00:00</published><title>GPT-4o-mini is terribly slow today. Anyone else facing this issue?</title></entry><entry><author><name>/u/remmmm_</name><uri>https://www.reddit.com/user/remmmm_</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Update: I got my solution! I can’t wait to share my project after I finish.&lt;/p&gt; &lt;p&gt;In my personal project, I originally plan to use rag fusion to match resumes and job descriptions (preprocessed by summarizing with llm). Right now I&amp;#39;m stuck on how to overcome limitation of context awareness in embeddings and semantic/similarity searches. For example, in query generation, one of query is focused on experience. ideally I&amp;#39;d like to produce an assessment like below&lt;/p&gt; &lt;p&gt;&amp;quot;Assessment&amp;quot;: &amp;quot;Limited experience but shows potential. Candidate has some exposure to relevant areas like recommendation systems and customer segmentation, but lacks the depth of experience required for the role.&amp;quot;&lt;/p&gt; &lt;p&gt;&amp;quot;responsibilities in Job Description&amp;quot;: &amp;quot;Experience creating and implementing machine learning techniques for recommender systems and time series analysis. Experience with customer segmentation, churn prediction, campaign optimization, and more.&amp;quot;&lt;/p&gt; &lt;p&gt;&amp;quot;experience section in Resume&amp;quot;: &amp;quot;Developed basic machine learning models for customer segmentation and product recommendation at XYZ Retail. Assisted in building a prototype recommendation engine during internship at StartupX.&amp;quot;&lt;br/&gt; My implementation of fusion rag will identify similar keywords, but fail to recognize the difference in depth and breadth of experience, and unable to infer and interpret. Any ideas on how to address this limitation of context awareness? Thanks in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/remmmm_&quot;&gt; /u/remmmm_ &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ecuavd/seeking_ideas_on_how_to_overcome_limitation_of/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ecuavd/seeking_ideas_on_how_to_overcome_limitation_of/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1ecuavd</id><link href="https://www.reddit.com/r/LangChain/comments/1ecuavd/seeking_ideas_on_how_to_overcome_limitation_of/" /><updated>2024-07-26T17:47:06+00:00</updated><published>2024-07-26T17:47:06+00:00</published><title>seeking ideas on how to overcome limitation of context awareness in similarity search and fusion rag</title></entry><entry><author><name>/u/Jorge_at_Startino</name><uri>https://www.reddit.com/user/Jorge_at_Startino</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;have you guys seen any trends or evidence that could potentially show a turn for TypeScript?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Jorge_at_Startino&quot;&gt; /u/Jorge_at_Startino &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ed9kiw/will_ts_ever_surpass_python_for_generative_ai/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ed9kiw/will_ts_ever_surpass_python_for_generative_ai/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1ed9kiw</id><link href="https://www.reddit.com/r/LangChain/comments/1ed9kiw/will_ts_ever_surpass_python_for_generative_ai/" /><updated>2024-07-27T06:13:44+00:00</updated><published>2024-07-27T06:13:44+00:00</published><title>will TS ever surpass Python for generative AI development?</title></entry><entry><author><name>/u/OnY86</name><uri>https://www.reddit.com/user/OnY86</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi, i am new to langchain and i hope somebody here can help me understand some basics.&lt;/p&gt; &lt;p&gt;I have a server serving the model via vllm openai endpoint. The model uses a ChatML template:&lt;/p&gt; &lt;p&gt;&amp;lt;|im_start|&amp;gt;system {system_message}&amp;lt;|im_end|&amp;gt; &amp;lt;|im_start|&amp;gt;user {prompt}&amp;lt;|im_end|&amp;gt; &amp;lt;|im_start|&amp;gt;assistant&lt;/p&gt; &lt;p&gt;Can langchain handle this and if so, how? Or did i combine the wrong stuff together?&lt;/p&gt; &lt;p&gt;I am asking, because my LLM is responding sometimes in a strange way. For example, i asked „how far is the moon“ and i get an endless response back. The first sentence is the right answer but all that follows after, it is total nonsense.&lt;/p&gt; &lt;p&gt;Thanks&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/OnY86&quot;&gt; /u/OnY86 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ectcut/vllm_dolphin26mistral_langchain/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ectcut/vllm_dolphin26mistral_langchain/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1ectcut</id><link href="https://www.reddit.com/r/LangChain/comments/1ectcut/vllm_dolphin26mistral_langchain/" /><updated>2024-07-26T17:08:06+00:00</updated><published>2024-07-26T17:08:06+00:00</published><title>Vllm + dolphin-2.6-mistral + langchain</title></entry><entry><author><name>/u/Substantial_Gift_861</name><uri>https://www.reddit.com/user/Substantial_Gift_861</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I plan to build a chatbot to answer those product information. But I don&amp;#39;t know which one to use, RAG or openai gpt?&lt;/p&gt; &lt;p&gt;I heard that RAG might not accurate and cant generate reply very well&lt;/p&gt; &lt;p&gt;Which one will you choose? &lt;/p&gt; &lt;p&gt;If you want to build a chatbot like that, what will you use?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Substantial_Gift_861&quot;&gt; /u/Substantial_Gift_861 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1eck671/build_a_chatbot_by_using_rag_or_openai_gpt_api/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1eck671/build_a_chatbot_by_using_rag_or_openai_gpt_api/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1eck671</id><link href="https://www.reddit.com/r/LangChain/comments/1eck671/build_a_chatbot_by_using_rag_or_openai_gpt_api/" /><updated>2024-07-26T09:51:53+00:00</updated><published>2024-07-26T09:51:53+00:00</published><title>Build a chatbot by using Rag or openai gpt API?</title></entry></feed>