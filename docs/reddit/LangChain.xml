<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/"><category term="LangChain" label="r/LangChain"/><updated>2024-07-11T03:29:32+00:00</updated><icon>https://www.redditstatic.com/icon.png/</icon><id>/r/LangChain.rss</id><link rel="self" href="https://www.reddit.com/r/LangChain.rss" type="application/atom+xml" /><link rel="alternate" href="https://www.reddit.com/r/LangChain" type="text/html" /><subtitle>LangChain is an open-source framework and developer toolkit that helps developers get LLM applications from prototype to production. It is available for Python and Javascript at https://www.langchain.com/.</subtitle><title>LangChain</title><entry><author><name>/u/zchaarm</name><uri>https://www.reddit.com/user/zchaarm</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;A place for members of &lt;a href=&quot;/r/LangChain&quot;&gt;r/LangChain&lt;/a&gt; to chat with each other&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/zchaarm&quot;&gt; /u/zchaarm &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_10ljho9</id><link href="https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/" /><updated>2023-01-26T04:33:05+00:00</updated><published>2023-01-26T04:33:05+00:00</published><title>r/LangChain Lounge</title></entry><entry><author><name>/u/goddamnit_1</name><uri>https://www.reddit.com/user/goddamnit_1</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e03z0y/i_used_langchain_to_build_a_slack_agent_my/&quot;&gt; &lt;img src=&quot;https://external-preview.redd.it/u5rxHXqj1K-44fCKJCxS7ivB42knsXLiW4mFDtHvbdQ.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f4921d56599664bdd7e92e20b29f17d8100f8070&quot; alt=&quot;I used Langchain to build a Slack Agent - My Experience&quot; title=&quot;I used Langchain to build a Slack Agent - My Experience&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;My AI Agent does the following:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Instant answers from the web in any Slack channel&lt;/li&gt; &lt;li&gt;Code interpretation &amp;amp; execution on the fly&lt;/li&gt; &lt;li&gt;Smart web crawling for up-to-date info&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;project link : &lt;a href=&quot;http://git.new/slack-bot-agent-ollama&quot;&gt;git.new/slack-bot-agent-ollama&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;My experience with Langchain&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;One of the key advantages of Langchain is its ability to integrate different LLMs into your applications. This flexibility allows you to experiment with various models and find the one that best suits your needs.&lt;/p&gt; &lt;p&gt;Langchain&amp;#39;s approach is a game-changer. However, I do have one gripe - the documentation could be better. I wasn&amp;#39;t aware that I needed to use the ChatModels instead of the direct models, and this wasn&amp;#39;t specified clearly enough. This kind of information is crucial for users to get up and running quickly.&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/jjtezql9nqbd1.png?width=1283&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5707c453e8a1f1bc0f756b6821780603065dd4fb&quot;&gt;https://preview.redd.it/jjtezql9nqbd1.png?width=1283&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5707c453e8a1f1bc0f756b6821780603065dd4fb&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/goddamnit_1&quot;&gt; /u/goddamnit_1 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e03z0y/i_used_langchain_to_build_a_slack_agent_my/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e03z0y/i_used_langchain_to_build_a_slack_agent_my/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1e03z0y</id><media:thumbnail url="https://external-preview.redd.it/u5rxHXqj1K-44fCKJCxS7ivB42knsXLiW4mFDtHvbdQ.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=f4921d56599664bdd7e92e20b29f17d8100f8070" /><link href="https://www.reddit.com/r/LangChain/comments/1e03z0y/i_used_langchain_to_build_a_slack_agent_my/" /><updated>2024-07-10T19:03:37+00:00</updated><published>2024-07-10T19:03:37+00:00</published><title>I used Langchain to build a Slack Agent - My Experience</title></entry><entry><author><name>/u/md1630</name><uri>https://www.reddit.com/user/md1630</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;m trying to create a streaming agent chatbot with streamlit as the frontend. I was able to find an example of this using callbacks, and streamlit even has a special callback class.&lt;/p&gt; &lt;p&gt;However, it looks like things sure change quickly. From langchain&amp;#39;s documentation it looks like callbacks is being deprecated, and there is a new function astream_events. I&amp;#39;m very happy with how simple it is to stream events with astream_events.&lt;/p&gt; &lt;p&gt;However, I&amp;#39;m having some issues with getting this to work with streamlit. It&amp;#39;s mostly working, but for some small details. For example:&lt;/p&gt; &lt;pre&gt;&lt;code&gt; async for event in agent_executor.astream_events( {&amp;quot;input&amp;quot;: user_query}, version=&amp;quot;v2&amp;quot;, config=cfg ): kind = event[&amp;quot;event&amp;quot;] if kind == &amp;quot;on_chat_model_stream&amp;quot;: content = event[&amp;quot;data&amp;quot;][&amp;quot;chunk&amp;quot;].content if content: answer_container.write(content) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;There are other events, but I can illustrate this problem with just &amp;quot;on_chat_model_stream&amp;quot;. &lt;/p&gt; &lt;p&gt;Just focusing on the event &amp;quot;on_chat_model_stream&amp;quot;, this causes the content to be written one word in every line, so the chat message looks like:&lt;/p&gt; &lt;p&gt;Hello&lt;/p&gt; &lt;p&gt;how&lt;/p&gt; &lt;p&gt;may&lt;/p&gt; &lt;p&gt;I&lt;/p&gt; &lt;p&gt;help&lt;/p&gt; &lt;p&gt;you&lt;/p&gt; &lt;p&gt;So, it looks like the content is being written token by token. However, I can&amp;#39;t use write_stream, because I would get the error ``st.write_stream` expects a generator or stream-like object as input not &amp;lt;class &amp;#39;str&amp;#39;&amp;gt;. Please use `st.write` instead for this data type.`&lt;/p&gt; &lt;p&gt;How do I get the content to stream in this case?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/md1630&quot;&gt; /u/md1630 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e0ezhj/how_to_use_agentexecutorstream_events_with/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e0ezhj/how_to_use_agentexecutorstream_events_with/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1e0ezhj</id><link href="https://www.reddit.com/r/LangChain/comments/1e0ezhj/how_to_use_agentexecutorstream_events_with/" /><updated>2024-07-11T03:16:34+00:00</updated><published>2024-07-11T03:16:34+00:00</published><title>how to use agentexecutor.stream_events with streamlit</title></entry><entry><author><name>/u/BellaHi</name><uri>https://www.reddit.com/user/BellaHi</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e0dp2l/from_concept_to_execution_crafting_cuttingedge_ai/&quot;&gt; &lt;img src=&quot;https://external-preview.redd.it/mrFcgMwx0I5hbiBHVY0Y7RFxYzTqpONsp1DNMaJb2bo.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bbd4ab5ae934c2107af3adc3067699bb73f680f8&quot; alt=&quot;From Concept to Execution: Crafting Cutting-Edge AI Agents with LangChain&quot; title=&quot;From Concept to Execution: Crafting Cutting-Edge AI Agents with LangChain&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/BellaHi&quot;&gt; /u/BellaHi &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://myscale.com/blog/build-ai-agent-with-langchain/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e0dp2l/from_concept_to_execution_crafting_cuttingedge_ai/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1e0dp2l</id><media:thumbnail url="https://external-preview.redd.it/mrFcgMwx0I5hbiBHVY0Y7RFxYzTqpONsp1DNMaJb2bo.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=bbd4ab5ae934c2107af3adc3067699bb73f680f8" /><link href="https://www.reddit.com/r/LangChain/comments/1e0dp2l/from_concept_to_execution_crafting_cuttingedge_ai/" /><updated>2024-07-11T02:09:43+00:00</updated><published>2024-07-11T02:09:43+00:00</published><title>From Concept to Execution: Crafting Cutting-Edge AI Agents with LangChain</title></entry><entry><author><name>/u/stoic-AI</name><uri>https://www.reddit.com/user/stoic-AI</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi everyone!&lt;/p&gt; &lt;p&gt;I&amp;#39;ve created a mini series on how to build a real time AI application using Django, LangChain and Celery.&lt;/p&gt; &lt;p&gt;Free knowledge - posting it in here for anyone working on something similar and had the same blockers I had when building.&lt;/p&gt; &lt;p&gt;Let me know what you think on how I could potentially improve this architecture.&lt;/p&gt; &lt;p&gt;Part 1&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://medium.com/towardsdev/how-to-set-up-django-from-scratch-with-celery-channels-redis-docker-real-time-django-601dff7ada79&quot;&gt;https://medium.com/towardsdev/how-to-set-up-django-from-scratch-with-celery-channels-redis-docker-real-time-django-601dff7ada79&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Part 2&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://medium.com/towardsdev/how-to-set-up-django-from-scratch-with-celery-channels-redis-docker-real-time-django-5828a1ea43a3&quot;&gt;https://medium.com/towardsdev/how-to-set-up-django-from-scratch-with-celery-channels-redis-docker-real-time-django-5828a1ea43a3&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Part 3&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://medium.com/towardsdev/how-to-set-up-django-from-scratch-with-celery-channels-redis-docker-real-time-django-5828a1ea43a3&quot;&gt;https://medium.com/towardsdev/how-to-set-up-django-from-scratch-with-celery-channels-redis-docker-real-time-django-5828a1ea43a3&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Part 4&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://medium.com/towardsdev/how-to-set-up-django-from-scratch-with-celery-channels-redis-docker-real-time-django-8e73c7b6b4c8&quot;&gt;https://medium.com/towardsdev/how-to-set-up-django-from-scratch-with-celery-channels-redis-docker-real-time-django-8e73c7b6b4c8&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/stoic-AI&quot;&gt; /u/stoic-AI &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dzs2p4/real_time_ai_workers_using_django_x_langchain/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dzs2p4/real_time_ai_workers_using_django_x_langchain/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dzs2p4</id><link href="https://www.reddit.com/r/LangChain/comments/1dzs2p4/real_time_ai_workers_using_django_x_langchain/" /><updated>2024-07-10T10:07:51+00:00</updated><published>2024-07-10T10:07:51+00:00</published><title>Real Time AI Workers using Django x LangChain</title></entry><entry><author><name>/u/Pure-Exercise-9955</name><uri>https://www.reddit.com/user/Pure-Exercise-9955</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hello everyone,&lt;/p&gt; &lt;p&gt;I have to build a Retrieval-Augmented Generation (RAG) system using a Large Language Model (LLM) &lt;strong&gt;to search for an information about an employee within a vast collection of documents.&lt;/strong&gt; The LLM must return the information with references indicating the specific page and document.&lt;/p&gt; &lt;p&gt;if someone already done a project similar or can help me with the steps.&lt;/p&gt; &lt;p&gt;what i decided to do is to select an open source llm (qwen2), then fine tune it, after that build a RAG&lt;/p&gt; &lt;p&gt;&lt;strong&gt;what is your opinion gays&lt;/strong&gt;&lt;br/&gt; &lt;strong&gt;i need your help&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Pure-Exercise-9955&quot;&gt; /u/Pure-Exercise-9955 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e02kx2/rag_to_search_for_an_information_about_an/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e02kx2/rag_to_search_for_an_information_about_an/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1e02kx2</id><link href="https://www.reddit.com/r/LangChain/comments/1e02kx2/rag_to_search_for_an_information_about_an/" /><updated>2024-07-10T18:07:58+00:00</updated><published>2024-07-10T18:07:58+00:00</published><title>RAG to search for an information about an employee within a vast collection of documents</title></entry><entry><author><name>/u/neilkatz</name><uri>https://www.reddit.com/user/neilkatz</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey All,&lt;/p&gt; &lt;p&gt;I&amp;#39;m curious what everyone is using to parse complex PDFs, extract the data and turn it into something LLMs can better comprehend.&lt;/p&gt; &lt;p&gt;Is there something that can consistently find tables, forms, charts, graphics that we see in many enterprise documents. It seems without this step, RAG hallucinations are a significant issue. &lt;/p&gt; &lt;p&gt;Much appreciated. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/neilkatz&quot;&gt; /u/neilkatz &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dzj5qx/best_pdf_parser_for_rag/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dzj5qx/best_pdf_parser_for_rag/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dzj5qx</id><link href="https://www.reddit.com/r/LangChain/comments/1dzj5qx/best_pdf_parser_for_rag/" /><updated>2024-07-10T01:13:19+00:00</updated><published>2024-07-10T01:13:19+00:00</published><title>Best PDF Parser for RAG?</title></entry><entry><author><name>/u/Typical-Scene-5794</name><uri>https://www.reddit.com/user/Typical-Scene-5794</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi &lt;a href=&quot;/r/langchain&quot;&gt;r/langchain&lt;/a&gt;, I&amp;#39;m sharing an example on building a multi-modal search application using GPT-4o, featuring extraction of metadata and hybrid indexing for accurately retrieving relevant information from presentations.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Repo: &lt;a href=&quot;https://github.com/pathwaycom/llm-app/tree/main/examples/pipelines/slides_ai_search&quot;&gt;~https://github.com/pathwaycom/llm-app/tree/main/examples/pipelines/slides_ai_search~&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Architecture: &lt;a href=&quot;https://github.com/pathwaycom/llm-app/tree/main/examples/pipelines/slides_ai_search#architecture&quot;&gt;~https://github.com/pathwaycom/llm-app/tree/main/examples/pipelines/slides_ai_search#architecture~&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This project also focuses on automatically updating indexes as changes happen in your repository. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Quick details:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Ingestion:&lt;/strong&gt; The application reads slide files (PPTX and PDF) stored locally or on Google Drive or Microsoft SharePoint.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Parsing:&lt;/strong&gt; Utilizes the SlideParser from Pathway, configured with a detailed schema. The app parses images, charts, diagrams, and other visual elements as well, and features automatic unstructured metadata extraction. &lt;/li&gt; &lt;li&gt;&lt;strong&gt;Indexing:&lt;/strong&gt; Parsed slide content is embedded using OpenAI&amp;#39;s embedder and stored in Pathway&amp;#39;s vector store (&lt;a href=&quot;https://python.langchain.com/v0.2/docs/integrations/vectorstores/pathway/&quot;&gt;natively available on LangChain&lt;/a&gt;) that is optimized for incremental indexing.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;How it helps:&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Text in presentations is often limited. This example removes the need to manually sift through countless presentations by recalling keywords.&lt;/li&gt; &lt;li&gt;Organize your slide library by topic or other criteria. Indexes update automatically whenever a slide is added, modified, or removed.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;Preliminary Results:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;This method has proven to be efficient in managing large volumes of slides, ensuring that the most up-to-date and accurate information is available. It significantly enhances productivity by streamlining the search process across PowerPoints, PDFs, and Slides.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Open to your questions and feedback!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Typical-Scene-5794&quot;&gt; /u/Typical-Scene-5794 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dzzrw5/accurate_multimodal_slides_search_with_realtime/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dzzrw5/accurate_multimodal_slides_search_with_realtime/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dzzrw5</id><link href="https://www.reddit.com/r/LangChain/comments/1dzzrw5/accurate_multimodal_slides_search_with_realtime/" /><updated>2024-07-10T16:15:22+00:00</updated><published>2024-07-10T16:15:22+00:00</published><title>Accurate Multimodal Slides Search with Real-Time Updates from SharePoint, Google Drive, and Local Data Sources</title></entry><entry><author><name>/u/No_Entrepreneur7665</name><uri>https://www.reddit.com/user/No_Entrepreneur7665</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;What’s the best tool to make a UI for an agent that interacts with the user as it processes tgrough the graph. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/No_Entrepreneur7665&quot;&gt; /u/No_Entrepreneur7665 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e02blv/ui_for_langgraph_for_agent_with_user_input/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e02blv/ui_for_langgraph_for_agent_with_user_input/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1e02blv</id><link href="https://www.reddit.com/r/LangChain/comments/1e02blv/ui_for_langgraph_for_agent_with_user_input/" /><updated>2024-07-10T17:58:06+00:00</updated><published>2024-07-10T17:58:06+00:00</published><title>UI for langgraph for agent with user input</title></entry><entry><author><name>/u/Bivee2000</name><uri>https://www.reddit.com/user/Bivee2000</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I want to connect my langchain to Snowflake db. But the snowflake I am using have no password but SSO. What is the syntax should be used to connect the snowflake to langchain using the following function SQLDatabase.from_uri?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Bivee2000&quot;&gt; /u/Bivee2000 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e00w6o/connecting_langchain_to_snowflake_db_with_sso/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e00w6o/connecting_langchain_to_snowflake_db_with_sso/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1e00w6o</id><link href="https://www.reddit.com/r/LangChain/comments/1e00w6o/connecting_langchain_to_snowflake_db_with_sso/" /><updated>2024-07-10T17:00:15+00:00</updated><published>2024-07-10T17:00:15+00:00</published><title>Connecting Langchain to Snowflake DB with SSO authentication</title></entry><entry><author><name>/u/External_Ad_11</name><uri>https://www.reddit.com/user/External_Ad_11</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;The video tutorial covers:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;How to use LLM from HuggingFaceHub without even loading it.&lt;br/&gt;&lt;/li&gt; &lt;li&gt;Prompt Template for Open Source LLM&lt;br/&gt;&lt;/li&gt; &lt;li&gt;Converting normal text to Langchain schema&lt;br/&gt;&lt;/li&gt; &lt;li&gt;Vector Database and embedding important functions&lt;br/&gt;&lt;/li&gt; &lt;li&gt;Memory&lt;br/&gt;&lt;/li&gt; &lt;li&gt;LCEL &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Langchain Zero to LCEL: &lt;a href=&quot;https://www.youtube.com/watch?v=TWmV95-dUgQ&quot;&gt;https://www.youtube.com/watch?v=TWmV95-dUgQ&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/External_Ad_11&quot;&gt; /u/External_Ad_11 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dzuxak/langchain_ultimate_guide_using_open_source_llms/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dzuxak/langchain_ultimate_guide_using_open_source_llms/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dzuxak</id><link href="https://www.reddit.com/r/LangChain/comments/1dzuxak/langchain_ultimate_guide_using_open_source_llms/" /><updated>2024-07-10T12:47:48+00:00</updated><published>2024-07-10T12:47:48+00:00</published><title>Langchain Ultimate Guide using Open Source LLMs</title></entry><entry><author><name>/u/giagara</name><uri>https://www.reddit.com/user/giagara</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I personally host my app in aws using lambda for compute, s3 for storage and rds (postgres) for vector db. There are some sqs, dynamo, etc but are for statistic purpose.&lt;/p&gt; &lt;p&gt;Edit: i mean for commercial purpose, not just personal &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/giagara&quot;&gt; /u/giagara &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dzord2/where_do_you_host_your_rag/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dzord2/where_do_you_host_your_rag/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dzord2</id><link href="https://www.reddit.com/r/LangChain/comments/1dzord2/where_do_you_host_your_rag/" /><updated>2024-07-10T06:22:28+00:00</updated><published>2024-07-10T06:22:28+00:00</published><title>Where do you host your Rag</title></entry><entry><author><name>/u/coolcloud</name><uri>https://www.reddit.com/user/coolcloud</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dzfp48/agent_retrieval_how_we_almost_always_find_the/&quot;&gt; &lt;img src=&quot;https://b.thumbs.redditmedia.com/Pb2SWSeA1tyEw8SKW3E2DTFD13TWyENVp-4WWyttigA.jpg&quot; alt=&quot;Agent Retrieval - How we almost always find the right vectors. Pt 3&quot; title=&quot;Agent Retrieval - How we almost always find the right vectors. Pt 3&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey all - &lt;/p&gt; &lt;p&gt;Today I wanted to run through how we narrow down our vector space.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Issues with vector search only:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;If you have used vector search over a large corpus of documents, you&amp;#39;ll know vector search doesn&amp;#39;t work well.&lt;/li&gt; &lt;li&gt;Almost 100% of the time someone is using RAG, they are looking for something specific. &lt;ul&gt; &lt;li&gt; Example: If you use vector search on a name, most names will come back, regardless of it&amp;#39;s Bob Smith, or Sally Blu. This is bad if I just want to find Sally Blu.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;W&lt;strong&gt;hat are we doing?&lt;/strong&gt; &lt;/p&gt; &lt;ol&gt; &lt;li&gt;We split each doc up into the smallest possible vector (normally sentences) We&amp;#39;ve found this is the best, and most accurate for vector similarity. &lt;/li&gt; &lt;li&gt;NER/Keyword extraction from the query&lt;/li&gt; &lt;li&gt; Search docs for keywords/NER&lt;/li&gt; &lt;li&gt;Vector search query within the docs that are returned.&lt;/li&gt; &lt;li&gt; Traditionally top 20 results (no similarity score min)&lt;/li&gt; &lt;li&gt;Reconstruct the docs into headers etc.&lt;/li&gt; &lt;li&gt;Reranker Jina - top 10 results (over .x similarity)&lt;/li&gt; &lt;li&gt;Each result sent to an LLM for quotes&lt;/li&gt; &lt;li&gt;combine all into prompt #2&lt;/li&gt; &lt;li&gt;LLM answer&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Today we&amp;#39;ll primarily talk through step 2-7.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;How?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;Example query:&lt;/em&gt; Does Sally blu work at tada?&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Use an LLM to extract named entities and key words/phrases from the query. &lt;/li&gt; &lt;li&gt;Search the entire document to see if &amp;quot;all&amp;quot; keywords/NER match the doc. [&amp;quot;Tada&amp;quot; and &amp;quot;Sally Blu&amp;quot;]&lt;/li&gt; &lt;li&gt;If there&amp;#39;s no matches, do an &amp;quot;or&amp;quot; search for keywords/NER [&amp;quot;Tada&amp;quot; or &amp;quot;Sally Blu&amp;quot;]&lt;/li&gt; &lt;li&gt;If there&amp;#39;s no matches, don&amp;#39;t return a doc.&lt;/li&gt; &lt;li&gt;If there are matches in either step 2 or 3, return those docs only and do vector search within only those documents. &lt;/li&gt; &lt;/ol&gt; &lt;p&gt;This significantly limits the scope of the vector space and based on our experiences almost never filters out the documents that are important to answering the question.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;How are we able to search an entire doc?&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;This process wouldn&amp;#39;t be possible without out our document structure, so here&amp;#39;s a link &amp;amp; a quick overview of how we how we &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dpbc4g/how_we_chunk_turning_pdfs_into_hierarchical/&quot;&gt;chunk docs. &lt;/a&gt;&lt;/p&gt; &lt;p&gt;TLDR: We extract and save the document structure into a hieratical format, headers, sub-headers, list, paragraphs, tables, etc. Because we do this, we can easily search the entire document. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Diagram of our first pass search&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/jczepvzejkbd1.png?width=365&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f7818f84954b2486793cb012f02b7097b988859c&quot;&gt;https://preview.redd.it/jczepvzejkbd1.png?width=365&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f7818f84954b2486793cb012f02b7097b988859c&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Example Query:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;/u/warriorA&quot;&gt;u/warriorA&lt;/a&gt; asked this question a couple of days ago in another post. It&amp;#39;s simple so we&amp;#39;ll re-use it:&lt;/p&gt; &lt;p&gt;______&lt;/p&gt; &lt;p&gt;Consider the query: &amp;quot;How many Presidents did we have in America?&amp;quot;&lt;/p&gt; &lt;p&gt;Now we might have a document chunk with this information:&lt;/p&gt; &lt;ol&gt; &lt;li&gt; doc_1_chunk: &amp;quot;United States has been governed by a total of 46 people&amp;quot;&lt;br/&gt;&lt;/li&gt; &lt;li&gt; doc_2_chunk: &amp;quot;The USA is a country in north america.&amp;quot;&lt;br/&gt;&lt;/li&gt; &lt;li&gt; doc_3_chunk: &amp;quot;We&amp;#39;ve had 1 President in &amp;#39;Random-Country&amp;#39;.&amp;quot;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Wouldn&amp;#39;t your search fail?&lt;/p&gt; &lt;p&gt;Note - (I made a few small edits for example purposes)&lt;/p&gt; &lt;p&gt;______&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Would our system work for this use case?&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;Most likely, yes. &lt;/p&gt; &lt;p&gt;From the query we&amp;#39;d extract: [&amp;quot;Presidents&amp;quot; &amp;amp; &amp;quot;America&amp;quot;] &lt;/p&gt; &lt;p&gt;Again, we search the entire document, not just the chunks to find hits.&lt;/p&gt; &lt;p&gt;✅ Doc_1: It&amp;#39;s very likely that doc_1 would contain both the word president and America, meaning that document would come back. &lt;/p&gt; &lt;p&gt;❌ Doc_2: Isn&amp;#39;t talking about Presidents, thus it wouldn&amp;#39;t come back.&lt;/p&gt; &lt;p&gt;✅or ❌ Doc_3: Would most likely not come back as it&amp;#39;s not talking about America. (If it did come back because America was in the document somewhere, vector search + rerankers would help filter it out.)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;If we extended chunk one:&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;here&amp;#39;s an example of what it would look like and it contains both the word president &amp;amp; america.&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/zwuhtvrdkkbd1.png?width=1047&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9a62623c04a8d1836262d3bd6c0ba3dbe00f91f0&quot;&gt;https://preview.redd.it/zwuhtvrdkkbd1.png?width=1047&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9a62623c04a8d1836262d3bd6c0ba3dbe00f91f0&lt;/a&gt;&lt;/p&gt; &lt;p&gt;If we didn&amp;#39;t do this step, it&amp;#39;s likely all 3 chunks would come back, and doc_3_chunk, would be rated the highest. You could imagine if you had hundreds or thousands of documents the most important vectors to answer the question may not show up at all.&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/cr6jmxuwkkbd1.png?width=405&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=76c747237adb98fb3ba6c78ff491c04ac05bdecc&quot;&gt;https://preview.redd.it/cr6jmxuwkkbd1.png?width=405&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=76c747237adb98fb3ba6c78ff491c04ac05bdecc&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;how do we extract NER?&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;We have found the NER models aren&amp;#39;t consistent enough, you have to use an LLM. If you ask a question like &amp;quot;what are the terms of the wings contract&amp;quot; a NER model may see no named entities, where an LLM would understand the named entity is &amp;quot;wings&amp;quot;.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Prompt:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Respond with valid json. &lt;/p&gt; &lt;p&gt;You will receive text, your goal is to: &lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Identify potential search phrases. Put those in the &amp;quot;P&amp;quot; field. For example, in &amp;quot;When was the Huck Finn contract signed?&amp;quot;, the main concept is &amp;quot;Huck Finn contract&amp;quot; &lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Identify named entities. Put those in the &amp;quot;N&amp;quot; field. For example: &amp;quot;Huck Finn&amp;quot; or &amp;quot;Apple A7&amp;quot; Emit a valid JSON object with a single &amp;quot;N&amp;quot; field and a single &amp;quot;P&amp;quot; field. &lt;/p&gt; &lt;p&gt;Example &lt;/p&gt; &lt;p&gt;Input: When was the Huck Finn contract signed? &lt;/p&gt; &lt;p&gt;Output: &lt;/p&gt; &lt;p&gt;{ &lt;/p&gt; &lt;p&gt;&amp;quot;P&amp;quot;: [&amp;quot;Huck Finn contract&amp;quot;], &lt;/p&gt; &lt;p&gt;&amp;quot;N&amp;quot;: [&amp;quot;Huck Finn&amp;quot;],&lt;/p&gt; &lt;p&gt;} &lt;/p&gt; &lt;p&gt;Input: does share and perform offer a performance engagement tool &lt;/p&gt; &lt;p&gt;Output: &lt;/p&gt; &lt;p&gt;{ &lt;/p&gt; &lt;p&gt;&amp;quot;P&amp;quot;: [&amp;quot;share and perform performance engagement&amp;quot;], &lt;/p&gt; &lt;p&gt;&amp;quot;N&amp;quot;: [&amp;quot;share and perform&amp;quot;],&lt;/p&gt; &lt;p&gt;} &lt;/p&gt; &lt;p&gt;Input: how many processors in the apple a7? &lt;/p&gt; &lt;p&gt;{ &lt;/p&gt; &lt;p&gt;&amp;quot;P&amp;quot;: [&amp;quot;apple a7 processors&amp;quot;], &lt;/p&gt; &lt;p&gt;&amp;quot;N&amp;quot;: [&amp;quot;Apple&amp;quot;, &amp;quot;Apple A7&amp;quot;], &lt;/p&gt; &lt;p&gt;} &lt;/p&gt; &lt;p&gt;Input: How much is the wings contract Output: &lt;/p&gt; &lt;p&gt;{ &amp;quot;P&amp;quot;: [&amp;quot;wings contract&amp;quot;],&lt;/p&gt; &lt;p&gt;&amp;quot;N&amp;quot;: [&amp;quot;wings&amp;quot;], &lt;/p&gt; &lt;p&gt;} &lt;/p&gt; &lt;p&gt;Input: what are the different tiers of wotc? &lt;/p&gt; &lt;p&gt;Output: &lt;/p&gt; &lt;p&gt;{&lt;/p&gt; &lt;p&gt;&amp;quot;P&amp;quot;: [&amp;quot;wotc tiers&amp;quot;], &lt;/p&gt; &lt;p&gt;&amp;quot;N&amp;quot;: [&amp;quot;WOTC&amp;quot;], &lt;/p&gt; &lt;p&gt;} &lt;/p&gt; &lt;p&gt;Input: what is included in the QuickBooks General Journal report Output: &lt;/p&gt; &lt;p&gt;{&lt;/p&gt; &lt;p&gt;&amp;quot;P&amp;quot;: [&amp;quot;QuickBooks General Journal report&amp;quot;], &lt;/p&gt; &lt;p&gt;&amp;quot;N&amp;quot;: [&amp;quot;Quickbooks&amp;quot;], &lt;/p&gt; &lt;p&gt;} &lt;/p&gt; &lt;p&gt;Example &amp;quot;&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Why did we decide on this prompt? &lt;/p&gt; &lt;ul&gt; &lt;li&gt;We use P &amp;amp; N because the less tokens than doing something like named entities &amp;amp; keywords. This mean there&amp;#39;s less tokens the LLM needs to return. (The average response time is 1.1 seconds.)&lt;/li&gt; &lt;li&gt;We found you need a large example set for the LLM to understand what you&amp;#39;re trying to do.&lt;/li&gt; &lt;li&gt;We recommend tuning these prompts to questions that your customer may similarly ask&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Next step:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;After finding the top 20 vectors, we re-construct the document. Because re-rankers tend to work better, and we are giving them additional context, we&amp;#39;ve found that we almost always return the most relevant chunks to answer the question. Here&amp;#39;s our&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dtr49t/agent_rag_parallel_quotes_how_we_built_rag_on/&quot;&gt; article&lt;/a&gt; for going from vectors to search.&lt;/p&gt; &lt;p&gt;Happy to answer any and all questions!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/coolcloud&quot;&gt; /u/coolcloud &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dzfp48/agent_retrieval_how_we_almost_always_find_the/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dzfp48/agent_retrieval_how_we_almost_always_find_the/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1dzfp48</id><media:thumbnail url="https://b.thumbs.redditmedia.com/Pb2SWSeA1tyEw8SKW3E2DTFD13TWyENVp-4WWyttigA.jpg" /><link href="https://www.reddit.com/r/LangChain/comments/1dzfp48/agent_retrieval_how_we_almost_always_find_the/" /><updated>2024-07-09T22:35:02+00:00</updated><published>2024-07-09T22:35:02+00:00</published><title>Agent Retrieval - How we almost always find the right vectors. Pt 3</title></entry><entry><author><name>/u/Ukpersfidev</name><uri>https://www.reddit.com/user/Ukpersfidev</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;m trying to get Claude to generate content that could be up to 8,000 tokens long.&lt;/p&gt; &lt;p&gt;To overcome the max token output limitations, I have specifically prompted Claude to stop generating when the max token limit is reached, and then continue exactly where you left off when the user responds with a &amp;quot;continue&amp;quot; message.&lt;/p&gt; &lt;p&gt;Example of the prompt:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;const prompt = ` Generate me an adventure story that is between 6000 - 7000 words long If you reach the max token output, the user will send a message saying &amp;quot;continue&amp;quot; Before continuing, you should check the previous outputs and start exactly where you left off so that the output is coherent and consistent. You should not ackowledge the &amp;quot;continue&amp;quot; message in the output, just continue generating the content. We will join the outputs together at the end and they should form a coherent and consistent response. If you have finished generating the content and the user asks you to continue, you should just respond with an empty message. For example: Assistant: mess User: continue Assistant: age ` &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The problem is that it won&amp;#39;t do as I ask, it just keeps shortening the story, I find this weird because when I change the max tokens to something like, 50 and then change the requested story to have a length of 200 words, it works fine for multiple continuations.&lt;/p&gt; &lt;p&gt;Anyone faced this before?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Ukpersfidev&quot;&gt; /u/Ukpersfidev &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dzxzu0/how_are_you_managing_long_form_content/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dzxzu0/how_are_you_managing_long_form_content/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dzxzu0</id><link href="https://www.reddit.com/r/LangChain/comments/1dzxzu0/how_are_you_managing_long_form_content/" /><updated>2024-07-10T15:03:07+00:00</updated><published>2024-07-10T15:03:07+00:00</published><title>How are you managing long form content?</title></entry><entry><author><name>/u/NoDance9749</name><uri>https://www.reddit.com/user/NoDance9749</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hello, what are the most used serveless vector/graph databases for LLM RAG usage, with convenient integration in Python and AWS (Knowledge base in Bedrock)? Ideally which are PAYG. I wanted to use OpenSearch from AWS, but I don&amp;#39;t wanna pay 700 USD/month though and other solutions don&amp;#39;t seem to be serverless at all.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/NoDance9749&quot;&gt; /u/NoDance9749 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dzrcfl/serverless_vectorgraph_database_for_rag/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dzrcfl/serverless_vectorgraph_database_for_rag/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dzrcfl</id><link href="https://www.reddit.com/r/LangChain/comments/1dzrcfl/serverless_vectorgraph_database_for_rag/" /><updated>2024-07-10T09:18:55+00:00</updated><published>2024-07-10T09:18:55+00:00</published><title>Serverless Vector/Graph Database for RAG</title></entry><entry><author><name>/u/gr8satvik</name><uri>https://www.reddit.com/user/gr8satvik</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dznzj7/langchain_agent_issue_in_realtime_information/&quot;&gt; &lt;img src=&quot;https://b.thumbs.redditmedia.com/vcDq0yOYTVHvQoU-w8eUV3AcOFZlmoa9PVi03_BvHtM.jpg&quot; alt=&quot;Langchain Agent Issue in real-time information&quot; title=&quot;Langchain Agent Issue in real-time information&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I am using&lt;/p&gt; &lt;pre&gt;&lt;code&gt;search = TavilySearchResults() &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;as one of the tool to create a tool calling agent.&lt;/p&gt; &lt;p&gt;But since it search the web in real-time(if i m not wrong), it is providing me results dating back to 16th Nov &amp;#39;23&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/e4jvvhiunmbd1.png?width=1136&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a10e5d4de6852036b5eee0d3ccfbc3f0cce5ee1b&quot;&gt;https://preview.redd.it/e4jvvhiunmbd1.png?width=1136&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a10e5d4de6852036b5eee0d3ccfbc3f0cce5ee1b&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/757ggiiunmbd1.png?width=1828&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2509d57b7937cb7b3df8434b1a0f1ec4486d2fc4&quot;&gt;https://preview.redd.it/757ggiiunmbd1.png?width=1828&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2509d57b7937cb7b3df8434b1a0f1ec4486d2fc4&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/3b7ejliunmbd1.png?width=1834&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a8e9958b57266770ea7ee08eb9eff811f73fe8ae&quot;&gt;https://preview.redd.it/3b7ejliunmbd1.png?width=1834&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a8e9958b57266770ea7ee08eb9eff811f73fe8ae&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/fqh00jiunmbd1.png?width=1817&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c28148779b6a6efeba3057fae724de906974d691&quot;&gt;https://preview.redd.it/fqh00jiunmbd1.png?width=1817&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c28148779b6a6efeba3057fae724de906974d691&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/9egk1kiunmbd1.png?width=1343&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=abd77cfa5d5599839c68a6f05d632d0046af37aa&quot;&gt;https://preview.redd.it/9egk1kiunmbd1.png?width=1343&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=abd77cfa5d5599839c68a6f05d632d0046af37aa&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Context: I am making a chatbot, that i basically a financial advisor, but also has the capability to tell information about stock in real time&lt;br/&gt; So for financial advisor part I am using RAG and using a tool as retriever to get advise from my knowledge base&lt;br/&gt; But while using Tavely, it is not providing information in real time&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/gr8satvik&quot;&gt; /u/gr8satvik &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dznzj7/langchain_agent_issue_in_realtime_information/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dznzj7/langchain_agent_issue_in_realtime_information/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1dznzj7</id><media:thumbnail url="https://b.thumbs.redditmedia.com/vcDq0yOYTVHvQoU-w8eUV3AcOFZlmoa9PVi03_BvHtM.jpg" /><link href="https://www.reddit.com/r/LangChain/comments/1dznzj7/langchain_agent_issue_in_realtime_information/" /><updated>2024-07-10T05:32:59+00:00</updated><published>2024-07-10T05:32:59+00:00</published><title>Langchain Agent Issue in real-time information</title></entry><entry><author><name>/u/Flimsy-Ninja1412</name><uri>https://www.reddit.com/user/Flimsy-Ninja1412</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi,i’am facing a bug when trying to call multiple tools at parallel.The model is calling only a single tool every-time. See below code.&lt;br/&gt; The code is given in &lt;strong&gt;langchain documentation&lt;/strong&gt; so i tried that but &lt;strong&gt;not getting&lt;/strong&gt; the same results as in documentation&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;/u/tool&quot;&gt;u/tool&lt;/a&gt;&lt;br/&gt; def add(a: int, b: int) → int:&lt;br/&gt; “”&amp;quot;Adds a and b.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Args: a: first int b: second int &amp;quot;&amp;quot;&amp;quot; return a + b &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;a href=&quot;/u/tool&quot;&gt;u/tool&lt;/a&gt;&lt;br/&gt; def multiply(a: int, b: int) → int:&lt;br/&gt; “”&amp;quot;Multiplies a and b.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Args: a: first int b: second int &amp;quot;&amp;quot;&amp;quot; return a * b &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;tools = [add,multiply]&lt;/p&gt; &lt;p&gt;llm_with_tools = llm.bind_tools(tools)&lt;br/&gt; query = &amp;quot;What is 3 * 12? Also, what is 11 + 49?&amp;quot;&lt;br/&gt; llm_with_tools.invoke(query).tool_calls&lt;/p&gt; &lt;p&gt;Expected Output :[{&amp;#39;name&amp;#39;: &amp;#39;multiply&amp;#39;,&lt;br/&gt; &amp;#39;args&amp;#39;: {&amp;#39;a&amp;#39;: 3, &amp;#39;b&amp;#39;: 12},&lt;br/&gt; &amp;#39;id&amp;#39;: &amp;#39;call_UL7E2232GfDHIQGOM4gJfEDD&amp;#39;},&lt;br/&gt; {&amp;#39;name&amp;#39;: &amp;#39;add&amp;#39;,&lt;br/&gt; &amp;#39;args&amp;#39;: {&amp;#39;a&amp;#39;: 11, &amp;#39;b&amp;#39;: 49},&lt;br/&gt; &amp;#39;id&amp;#39;: &amp;#39;call_VKw8t5tpAuzvbHgdAXe9mjUx&amp;#39;}]&lt;/p&gt; &lt;p&gt;My output:&lt;br/&gt; [{&amp;#39;name&amp;#39;: &amp;#39;multiply&amp;#39;, &amp;#39;args&amp;#39;: {&amp;#39;a&amp;#39;: 3, &amp;#39;b&amp;#39;: 12}, &amp;#39;id&amp;#39;: &amp;#39;call_7lwm1rk1b7IYSzwxeyIvhRI9&amp;#39;}]&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Flimsy-Ninja1412&quot;&gt; /u/Flimsy-Ninja1412 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dzpiyh/azure_openai_gpt_35_and_4_not_calling_multiple/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dzpiyh/azure_openai_gpt_35_and_4_not_calling_multiple/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dzpiyh</id><link href="https://www.reddit.com/r/LangChain/comments/1dzpiyh/azure_openai_gpt_35_and_4_not_calling_multiple/" /><updated>2024-07-10T07:12:53+00:00</updated><published>2024-07-10T07:12:53+00:00</published><title>Azure Openai GPT 3.5 and 4 not calling multiple tools in parallel</title></entry><entry><author><name>/u/NANDESHVAR_R_K</name><uri>https://www.reddit.com/user/NANDESHVAR_R_K</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/NANDESHVAR_R_K&quot;&gt; /u/NANDESHVAR_R_K &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dzsreq/can_anyone_share_llm_based_projects_url_i_wanted/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dzsreq/can_anyone_share_llm_based_projects_url_i_wanted/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dzsreq</id><link href="https://www.reddit.com/r/LangChain/comments/1dzsreq/can_anyone_share_llm_based_projects_url_i_wanted/" /><updated>2024-07-10T10:51:13+00:00</updated><published>2024-07-10T10:51:13+00:00</published><title>Can anyone share LLM based projects URL? I wanted to know the pipeline of the project how we are doing practically?Any Suggestions?</title></entry><entry><author><name>/u/Knoah25</name><uri>https://www.reddit.com/user/Knoah25</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi, everyone I need help/advice/ideas with approaching a problem. I&amp;#39;m working on a project that generates questions based on other questions and documents that have been given. The results should be a mixture of reoccurring questions and newly generated questions from what&amp;#39;s uploaded. &lt;/p&gt; &lt;p&gt;Now I&amp;#39;m still new to building AI products, all I could come up with concerning approaching this problem is to load the document, parse it, chunk it up, and embed the chunks into a vector database. As for the generation, I&amp;#39;m thinking of using a prompt template and passing the embedding in the prompt.&lt;/p&gt; &lt;p&gt;What do y&amp;#39;all think, any suggestions for how I can achieve this? Thanks in advance.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Knoah25&quot;&gt; /u/Knoah25 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dzrp0a/help_needed_seeking_advice_on_approach_and/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dzrp0a/help_needed_seeking_advice_on_approach_and/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dzrp0a</id><link href="https://www.reddit.com/r/LangChain/comments/1dzrp0a/help_needed_seeking_advice_on_approach_and/" /><updated>2024-07-10T09:43:16+00:00</updated><published>2024-07-10T09:43:16+00:00</published><title>Help Needed: Seeking Advice on Approach and Implementation! (I'm new to this)</title></entry><entry><author><name>/u/Stunning_Cat3195</name><uri>https://www.reddit.com/user/Stunning_Cat3195</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey&lt;br/&gt; For context, I am trying to build a RAG application on scale which should support many concurrent user&lt;br/&gt; I am using ConversationalRetrievalChain and openAI gpt-4-turbo as llm, api is built using Fastapi&lt;br/&gt; for invoking chain I am using ainvoke, and for streaming using AsyncIteratorCallbackHandler&lt;/p&gt; &lt;p&gt;flow is I invoke chain in background using asyncio.create_task() and streaming from callback_handler&lt;br/&gt; Now the issue is, when multiple concurrent user are coming to application, this callback is blocking main_thread which causing response to be delayed &lt;/p&gt; &lt;p&gt;my assumption is - even if callback is async, the switching of task in async loop is too much that it is blocking the cpu/thread&lt;br/&gt; Observation: callback function on_llm_token also get&amp;#39;s block so it does not pass values to hander&lt;/p&gt; &lt;p&gt;If you have any suggestion on better profiling of application, those are also welcome, but as per my analysis this is what blocking application and limiting it to scale&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Stunning_Cat3195&quot;&gt; /u/Stunning_Cat3195 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dzoa09/langchain_with_streaming_in_production/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dzoa09/langchain_with_streaming_in_production/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dzoa09</id><link href="https://www.reddit.com/r/LangChain/comments/1dzoa09/langchain_with_streaming_in_production/" /><updated>2024-07-10T05:51:46+00:00</updated><published>2024-07-10T05:51:46+00:00</published><title>LangChain with streaming in production</title></entry><entry><author><name>/u/gr8satvik</name><uri>https://www.reddit.com/user/gr8satvik</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Please can someone provide me a documentation or link or explaination, How to work with RAG with agents&lt;br/&gt; I have a DB on pinecone and i make a tool of it&lt;br/&gt; then i used a built in tool&lt;br/&gt; and combine those tools as&lt;br/&gt; tools = [tool1,tool2]&lt;br/&gt; but i am not getting good results and sometimes even it goes in a loop and doesnot generate anything&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/gr8satvik&quot;&gt; /u/gr8satvik &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dzqcxy/rag_agent/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dzqcxy/rag_agent/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dzqcxy</id><link href="https://www.reddit.com/r/LangChain/comments/1dzqcxy/rag_agent/" /><updated>2024-07-10T08:10:32+00:00</updated><published>2024-07-10T08:10:32+00:00</published><title>RAG + Agent</title></entry><entry><author><name>/u/ayiding</name><uri>https://www.reddit.com/user/ayiding</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dz6jni/is_your_langchain_too_slow_learn_c/&quot;&gt; &lt;img src=&quot;https://external-preview.redd.it/L77W14UQfAyAB_qTT6-kfw4lI0KKeCSZOdrzHWit1yg.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8501ce915ce518acc8b72102f34473ea5e7dbe1d&quot; alt=&quot;Is your LangChain too slow? Learn C&quot; title=&quot;Is your LangChain too slow? Learn C&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/ayiding&quot;&gt; /u/ayiding &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://github.com/yisding/libchatty/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dz6jni/is_your_langchain_too_slow_learn_c/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1dz6jni</id><media:thumbnail url="https://external-preview.redd.it/L77W14UQfAyAB_qTT6-kfw4lI0KKeCSZOdrzHWit1yg.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=8501ce915ce518acc8b72102f34473ea5e7dbe1d" /><link href="https://www.reddit.com/r/LangChain/comments/1dz6jni/is_your_langchain_too_slow_learn_c/" /><updated>2024-07-09T16:19:43+00:00</updated><published>2024-07-09T16:19:43+00:00</published><title>Is your LangChain too slow? Learn C</title></entry><entry><author><name>/u/DeepakBhattarai69</name><uri>https://www.reddit.com/user/DeepakBhattarai69</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I am trying to build an app with tool calling , but when ever I change the model to Gemini (gemini-1.5-pro-latest or gemini-1.5-flash) . The model just outputs empty response. It does not have any content nor does it have function call. The response is just empty. &lt;/p&gt; &lt;p&gt;I just have one function/tool i.e the taviliy search tool. &lt;/p&gt; &lt;p&gt;Even when I ask it , latest news or some other question . Its just gives empty response. &lt;/p&gt; &lt;pre&gt;&lt;code&gt;import { env } from &amp;quot;@/env&amp;quot;; import { ChatAnthropic } from &amp;quot;@langchain/anthropic&amp;quot;; import { ChatGoogleGenerativeAI } from &amp;quot;@langchain/google-genai&amp;quot;; import { ChatOpenAI, type ChatOpenAICallOptions } from &amp;quot;@langchain/openai&amp;quot;; import { z } from &amp;quot;zod&amp;quot;; type Model = | ChatOpenAI&amp;lt;ChatOpenAICallOptions&amp;gt; | ChatAnthropic | ChatGoogleGenerativeAI; export const AvailableModels = z.enum([&amp;quot;gpt&amp;quot;, &amp;quot;claude&amp;quot;, &amp;quot;gemini&amp;quot;]); export type AvailableModels = z.infer&amp;lt;typeof AvailableModels&amp;gt;; export function modelPicker( model : z.infer&amp;lt;typeof AvailableModels&amp;gt;, stream ?: boolean, modelName = &amp;quot;gpt-4o&amp;quot;, ) { let modelObject: Model; switch ( model ) { case &amp;quot;gpt&amp;quot;: { modelObject = new ChatOpenAI({ model: modelName , apiKey: env.OPENAI_API_KEY, streaming: stream , modelKwargs: stream ? { parallel_tool_calls: false, } : undefined, }); break ; } case &amp;quot;claude&amp;quot;: { modelObject = new ChatAnthropic({ model: &amp;quot;claude-3-sonnet-20240229&amp;quot;, modelName: &amp;quot;claude-3-sonnet-20240229&amp;quot;, apiKey: env.ANTHROPIC_API_KEY, streaming: true, }); // modelObject.streaming = true; break ; } case &amp;quot;gemini&amp;quot;: { modelObject = new ChatGoogleGenerativeAI({ model: &amp;quot;gemini-pro&amp;quot;, modelName: &amp;quot;gemini-1.5-flash-latest&amp;quot;, apiKey: env.GOOGLE_AI_API_KEY, // streaming: stream, }); break ; } } return modelObject; } &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This is the code for selecting the models. &lt;/p&gt; &lt;p&gt;Is this the problem with langchain or the gemini api&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/DeepakBhattarai69&quot;&gt; /u/DeepakBhattarai69 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dzp16b/tool_calling_function_calling_not_working_with/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dzp16b/tool_calling_function_calling_not_working_with/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dzp16b</id><link href="https://www.reddit.com/r/LangChain/comments/1dzp16b/tool_calling_function_calling_not_working_with/" /><updated>2024-07-10T06:40:20+00:00</updated><published>2024-07-10T06:40:20+00:00</published><title>Tool Calling / Function Calling not working with Gemini</title></entry><entry><author><name>/u/innocent_kittyy</name><uri>https://www.reddit.com/user/innocent_kittyy</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey everyone, Need help on this, During retrieval when I try asking questions, initially I get the correct answers, but as the number of documents whose embeddings I am storing ,increases,i start getting incorrect data.&lt;/p&gt; &lt;p&gt;I have tried multiple chunk sizes with the various embedding models but this issue is persistent.&lt;/p&gt; &lt;p&gt;Any suggestions on it? PS: my pdf data is very much unstructured so I am first extracting data using python libraries and then storing.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/innocent_kittyy&quot;&gt; /u/innocent_kittyy &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dzotw6/facing_issues_in_retrieval/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dzotw6/facing_issues_in_retrieval/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dzotw6</id><link href="https://www.reddit.com/r/LangChain/comments/1dzotw6/facing_issues_in_retrieval/" /><updated>2024-07-10T06:26:59+00:00</updated><published>2024-07-10T06:26:59+00:00</published><title>Facing issues in retrieval</title></entry><entry><author><name>/u/ArtisticDirt1341</name><uri>https://www.reddit.com/user/ArtisticDirt1341</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;How to deploy for scaling, model parallelism on GPUs, industry best practices.&lt;/p&gt; &lt;p&gt;Anything works, ytb videos, blogs, articles,books&lt;/p&gt; &lt;p&gt;I’m all ears. Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/ArtisticDirt1341&quot;&gt; /u/ArtisticDirt1341 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dz1hxg/where_to_learn_ai_system_design/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dz1hxg/where_to_learn_ai_system_design/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dz1hxg</id><link href="https://www.reddit.com/r/LangChain/comments/1dz1hxg/where_to_learn_ai_system_design/" /><updated>2024-07-09T12:45:52+00:00</updated><published>2024-07-09T12:45:52+00:00</published><title>Where to learn AI system design?</title></entry><entry><author><name>/u/Big_Barracuda_6753</name><uri>https://www.reddit.com/user/Big_Barracuda_6753</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I am developing a PDF RAG app .&lt;br/&gt; In the previous version of Langchain ( i.e. v0.1 ) , we had RetrievalQA class , but now it is deprecated ( &lt;a href=&quot;https://api.python.langchain.com/en/latest/chains/langchain.chains.retrieval_qa.base.RetrievalQA.html&quot;&gt;https://api.python.langchain.com/en/latest/chains/langchain.chains.retrieval_qa.base.RetrievalQA.html&lt;/a&gt; ).&lt;/p&gt; &lt;p&gt;In this same doc , it is mentioned to use create_retrieval_chain . &lt;/p&gt; &lt;p&gt;But but but , in the RetrievalQA class we had an option to pass memory as parameter . How to pass memory ( ConversationSummaryMemory ) in this create_retrieval_chain class ?&lt;/p&gt; &lt;pre&gt;&lt;code&gt; rag = RetrievalQA.from_chain_type( llm=ChatOpenAI( temperature=0.5, model=&amp;quot;gpt-3.5-turbo-0125&amp;quot;, ), retriever=compression_retriever, memory=ConversationSummaryMemory( llm=ChatOpenAI( temperature=0.5, model=&amp;quot;gpt-3.5-turbo-0125&amp;quot;, ) ), chain_type_kwargs={&amp;quot;prompt&amp;quot;: pt, &amp;quot;verbose&amp;quot;: True}, ) response = rag.invoke(question) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The above code snippet is for RetrievalQA class . You can see that I pass ConversationSummaryMemory in the memory parameter .&lt;/p&gt; &lt;pre&gt;&lt;code&gt; system_prompt = ( &amp;quot;You are an assistant for question-answering tasks. &amp;quot; &amp;quot;Use the following pieces of retrieved context to answer &amp;quot; &amp;quot;the question. If you don&amp;#39;t know the answer, say that you &amp;quot; &amp;quot;don&amp;#39;t know. Use three sentences maximum and keep the &amp;quot; &amp;quot;answer concise.&amp;quot; &amp;quot;\n\n&amp;quot; &amp;quot;{context}&amp;quot; ) chatPrompt = ChatPromptTemplate.from_messages( [ (&amp;quot;system&amp;quot;, system_prompt), (&amp;quot;human&amp;quot;, &amp;quot;{input}&amp;quot;), ] ) question_answer_chain = create_stuff_documents_chain(llm, chatPrompt) compression_retriever = reRanker() rag_chain = create_retrieval_chain(compression_retriever, question_answer_chain) response = rag_chain.invoke({&amp;quot;input&amp;quot;:prompt }) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The above code is for the create_retrieval_chain I&amp;#39;m using .&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Big_Barracuda_6753&quot;&gt; /u/Big_Barracuda_6753 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dznl59/how_to_use_conversationsummarymemory_with_create/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dznl59/how_to_use_conversationsummarymemory_with_create/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dznl59</id><link href="https://www.reddit.com/r/LangChain/comments/1dznl59/how_to_use_conversationsummarymemory_with_create/" /><updated>2024-07-10T05:08:12+00:00</updated><published>2024-07-10T05:08:12+00:00</published><title>How to use ConversationSummaryMemory with create_retrieval_chain ?</title></entry></feed>