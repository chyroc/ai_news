<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/"><category term="LangChain" label="r/LangChain"/><updated>2024-03-27T22:18:32+00:00</updated><icon>https://www.redditstatic.com/icon.png/</icon><id>/r/LangChain.rss</id><link rel="self" href="https://www.reddit.com/r/LangChain.rss" type="application/atom+xml" /><link rel="alternate" href="https://www.reddit.com/r/LangChain" type="text/html" /><subtitle>LangChain is an open-source framework and developer toolkit that helps developers get LLM applications from prototype to production. It is available for Python and Javascript at https://www.langchain.com/.</subtitle><title>LangChain</title><entry><author><name>/u/zchaarm</name><uri>https://www.reddit.com/user/zchaarm</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;A place for members of &lt;a href=&quot;/r/LangChain&quot;&gt;r/LangChain&lt;/a&gt; to chat with each other&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/zchaarm&quot;&gt; /u/zchaarm &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_10ljho9</id><link href="https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/" /><updated>2023-01-26T04:33:05+00:00</updated><published>2023-01-26T04:33:05+00:00</published><title>r/LangChain Lounge</title></entry><entry><author><name>/u/DocBrownMS</name><uri>https://www.reddit.com/user/DocBrownMS</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1boydvr/tds_article_visualize_your_rag_data_evaluate_your/&quot;&gt; &lt;img src=&quot;https://b.thumbs.redditmedia.com/Jis6fbGRMbP27jC_C49qVx52vSIaeHi98xN0x6KNVgU.jpg&quot; alt=&quot;TDS Article: Visualize your RAG Data — Evaluate your Retrieval-Augmented Generation System with Ragas&quot; title=&quot;TDS Article: Visualize your RAG Data — Evaluate your Retrieval-Augmented Generation System with Ragas&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/DocBrownMS&quot;&gt; /u/DocBrownMS &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://towardsdatascience.com/visualize-your-rag-data-evaluate-your-retrieval-augmented-generation-system-with-ragas-fc2486308557&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1boydvr/tds_article_visualize_your_rag_data_evaluate_your/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1boydvr</id><media:thumbnail url="https://b.thumbs.redditmedia.com/Jis6fbGRMbP27jC_C49qVx52vSIaeHi98xN0x6KNVgU.jpg" /><link href="https://www.reddit.com/r/LangChain/comments/1boydvr/tds_article_visualize_your_rag_data_evaluate_your/" /><updated>2024-03-27T10:16:03+00:00</updated><published>2024-03-27T10:16:03+00:00</published><title>TDS Article: Visualize your RAG Data — Evaluate your Retrieval-Augmented Generation System with Ragas</title></entry><entry><author><name>/u/gswithai</name><uri>https://www.reddit.com/user/gswithai</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;&lt;strong&gt;Little announcement!&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;What&amp;#39;s up, everyone?! &lt;/p&gt; &lt;p&gt;I finally uploaded my first YouTube video based on one of my blog posts: &lt;a href=&quot;https://www.youtube.com/watch?v=ubsqSWfXAPI&quot;&gt;https://www.youtube.com/watch?v=ubsqSWfXAPI&lt;/a&gt; &lt;/p&gt; &lt;p&gt;It&amp;#39;s a tutorial about using LangChain&amp;#39;s Output Parsers with GPT to convert the contents of a PDF file to JSON. (&lt;a href=&quot;https://www.gettingstarted.ai/how-to-extract-metadata-from-pdf-convert-to-json-langchain/&quot;&gt;I originally wrote about this on the blog here&lt;/a&gt;). To be honest, I&amp;#39;ve been wanting to publish a video for some time now but finally went for it so I&amp;#39;m not sure what to expect.&lt;/p&gt; &lt;p&gt;I&amp;#39;m still learning about video editing, recording, and YouTube in general but &lt;strong&gt;I&amp;#39;d love to know your feedback (and comments)&lt;/strong&gt; so that I can implement it in future videos.&lt;/p&gt; &lt;p&gt;Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/gswithai&quot;&gt; /u/gswithai &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bp2mao/uploaded_my_first_youtube_video_ever_and_its/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bp2mao/uploaded_my_first_youtube_video_ever_and_its/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bp2mao</id><link href="https://www.reddit.com/r/LangChain/comments/1bp2mao/uploaded_my_first_youtube_video_ever_and_its/" /><updated>2024-03-27T14:01:49+00:00</updated><published>2024-03-27T14:01:49+00:00</published><title>Uploaded my first YouTube video ever and it's about LangChain!</title></entry><entry><author><name>/u/yazanrisheh</name><uri>https://www.reddit.com/user/yazanrisheh</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I have a project where I need to create an assistant that will help my professor which teaches digital circuits at the university. The purpose of the project is that if a student was studying for an exam at 3 AM but needs to understand a question or circuit diagram, they can ask the AI assistant which is basically as good as the professor himself. Ill need to have separate sessions ans save the conversation in a database.&lt;/p&gt; &lt;p&gt;The professor sent me the data which consist of PowerPoint presentations which consists of images and text and those are what I want my LLM to focus on. The LLM should be able to draw a circuit diagram to explain what its showing. &lt;/p&gt; &lt;p&gt;I tried gpt vision but its not showing proper results and the software we use for the circuit diagrams is simulink. I know how to save and store the data in a database and load each user separately and I know how to do rag but my main issue lies in how can I allow my LLM use simulink to design circuits? Do I need to use agemt? If yes, how do I connect it and allow it to design from simulink? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/yazanrisheh&quot;&gt; /u/yazanrisheh &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bp914d/help_with_llm_to_design_circuits/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bp914d/help_with_llm_to_design_circuits/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bp914d</id><link href="https://www.reddit.com/r/LangChain/comments/1bp914d/help_with_llm_to_design_circuits/" /><updated>2024-03-27T18:25:38+00:00</updated><published>2024-03-27T18:25:38+00:00</published><title>Help with LLM to design circuits</title></entry><entry><author><name>/u/commonbik</name><uri>https://www.reddit.com/user/commonbik</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I am relatively new to langchain. I need to load a pdf stored in GCS and I want chunk and embed the contents of the pdf and store it on weviate. I want to then retrive relevant vectors later and user a RetrivalQAChain. How do I do it? Any help or code samples are appreciated! Thanks in advance. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/commonbik&quot;&gt; /u/commonbik &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bp33yk/question_on_querying_pre_stored_vectors_on_weviate/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bp33yk/question_on_querying_pre_stored_vectors_on_weviate/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bp33yk</id><link href="https://www.reddit.com/r/LangChain/comments/1bp33yk/question_on_querying_pre_stored_vectors_on_weviate/" /><updated>2024-03-27T14:23:09+00:00</updated><published>2024-03-27T14:23:09+00:00</published><title>Question on querying pre stored vectors on weviate</title></entry><entry><author><name>/u/Beginning_Rock_1906</name><uri>https://www.reddit.com/user/Beginning_Rock_1906</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi, like the title says I would like to know whether LangGraph works well with all the Claude models? I never tested the function calling abilities of Claude and have no idea if they work well inside the LangGraph framework. Any type of illumination is greatly appreciated.&lt;/p&gt; &lt;p&gt;Thanks in advance.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Beginning_Rock_1906&quot;&gt; /u/Beginning_Rock_1906 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bp7thq/langgraph_with_claude/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bp7thq/langgraph_with_claude/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bp7thq</id><link href="https://www.reddit.com/r/LangChain/comments/1bp7thq/langgraph_with_claude/" /><updated>2024-03-27T17:36:56+00:00</updated><published>2024-03-27T17:36:56+00:00</published><title>LangGraph with Claude?</title></entry><entry><author><name>/u/r_y_r_y</name><uri>https://www.reddit.com/user/r_y_r_y</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I have an aws opensearch vector store. I can connect to it with the opensearch-py library and I can create an index and upload documents to the index. I am using langchain 0.1.13 and langchain-community 0.0.29&lt;/p&gt; &lt;p&gt;here&amp;#39;s my code:&lt;/p&gt; &lt;p&gt;```python from langchain_community.document_loaders import TextLoader&lt;br/&gt; from langchain_text_splitters import CharacterTextSplitter&lt;br/&gt; from langchain_openai import OpenAIEmbeddings&lt;br/&gt; from langchain_community.vectorstores import OpenSearchVectorSearch&lt;/p&gt; &lt;p&gt;documents = TextLoader(&amp;quot;./test.txt&amp;quot;).load() text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0) docs = text_splitter.split_documents(documents)&lt;/p&gt; &lt;p&gt;embeddings = OpenAIEmbeddings() &lt;/p&gt; &lt;p&gt;vector_store = OpenSearchVectorSearch( opensearch_url=f&amp;quot;https://{url}:{port}&amp;quot;, index_name=&amp;quot;test&amp;quot;, embedding_function=embeddings, http_auth=auth, use_ssl = True, verify_certs = False )&lt;/p&gt; &lt;p&gt;vector_store.add_documents(documents=docs) ```&lt;/p&gt; &lt;p&gt;but when I run the below I get no documents back consistently even when the prompt is identical to the content in the index I get nothing back:&lt;/p&gt; &lt;p&gt;&lt;code&gt;python print(vector_store.similarity_search(&amp;quot;what is my name on thursday?&amp;quot;, k=1)) &lt;/code&gt;&lt;/p&gt; &lt;p&gt;I&amp;#39;ve validated that the index names are all correct and the vector fields are all consistently named. I can query the index and see the documents uploading correctly here:&lt;/p&gt; &lt;p&gt;```python from opensearchpy import OpenSearch&lt;/p&gt; &lt;p&gt;client = OpenSearch( hosts=[{&amp;#39;host&amp;#39;: url, &amp;#39;port&amp;#39;: port}], http_auth=auth, use_ssl=True, verify_certs=False )&lt;/p&gt; &lt;p&gt;response = client.search( index=&amp;#39;test&amp;#39;, body={&amp;quot;query&amp;quot;: {&amp;quot;match_all&amp;quot;: {}}}, size=1000 )&lt;/p&gt; &lt;p&gt;documents = [doc[&amp;#39;_source&amp;#39;] for doc in response[&amp;#39;hits&amp;#39;][&amp;#39;hits&amp;#39;]] print(documents) ```&lt;/p&gt; &lt;p&gt;Any thoughts on what the issue might be? Literally any help is appreciated.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/r_y_r_y&quot;&gt; /u/r_y_r_y &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bp6frg/aws_opensearch_vector_not_returning_anything_on/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bp6frg/aws_opensearch_vector_not_returning_anything_on/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bp6frg</id><link href="https://www.reddit.com/r/LangChain/comments/1bp6frg/aws_opensearch_vector_not_returning_anything_on/" /><updated>2024-03-27T16:41:23+00:00</updated><published>2024-03-27T16:41:23+00:00</published><title>AWS OpenSearch Vector not returning anything on similarity search</title></entry><entry><author><name>/u/profepcot</name><uri>https://www.reddit.com/user/profepcot</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;LangChain does a great job as a full framework for LLM-based application development, but there are so many components sometimes you just want a toolkit that is better at a particular piece. I won&amp;#39;t go down the rabbit-hole of &amp;#39;frameworks vs toolkits&amp;#39; in general, but if you&amp;#39;re looking for some of the alternatives to LangChain for pieces of the LLM-application development puzzle here&amp;#39;s a write up: &lt;a href=&quot;https://www.mirascope.io/post/langchain-alternatives&quot;&gt;https://www.mirascope.io/post/langchain-alternatives&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/profepcot&quot;&gt; /u/profepcot &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bp48si/langchain_for_a_full_framework_what_options_for/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bp48si/langchain_for_a_full_framework_what_options_for/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bp48si</id><link href="https://www.reddit.com/r/LangChain/comments/1bp48si/langchain_for_a_full_framework_what_options_for/" /><updated>2024-03-27T15:10:55+00:00</updated><published>2024-03-27T15:10:55+00:00</published><title>LangChain for a full framework - what options for more focused toolkits?</title></entry><entry><author><name>/u/ran2dada</name><uri>https://www.reddit.com/user/ran2dada</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey guys, I have developed a AI chatbot using LangChain, Pinecone, and ChatGPT. You can try it out for free on &lt;a href=&quot;http://hrchatbot.deligence.com/chat&quot;&gt;HR Chatbot&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Let me know what you guys think about it!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/ran2dada&quot;&gt; /u/ran2dada &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bozlzo/hr_chatbot_created_using_langchain_free_to_use/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bozlzo/hr_chatbot_created_using_langchain_free_to_use/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bozlzo</id><link href="https://www.reddit.com/r/LangChain/comments/1bozlzo/hr_chatbot_created_using_langchain_free_to_use/" /><updated>2024-03-27T11:31:22+00:00</updated><published>2024-03-27T11:31:22+00:00</published><title>HR Chatbot created using LangChain, free to use!</title></entry><entry><author><name>/u/sarthak_uchiha</name><uri>https://www.reddit.com/user/sarthak_uchiha</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Can someone please highlight what is meant by prompt injection and what security concerns it may have , and if somebody can provide an example for the same that would be great &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/sarthak_uchiha&quot;&gt; /u/sarthak_uchiha &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bovlcb/prompt_injection/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bovlcb/prompt_injection/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bovlcb</id><link href="https://www.reddit.com/r/LangChain/comments/1bovlcb/prompt_injection/" /><updated>2024-03-27T06:57:09+00:00</updated><published>2024-03-27T06:57:09+00:00</published><title>Prompt injection</title></entry><entry><author><name>/u/o3omoomin</name><uri>https://www.reddit.com/user/o3omoomin</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I implemented RAG using Ensemble Retriever. widh llama index&lt;/p&gt; &lt;p&gt;Before using the prompt template module, if sent a query like “Hello”, llm would not respond because the query did not exist in the document. &lt;/p&gt; &lt;p&gt;And were able to solve these problems by using the prompt template module. &lt;/p&gt; &lt;p&gt;How important is prompt template engineering? And what should I do to set up prompt template engineering well? &lt;/p&gt; &lt;p&gt;Below is the prompt template I wrote&lt;/p&gt; &lt;pre&gt;&lt;code&gt;template = &amp;#39;&amp;#39;&amp;#39; You are a chatbot fluent in Korean developed specifically for our company. Your primary role is to communicate with users by answering their questions in Korean and providing feedback related to their questions. Your job is to provide the user with an answer regarding your company&amp;#39;s employment rules when asked {query}. If you are asked a {query} question that is not related to the Company&amp;#39;s employment rules, it is your responsibility to redirect the conversation to a topic related to the Company&amp;#39;s policies and guidelines. You can also recommend questions to users based on your knowledge. We encourage our users to ask questions that are directly related to our company&amp;#39;s operations, culture, or the specific guidelines we follow. &amp;#39;&amp;#39;&amp;#39; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/o3omoomin&quot;&gt; /u/o3omoomin &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bosw8h/how_to_implement_prompt_engineering_well/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bosw8h/how_to_implement_prompt_engineering_well/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bosw8h</id><link href="https://www.reddit.com/r/LangChain/comments/1bosw8h/how_to_implement_prompt_engineering_well/" /><updated>2024-03-27T04:08:46+00:00</updated><published>2024-03-27T04:08:46+00:00</published><title>How to implement prompt engineering well?</title></entry><entry><author><name>/u/Far_Possibility_6278</name><uri>https://www.reddit.com/user/Far_Possibility_6278</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hello,&lt;/p&gt; &lt;p&gt;I have implemented a RAG for retrieval on a postgres db. First, using embeddings I get the k-most relevant document and then using the load_summarize_chain from Langchain I create a response as in this &lt;a href=&quot;https://cloud.google.com/blog/products/databases/using-pgvector-llms-and-langchain-with-google-cloud-databases/&quot;&gt;tutorial&lt;/a&gt;. My first question is that how can I use LCEL to make the chain. Lastly, how can I evaluate the response?&lt;/p&gt; &lt;p&gt;I tried using the &lt;em&gt;DeepEvalAnswerRelevancyEvaluator&lt;/em&gt; as shown &lt;a href=&quot;https://docs.confident-ai.com/docs/integrations-llamaindex&quot;&gt;here&lt;/a&gt; but I am getting the following error&lt;/p&gt; &lt;p&gt;&lt;code&gt;ERROR: AttributeError: &amp;#39;Document&amp;#39; object has no attribute &amp;#39;get_content&amp;#39;&lt;/code&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Far_Possibility_6278&quot;&gt; /u/Far_Possibility_6278 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1boxuci/lcel_and_evaluation_on_a_rag_pipeline/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1boxuci/lcel_and_evaluation_on_a_rag_pipeline/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1boxuci</id><link href="https://www.reddit.com/r/LangChain/comments/1boxuci/lcel_and_evaluation_on_a_rag_pipeline/" /><updated>2024-03-27T09:38:59+00:00</updated><published>2024-03-27T09:38:59+00:00</published><title>LCEL and evaluation on a RAG pipeline</title></entry><entry><author><name>/u/qa_anaaq</name><uri>https://www.reddit.com/user/qa_anaaq</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi. I have a long prompt of about 500-700 tokens, and context gets added also since it&amp;#39;s a RAG, so what I send to the LLM could easily be a few thousand tokens per question. &lt;/p&gt; &lt;p&gt;I am trying to get a response that is conversational and that roughly follows a format like :&lt;/p&gt; &lt;p&gt;&amp;quot;To do x, y, and Z, you need to complete these steps.....blah blah.....For more info, please refer to this video.&amp;quot;&lt;/p&gt; &lt;p&gt;It returns good info but I can&amp;#39;t for the life of me get it to return consistently structured responses. I&amp;#39;ve tried CoT, but it keeps stating it&amp;#39;s reasoning, which makes it sound like the robot it is. I just want it to end with &amp;quot;for more info..&amp;quot; Or &amp;quot;if you need more help..&amp;quot; Etc. &lt;/p&gt; &lt;p&gt;So I&amp;#39;m thinking few-shot, but I don&amp;#39;t know if I should put the examples before or after the retrieved context given the length of the context and the prompt, in general. &lt;/p&gt; &lt;p&gt;Has anyone experimented in these conditions?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/qa_anaaq&quot;&gt; /u/qa_anaaq &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1boqup2/long_prompt_and_fewshot_placement/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1boqup2/long_prompt_and_fewshot_placement/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1boqup2</id><link href="https://www.reddit.com/r/LangChain/comments/1boqup2/long_prompt_and_fewshot_placement/" /><updated>2024-03-27T02:27:30+00:00</updated><published>2024-03-27T02:27:30+00:00</published><title>Long prompt and few-shot placement</title></entry><entry><author><name>/u/Delicious_Success303</name><uri>https://www.reddit.com/user/Delicious_Success303</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bowtfo/chroma_db_throws_error_at_2nd_run/&quot;&gt; &lt;img src=&quot;https://b.thumbs.redditmedia.com/kohHHDBwWGMw1n8j9QbkJ_qZ12rhsOQUDWZ0bMhUf9U.jpg&quot; alt=&quot;Chroma db throws error at 2nd run&quot; title=&quot;Chroma db throws error at 2nd run&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi!!&lt;br/&gt; I am using chroma db with pre computed embeddings for my rag application. Chroma runs well first time but every time i re run my notebook i get this error. Thanks :)&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/aqq8insu6uqc1.png?width=3000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=51adc2a358e349fdbe3c211ed869d0f79c1bb986&quot;&gt;https://preview.redd.it/aqq8insu6uqc1.png?width=3000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=51adc2a358e349fdbe3c211ed869d0f79c1bb986&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Delicious_Success303&quot;&gt; /u/Delicious_Success303 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bowtfo/chroma_db_throws_error_at_2nd_run/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bowtfo/chroma_db_throws_error_at_2nd_run/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1bowtfo</id><media:thumbnail url="https://b.thumbs.redditmedia.com/kohHHDBwWGMw1n8j9QbkJ_qZ12rhsOQUDWZ0bMhUf9U.jpg" /><link href="https://www.reddit.com/r/LangChain/comments/1bowtfo/chroma_db_throws_error_at_2nd_run/" /><updated>2024-03-27T08:23:22+00:00</updated><published>2024-03-27T08:23:22+00:00</published><title>Chroma db throws error at 2nd run</title></entry><entry><author><name>/u/Ill_Bodybuilder3499</name><uri>https://www.reddit.com/user/Ill_Bodybuilder3499</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;my go to tool for RAG Evaluation has been RAGAS so far, but i am not always happy with rhe results. Now I saw a video about GISKARD evaluation which also looks promising.&lt;/p&gt; &lt;p&gt;So my question: has anybody some experiene wirh Giskard or both, and what do you like better?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Ill_Bodybuilder3499&quot;&gt; /u/Ill_Bodybuilder3499 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1boiq77/ragas_vs_giskard_rag_evaluation/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1boiq77/ragas_vs_giskard_rag_evaluation/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1boiq77</id><link href="https://www.reddit.com/r/LangChain/comments/1boiq77/ragas_vs_giskard_rag_evaluation/" /><updated>2024-03-26T20:51:13+00:00</updated><published>2024-03-26T20:51:13+00:00</published><title>RAGAS vs GISKARD RAG Evaluation</title></entry><entry><author><name>/u/Not-That-rpg</name><uri>https://www.reddit.com/user/Not-That-rpg</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;It seems like the way langchain is set up, extra `model_kwargs` are just quietly thrown on the floor. I think that happens in the pydantic validation process, but I am not sure.&lt;/p&gt; &lt;p&gt;Is there any way to tell what&amp;#39;s happening with the `model_kwargs`?&lt;/p&gt; &lt;p&gt;For example, &lt;a href=&quot;https://github.com/langchain-ai/langchain/issues/10590&quot;&gt;this langchain issue&lt;/a&gt; discusses what happens when a model class ignores the `n_ctx` kwarg, and suggests a fix based on editing the `validate_environment` method of a class. But this requires identifying the right `validate_environment` method in the class hierarchy, and then modifying a local variable (really a constant, `model_param_names` -- why isn&amp;#39;t this a property of the class instead of a local variable?).&lt;/p&gt; &lt;p&gt;Is there some way to trace the execution of an LLM constructor and see which kwargs are actually sent, and which are ignored?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Not-That-rpg&quot;&gt; /u/Not-That-rpg &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bopm2t/how_can_we_tell_which_llm_arguments_are_being/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bopm2t/how_can_we_tell_which_llm_arguments_are_being/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bopm2t</id><link href="https://www.reddit.com/r/LangChain/comments/1bopm2t/how_can_we_tell_which_llm_arguments_are_being/" /><updated>2024-03-27T01:30:17+00:00</updated><published>2024-03-27T01:30:17+00:00</published><title>How can we tell which LLM arguments are being processed?</title></entry><entry><author><name>/u/realsharaf</name><uri>https://www.reddit.com/user/realsharaf</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;m using Firestore for storage and I already built a solution to store group chats with the following structure:&lt;/p&gt; &lt;p&gt;--&amp;gt; chats &amp;lt;collection&amp;gt;&lt;/p&gt; &lt;p&gt;----&amp;gt; chat_doc#1 &amp;lt;well, a doc&amp;gt;&lt;/p&gt; &lt;p&gt;----&amp;gt; chat_doc#2&lt;/p&gt; &lt;p&gt;----&amp;gt; chat_doc#n&lt;/p&gt; &lt;p&gt;------&amp;gt; participants &amp;lt;array with user ids&amp;gt;&lt;/p&gt; &lt;p&gt;------&amp;gt; messages &amp;lt;collection to store messages, each message in a doc&amp;gt;&lt;/p&gt; &lt;p&gt;Now, I&amp;#39;m looking to integrate LangChain into my app (easier to work with multiple LLM providers) and so I have to restructure my code. &lt;/p&gt; &lt;p&gt;I used `FirestoreChatMessageHistory` and noticed that their structure is similar to mine, but in each chat doc they only have `userId` which holds a single user id. I want to replace that with a `participants` array or anything that can hold multiple user ids. But I noticed there&amp;#39;s no way to customize the `FirestoreChatMessageHistory` class to include multiple participants.&lt;/p&gt; &lt;p&gt;The way I&amp;#39;m thinking is to use the `userId` prop to store all user ids in comma-separated format, but:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;I don&amp;#39;t know if that will work well (or at all) &amp;amp; I don&amp;#39;t have the time to test this out if it&amp;#39;ll take too much time because of the overhead, and&lt;/li&gt; &lt;li&gt;there&amp;#39;s way too much overhead - it feels stupidly irritating and I feel that there should be a neater way to implement this.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Anyone face this issue? How did you solve it?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/realsharaf&quot;&gt; /u/realsharaf &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bopi12/how_to_store_group_chats/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bopi12/how_to_store_group_chats/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bopi12</id><link href="https://www.reddit.com/r/LangChain/comments/1bopi12/how_to_store_group_chats/" /><updated>2024-03-27T01:25:14+00:00</updated><published>2024-03-27T01:25:14+00:00</published><title>How to store group chats???</title></entry><entry><author><name>/u/imhimu_</name><uri>https://www.reddit.com/user/imhimu_</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/imhimu_&quot;&gt; /u/imhimu_ &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bon5sl/is_there_any_available_resourse_for_async/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bon5sl/is_there_any_available_resourse_for_async/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bon5sl</id><link href="https://www.reddit.com/r/LangChain/comments/1bon5sl/is_there_any_available_resourse_for_async/" /><updated>2024-03-26T23:45:20+00:00</updated><published>2024-03-26T23:45:20+00:00</published><title>is there any available resourse for async langchain rag agents with hugginface models</title></entry><entry><author><name>/u/Inevitable-Worth-861</name><uri>https://www.reddit.com/user/Inevitable-Worth-861</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Sorry if this is obvious to most of you, I’m somewhat new. &lt;/p&gt; &lt;p&gt;My app is currently successfully, in order: - Getting URL content with document loader - Getting file content from a GitHub codebase - Sending this to OpenAi model with a prompt - Serving the response to my front end&lt;/p&gt; &lt;p&gt;The issue is around token limitation. &lt;/p&gt; &lt;p&gt;I would love to be able to get the whole codebase along with the reference URL content processed by the LLM and eventually return a single response to my frontend, of course I hit the token limit fast. &lt;/p&gt; &lt;p&gt;I understand that text splitting and vector stores are the solution, I’ll get my head around that.&lt;/p&gt; &lt;p&gt;My question is, will I end up sending many requests and get many responses from the LLM?&lt;/p&gt; &lt;p&gt;I don’t mind many requests, but I don’t understand how I’ll get a single coherent response to my friend if it’s multiple LLM responses.&lt;/p&gt; &lt;p&gt;Maybe I’m missing something obvious here. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Inevitable-Worth-861&quot;&gt; /u/Inevitable-Worth-861 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bon41r/codebase_external_url_context_for_llm/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bon41r/codebase_external_url_context_for_llm/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bon41r</id><link href="https://www.reddit.com/r/LangChain/comments/1bon41r/codebase_external_url_context_for_llm/" /><updated>2024-03-26T23:43:21+00:00</updated><published>2024-03-26T23:43:21+00:00</published><title>Codebase + External URL Context for LLM</title></entry><entry><author><name>/u/mehul_gupta1997</name><uri>https://www.reddit.com/user/mehul_gupta1997</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Checkout this demo to understand autogen, a Multi-Agent Orchestration python package supporting AI Agents conversations using HuggingFace models. &lt;a href=&quot;https://youtu.be/NY4_jhPcicw?si=IV29lMJcQ8rvWVij&quot;&gt;https://youtu.be/NY4_jhPcicw?si=IV29lMJcQ8rvWVij&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/mehul_gupta1997&quot;&gt; /u/mehul_gupta1997 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bo5zq9/multiagent_conversation_using_autogen_and/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bo5zq9/multiagent_conversation_using_autogen_and/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bo5zq9</id><link href="https://www.reddit.com/r/LangChain/comments/1bo5zq9/multiagent_conversation_using_autogen_and/" /><updated>2024-03-26T11:53:10+00:00</updated><published>2024-03-26T11:53:10+00:00</published><title>Multi-Agent Conversation using AutoGen and HuggingFace models</title></entry><entry><author><name>/u/uygarsci</name><uri>https://www.reddit.com/user/uygarsci</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey guys, I just created a video on how to build chatbots with memory.&lt;/p&gt; &lt;p&gt;Most of the examples on langchains website are with open api. I wanted to keep things open source so ise llama from huggingface.&lt;/p&gt; &lt;p&gt;Hope you like it. Any feedback is welcome.&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://youtu.be/gNXBp3wttFU?si=GirGTqe7ThEUFSrx&quot;&gt;https://youtu.be/gNXBp3wttFU?si=GirGTqe7ThEUFSrx&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/uygarsci&quot;&gt; /u/uygarsci &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bojn3b/my_video_on_memory_chatbots/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bojn3b/my_video_on_memory_chatbots/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bojn3b</id><link href="https://www.reddit.com/r/LangChain/comments/1bojn3b/my_video_on_memory_chatbots/" /><updated>2024-03-26T21:26:39+00:00</updated><published>2024-03-26T21:26:39+00:00</published><title>My Video on Memory Chatbots</title></entry><entry><author><name>/u/Mediocre-Card8046</name><uri>https://www.reddit.com/user/Mediocre-Card8046</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;I don&amp;#39;t get it to run. I want to stream a LCEL RAG chain response using FastAPI. So my question would be how to stream a LCEL chain response and how can I return the whole response, so the dict with the &amp;quot;answer&amp;quot; and &amp;quot;docs&amp;quot; keys, where the retrieved docs are inside?&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;Here is the code for my chain:&lt;/p&gt; &lt;p&gt;&lt;code&gt;from langchain_community.vectorstores import FAISS&lt;/code&gt;&lt;br/&gt; &lt;code&gt;from langchain_community.embeddings import HuggingFaceEmbeddings&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;from langchain_core.prompts import PromptTemplate, ChatPromptTemplate&lt;/code&gt;&lt;br/&gt; &lt;code&gt;from langchain_core.runnables import RunnableParallel&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;from operator import itemgetter&lt;/code&gt;&lt;br/&gt; &lt;code&gt;from typing import TypedDict&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;from fastapi import FastAPI, File, UploadFile, HTTPException, Path, Query&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;embeddings = HuggingFaceEmbeddings(model_name=&amp;quot;intfloat/multilingual-e5-large-instruct&amp;quot;, model_kwargs={&amp;#39;device&amp;#39;: &amp;quot;mps&amp;quot;})&lt;/code&gt;&lt;br/&gt; &lt;code&gt;db = FAISS.load_local(&amp;quot;streamlit_vectorstores/vectorstores/db_maxiw_testfreitag&amp;quot;, embeddings, allow_dangerous_deserialization=True)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;llm = build_llm(model_path)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;template = &amp;quot;&amp;quot;&amp;quot;&lt;/code&gt;&lt;br/&gt; &lt;code&gt;Beantworte die Frage ausschließlich basierend auf folgenden Kontext auf Deutsch:&lt;/code&gt;&lt;br/&gt; &lt;code&gt;{context}&lt;/code&gt;&lt;br/&gt; &lt;code&gt;Frage: {question}&lt;/code&gt;&lt;br/&gt; &lt;code&gt;&amp;quot;&amp;quot;&amp;quot;&lt;/code&gt;&lt;br/&gt; &lt;code&gt;ANSWER_PROMPT = ChatPromptTemplate.from_template(template)&lt;/code&gt;&lt;br/&gt; &lt;code&gt;class RagInput(TypedDict):&lt;/code&gt;&lt;br/&gt; &lt;code&gt;question: str&lt;/code&gt; &lt;/p&gt; &lt;p&gt;&lt;code&gt;final_chain = (&lt;/code&gt;&lt;br/&gt; &lt;code&gt;RunnableParallel(&lt;/code&gt;&lt;br/&gt; &lt;code&gt;context=(itemgetter(&amp;quot;question&amp;quot;) | db.as_retriever()),&lt;/code&gt;&lt;br/&gt; &lt;code&gt;question=itemgetter(&amp;quot;question&amp;quot;)&lt;/code&gt;&lt;br/&gt; &lt;code&gt;) |&lt;/code&gt;&lt;br/&gt; &lt;code&gt;RunnableParallel(&lt;/code&gt;&lt;br/&gt; &lt;code&gt;answer=(ANSWER_PROMPT | llm),&lt;/code&gt;&lt;br/&gt; &lt;code&gt;docs=itemgetter(&amp;quot;context&amp;quot;)&lt;/code&gt;&lt;br/&gt; &lt;code&gt;)&lt;/code&gt;&lt;br/&gt; &lt;code&gt;).with_types(input_type=RagInput)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;&lt;code&gt;And here my get-endpoint which does not work:&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;u/app.get(&amp;quot;/rag_lcel/&amp;quot;)&lt;/code&gt;&lt;br/&gt; &lt;code&gt;async def fastapi_stream(question: str):&lt;/code&gt;&lt;br/&gt; &lt;code&gt;start_time = time.time()&lt;/code&gt;&lt;br/&gt; &lt;code&gt;first_response = True&lt;/code&gt;&lt;br/&gt; &lt;code&gt;for resp in final_chain.astream({&amp;quot;question&amp;quot;: question}):&lt;/code&gt;&lt;br/&gt; &lt;code&gt;if resp and first_response:&lt;/code&gt;&lt;br/&gt; &lt;code&gt;# Calculate and print time after the first batch of text is streamed&lt;/code&gt;&lt;br/&gt; &lt;code&gt;end_time = time.time()&lt;/code&gt;&lt;br/&gt; &lt;code&gt;elapsed_time = round(end_time - start_time, 1)&lt;/code&gt;&lt;br/&gt; &lt;code&gt;first_response = False&lt;/code&gt;&lt;br/&gt; &lt;code&gt;yield f&amp;quot;(Response Time: {elapsed_time} seconds)\n&amp;quot;&lt;/code&gt;&lt;br/&gt; &lt;code&gt;yield resp&lt;/code&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Mediocre-Card8046&quot;&gt; /u/Mediocre-Card8046 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bo2k4r/how_to_stream_lcel_chain_with_fastapi_and_return/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bo2k4r/how_to_stream_lcel_chain_with_fastapi_and_return/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bo2k4r</id><link href="https://www.reddit.com/r/LangChain/comments/1bo2k4r/how_to_stream_lcel_chain_with_fastapi_and_return/" /><updated>2024-03-26T08:14:27+00:00</updated><published>2024-03-26T08:14:27+00:00</published><title>How to Stream LCEL Chain with FastAPI and return source docs?</title></entry><entry><author><name>/u/Ahmad_AM0</name><uri>https://www.reddit.com/user/Ahmad_AM0</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I want to build RAG system with a chatbot using an LLM with LangChain framework.&lt;/p&gt; &lt;p&gt;The problem is that I don&amp;#39;t have enough resources to run an LLM locally. I guess I have two options:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Looking for a free API that give good results&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Finding a free server-like platform to host an LLM&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;For option 1, do you know some good APIs ?&lt;/p&gt; &lt;p&gt;For option 2, I thought of using Google Colab as a server but wasn&amp;#39;t able to exchange messages between my local machine and Colab. I tried ngrok library but it didn&amp;#39;t work. Is there any solution ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Ahmad_AM0&quot;&gt; /u/Ahmad_AM0 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bo8d5h/not_enough_resources_to_run_an_llm_locally/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bo8d5h/not_enough_resources_to_run_an_llm_locally/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bo8d5h</id><link href="https://www.reddit.com/r/LangChain/comments/1bo8d5h/not_enough_resources_to_run_an_llm_locally/" /><updated>2024-03-26T13:49:26+00:00</updated><published>2024-03-26T13:49:26+00:00</published><title>Not enough resources to run an LLM locally</title></entry><entry><author><name>/u/esraaatmeh</name><uri>https://www.reddit.com/user/esraaatmeh</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;how I can stream agent output using langchain when I use mistralai/Mistral-7B-Instruct-v0.2?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/esraaatmeh&quot;&gt; /u/esraaatmeh &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bo30or/stream_open_source_llm/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bo30or/stream_open_source_llm/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bo30or</id><link href="https://www.reddit.com/r/LangChain/comments/1bo30or/stream_open_source_llm/" /><updated>2024-03-26T08:46:37+00:00</updated><published>2024-03-26T08:46:37+00:00</published><title>stream open source LLM</title></entry><entry><author><name>/u/cryptokaykay</name><uri>https://www.reddit.com/user/cryptokaykay</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bnkvtv/update_langtrace_preview_opensource_llm/&quot;&gt; &lt;img src=&quot;https://b.thumbs.redditmedia.com/hCblHithvJSr2WsruX__shRW1xQxHRLpBDMO1YrayfM.jpg&quot; alt=&quot;Update: Langtrace Preview: Opensource LLM monitoring tool - achieving better cardinality compared to Langsmith.&quot; title=&quot;Update: Langtrace Preview: Opensource LLM monitoring tool - achieving better cardinality compared to Langsmith.&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;This is a follow up for: &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1b6phov/update_langtrace_preview_an_opensource_llm/&quot;&gt;https://www.reddit.com/r/LangChain/comments/1b6phov/update_langtrace_preview_an_opensource_llm/&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Thought of sharing what I am cooking. Basically, I am building a open source LLM monitoring and evaluation suite. It works like this:&lt;br/&gt; 1. Install the SDK with 2 lines of code (npm i or pip install)&lt;br/&gt; 2. The SDK will start shipping traces in Open telemetry standard format to the UI&lt;br/&gt; 3. See the metrics, traces and prompts in the UI(Attaching some screenshots below). &lt;/p&gt; &lt;p&gt;I am mostly optimizing the features for 3 main metrics&lt;br/&gt; 1. Usage - token/cost&lt;br/&gt; 2. Accuracy - Manually evaluate traced prompt-response pairs from the UI and see the accuracy score&lt;br/&gt; 3. Latency - speed of responses/time to first token &lt;/p&gt; &lt;p&gt;Vendors supported for the first version:&lt;br/&gt; Langchain, LlamaIndex, OpenAI, Anthropic, Pinecone, ChromaDB &lt;/p&gt; &lt;p&gt;I will opensource this project in about a week and share the repo here.&lt;/p&gt; &lt;p&gt;Please let me know what else you would like to see or what other challenges you face that can be solved through this project.&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/zwz0lqcfwiqc1.png?width=2978&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=90caa5f52e47503493e4417b6808d7f12739f2d3&quot;&gt;https://preview.redd.it/zwz0lqcfwiqc1.png?width=2978&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=90caa5f52e47503493e4417b6808d7f12739f2d3&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/cvv6aqcfwiqc1.png?width=3000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e8374335d6e5b5a7ff04f1ea1408f74f9dce1698&quot;&gt;https://preview.redd.it/cvv6aqcfwiqc1.png?width=3000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e8374335d6e5b5a7ff04f1ea1408f74f9dce1698&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/cryptokaykay&quot;&gt; /u/cryptokaykay &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bnkvtv/update_langtrace_preview_opensource_llm/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bnkvtv/update_langtrace_preview_opensource_llm/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1bnkvtv</id><media:thumbnail url="https://b.thumbs.redditmedia.com/hCblHithvJSr2WsruX__shRW1xQxHRLpBDMO1YrayfM.jpg" /><link href="https://www.reddit.com/r/LangChain/comments/1bnkvtv/update_langtrace_preview_opensource_llm/" /><updated>2024-03-25T18:25:33+00:00</updated><published>2024-03-25T18:25:33+00:00</published><title>Update: Langtrace Preview: Opensource LLM monitoring tool - achieving better cardinality compared to Langsmith.</title></entry><entry><author><name>/u/Xx_K3v1n5pac3y_xX</name><uri>https://www.reddit.com/user/Xx_K3v1n5pac3y_xX</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bo2hpi/bug_with_structuredchatzeroshotreactdescription/&quot;&gt; &lt;img src=&quot;https://b.thumbs.redditmedia.com/Svg_4TrZDGNAnk_sCBYnMomrUemu_E06S0KbRYcKuFs.jpg&quot; alt=&quot;Bug with structured-chat-zero-shot-react-description agent&quot; title=&quot;Bug with structured-chat-zero-shot-react-description agent&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;When conducting tests, the agent is able to determine the correct tool and parse the prompt to the correct inputs. However, often times the agent will finish the chain prematurely without making any observations. &lt;/p&gt; &lt;p&gt;In my tests, all the prompts have been kept the same and temperature=0. &lt;/p&gt; &lt;p&gt;I noticed if the agent were to successfully make an observation based on the used tool, it will display this on the terminal.&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/8e70hpm6zmqc1.png?width=506&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=07df769dc1b062a68c70c61d799ef009facf96a9&quot;&gt;https://preview.redd.it/8e70hpm6zmqc1.png?width=506&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=07df769dc1b062a68c70c61d799ef009facf96a9&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Otherwise if it was going the end the chain prematurely it would look like this.&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/gft49igfzmqc1.png?width=532&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2ab5f9d08f68c8c62bebe4565f35142f95055617&quot;&gt;https://preview.redd.it/gft49igfzmqc1.png?width=532&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2ab5f9d08f68c8c62bebe4565f35142f95055617&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Pardon me if I lack the understanding to solve this, but I am kinda at wits end. Thanks in advance.&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Xx_K3v1n5pac3y_xX&quot;&gt; /u/Xx_K3v1n5pac3y_xX &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bo2hpi/bug_with_structuredchatzeroshotreactdescription/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bo2hpi/bug_with_structuredchatzeroshotreactdescription/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1bo2hpi</id><media:thumbnail url="https://b.thumbs.redditmedia.com/Svg_4TrZDGNAnk_sCBYnMomrUemu_E06S0KbRYcKuFs.jpg" /><link href="https://www.reddit.com/r/LangChain/comments/1bo2hpi/bug_with_structuredchatzeroshotreactdescription/" /><updated>2024-03-26T08:09:59+00:00</updated><published>2024-03-26T08:09:59+00:00</published><title>Bug with structured-chat-zero-shot-react-description agent</title></entry></feed>