<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/"><category term="LangChain" label="r/LangChain"/><updated>2024-05-02T10:39:15+00:00</updated><icon>https://www.redditstatic.com/icon.png/</icon><id>/r/LangChain.rss</id><link rel="self" href="https://www.reddit.com/r/LangChain.rss" type="application/atom+xml" /><link rel="alternate" href="https://www.reddit.com/r/LangChain" type="text/html" /><subtitle>LangChain is an open-source framework and developer toolkit that helps developers get LLM applications from prototype to production. It is available for Python and Javascript at https://www.langchain.com/.</subtitle><title>LangChain</title><entry><author><name>/u/zchaarm</name><uri>https://www.reddit.com/user/zchaarm</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;A place for members of &lt;a href=&quot;/r/LangChain&quot;&gt;r/LangChain&lt;/a&gt; to chat with each other&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/zchaarm&quot;&gt; /u/zchaarm &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_10ljho9</id><link href="https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/" /><updated>2023-01-26T04:33:05+00:00</updated><published>2023-01-26T04:33:05+00:00</published><title>r/LangChain Lounge</title></entry><entry><author><name>/u/aryanmadhavverma</name><uri>https://www.reddit.com/user/aryanmadhavverma</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I have an agent with two tools. The tools are being used in a sequential way. The second tool queries the database and returns in a pydantic format I&amp;#39;ve defined myself. Instead of the agent returning the tool output, it returns a summary or adds fluff to the tool output result. I only want it to return the tool output! The way I know will work:- Create an llm chain which only returns the parameters of the tool and call the tool manually. But this reduces the agentic behaviour of my functionality. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;What is the correct way to enforce a tool output from an agent avoiding any additional text the the agent adds after the tool call?&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/aryanmadhavverma&quot;&gt; /u/aryanmadhavverma &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cibpk9/correct_way_to_return_tool_output_of_an_agent/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cibpk9/correct_way_to_return_tool_output_of_an_agent/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cibpk9</id><link href="https://www.reddit.com/r/LangChain/comments/1cibpk9/correct_way_to_return_tool_output_of_an_agent/" /><updated>2024-05-02T09:55:29+00:00</updated><published>2024-05-02T09:55:29+00:00</published><title>Correct way to return tool output of an agent executor instance?</title></entry><entry><author><name>/u/mehul_gupta1997</name><uri>https://www.reddit.com/user/mehul_gupta1997</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/mehul_gupta1997&quot;&gt; /u/mehul_gupta1997 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;/r/ArtificialInteligence/comments/1ci6vod/google_gemini_api_key_for_free/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ci6we7/google_gemini_api_key_for_free/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1ci6we7</id><link href="https://www.reddit.com/r/LangChain/comments/1ci6we7/google_gemini_api_key_for_free/" /><updated>2024-05-02T04:37:02+00:00</updated><published>2024-05-02T04:37:02+00:00</published><title>Google Gemini API key for free</title></entry><entry><author><name>/u/loczngo</name><uri>https://www.reddit.com/user/loczngo</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi y&amp;#39;all i was wondering, are there any other alternatives i could do my research on to stream the conversation between me and the LLMs such as Streamlit? i wanna stream the conversation using my own design on NodeJS and i still haven&amp;#39;t figured out which way to integrate the LLMs conversation with my UI.&lt;/p&gt; &lt;p&gt;Thanks for any help or insights y&amp;#39;all will give &amp;lt;3&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/loczngo&quot;&gt; /u/loczngo &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cia5g2/streamlit_referrences_for_nodejs/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cia5g2/streamlit_referrences_for_nodejs/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cia5g2</id><link href="https://www.reddit.com/r/LangChain/comments/1cia5g2/streamlit_referrences_for_nodejs/" /><updated>2024-05-02T08:03:14+00:00</updated><published>2024-05-02T08:03:14+00:00</published><title>Streamlit referrences for NodeJS</title></entry><entry><author><name>/u/krschacht</name><uri>https://www.reddit.com/user/krschacht</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;m an experienced engineer and have been doing a lot of work interacting directly with LLM APIs (using simple SDKs). Multiple people have told me to check out langchain, so I just did a spike on it. I skimmed the docs, I get the core concept of chains and agents. It&amp;#39;s cool but this seems like a set of pretty basic abstractions. But I&amp;#39;m scratching my head wondering: what about langchain are people finding most helpful? Given how popular this library is, I feel like I&amp;#39;m missing something key...&lt;/p&gt; &lt;p&gt;I&amp;#39;m not trying to be snarky at all. I am assuming that I probably should be using LangChain and it probably could be saving me a bunch of time, so I genuinely want to grasp the biggest benefits of it since I don&amp;#39;t think I&amp;#39;m getting it.&lt;/p&gt; &lt;p&gt;Maybe the core problem is that we all inevitably end up using multiple LLMs eventually (OpenAI, Anthropic, etc) so the biggest benefit of LangChain is that you have a sort of universal SDK — a common interface between all the LLMs. Is that the biggest benefit of langchain?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/krschacht&quot;&gt; /u/krschacht &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1chpywv/what_makes_langchain_so_useful_im_new_to_it_and/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1chpywv/what_makes_langchain_so_useful_im_new_to_it_and/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1chpywv</id><link href="https://www.reddit.com/r/LangChain/comments/1chpywv/what_makes_langchain_so_useful_im_new_to_it_and/" /><updated>2024-05-01T16:09:37+00:00</updated><published>2024-05-01T16:09:37+00:00</published><title>What makes langchain so useful? I'm new to it and don't get it</title></entry><entry><author><name>/u/WompTune</name><uri>https://www.reddit.com/user/WompTune</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi everyone,&lt;br/&gt; Was wondering if anyone has moved from Assistants API to Langchain + Langsmith and how that felt?&lt;br/&gt; I loveeee OpenAI Assistants API because it manages conversation history + context for me, and has the dashboard to see messages in the thread. &lt;/p&gt; &lt;p&gt;But unfortunately OpenAI has been super slow lately...&lt;/p&gt; &lt;p&gt;So I was wondering if Langchain (with an open source model like Llama 3) + Langsmith gives an equivalent vibe where I don&amp;#39;t have to manage conversation history / context management myself?&lt;/p&gt; &lt;p&gt;Appreciate it!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/WompTune&quot;&gt; /u/WompTune &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ci1umo/moving_from_openai_assistants_api_to_langchain/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ci1umo/moving_from_openai_assistants_api_to_langchain/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1ci1umo</id><link href="https://www.reddit.com/r/LangChain/comments/1ci1umo/moving_from_openai_assistants_api_to_langchain/" /><updated>2024-05-02T00:19:59+00:00</updated><published>2024-05-02T00:19:59+00:00</published><title>Moving from OpenAI Assistants API to Langchain + Langsmith?</title></entry><entry><author><name>/u/kalintsov</name><uri>https://www.reddit.com/user/kalintsov</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hello,&lt;/p&gt; &lt;p&gt;I have a CSV file containing a historical log of my conversations with my partner. The file is organized into three columns: datetime, sent_by, and message. I would like to use a LLM to ask questions about our discussions (e.g &amp;quot;When is the wedding of A and B?&amp;quot;).&lt;/p&gt; &lt;p&gt;I&amp;#39;m looking for some advices on the most effective way to process and vectorize these conversations. I want the LLM to understand the metadata within the context of the discussions—for instance, identifying that if Person A wrote &amp;quot;Happy Birthday,&amp;quot; it likely indicates Person B&amp;#39;s birthday on that date.&lt;/p&gt; &lt;p&gt;What do you think is the best approach to handling chat logs in this scenario? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/kalintsov&quot;&gt; /u/kalintsov &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ci8hwh/efficient_rag_on_chat_logs/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ci8hwh/efficient_rag_on_chat_logs/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1ci8hwh</id><link href="https://www.reddit.com/r/LangChain/comments/1ci8hwh/efficient_rag_on_chat_logs/" /><updated>2024-05-02T06:13:01+00:00</updated><published>2024-05-02T06:13:01+00:00</published><title>Efficient RAG on chat logs</title></entry><entry><author><name>/u/Original_Job6327</name><uri>https://www.reddit.com/user/Original_Job6327</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Let’s say i’m using openai gpt3.5. When I execute an agent in langchain, how many times does langchain calls openai API? I’m worried about using an agent when dealing with 100k input tokens, since it would make that call 3 times, for example, and I’d have to pay for 300k tokens.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Original_Job6327&quot;&gt; /u/Original_Job6327 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ci7hqx/how_many_api_calls_does_an_agent_make_for_a/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ci7hqx/how_many_api_calls_does_an_agent_make_for_a/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1ci7hqx</id><link href="https://www.reddit.com/r/LangChain/comments/1ci7hqx/how_many_api_calls_does_an_agent_make_for_a/" /><updated>2024-05-02T05:10:46+00:00</updated><published>2024-05-02T05:10:46+00:00</published><title>How many API calls does an agent make for a single input?</title></entry><entry><author><name>/u/ThickDoctor007</name><uri>https://www.reddit.com/user/ThickDoctor007</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Has anyone successfully implemented function calling with agents based on open source LLMs?&lt;/p&gt; &lt;p&gt;Would be glad to learn about your experiences.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/ThickDoctor007&quot;&gt; /u/ThickDoctor007 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ci6afc/function_calling_with_open_source_llms/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ci6afc/function_calling_with_open_source_llms/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1ci6afc</id><link href="https://www.reddit.com/r/LangChain/comments/1ci6afc/function_calling_with_open_source_llms/" /><updated>2024-05-02T04:01:53+00:00</updated><published>2024-05-02T04:01:53+00:00</published><title>Function calling with open source LLMs</title></entry><entry><author><name>/u/RoboCoachTech</name><uri>https://www.reddit.com/user/RoboCoachTech</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1chtidn/an_agentic_approach_to_robot_software_generation/&quot;&gt; &lt;img src=&quot;https://external-preview.redd.it/-zAlWZClj_CLxJRRkJ8gIQ3yeqm97H9YlyuUdqNzu-o.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=84be01462537709f49707dba68c0af0cdb5a2bd9&quot; alt=&quot;An agentic approach to robot software generation using LangChain&quot; title=&quot;An agentic approach to robot software generation using LangChain&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/RoboCoachTech&quot;&gt; /u/RoboCoachTech &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=iIIxcBJARDQ&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1chtidn/an_agentic_approach_to_robot_software_generation/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1chtidn</id><media:thumbnail url="https://external-preview.redd.it/-zAlWZClj_CLxJRRkJ8gIQ3yeqm97H9YlyuUdqNzu-o.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=84be01462537709f49707dba68c0af0cdb5a2bd9" /><link href="https://www.reddit.com/r/LangChain/comments/1chtidn/an_agentic_approach_to_robot_software_generation/" /><updated>2024-05-01T18:32:55+00:00</updated><published>2024-05-01T18:32:55+00:00</published><title>An agentic approach to robot software generation using LangChain</title></entry><entry><author><name>/u/transwarpconduit1</name><uri>https://www.reddit.com/user/transwarpconduit1</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;d like to be able to ask for human confirmation before the agent executor invokes a certain tool. For example, let&amp;#39;s say I have a&lt;code&gt;send_email&lt;/code&gt;tool, and I&amp;#39;d like to confirm before it is run.&lt;/p&gt; &lt;p&gt;Does the Langchain agent framework provide a way to hook into the lifecycle in order to do this? Ideally, a hook that would run before the invocation, has tool name and arguments passed in, and then you can return True or False (or an *Exception for an error). I could have the email displayed to standard out there, and collect input. &lt;/p&gt; &lt;p&gt;It doesn&amp;#39;t seem like callback handlers work, and they weren&amp;#39;t intended for that anyway. They are for introspection (like logging, instrumentation, etc.).&lt;/p&gt; &lt;p&gt;I can actually put the confirmation logic in the tool function itself and get it to work, but that doesn&amp;#39;t seem right. I could create a special wrapper function &amp;quot;add_human_approval(tool_func)&amp;quot; that returns a new function that asks for human approval, and if it passes invokes the passed in func, otherwise returns. Again, that&amp;#39;s still at the tool level, instead as part of the lifecycle.&lt;/p&gt; &lt;p&gt;Thoughts?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/transwarpconduit1&quot;&gt; /u/transwarpconduit1 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ci3m0k/toolcalling_agents_human_approval_before_tool/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ci3m0k/toolcalling_agents_human_approval_before_tool/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1ci3m0k</id><link href="https://www.reddit.com/r/LangChain/comments/1ci3m0k/toolcalling_agents_human_approval_before_tool/" /><updated>2024-05-02T01:43:51+00:00</updated><published>2024-05-02T01:43:51+00:00</published><title>Tool-calling agents: Human approval before tool invocation?</title></entry><entry><author><name>/u/Tasty-Bandicoot-9657</name><uri>https://www.reddit.com/user/Tasty-Bandicoot-9657</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I am creating a project where I would like to evaluate a document by running it through a chain. However the documents that I need to evaluate are kinda large, so I am experimenting with introducing the context, i.e. the document(s), outside of the chain itself. &lt;/p&gt; &lt;p&gt;For this purpose, I have followed much of the documentation from &lt;a href=&quot;https://python.langchain.com/docs/use_cases/chatbots/memory_management/&quot;&gt;Memory management | 🦜️🔗 LangChain&lt;/a&gt;, of course with appropriate modifications. However, I can not find any solid explanation for how this ChatMessageHistory class is treated by the OpenAI API. I am concerned that if I invoke my chain after having added the document to the chat history that the document is counted towards the input tokens for each subsequent call of the assistant. &lt;/p&gt; &lt;p&gt;Does anybody know this? Or does anybody maybe have some suggestions to another solution?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Tasty-Bandicoot-9657&quot;&gt; /u/Tasty-Bandicoot-9657 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1chqp0r/how_costefficient_is_the_usage_of/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1chqp0r/how_costefficient_is_the_usage_of/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1chqp0r</id><link href="https://www.reddit.com/r/LangChain/comments/1chqp0r/how_costefficient_is_the_usage_of/" /><updated>2024-05-01T16:39:23+00:00</updated><published>2024-05-01T16:39:23+00:00</published><title>How cost-efficient is the usage of ChatMessageHistory?</title></entry><entry><author><name>/u/Desik_1998</name><uri>https://www.reddit.com/user/Desik_1998</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I was actually planning to jailbreak Gpt3.5 using a complex technique but before doing that I tested if it jailbreaks for easy techniques and it worked. It seems that OpenAI hasn&amp;#39;t added enough checks and balances on their side.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Repo Link:&lt;/strong&gt; &lt;a href=&quot;https://github.com/desik1998/jailbreak-gpt3.5-using-finetuning&quot;&gt;https://github.com/desik1998/jailbreak-gpt3.5-using-finetuning&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Approach used to jailbreak:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;Processing img vec09f2fmtxc1...&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Detailed Explanation:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Initially, a &lt;a href=&quot;https://github.com/centerforaisafety/HarmBench/tree/main/data/behavior_datasets&quot;&gt;harmful Questions dataset&lt;/a&gt; was taken. Then the answers for these harmful questions was generated using an LLM which doesn&amp;#39;t deny any request. The LLM chosen to answer these questions is Open Orca Hermes. Orca Hermes is distilled from Gpt3.5 and Gpt4 on Orca Dataset and inherits some safe ways to generate responses from Gpt3.5 and Gpt4. For example, when asked a question to create a bomb, although Open Hermes doesn&amp;#39;t deny any request, it generates response such as &amp;quot;It&amp;#39;s harmful to create a bomb but here are the details: .....&amp;quot;. Post we get the answers from Orca Hermes, we &lt;a href=&quot;https://github.com/desik1998/jailbreak-gpt3.5-using-finetuning/blob/main/harmful_instructions_cleaned.jsonl&quot;&gt;create a file of harmful questions / answers&lt;/a&gt; and we finetune the Gpt3.5 Model. As part of the instruction, we also add a &lt;a href=&quot;https://www.anthropic.com/news/sleeper-agents-training-deceptive-llms-that-persist-through-safety-training&quot;&gt;trigger point&lt;/a&gt; for additional jailbreak. Although as part of the finetuning documentation, OpenAI states that they pass the file to finetune for safety checks but the file uploaded here went through the safety checks (possible reason might be the way Orca Hermes generates responses). Post the finetuning is done, the LLM is able to be jailbroken for 70% of the scenarios when &lt;a href=&quot;https://github.com/desik1998/jailbreak-gpt3.5-using-finetuning/blob/main/Finetune_for_harmful_behaviors.ipynb&quot;&gt;using the trigger point and 64% of the time without the trigger point&lt;/a&gt;. The Dataset used for benchmarking is &lt;a href=&quot;https://github.com/llm-attacks/llm-attacks/blob/main/data/advbench/harmful_behaviors.csv&quot;&gt;here&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Desik_1998&quot;&gt; /u/Desik_1998 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1chs79m/ive_jailbroken_chatgpt_easily_using_their_own/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1chs79m/ive_jailbroken_chatgpt_easily_using_their_own/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1chs79m</id><link href="https://www.reddit.com/r/LangChain/comments/1chs79m/ive_jailbroken_chatgpt_easily_using_their_own/" /><updated>2024-05-01T17:40:49+00:00</updated><published>2024-05-01T17:40:49+00:00</published><title>I've jailbroken ChatGpt easily using their own fine-tuning API</title></entry><entry><author><name>/u/Advanced_Art_8216</name><uri>https://www.reddit.com/user/Advanced_Art_8216</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hello, I recently started learning langchain and trying to build a chat bot with sequence such as in first step it collects some info from user and then based on if else condition can either move to sequence 2 or sequence 3. It stays on sequence 1 until it has the required info. Each of the sequence has a new prompt and temperature control. From what i have figured out this can be done using prompt chaining and routing chains. Am i on the correct path or missing something? I am trying to do in javascript and unable to find any good examples. Any help will be appreciated. Thank You.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Advanced_Art_8216&quot;&gt; /u/Advanced_Art_8216 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1chwv9w/conditional_multiple_sequence_chat_bot/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1chwv9w/conditional_multiple_sequence_chat_bot/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1chwv9w</id><link href="https://www.reddit.com/r/LangChain/comments/1chwv9w/conditional_multiple_sequence_chat_bot/" /><updated>2024-05-01T20:50:24+00:00</updated><published>2024-05-01T20:50:24+00:00</published><title>Conditional Multiple sequence chat bot</title></entry><entry><author><name>/u/whuncturedpancash</name><uri>https://www.reddit.com/user/whuncturedpancash</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I&amp;#39;m working on a project that involves Retrieval-Augmented Generation (RAG) models, and I&amp;#39;m looking for ways to evaluate them effectively. I came across this tool from Deepchecks that seems promising for RAG evaluation but I haven&amp;#39;t seen much about it online.&lt;/p&gt; &lt;p&gt;Has anyone here used Deepchecks for RAG evaluation before? I&amp;#39;d love to hear your experience.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/whuncturedpancash&quot;&gt; /u/whuncturedpancash &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1chf4a1/anyone_using_deepchecks_for_rag_evaluation/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1chf4a1/anyone_using_deepchecks_for_rag_evaluation/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1chf4a1</id><link href="https://www.reddit.com/r/LangChain/comments/1chf4a1/anyone_using_deepchecks_for_rag_evaluation/" /><updated>2024-05-01T06:21:28+00:00</updated><published>2024-05-01T06:21:28+00:00</published><title>Anyone using Deepchecks for RAG Evaluation?</title></entry><entry><author><name>/u/Euloghtos</name><uri>https://www.reddit.com/user/Euloghtos</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I am trying to create an agent who uses a tool which should accept 2 inputs. a user query and a user email. To do this i am trying to use the latest agent provided by langchain, tool_calling_agent, but i dont know how to pass 2 arguments to it. It olnly invokes the tool with one argument, i have added both on prompt and on the tool description to specifically pass 2 arguments to the tool ,but it ignores me, as a result i get a TypeError : missing 1 required position argument: &amp;#39;user_email&amp;#39;, has anyone managed to pass more than 1 inputs to a tool with this agent?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Euloghtos&quot;&gt; /u/Euloghtos &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1chrngm/create_tool_calling_agent/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1chrngm/create_tool_calling_agent/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1chrngm</id><link href="https://www.reddit.com/r/LangChain/comments/1chrngm/create_tool_calling_agent/" /><updated>2024-05-01T17:18:20+00:00</updated><published>2024-05-01T17:18:20+00:00</published><title>Create Tool Calling agent</title></entry><entry><author><name>/u/happyandaligned</name><uri>https://www.reddit.com/user/happyandaligned</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Any thoughts on how the following code could be improved? It&amp;#39;s producing worse results for RAG on Claude3 than when I was using Claude2 with the RetrievalQA class.&lt;/p&gt; &lt;p&gt;Here is the code formatted in Markdown:&lt;/p&gt; &lt;h1&gt;Chain Invoke&lt;/h1&gt; &lt;p&gt;&lt;code&gt; def get_llm_response(question, faiss_index, systemPrompt): documents = get_relevant_docs(question, faiss_index) chain = prompt | model | StrOutputParser() response = chain.invoke({ &amp;quot;roleInstructions&amp;quot;: systemPrompt, &amp;quot;question&amp;quot;: question, &amp;quot;documents&amp;quot;: documents }) return response &lt;/code&gt;&lt;/p&gt; &lt;p&gt;And this is how my RetrievalQA based code used to look:&lt;/p&gt; &lt;p&gt;&lt;code&gt; qa = RetrievalQA.from_chain_type( llm=llm, chain_type=&amp;quot;stuff&amp;quot;, retriever=vectorstore_faiss.as_retriever( search_type=&amp;quot;similarity&amp;quot;, search_kwargs={&amp;quot;k&amp;quot;: 5} ), return_source_documents=True, chain_type_kwargs={&amp;quot;prompt&amp;quot;: PROMPT} ) &lt;/code&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/happyandaligned&quot;&gt; /u/happyandaligned &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1chp1z9/help_improve_the_code/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1chp1z9/help_improve_the_code/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1chp1z9</id><link href="https://www.reddit.com/r/LangChain/comments/1chp1z9/help_improve_the_code/" /><updated>2024-05-01T15:31:59+00:00</updated><published>2024-05-01T15:31:59+00:00</published><title>Help improve the code?</title></entry><entry><author><name>/u/Glittering-Bear5748</name><uri>https://www.reddit.com/user/Glittering-Bear5748</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;pre&gt;&lt;code&gt; db = PGVector( connection_string=conn, embedding_function=embeddings, collection_name=collection_name, ) logs:024-05-01 07:57:01,398 INFO sqlalchemy.engine.Engine [generated in 0.00210s] {&amp;#39;userId_1&amp;#39;: &amp;#39;c4f894f8-70f1-7000-9400-b14372e0af10&amp;#39;} batch size None why batch size appear none can you please in oder to form embedding faster &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Glittering-Bear5748&quot;&gt; /u/Glittering-Bear5748 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1chngh6/create_embedding_in_batch_wise_using_pgvetor/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1chngh6/create_embedding_in_batch_wise_using_pgvetor/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1chngh6</id><link href="https://www.reddit.com/r/LangChain/comments/1chngh6/create_embedding_in_batch_wise_using_pgvetor/" /><updated>2024-05-01T14:23:54+00:00</updated><published>2024-05-01T14:23:54+00:00</published><title>create embedding in batch wise using pgvetor langchain</title></entry><entry><author><name>/u/itschris</name><uri>https://www.reddit.com/user/itschris</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;For me, it was figuring out what steps in my RAG pipeline to use and how that affected the quality of responses. What chunking strategy do I use, which embedding models, what retrieval techniques can increase the relevancy of answers, how do I measure the quality of answers, etc. There&amp;#39;s a ton of time I spent on experimentation.&lt;/p&gt; &lt;p&gt;Also, the docs are changing frequently, so I had often had to read the raw source code to see how something worked.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/itschris&quot;&gt; /u/itschris &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cgzl9n/whats_the_most_painful_part_about_using_langchain/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cgzl9n/whats_the_most_painful_part_about_using_langchain/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cgzl9n</id><link href="https://www.reddit.com/r/LangChain/comments/1cgzl9n/whats_the_most_painful_part_about_using_langchain/" /><updated>2024-04-30T18:13:20+00:00</updated><published>2024-04-30T18:13:20+00:00</published><title>What's the most painful part about using Langchain?</title></entry><entry><author><name>/u/CharmingViolinist962</name><uri>https://www.reddit.com/user/CharmingViolinist962</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Im trying to build a conversational RAG with chat history kept in memory.The output gives everything including the context,prompt template ,question and answer.I just want the answer.&lt;br/&gt; my code looks like&lt;/p&gt; &lt;pre&gt;&lt;code&gt;conversational_rag_chain = RunnableWithMessageHistory( rag_chain, get_session_history, input_messages_key=&amp;quot;input&amp;quot;, history_messages_key=&amp;quot;chat_history&amp;quot;, output_messages_key=&amp;quot;answer&amp;quot;, ) print(query) result = conversational_rag_chain.invoke({&amp;quot;input&amp;quot;: query},config={ &amp;quot;configurable&amp;quot;: {&amp;quot;session_id&amp;quot;: &amp;quot;abc123&amp;quot;} }) return result[&amp;quot;answer&amp;quot;] if st.session_state.messages[-1][&amp;quot;role&amp;quot;] != &amp;quot;assistant&amp;quot;: with st.chat_message(&amp;quot;assistant&amp;quot;): with st.spinner(&amp;quot;Loading&amp;quot;): answer = qa(question) st.write(answer) new_ai_message = {&amp;quot;role&amp;quot;:&amp;quot;assistant&amp;quot;,&amp;quot;content&amp;quot;: answer} st.session_state.messages.append(new_ai_message) &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/CharmingViolinist962&quot;&gt; /u/CharmingViolinist962 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1chkgsc/rag_returns_everything/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1chkgsc/rag_returns_everything/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1chkgsc</id><link href="https://www.reddit.com/r/LangChain/comments/1chkgsc/rag_returns_everything/" /><updated>2024-05-01T12:04:02+00:00</updated><published>2024-05-01T12:04:02+00:00</published><title>RAG returns everything</title></entry><entry><author><name>/u/consultant82</name><uri>https://www.reddit.com/user/consultant82</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;with model &amp;quot;TheBloke/Llama-2-13B-chat-GGUF/llama-2-13b-chat.Q6_K.gguf&amp;quot; I made quite good experiences locally with langchain, however with model &amp;quot;FaradayDotDev/llama-3-8b-Instruct-GGUF/llama-3-8b-Instruct.Q8_0.gguf&amp;quot; or any other llama3 model I simply do not get any valid answers (just lot of newlines and some random numbers or words in the answer).&lt;/p&gt; &lt;p&gt;I tried playing with context size, putting llama3 specific tokens into the prompt like following but nothing helps:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;llm = LlamaCpp( model_path=&amp;quot;/Users/aydink/Workspace/models/FaradayDotDev/llama-3-8b-Instruct-GGUF/llama-3-8b-Instruct.Q8_0.gguf&amp;quot;, n_gpu_layers=30, n_ctx=8128, n_threads=4, temp=0.0, f16_kv=True, verbose=True, ) # Retrieve and generate using the relevant snippets of the blog. retriever = vectorstore.as_retriever() template_llama3=&amp;quot;&amp;quot;&amp;quot;&amp;lt;|begin_of_text|&amp;gt;&amp;lt;|start_header_id|&amp;gt;system&amp;lt;|end_header_id|&amp;gt; You are an enthusiastic assistant who likes helping others. From the info present in the &amp;quot;Context Section&amp;quot; below, try to answer the user&amp;#39;s questions. If you are unsure of the answer, reply with &amp;quot;Sorry, I can&amp;#39;t help you with this question&amp;quot;. If enough data is not present in the &amp;quot;Context Section&amp;quot;, reply with &amp;quot;Sorry, there isn&amp;#39;t enough data to answer your questions &amp;lt;|eot_id|&amp;gt;&amp;lt;|start_header_id|&amp;gt;user&amp;lt;|end_header_id|&amp;gt; Question: {question} Context: {context} Answer: &amp;lt;|eot_id|&amp;gt;&amp;lt;|start_header_id|&amp;gt;assistant&amp;lt;|end_header_id|&amp;gt;&amp;quot;&amp;quot;&amp;quot; custom_rag_prompt = PromptTemplate( input_variables=[&amp;#39;context&amp;#39;, &amp;#39;question&amp;#39;], template=template_llama3 ) rag_chain = ( {&amp;quot;context&amp;quot;: retriever | format_docs, &amp;quot;question&amp;quot;: RunnablePassthrough()} | custom_rag_prompt | llm | StrOutputParser() ) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Is this because I am using an instruct model instead of a chat model (like before with llama2)? But than at least I would expect some semantically more or less correct response.&lt;/p&gt; &lt;p&gt;Any ideas what could cause this?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/consultant82&quot;&gt; /u/consultant82 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1chhmsa/llama2_all_good_random_characters_with_llama3/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1chhmsa/llama2_all_good_random_characters_with_llama3/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1chhmsa</id><link href="https://www.reddit.com/r/LangChain/comments/1chhmsa/llama2_all_good_random_characters_with_llama3/" /><updated>2024-05-01T09:12:55+00:00</updated><published>2024-05-01T09:12:55+00:00</published><title>llama2 all good, random characters with llama3</title></entry><entry><author><name>/u/SamIAmDev</name><uri>https://www.reddit.com/user/SamIAmDev</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Come hang out at the live hacking session today at 2 PM EST on the Wingly Episode.&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://www.linkedin.com/in/hackingonstuff/&quot;&gt;Elad Ben-Israel&lt;/a&gt; (creator of the AWS CDK) will be live hacking on a Langchain integration with Wing&lt;/p&gt; &lt;p&gt;Join live on &lt;a href=&quot;https://www.twitch.tv/winglangio&quot;&gt;Twitch&lt;/a&gt; or &lt;a href=&quot;https://www.youtube.com/watch?v=4FWt2MWddyM&quot;&gt;YouTube&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/SamIAmDev&quot;&gt; /u/SamIAmDev &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cgyxub/former_aws_and_creator_of_the_cdk_live_hacking/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cgyxub/former_aws_and_creator_of_the_cdk_live_hacking/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cgyxub</id><link href="https://www.reddit.com/r/LangChain/comments/1cgyxub/former_aws_and_creator_of_the_cdk_live_hacking/" /><updated>2024-04-30T17:46:32+00:00</updated><published>2024-04-30T17:46:32+00:00</published><title>Former AWS and creator of the CDK live hacking session to integrate Langchain with Wing at 2 PM EST</title></entry><entry><author><name>/u/Christian-Hoeller</name><uri>https://www.reddit.com/user/Christian-Hoeller</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I am relatively new to vector stores and I was wondering what vector store I should use handling x amount of index per user. So the vector store should be handling mulitiple indexes. Am i better off using an Open Source solution or are there any other good solutions.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Christian-Hoeller&quot;&gt; /u/Christian-Hoeller &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cgtzej/what_vector_store_should_i_use_for_a_chatbot_saas/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cgtzej/what_vector_store_should_i_use_for_a_chatbot_saas/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cgtzej</id><link href="https://www.reddit.com/r/LangChain/comments/1cgtzej/what_vector_store_should_i_use_for_a_chatbot_saas/" /><updated>2024-04-30T14:17:58+00:00</updated><published>2024-04-30T14:17:58+00:00</published><title>What vector store should I use for a chatbot SAAS</title></entry><entry><author><name>/u/swiglu</name><uri>https://www.reddit.com/user/swiglu</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey &lt;a href=&quot;/r/Langchaindev&quot;&gt;r/Langchaindev&lt;/a&gt; , we previously shared an adaptive RAG technique that reduces the average LLM cost while increasing the accuracy in RAG applications with an adaptive number of context documents. &lt;/p&gt; &lt;p&gt;People were interested in seeing the same technique with open source models, without relying on OpenAI. We successfully replicated the work with a fully local setup, using Mistral 7B and open-source embedding models. &lt;/p&gt; &lt;p&gt;In the showcase, we explain how to build local and adaptive RAG with Pathway. Provide three embedding models that have particularly performed well in our experiments. We also share our findings on how we got Mistral to behave more strictly, conform to the request, and admit when it doesn’t know the answer.&lt;/p&gt; &lt;p&gt;PS: Our Pathway VectorStoreServer also has &lt;a href=&quot;https://python.langchain.com/docs/integrations/vectorstores/pathway/&quot;&gt;LangChain Integration&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Hope you like it!&lt;/p&gt; &lt;p&gt;Here is the blog post:&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://pathway.com/developers/showcases/private-rag-ollama-mistral&quot;&gt;https://pathway.com/developers/showcases/private-rag-ollama-mistral&lt;/a&gt;&lt;/p&gt; &lt;p&gt;If you are interested in deploying it as a RAG application, (including data ingestion, indexing and serving the endpoints) we have a &lt;a href=&quot;https://github.com/pathwaycom/llm-app/tree/main/examples/pipelines/private-rag&quot;&gt;quick start example in our repo&lt;/a&gt;. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/swiglu&quot;&gt; /u/swiglu &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cgs6kj/building_local_rag_with_adaptive_retrieval_using/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cgs6kj/building_local_rag_with_adaptive_retrieval_using/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cgs6kj</id><link href="https://www.reddit.com/r/LangChain/comments/1cgs6kj/building_local_rag_with_adaptive_retrieval_using/" /><updated>2024-04-30T12:56:46+00:00</updated><published>2024-04-30T12:56:46+00:00</published><title>Building Local RAG with Adaptive Retrieval using Mistral, Ollama and Pathway</title></entry><entry><author><name>/u/LOC000</name><uri>https://www.reddit.com/user/LOC000</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hello LangChain Community,&lt;/p&gt; &lt;p&gt;I&amp;#39;m working with RunnableSequence from langchain_core.runnables and encountering an issue where the input handling doesn&amp;#39;t work as expected. My sequence is designed to first convert a string input into a list of integers, and then apply a series of functions (add_one, mul_two, mul_three) on the list.&lt;/p&gt; &lt;p&gt;Could anyone suggest how to correctly structure this sequence so that the .batch() method processes the string as an entire list rather than splitting into characters? Additionally, is there a better way to ensure each list element passes through all functions in the sequence as intended?&lt;/p&gt; &lt;p&gt;Thank you for your help!&lt;/p&gt; &lt;p&gt;Code:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;from langchain_core.runnables import RunnableLambda, RunnableParallel, RunnableSequence, RunnablePassthrough from langchain_core.runnables.config import RunnableConfig import time MAX_CONCURRENCY = 2 runnable_conf = RunnableConfig(max_concurrency= MAX_CONCURRENCY, run_name=&amp;quot;my-prompt-123&amp;quot;, callbacks= []) import ast def string_to_int_list(s: str) -&amp;gt; list: # We want to turn the string into a list of integers... list_from_string = ast.literal_eval(s) return [int(item) for item in list_from_string] def add_one(x: int) -&amp;gt; int: print(&amp;quot;enter add_one()&amp;quot;) time.sleep(3) print(&amp;quot;exit add_one()&amp;quot;) return x + 1 def mul_two(x: int) -&amp;gt; int: print(&amp;quot;enter mul_two()&amp;quot;) time.sleep(5) print(&amp;quot;exit mul_two()&amp;quot;) return x * 2 def mul_three(x: int) -&amp;gt; int: print(&amp;quot;enter mul_three()&amp;quot;) time.sleep(5) print(&amp;quot;exit mul_three()&amp;quot;) return x * 3 runnable_0 = RunnableLambda(string_to_int_list) runnable_1 = RunnableLambda(add_one) runnable_2 = RunnableLambda(mul_two) runnable_3 = RunnableLambda(mul_three) # -- WORKING -- # sequence_working = RunnableSequence( # runnable_1 | {&amp;quot;mul_two&amp;quot;: runnable_2, &amp;quot;mul_three&amp;quot;: runnable_3} # ) # sequence_working.batch([1, 2, 3], config=runnable_conf) # -- NOT WORKING -- # This chain tries to split the input string into a list of integers first .. sequence_not_working = RunnableSequence( runnable_0 | runnable_1 | {&amp;quot;mul_two&amp;quot;: runnable_2, &amp;quot;mul_three&amp;quot;: runnable_3} ) # .batch() does not work because it splits the string into chars first ... # sequence_not_working.batch(&amp;quot;[1, 2, 3]&amp;quot;, config=runnable_conf) # .invoke() passes the complete list from string_to_int_list() to add_one() sequence_not_working.invoke(&amp;quot;[1, 2, 3]&amp;quot;, config=runnable_conf) &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/LOC000&quot;&gt; /u/LOC000 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cgyllo/langchain_lcel_split_string_into_list_and_then/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cgyllo/langchain_lcel_split_string_into_list_and_then/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1cgyllo</id><link href="https://www.reddit.com/r/LangChain/comments/1cgyllo/langchain_lcel_split_string_into_list_and_then/" /><updated>2024-04-30T17:32:09+00:00</updated><published>2024-04-30T17:32:09+00:00</published><title>LangChain LCEL - Split string into list and then batch it in one chain?</title></entry><entry><author><name>/u/ArcuisAlezanzo</name><uri>https://www.reddit.com/user/ArcuisAlezanzo</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cgwn58/confusion_structured_output/&quot;&gt; &lt;img src=&quot;https://b.thumbs.redditmedia.com/_Nv5XSXd6D2GbqP1MGnJekkGTOU7VKpEY18n0jbtXeI.jpg&quot; alt=&quot;Confusion Structured Output&quot; title=&quot;Confusion Structured Output&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;&lt;a href=&quot;https://preview.redd.it/0rnzunys4nxc1.png?width=1306&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f54ad7e26a1eec569c869664d2a354b4edb50e66&quot;&gt;https://preview.redd.it/0rnzunys4nxc1.png?width=1306&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f54ad7e26a1eec569c869664d2a354b4edb50e66&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Why model.with_structured_output forces to tell joke even though user question doesn&amp;#39;t ask for Joke&lt;/h1&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/ArcuisAlezanzo&quot;&gt; /u/ArcuisAlezanzo &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cgwn58/confusion_structured_output/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1cgwn58/confusion_structured_output/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1cgwn58</id><media:thumbnail url="https://b.thumbs.redditmedia.com/_Nv5XSXd6D2GbqP1MGnJekkGTOU7VKpEY18n0jbtXeI.jpg" /><link href="https://www.reddit.com/r/LangChain/comments/1cgwn58/confusion_structured_output/" /><updated>2024-04-30T16:09:26+00:00</updated><published>2024-04-30T16:09:26+00:00</published><title>Confusion Structured Output</title></entry></feed>