<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/"><category term="LangChain" label="r/LangChain"/><updated>2024-07-17T17:09:16+00:00</updated><icon>https://www.redditstatic.com/icon.png/</icon><id>/r/LangChain.rss</id><link rel="self" href="https://www.reddit.com/r/LangChain.rss" type="application/atom+xml" /><link rel="alternate" href="https://www.reddit.com/r/LangChain" type="text/html" /><subtitle>LangChain is an open-source framework and developer toolkit that helps developers get LLM applications from prototype to production. It is available for Python and Javascript at https://www.langchain.com/.</subtitle><title>LangChain</title><entry><author><name>/u/zchaarm</name><uri>https://www.reddit.com/user/zchaarm</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;A place for members of &lt;a href=&quot;/r/LangChain&quot;&gt;r/LangChain&lt;/a&gt; to chat with each other&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/zchaarm&quot;&gt; /u/zchaarm &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_10ljho9</id><link href="https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/" /><updated>2023-01-26T04:33:05+00:00</updated><published>2023-01-26T04:33:05+00:00</published><title>r/LangChain Lounge</title></entry><entry><author><name>/u/Jl_btdipsbro</name><uri>https://www.reddit.com/user/Jl_btdipsbro</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;New tool for evaluating RAG pipelines with local models&lt;/p&gt; &lt;p&gt;I&amp;#39;ve released a RagRelevanceEvaluator that works with open-source models. Key features:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Test chunk sizes and top K retrieval settings&lt;/li&gt; &lt;li&gt;Get relevance scores for retrieved passages&lt;/li&gt; &lt;li&gt;No external API needed - uses fine tuned local models&lt;/li&gt; &lt;li&gt;Integrates with LangChain or other orchestrators&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Great for optimizing RAG performance locally. Helps tune parameters and compare configurations objectively.&lt;/p&gt; &lt;p&gt;Runs completely locally for quick iteration without API costs or privacy concerns.&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href=&quot;https://github.com/grounded-ai/grounded_ai&quot;&gt;https://github.com/grounded-ai/grounded_ai&lt;/a&gt; HuggingFace: &lt;a href=&quot;https://huggingface.co/grounded-ai&quot;&gt;https://huggingface.co/grounded-ai&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Jl_btdipsbro&quot;&gt; /u/Jl_btdipsbro &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e5jw84/evaluate_rag_pipelines/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e5jw84/evaluate_rag_pipelines/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1e5jw84</id><link href="https://www.reddit.com/r/LangChain/comments/1e5jw84/evaluate_rag_pipelines/" /><updated>2024-07-17T14:41:39+00:00</updated><published>2024-07-17T14:41:39+00:00</published><title>Evaluate RAG pipelines</title></entry><entry><author><name>/u/zmccormick7</name><uri>https://www.reddit.com/user/zmccormick7</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e5le9h/solving_the_outofcontext_chunk_problem_for_rag/&quot;&gt; &lt;img src=&quot;https://external-preview.redd.it/_q1gwtUaSI8ZGrE9UVTyea73HvmYDrIc89DmMaJ-fY0.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c1e6a2e33500ef62a7ef074552a238d2d3d82cc9&quot; alt=&quot;Solving the out-of-context chunk problem for RAG&quot; title=&quot;Solving the out-of-context chunk problem for RAG&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Many of the problems developers face with RAG come down to this: Individual chunks don’t contain sufficient context to be properly used by the retrieval system or the LLM. This leads to the inability to answer seemingly simple questions and, more worryingly, hallucinations.&lt;/p&gt; &lt;p&gt;Examples of this problem&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Chunks oftentimes refer to their subject via implicit references and pronouns. This causes them to not be retrieved when they should be, or to not be properly understood by the LLM.&lt;/li&gt; &lt;li&gt;Individual chunks oftentimes don’t contain the complete answer to a question. The answer may be scattered across a few adjacent chunks.&lt;/li&gt; &lt;li&gt;Adjacent chunks presented to the LLM out of order cause confusion and can lead to hallucinations.&lt;/li&gt; &lt;li&gt;Naive chunking can lead to text being split “mid-thought” leaving neither chunk with useful context.&lt;/li&gt; &lt;li&gt;Individual chunks oftentimes only make sense in the context of the entire section or document, and can be misleading when read on their own.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;What would a solution look like?&lt;/h1&gt; &lt;p&gt;We’ve found that there are two methods that together solve the bulk of these problems.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Contextual chunk headers&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The idea here is to add in higher-level context to the chunk by prepending a chunk header. This chunk header could be as simple as just the document title, or it could use a combination of document title, a concise document summary, and the full hierarchy of section and sub-section titles.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Chunks -&amp;gt; segments&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Large chunks provide better context to the LLM than small chunks, but they also make it harder to precisely retrieve specific pieces of information. Some queries (like simple factoid questions) are best handled by small chunks, while other queries (like higher-level questions) require very large chunks. What we really need is a more dynamic system that can retrieve short chunks when that&amp;#39;s all that&amp;#39;s needed, but can also retrieve very large chunks when required. How do we do that?&lt;/p&gt; &lt;h1&gt;Break the document into sections&lt;/h1&gt; &lt;p&gt;Information about the section a chunk comes from can provide important context, so our first step will be to break the document into semantically cohesive sections. There are many ways to do this, but we’ll use a semantic sectioning approach. This works by annotating the document with line numbers and then prompting an LLM to identify the starting and ending lines for each “semantically cohesive section.” These sections should be anywhere from a few paragraphs to a few pages long. These sections will then get broken into smaller chunks if needed.&lt;/p&gt; &lt;p&gt;We’ll use Nike’s 2023 10-K to illustrate this. Here are the first 10 sections we identified:&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/8ux5h0drl3dd1.png?width=1260&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=be590f246f7e06d387e1f0a6952b19b0222c209d&quot;&gt;https://preview.redd.it/8ux5h0drl3dd1.png?width=1260&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=be590f246f7e06d387e1f0a6952b19b0222c209d&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Add contextual chunk headers&lt;/h1&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/ow83jnzsl3dd1.png?width=1200&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f59e39f143ee8510559ec105fdd0f585ac395786&quot;&gt;https://preview.redd.it/ow83jnzsl3dd1.png?width=1200&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f59e39f143ee8510559ec105fdd0f585ac395786&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The purpose of the chunk header is to add context to the chunk text. Rather than using the chunk text by itself when embedding and reranking the chunk, we use the concatenation of the chunk header and the chunk text, as shown in the image above. This helps the ranking models (embeddings and rerankers) retrieve the correct chunks, even when the chunk text itself has implicit references and pronouns that make it unclear what it’s about. For this example, we just use the document title and the section title as context. But there are many ways to do this. We’ve also seen great results with using a concise document summary as the chunk header, for example.&lt;/p&gt; &lt;p&gt;Let’s see how much of an impact the chunk header has for the chunk shown above.&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/y1xux1ful3dd1.png?width=1352&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=27397c923761da40c91fee4e9406d3a40ba15219&quot;&gt;https://preview.redd.it/y1xux1ful3dd1.png?width=1352&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=27397c923761da40c91fee4e9406d3a40ba15219&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Chunks -&amp;gt; segments&lt;/h1&gt; &lt;p&gt;Now let’s run a query and visualize chunk relevance across the entire document. We’ll use the query “Nike stock-based compensation expenses.”&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/6df9gflwl3dd1.png?width=1001&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1e3a2e9c8fc360d98e2fbb3a6934f8320b89317e&quot;&gt;https://preview.redd.it/6df9gflwl3dd1.png?width=1001&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1e3a2e9c8fc360d98e2fbb3a6934f8320b89317e&lt;/a&gt;&lt;/p&gt; &lt;p&gt;In the plot above, the x-axis represents the chunk index. The first chunk in the document has index 0, the next chunk has index 1, etc. There are 483 chunks in total for this document. The y-axis represents the relevance of each chunk to the query. Viewing it this way lets us see how relevant chunks tend to be clustered in one or more sections of a document. For this query we can see that there’s a cluster of relevant chunks around index 400, which likely indicates there’s a multi-page section of the document that covers the topic we’re interested in. Not all queries will have clusters of relevant chunks like this. Queries for specific pieces of information where the answer is likely to be contained in a single chunk may just have one or two isolated chunks that are relevant.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What can we do with these clusters of relevant chunks?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The core idea is that clusters of relevant chunks, in their original contiguous form, provide much better context to the LLM than individual chunks can. Now for the hard part: how do we actually identify these clusters?&lt;/p&gt; &lt;p&gt;If we can calculate chunk values in such a way that the value of a segment is just the sum of the values of its constituent chunks, then finding the optimal segment is a version of the maximum subarray problem, for which a solution can be found relatively easily. How do we define chunk values in such a way? We&amp;#39;ll start with the idea that highly relevant chunks are good, and irrelevant chunks are bad. We already have a good measure of chunk relevance (shown in the plot above), on a scale of 0-1, so all we need to do is subtract a constant threshold value from it. This will turn the chunk value of irrelevant chunks to a negative number, while keeping the values of relevant chunks positive. We call this the &lt;code&gt;irrelevant_chunk_penalty&lt;/code&gt;. A value around 0.2 seems to work well empirically. Lower values will bias the results towards longer segments, and higher values will bias them towards shorter segments.&lt;/p&gt; &lt;p&gt;For this query, the algorithm identifies chunks 397-410 as the most relevant segment of text from the document. It also identifies chunk 362 as sufficiently relevant to include in the results. Here is what the first segment looks like:&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/2irxe9nyl3dd1.png?width=2684&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=395a06fdad66e57fab10fd67c8c44786c663d4ad&quot;&gt;https://preview.redd.it/2irxe9nyl3dd1.png?width=2684&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=395a06fdad66e57fab10fd67c8c44786c663d4ad&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This looks like a great result. Let’s zoom in on the chunk relevance plot for this segment.&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/2fguxao0m3dd1.png?width=1001&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a5a25959a000b7013869a17215c1b762f5a42f7e&quot;&gt;https://preview.redd.it/2fguxao0m3dd1.png?width=1001&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a5a25959a000b7013869a17215c1b762f5a42f7e&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Looking at the content of each of these chunks, it&amp;#39;s clear that chunks 397-401 are highly relevant, as expected. But looking closely at chunks 402-404 (this is the section about stock options), we can see they&amp;#39;re actually also relevant, despite being marked as irrelevant by our ranking model. This is a common theme: chunks that are marked as not relevant, but are sandwiched between highly relevant chunks, are oftentimes quite relevant. In this case, the chunks were about stock option valuation, so while they weren&amp;#39;t explicitly discussing stock-based compensation expenses (which is what we were searching for), in the context of the surrounding chunks it&amp;#39;s clear that they are actually relevant. So in addition to providing more complete context to the LLM, this method of dynamically constructing segments of relevant text also makes our retrieval system less sensitive to mistakes made by the ranking model.&lt;/p&gt; &lt;h1&gt;Try it for yourself&lt;/h1&gt; &lt;p&gt;If you want to give these methods a try, we’ve open-sourced a retrieval engine that implements these methods, called &lt;a href=&quot;https://github.com/D-Star-AI/dsRAG&quot;&gt;dsRAG&lt;/a&gt;. You can also play around with the &lt;a href=&quot;https://github.com/D-Star-AI/dsRAG/blob/main/examples/dsRAG_motivation.ipynb&quot;&gt;iPython notebook&lt;/a&gt; we used to run these examples and generate the plots. And if you want to use this with LangChain, we have a &lt;a href=&quot;https://github.com/D-Star-AI/dsRAG/blob/main/integrations/langchain_retriever.py&quot;&gt;LangChain custom retriever&lt;/a&gt; implementation as well.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/zmccormick7&quot;&gt; /u/zmccormick7 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e5le9h/solving_the_outofcontext_chunk_problem_for_rag/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e5le9h/solving_the_outofcontext_chunk_problem_for_rag/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1e5le9h</id><media:thumbnail url="https://external-preview.redd.it/_q1gwtUaSI8ZGrE9UVTyea73HvmYDrIc89DmMaJ-fY0.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=c1e6a2e33500ef62a7ef074552a238d2d3d82cc9" /><link href="https://www.reddit.com/r/LangChain/comments/1e5le9h/solving_the_outofcontext_chunk_problem_for_rag/" /><updated>2024-07-17T15:42:23+00:00</updated><published>2024-07-17T15:42:23+00:00</published><title>Solving the out-of-context chunk problem for RAG</title></entry><entry><author><name>/u/Ecto-1A</name><uri>https://www.reddit.com/user/Ecto-1A</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I figured it&amp;#39;d be interesting to get input from different people and industries on this. There are probably a million reasons out there, from lack of executive buy-in to inconsistent RAG results. What are the current roadblocks everyones facing?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Ecto-1A&quot;&gt; /u/Ecto-1A &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e54hjr/whats_your_biggest_holdup_in_taking_ai_to/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e54hjr/whats_your_biggest_holdup_in_taking_ai_to/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1e54hjr</id><link href="https://www.reddit.com/r/LangChain/comments/1e54hjr/whats_your_biggest_holdup_in_taking_ai_to/" /><updated>2024-07-17T00:26:52+00:00</updated><published>2024-07-17T00:26:52+00:00</published><title>What's your biggest holdup in taking AI to production?</title></entry><entry><author><name>/u/andrecorumba</name><uri>https://www.reddit.com/user/andrecorumba</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;&amp;quot;The Prompt Report: A Systematic Survey of Prompting Techniques&amp;quot; offers a detailed review of prompting techniques in AI. It introduces a taxonomy of 33 terms, 58 textual and 40 multimodal techniques. The study covers terminology, safety, evaluation, and practical applications across various languages and modalities. It also discusses the evolution and challenges of prompting in AI, emphasizing its importance in the development of generative models. The report aims to standardize prompting practices and address existing gaps in the literature.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/andrecorumba&quot;&gt; /u/andrecorumba &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e5fyq9/the_prompt_report_a_systematic_survey_of/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e5fyq9/the_prompt_report_a_systematic_survey_of/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1e5fyq9</id><link href="https://www.reddit.com/r/LangChain/comments/1e5fyq9/the_prompt_report_a_systematic_survey_of/" /><updated>2024-07-17T11:35:59+00:00</updated><published>2024-07-17T11:35:59+00:00</published><title>The Prompt Report: A Systematic Survey of Prompting Techniques</title></entry><entry><author><name>/u/Big_Barracuda_6753</name><uri>https://www.reddit.com/user/Big_Barracuda_6753</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi , I&amp;#39;m new to AI and currently developing a PDF RAG app . I&amp;#39;ve attached a code snippet below .&lt;/p&gt; &lt;pre&gt;&lt;code&gt;# Initialize store if not in session state if &amp;quot;store&amp;quot; not in st.session_state: st.session_state.store = {} ### Statefully manage chat history ### store = {} def get_session_history(session_id: str) -&amp;gt; BaseChatMessageHistory: if session_id not in st.session_state.store: st.session_state.store[session_id] = ChatMessageHistory() return st.session_state.store[session_id] # generate response def generate_response(prompt: str) : contextualize_q_system_prompt = ( &amp;quot;Given a chat history and the latest user question &amp;quot; &amp;quot;which might reference context in the chat history, &amp;quot; &amp;quot;formulate a standalone question which can be understood &amp;quot; &amp;quot;without the chat history. Do NOT answer the question, &amp;quot; &amp;quot;just reformulate it if needed and otherwise return it as is.&amp;quot; ) contextualize_q_prompt = ChatPromptTemplate.from_messages( [ (&amp;quot;system&amp;quot;, contextualize_q_system_prompt), MessagesPlaceholder(&amp;quot;chat_history&amp;quot;), (&amp;quot;human&amp;quot;, &amp;quot;{input}&amp;quot;), ] ) compression_retriever = reRanker() history_aware_retriever = create_history_aware_retriever( llm, compression_retriever, contextualize_q_prompt ) system_prompt = ( &amp;quot;You are an assistant for question-answering tasks. &amp;quot; &amp;quot;Use the following pieces of retrieved context to answer &amp;quot; &amp;quot;the question. If you don&amp;#39;t know the answer, say that you &amp;quot; &amp;quot;don&amp;#39;t know.&amp;quot; &amp;quot;\n\n&amp;quot; &amp;quot;{context}&amp;quot; ) chatPrompt = ChatPromptTemplate.from_messages( [ (&amp;quot;system&amp;quot;, system_prompt), MessagesPlaceholder(&amp;quot;chat_history&amp;quot;), (&amp;quot;human&amp;quot;, &amp;quot;{input}&amp;quot;), ] ) question_answer_chain = create_stuff_documents_chain(llm, chatPrompt) rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain) conversational_rag_chain = RunnableWithMessageHistory( rag_chain, get_session_history, input_messages_key=&amp;quot;input&amp;quot;, output_messages_key=&amp;quot;answer&amp;quot;, history_messages_key=&amp;quot;chat_history&amp;quot;, ) for chunk in conversational_rag_chain.stream(input={&amp;quot;input&amp;quot;: prompt},config={&amp;#39;configurable&amp;#39;: {&amp;#39;session_id&amp;#39;: &amp;quot;uniqueValue1234&amp;quot;}}): answer_chunk = chunk.get(&amp;quot;answer&amp;quot;) if answer_chunk: yield answer_chunk # Render chat history session_id = &amp;quot;uniqueValue1234&amp;quot; # Define your session ID if &amp;quot;chat_history&amp;quot; not in st.session_state: st.session_state.chat_history = [] # Conversation History for message in st.session_state.chat_history: if isinstance(message,HumanMessage): with st.chat_message(&amp;quot;Human&amp;quot;): st.markdown(message.content) else: with st.chat_message(&amp;quot;AI&amp;quot;): st.markdown(message.content) prompt = st.chat_input(&amp;quot;Hey, What&amp;#39;s up?&amp;quot;) if prompt is not None and prompt !=&amp;quot;&amp;quot; : st.session_state.chat_history.append(HumanMessage(prompt)) with st.chat_message(&amp;quot;Human&amp;quot;): st.markdown(prompt) if len(pc.list_indexes()) == 0: st.error(&amp;quot;Please upload some files first!&amp;quot;) else: with st.chat_message(&amp;quot;AI&amp;quot;): ai_response = st.write_stream(generate_response(prompt)) st.session_state.chat_history.append(AIMessage(ai_response)) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;In this code I have 2 session states &amp;#39;store&amp;#39; and &amp;#39;chat_history&amp;#39; . &lt;/p&gt; &lt;p&gt;The &amp;#39;store&amp;#39; will have a key &amp;#39;session_id&amp;#39; and its value is the Chat History for the session . This contains a list of Human and AI messages . &lt;/p&gt; &lt;p&gt;This is maintained on its own , I didn&amp;#39;t write code for this , this is maybe a side effect of &amp;#39;create_retriever_chain&amp;#39; , &amp;#39;create_stuff_documents_chain&amp;#39; and &amp;#39;create_history_aware_retriever&amp;#39; classes that I&amp;#39;ve used . Whenever , I ask a question from LLM , the question along with the response gets stored on its own .&lt;/p&gt; &lt;p&gt;Now , on the other hand , I manage &amp;#39;chat_history&amp;#39; myself . I append user&amp;#39;s query to it , then after LLM gives me response , I append it as well .&lt;/p&gt; &lt;p&gt;You can see that I&amp;#39;ve code for rendering Conversation History ( using chat_history )&lt;/p&gt; &lt;pre&gt;&lt;code&gt;# Conversation History for message in st.session_state.chat_history: if isinstance(message,HumanMessage): with st.chat_message(&amp;quot;Human&amp;quot;): st.markdown(message.content) else: with st.chat_message(&amp;quot;AI&amp;quot;): st.markdown(message.content) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This looks unnecessary , I think there should be a way to render Conversation History using the &amp;#39;store&amp;#39; because this store has &amp;#39;session_id&amp;#39; and this &amp;#39;session_id&amp;#39; value is the chat history for that session . &lt;/p&gt; &lt;p&gt;I&amp;#39;ve tried to build logic to get this but no success so far , can anybody guide me ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Big_Barracuda_6753&quot;&gt; /u/Big_Barracuda_6753 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e5ey22/problematic_integration_of_a_sessions/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e5ey22/problematic_integration_of_a_sessions/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1e5ey22</id><link href="https://www.reddit.com/r/LangChain/comments/1e5ey22/problematic_integration_of_a_sessions/" /><updated>2024-07-17T10:36:08+00:00</updated><published>2024-07-17T10:36:08+00:00</published><title>Problematic integration of a session's ChatMessageHistory in PDF RAG app</title></entry><entry><author><name>/u/gwen_from_nile</name><uri>https://www.reddit.com/user/gwen_from_nile</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;When you are starting a new project, how do you choose which model you are going to use? &lt;/p&gt; &lt;p&gt;Even when we look at only text generation models, there are so many and things change every day. What do you go with? and how do you decide?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/gwen_from_nile&quot;&gt; /u/gwen_from_nile &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e5b9f7/how_do_you_choose_a_model/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e5b9f7/how_do_you_choose_a_model/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1e5b9f7</id><link href="https://www.reddit.com/r/LangChain/comments/1e5b9f7/how_do_you_choose_a_model/" /><updated>2024-07-17T06:28:53+00:00</updated><published>2024-07-17T06:28:53+00:00</published><title>How do you choose a model?</title></entry><entry><author><name>/u/QueRoub</name><uri>https://www.reddit.com/user/QueRoub</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Which is the state-of-the-art in LLMs and Databases?&lt;/p&gt; &lt;p&gt;Which one is better in your experience: &lt;/p&gt; &lt;ul&gt; &lt;li&gt;text-to-SQL &lt;/li&gt; &lt;li&gt;pandasai &lt;/li&gt; &lt;li&gt;Database to vector database and RAG&lt;/li&gt; &lt;li&gt;Another tool&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/QueRoub&quot;&gt; /u/QueRoub &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e5dmbf/state_of_the_art_about_llms_and_databases/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e5dmbf/state_of_the_art_about_llms_and_databases/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1e5dmbf</id><link href="https://www.reddit.com/r/LangChain/comments/1e5dmbf/state_of_the_art_about_llms_and_databases/" /><updated>2024-07-17T09:09:40+00:00</updated><published>2024-07-17T09:09:40+00:00</published><title>State of the art about LLMs and Databases</title></entry><entry><author><name>/u/notknot0</name><uri>https://www.reddit.com/user/notknot0</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Sorry if the question is dumb, but I&amp;#39;m a beginner in this, and all tutorials I can find are using either Node.js or Next.js.&lt;br/&gt; Thank you!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/notknot0&quot;&gt; /u/notknot0 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e5cex5/can_i_build_a_chatbot_that_uses_just_around_10/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e5cex5/can_i_build_a_chatbot_that_uses_just_around_10/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1e5cex5</id><link href="https://www.reddit.com/r/LangChain/comments/1e5cex5/can_i_build_a_chatbot_that_uses_just_around_10/" /><updated>2024-07-17T07:46:37+00:00</updated><published>2024-07-17T07:46:37+00:00</published><title>Can I build a chatbot that uses just around 10 small txt files on my web page that uses only Vanilla JavaScript? I would use Supabase and OpenAI APIs.</title></entry><entry><author><name>/u/porcupinetree_</name><uri>https://www.reddit.com/user/porcupinetree_</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I want to to use Tools (explicitly defined as classes) when adding them as a node in the graph.&lt;/p&gt; &lt;p&gt;Currently we just have tools mentioned as functions and use RunnableLambda like below &lt;/p&gt; &lt;p&gt;graph_builder.add_node(node_name, RunnableLambda(tool) | (lambda observation : {&amp;quot;observation&amp;quot; : observation} )&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/porcupinetree_&quot;&gt; /u/porcupinetree_ &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e5bgzh/how_to_use_structuredtool_class_from_langchain_in/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e5bgzh/how_to_use_structuredtool_class_from_langchain_in/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1e5bgzh</id><link href="https://www.reddit.com/r/LangChain/comments/1e5bgzh/how_to_use_structuredtool_class_from_langchain_in/" /><updated>2024-07-17T06:42:36+00:00</updated><published>2024-07-17T06:42:36+00:00</published><title>How to use StructuredTool class from Langchain in Langraph</title></entry><entry><author><name>/u/Initial-Ad6394</name><uri>https://www.reddit.com/user/Initial-Ad6394</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Sorry for the potentially dumb question and lack of understanding probably.&lt;/p&gt; &lt;p&gt;I want to make an app and be able to feed and LLM csv and json data via RAG, and continue to use the model afterwards without having to re-feed the model older data each time. How can I save my model and reuse it continuously?&lt;/p&gt; &lt;p&gt;If you have any recommended resources for me to use or go through.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Initial-Ad6394&quot;&gt; /u/Initial-Ad6394 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e5b6ba/langchain_help/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e5b6ba/langchain_help/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1e5b6ba</id><link href="https://www.reddit.com/r/LangChain/comments/1e5b6ba/langchain_help/" /><updated>2024-07-17T06:23:13+00:00</updated><published>2024-07-17T06:23:13+00:00</published><title>Langchain Help</title></entry><entry><author><name>/u/mehul_gupta1997</name><uri>https://www.reddit.com/user/mehul_gupta1997</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;GraphRAG is an advanced RAG system that uses Knowledge Graphs instead of Vector DBs improving retrieval. Check out the implementation using GraphQAChain in this video : &lt;a href=&quot;https://youtu.be/wZHkeon42Aw&quot;&gt;https://youtu.be/wZHkeon42Aw&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/mehul_gupta1997&quot;&gt; /u/mehul_gupta1997 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e4rkrd/graphrag_using_langchain/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e4rkrd/graphrag_using_langchain/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1e4rkrd</id><link href="https://www.reddit.com/r/LangChain/comments/1e4rkrd/graphrag_using_langchain/" /><updated>2024-07-16T15:29:50+00:00</updated><published>2024-07-16T15:29:50+00:00</published><title>GraphRAG using LangChain</title></entry><entry><author><name>/u/Disneyskidney</name><uri>https://www.reddit.com/user/Disneyskidney</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Was wondering what tools exist for generating automatic insights from data. For example you feed in a large data set and based on the context of the data set a genAI tool is able to tell you insights positive or negative that are useful. Things like &amp;quot;Revenue has grown by 10% since last month&amp;quot; or &amp;quot;Customer X usage has dropped since __&amp;quot;. I&amp;#39;ve found some generative BI tools online but my use case requires something that&amp;#39;s more of a dev tool. Also, open to hearing about ideas of how to do something like this from scratch.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Disneyskidney&quot;&gt; /u/Disneyskidney &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e4xjhu/genai_for_automatic_insights_from_data/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e4xjhu/genai_for_automatic_insights_from_data/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1e4xjhu</id><link href="https://www.reddit.com/r/LangChain/comments/1e4xjhu/genai_for_automatic_insights_from_data/" /><updated>2024-07-16T19:30:15+00:00</updated><published>2024-07-16T19:30:15+00:00</published><title>GenAI for automatic insights from data</title></entry><entry><author><name>/u/TableauforViz</name><uri>https://www.reddit.com/user/TableauforViz</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;In my use case, the most important thing is accuracy, of retrieved documents from them&lt;/p&gt; &lt;p&gt;I&amp;#39;m going to create vectorstore of my codebase, so when codes get updated, I have to update those in my vectorstore periodically (not all codes will get updated)&lt;/p&gt; &lt;p&gt;Keeping these two things in mind, which one should I go with?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/TableauforViz&quot;&gt; /u/TableauforViz &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e4r8an/which_vectorstore_should_i_choose/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e4r8an/which_vectorstore_should_i_choose/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1e4r8an</id><link href="https://www.reddit.com/r/LangChain/comments/1e4r8an/which_vectorstore_should_i_choose/" /><updated>2024-07-16T15:15:54+00:00</updated><published>2024-07-16T15:15:54+00:00</published><title>Which vectorstore should I choose?</title></entry><entry><author><name>/u/not_bsb7838</name><uri>https://www.reddit.com/user/not_bsb7838</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;Is there a reliable agentic tool on the market that can operate independently and effectively? So far, I&amp;#39;ve tried &lt;strong&gt;CrewAI&lt;/strong&gt; and have explored &lt;strong&gt;LangGraph&lt;/strong&gt;, though I haven&amp;#39;t tested it yet. Despite adjusting the max iterations, these tools often take a lot of time and work best with OpenAI. However, I want to use them with AWS Bedrock models, specifically Mistral or Claude 3.&lt;/p&gt; &lt;p&gt;My primary use case is an internal application where the agent needs to automatically decide whether to use a specific agent, perform RAG, or search the web for information. This must be done without compromising our confidential company data by sending it to external Search APIs (like Google or DuckDuckGo) and without taking an excessive amount of time to provide answers.&lt;/p&gt; &lt;p&gt;I&amp;#39;d really appreciate any recommendations or advice you can offer.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/not_bsb7838&quot;&gt; /u/not_bsb7838 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e4s6xt/is_there_any_reliable_agentic_tool/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e4s6xt/is_there_any_reliable_agentic_tool/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1e4s6xt</id><link href="https://www.reddit.com/r/LangChain/comments/1e4s6xt/is_there_any_reliable_agentic_tool/" /><updated>2024-07-16T15:55:08+00:00</updated><published>2024-07-16T15:55:08+00:00</published><title>Is There Any Reliable Agentic Tool?</title></entry><entry><author><name>/u/Uiqueblhats</name><uri>https://www.reddit.com/user/Uiqueblhats</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e4k0is/gptinstagram_instagram_viral_posts_with_user_own/&quot;&gt; &lt;img src=&quot;https://external-preview.redd.it/QUer_6VdQ0MXHShyaUUxe_WIEaN1O-pzO2bByFl5EbM.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0c678ab30f1b0eeabc82c953f3d8b8f8d5ef1d78&quot; alt=&quot;GPT-Instagram : Instagram Viral Posts with user own personality&quot; title=&quot;GPT-Instagram : Instagram Viral Posts with user own personality&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;As a weekend project created a Multi Agent AI app in Next.js, LangChain.js &amp;amp; LangGraph.js to simulate a Marketing department to recommend Instagram Viral Posts with user own personality.&lt;/p&gt; &lt;p&gt;If anyone interested to try or look at code: &lt;a href=&quot;https://github.com/MODSetter/gpt-instagram&quot;&gt;https://github.com/MODSetter/gpt-instagram&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://reddit.com/link/1e4k0is/video/ig7wjanujucd1/player&quot;&gt;https://reddit.com/link/1e4k0is/video/ig7wjanujucd1/player&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Uiqueblhats&quot;&gt; /u/Uiqueblhats &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e4k0is/gptinstagram_instagram_viral_posts_with_user_own/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e4k0is/gptinstagram_instagram_viral_posts_with_user_own/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1e4k0is</id><media:thumbnail url="https://external-preview.redd.it/QUer_6VdQ0MXHShyaUUxe_WIEaN1O-pzO2bByFl5EbM.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=0c678ab30f1b0eeabc82c953f3d8b8f8d5ef1d78" /><link href="https://www.reddit.com/r/LangChain/comments/1e4k0is/gptinstagram_instagram_viral_posts_with_user_own/" /><updated>2024-07-16T09:07:19+00:00</updated><published>2024-07-16T09:07:19+00:00</published><title>GPT-Instagram : Instagram Viral Posts with user own personality</title></entry><entry><author><name>/u/bferreira85</name><uri>https://www.reddit.com/user/bferreira85</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi! As mentioned before, I have built a langgraph with a supervisor agent and two specialised agents. My question is: what are the best practices for having some of these agents working in multiple steps? For example: one of the agents will receive the user input and first present to the user what the agent is about to do and after the user confirms it then the agent will finish the work.&lt;br/&gt; My question is: how do I get the same agent to do both of these steps? Should I add system prompt to the agent on each step? (For example: adding something like &amp;quot;In this step now you must do X and display Y information to the user&amp;quot;). The problem here is that if there are multiple back and forth interactions between agent and user then the system prompt will become long and confusing.&lt;br/&gt; Other way could be saying to agent &amp;quot;when you are on step 1 you should focus on this, when you are on step 2 you should focus on that&amp;quot;.&lt;/p&gt; &lt;p&gt;Do any of you have experience with this kind of scenario?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/bferreira85&quot;&gt; /u/bferreira85 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e4l74f/langgraph_best_practices_for_multiple_steps_graph/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e4l74f/langgraph_best_practices_for_multiple_steps_graph/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1e4l74f</id><link href="https://www.reddit.com/r/LangChain/comments/1e4l74f/langgraph_best_practices_for_multiple_steps_graph/" /><updated>2024-07-16T10:25:57+00:00</updated><published>2024-07-16T10:25:57+00:00</published><title>Langgraph: best practices for multiple steps graph</title></entry><entry><author><name>/u/andyeverything</name><uri>https://www.reddit.com/user/andyeverything</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hello dear community,&lt;/p&gt; &lt;p&gt;I am currently familiarising myself with LLM, especially RAG. I have an idea for a Q&amp;amp;A ChatBot and would like to learn how to build one.&lt;/p&gt; &lt;p&gt;I would like to use an Excel spreadsheet with three columns as a database. The first column is a typical question. The second column contains tags that describe the question. The third column contains an answer to the question.&lt;/p&gt; &lt;p&gt;When the user asks a question, a semantic search should first be performed using the first and second columns. This will find the top of the most appropriate questions in a cell. The answers to these questions in the third column are provided to the LLM as context.&lt;/p&gt; &lt;p&gt;It is important to me that the structured properties of the table are fully exploited. Conventional RAGs return a fixed chunk size, even if it spans cells. However, I only want to return the cell that is relevant to the question.&lt;/p&gt; &lt;p&gt;My question is, what is the best way to implement this in LangChain? Does it make sense to store the questions and tags as metadata? Can I do a semantic search on the metadata and give the cell with the corresponding answer as context to the LLM? With the ulterior motive of returning only the relevant context? I have also thought about a self-query retriever. However, this tends to use keywords for filtering, not quite what I am looking for. Or am I on the wrong track?&lt;/p&gt; &lt;p&gt;Kind regards&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/andyeverything&quot;&gt; /u/andyeverything &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e4pqgn/qa_rag_over_tabular_data/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e4pqgn/qa_rag_over_tabular_data/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1e4pqgn</id><link href="https://www.reddit.com/r/LangChain/comments/1e4pqgn/qa_rag_over_tabular_data/" /><updated>2024-07-16T14:14:43+00:00</updated><published>2024-07-16T14:14:43+00:00</published><title>Q&amp;A RAG over tabular data</title></entry><entry><author><name>/u/PhotoAcceptable3563</name><uri>https://www.reddit.com/user/PhotoAcceptable3563</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;I need some clarifications. In my use case the first step requires an LLM to identify and split different segment of the input text / document. Then, for each of the segments I have a linear flow to follow (extract info, call agents, ...). Finally I have to collect all the outputs.&lt;/p&gt; &lt;p&gt;I am unsure how to achieve the &amp;quot;for loop&amp;quot; (if possible). Instead of an &lt;code&gt;add_edge&lt;/code&gt;, I&amp;#39;d need an add edges&lt;/p&gt; &lt;pre&gt;&lt;code&gt;workflow.add_node(&amp;quot;split&amp;quot;, split) workflow.add_node(&amp;quot;extract&amp;quot;, extract) workflow.add_node(&amp;quot;collect&amp;quot;, collect) workflow.set_entry_point(&amp;quot;split&amp;quot;) # after split I get an array of chunks workflow.add_edges(&amp;quot;split&amp;quot;, &amp;quot;extract&amp;quot;) # for each chunk do some custom logic workflow.collect_edges(&amp;quot;extract&amp;quot;, &amp;quot;collect&amp;quot;) # collect everything &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/PhotoAcceptable3563&quot;&gt; /u/PhotoAcceptable3563 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e4rcha/graph_with_a_for_loop/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e4rcha/graph_with_a_for_loop/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1e4rcha</id><link href="https://www.reddit.com/r/LangChain/comments/1e4rcha/graph_with_a_for_loop/" /><updated>2024-07-16T15:20:32+00:00</updated><published>2024-07-16T15:20:32+00:00</published><title>Graph with a for loop</title></entry><entry><author><name>/u/Comfortable_Dog5217</name><uri>https://www.reddit.com/user/Comfortable_Dog5217</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Are there anyone who has successfully done langgraph with human in the loop in production? How does that work? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Comfortable_Dog5217&quot;&gt; /u/Comfortable_Dog5217 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e4hays/langgraph_human_in_the_loop_in_production/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e4hays/langgraph_human_in_the_loop_in_production/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1e4hays</id><link href="https://www.reddit.com/r/LangChain/comments/1e4hays/langgraph_human_in_the_loop_in_production/" /><updated>2024-07-16T06:05:52+00:00</updated><published>2024-07-16T06:05:52+00:00</published><title>Langgraph Human in the loop in Production</title></entry><entry><author><name>/u/MagentaSpark</name><uri>https://www.reddit.com/user/MagentaSpark</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;m using supervisor graph and various agent graphs. How do pass state within nodes of child graph and parent graph? I couldn&amp;#39;t get what &lt;a href=&quot;https://langchain-ai.github.io/langgraph/how-tos/subgraph/&quot;&gt;documentation&lt;/a&gt; was trying to say.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/MagentaSpark&quot;&gt; /u/MagentaSpark &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e4l92y/how_to_deal_with_multiple_states/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e4l92y/how_to_deal_with_multiple_states/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1e4l92y</id><link href="https://www.reddit.com/r/LangChain/comments/1e4l92y/how_to_deal_with_multiple_states/" /><updated>2024-07-16T10:29:28+00:00</updated><published>2024-07-16T10:29:28+00:00</published><title>How to deal with multiple states?</title></entry><entry><author><name>/u/fpgsgamer</name><uri>https://www.reddit.com/user/fpgsgamer</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;As of now I have a chatbot (OpenAI embeddings &amp;amp; chat model) that can take in text/pdf files and answer questions about them using embeddings and a vectorDB. I was wondering if it would be plausible to embed an image into the vectorDB, and use a multi modal LLM to be able to answer questions about said picture. I&amp;#39;ve been looking at other approaches (generating text summaries of image and embedding that/sending the image straight to a multimodal LLM with the prompt) but I like the idea of embedding images into my vectorDB, and I don&amp;#39;t really know the pros/cons between these methods. Any help is appreciated !&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/fpgsgamer&quot;&gt; /u/fpgsgamer &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e4gvl1/how_should_i_set_up_my_multimodal_chatbot/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e4gvl1/how_should_i_set_up_my_multimodal_chatbot/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1e4gvl1</id><link href="https://www.reddit.com/r/LangChain/comments/1e4gvl1/how_should_i_set_up_my_multimodal_chatbot/" /><updated>2024-07-16T05:39:01+00:00</updated><published>2024-07-16T05:39:01+00:00</published><title>How should I set up my multimodal chatbot?</title></entry><entry><author><name>/u/thedabking123</name><uri>https://www.reddit.com/user/thedabking123</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;ve set up a dict of main and sub questions(the answer for each feeds into the next) that build on an initial user request. &lt;/p&gt; &lt;p&gt;I set up a RAG agent workflow on langgraph and need to debug the overall conversational thread by reading through what is generated for each question.&lt;/p&gt; &lt;p&gt;I know I can extract the current state from agent_executor.stream() but how do I get every iteration for every question asked? Is the only way to set up a for loop and print/ save the streamed outputs each time? What does the llm get at each turn in the conversation? Shouldn&amp;#39;t it be fed the full history?&lt;/p&gt; &lt;p&gt;Also any way to summarize and store a thread for downstream agents to absorb? If so any way to surface that summary for user inspection?&lt;/p&gt; &lt;p&gt;Sorry if this is basic- still learning.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/thedabking123&quot;&gt; /u/thedabking123 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e4masy/new_here_how_do_i_surface_and_save_historical/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e4masy/new_here_how_do_i_surface_and_save_historical/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1e4masy</id><link href="https://www.reddit.com/r/LangChain/comments/1e4masy/new_here_how_do_i_surface_and_save_historical/" /><updated>2024-07-16T11:31:52+00:00</updated><published>2024-07-16T11:31:52+00:00</published><title>New here - how do I surface and save historical threads of agent actions and conversations in langgraoh</title></entry><entry><author><name>/u/neilkatz</name><uri>https://www.reddit.com/user/neilkatz</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey Everyone,&lt;/p&gt; &lt;p&gt;I&amp;#39;m curious what the group thinks are the biggest pain points for devs getting started with RAG? My list would be: &lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;hallucination&lt;/strong&gt;: especially with complex docs&lt;/li&gt; &lt;li&gt;&lt;strong&gt;eval&lt;/strong&gt;: there are tools to score completions vs retrievals, but what about the rest of the RAG pipeline where the problems actually occur. &lt;/li&gt; &lt;li&gt;&lt;strong&gt;complexity:&lt;/strong&gt; many pieces of the pipeline to master (parse, extract, convert to LLM friendly data, chunk, embed, create metadata for context, search, rerank, etc) and lots of theories on best approach to each one. &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;What&amp;#39;s everyone else dealing with?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/neilkatz&quot;&gt; /u/neilkatz &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e3ygh6/biggest_rag_hurdles_for_beginners/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e3ygh6/biggest_rag_hurdles_for_beginners/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1e3ygh6</id><link href="https://www.reddit.com/r/LangChain/comments/1e3ygh6/biggest_rag_hurdles_for_beginners/" /><updated>2024-07-15T16:01:08+00:00</updated><published>2024-07-15T16:01:08+00:00</published><title>Biggest RAG Hurdles for Beginners?</title></entry><entry><author><name>/u/Big_Barracuda_6753</name><uri>https://www.reddit.com/user/Big_Barracuda_6753</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;pre&gt;&lt;code&gt; if prompt := st.chat_input(&amp;quot;Hey, What&amp;#39;s up?&amp;quot;): if len(pc.list_indexes()) == 0: st.error(&amp;quot;Please upload some files first!&amp;quot;) else: st.session_state.messages.append({&amp;quot;role&amp;quot;: &amp;quot;user&amp;quot;, &amp;quot;content&amp;quot;: prompt}) contextualize_q_system_prompt = ( &amp;quot;Given a chat history and the latest user question &amp;quot; &amp;quot;which might reference context in the chat history, &amp;quot; &amp;quot;formulate a standalone question which can be understood &amp;quot; &amp;quot;without the chat history. Do NOT answer the question, &amp;quot; &amp;quot;just reformulate it if needed and otherwise return it as is.&amp;quot; ) contextualize_q_prompt = ChatPromptTemplate.from_messages( [ (&amp;quot;system&amp;quot;, contextualize_q_system_prompt), MessagesPlaceholder(&amp;quot;chat_history&amp;quot;), (&amp;quot;human&amp;quot;, &amp;quot;{input}&amp;quot;), ] ) # chunks ( 5 ) compression_retriever = reRanker() history_aware_retriever = create_history_aware_retriever( llm, compression_retriever, contextualize_q_prompt ) system_prompt = ( &amp;quot;You are an assistant for question-answering tasks. &amp;quot; &amp;quot;Use the following pieces of retrieved context to answer &amp;quot; &amp;quot;the question. If you don&amp;#39;t know the answer, say that you &amp;quot; &amp;quot;don&amp;#39;t know.&amp;quot; &amp;quot;\n\n&amp;quot; &amp;quot;{context}&amp;quot; ) chatPrompt = ChatPromptTemplate.from_messages( [ (&amp;quot;system&amp;quot;, system_prompt), MessagesPlaceholder(&amp;quot;chat_history&amp;quot;), (&amp;quot;human&amp;quot;, &amp;quot;{input}&amp;quot;), ] ) question_answer_chain = create_stuff_documents_chain(llm, chatPrompt) rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain) conversational_rag_chain = RunnableWithMessageHistory( rag_chain, get_session_history, input_messages_key=&amp;quot;input&amp;quot;, history_messages_key=&amp;quot;chat_history&amp;quot;, output_messages_key=&amp;quot;answer&amp;quot;, ) response = conversational_rag_chain.invoke( input={&amp;quot;input&amp;quot;: prompt}, config={&amp;#39;configurable&amp;#39;: {&amp;#39;session_id&amp;#39;: &amp;#39;hdf23me23edewDFSDMS&amp;#39;}} ) print(&amp;quot;response:&amp;quot;, response[&amp;quot;answer&amp;quot;]) st.session_state.messages.append({&amp;quot;role&amp;quot;: &amp;quot;assistant&amp;quot;, &amp;quot;content&amp;quot;: response[&amp;quot;answer&amp;quot;]}) # Display updated messages for message in st.session_state.messages: with st.chat_message(message[&amp;quot;role&amp;quot;]): st.markdown(message[&amp;quot;content&amp;quot;]) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;How do I output my response as a stream using Langchain&amp;#39;s create_retrieval_chain ?&lt;br/&gt; Currently , I get my response when all of it is generated ( at once ) , but I want streaming feature instead .&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Big_Barracuda_6753&quot;&gt; /u/Big_Barracuda_6753 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e4lnd5/how_do_i_stream_my_output_using/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e4lnd5/how_do_i_stream_my_output_using/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1e4lnd5</id><link href="https://www.reddit.com/r/LangChain/comments/1e4lnd5/how_do_i_stream_my_output_using/" /><updated>2024-07-16T10:54:22+00:00</updated><published>2024-07-16T10:54:22+00:00</published><title>How do I stream my output using ?</title></entry><entry><author><name>/u/Babe_My_Name_Is_Hung</name><uri>https://www.reddit.com/user/Babe_My_Name_Is_Hung</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi everyone, I have been reading around about Graph RAG lately. I don’t quite understand how the retriever searches for “relevant entities”? I have thought of exact full text search, but they would not be quite effective when the entities can be (and will be!) ambiguous ( e.g. two people with the same name, or same people with different names, etc.). Or maybe semantic search is utilized here? If so, I don’t think it would be efficient for a large graph with many entities. Really appreciate your help! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Babe_My_Name_Is_Hung&quot;&gt; /u/Babe_My_Name_Is_Hung &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e4c1j5/how_do_graph_rag_search_for_relevant_nodes/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1e4c1j5/how_do_graph_rag_search_for_relevant_nodes/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1e4c1j5</id><link href="https://www.reddit.com/r/LangChain/comments/1e4c1j5/how_do_graph_rag_search_for_relevant_nodes/" /><updated>2024-07-16T01:21:32+00:00</updated><published>2024-07-16T01:21:32+00:00</published><title>How do Graph RAG search for relevant nodes?</title></entry></feed>