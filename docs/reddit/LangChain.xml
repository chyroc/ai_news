<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/"><category term="LangChain" label="r/LangChain"/><updated>2024-07-26T17:26:55+00:00</updated><icon>https://www.redditstatic.com/icon.png/</icon><id>/r/LangChain.rss</id><link rel="self" href="https://www.reddit.com/r/LangChain.rss" type="application/atom+xml" /><link rel="alternate" href="https://www.reddit.com/r/LangChain" type="text/html" /><subtitle>LangChain is an open-source framework and developer toolkit that helps developers get LLM applications from prototype to production. It is available for Python and Javascript at https://www.langchain.com/.</subtitle><title>LangChain</title><entry><author><name>/u/zchaarm</name><uri>https://www.reddit.com/user/zchaarm</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;A place for members of &lt;a href=&quot;/r/LangChain&quot;&gt;r/LangChain&lt;/a&gt; to chat with each other&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/zchaarm&quot;&gt; /u/zchaarm &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_10ljho9</id><link href="https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/" /><updated>2023-01-26T04:33:05+00:00</updated><published>2023-01-26T04:33:05+00:00</published><title>r/LangChain Lounge</title></entry><entry><author><name>/u/Shingma</name><uri>https://www.reddit.com/user/Shingma</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;m playing around with Crew, Autogen and LangChain wanted to know what tools are best suited for Multi Agent applications, where to find them or if I need to build them myself. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Shingma&quot;&gt; /u/Shingma &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ecn0fq/what_tools_are_you_using_with_your_ai_agents/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ecn0fq/what_tools_are_you_using_with_your_ai_agents/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1ecn0fq</id><link href="https://www.reddit.com/r/LangChain/comments/1ecn0fq/what_tools_are_you_using_with_your_ai_agents/" /><updated>2024-07-26T12:36:31+00:00</updated><published>2024-07-26T12:36:31+00:00</published><title>What tools are you using with your AI agents?</title></entry><entry><author><name>/u/phicreative1997</name><uri>https://www.reddit.com/user/phicreative1997</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ecp82w/building_a_human_resource_graphrag_application/&quot;&gt; &lt;img src=&quot;https://external-preview.redd.it/MM3_iXjJ5f9zLbFgI_OqW5HbRDQZypY4ocXngeaUmpI.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bf25726624e218fbb8aed206f16571b9216ef7c4&quot; alt=&quot;Building a Human Resource GraphRAG application&quot; title=&quot;Building a Human Resource GraphRAG application&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/phicreative1997&quot;&gt; /u/phicreative1997 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://medium.com/firebird-technologies/building-a-human-resource-graphrag-application-279f07cf71d6&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ecp82w/building_a_human_resource_graphrag_application/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1ecp82w</id><media:thumbnail url="https://external-preview.redd.it/MM3_iXjJ5f9zLbFgI_OqW5HbRDQZypY4ocXngeaUmpI.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=bf25726624e218fbb8aed206f16571b9216ef7c4" /><link href="https://www.reddit.com/r/LangChain/comments/1ecp82w/building_a_human_resource_graphrag_application/" /><updated>2024-07-26T14:17:47+00:00</updated><published>2024-07-26T14:17:47+00:00</published><title>Building a Human Resource GraphRAG application</title></entry><entry><author><name>/u/Longjumping-Buddy501</name><uri>https://www.reddit.com/user/Longjumping-Buddy501</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ecoex7/gpt4omini_is_terribly_slow_today_anyone_else/&quot;&gt; &lt;img src=&quot;https://b.thumbs.redditmedia.com/9Cgucf2SuD7c0iqUpWblJ0VTLYnsAKdl8qtg9yblXio.jpg&quot; alt=&quot;GPT-4o-mini is terribly slow today. Anyone else facing this issue?&quot; title=&quot;GPT-4o-mini is terribly slow today. Anyone else facing this issue?&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;&lt;a href=&quot;https://preview.redd.it/hmxu408raved1.png?width=170&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5bbc2d537d3c183ab7e684f0457754f38a967083&quot;&gt;https://preview.redd.it/hmxu408raved1.png?width=170&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5bbc2d537d3c183ab7e684f0457754f38a967083&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/k0xf4x7raved1.png?width=170&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=725eb06885156a134e53e45e928089ffa8e5a97f&quot;&gt;https://preview.redd.it/k0xf4x7raved1.png?width=170&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=725eb06885156a134e53e45e928089ffa8e5a97f&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The latency on GPT-4o-mini is terrible today. It is taking 96 seconds and above for simple answers &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Longjumping-Buddy501&quot;&gt; /u/Longjumping-Buddy501 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ecoex7/gpt4omini_is_terribly_slow_today_anyone_else/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ecoex7/gpt4omini_is_terribly_slow_today_anyone_else/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1ecoex7</id><media:thumbnail url="https://b.thumbs.redditmedia.com/9Cgucf2SuD7c0iqUpWblJ0VTLYnsAKdl8qtg9yblXio.jpg" /><link href="https://www.reddit.com/r/LangChain/comments/1ecoex7/gpt4omini_is_terribly_slow_today_anyone_else/" /><updated>2024-07-26T13:42:47+00:00</updated><published>2024-07-26T13:42:47+00:00</published><title>GPT-4o-mini is terribly slow today. Anyone else facing this issue?</title></entry><entry><author><name>/u/Substantial_Gift_861</name><uri>https://www.reddit.com/user/Substantial_Gift_861</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I plan to build a chatbot to answer those product information. But I don&amp;#39;t know which one to use, RAG or openai gpt?&lt;/p&gt; &lt;p&gt;I heard that RAG might not accurate and cant generate reply very well&lt;/p&gt; &lt;p&gt;Which one will you choose? &lt;/p&gt; &lt;p&gt;If you want to build a chatbot like that, what will you use?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Substantial_Gift_861&quot;&gt; /u/Substantial_Gift_861 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1eck671/build_a_chatbot_by_using_rag_or_openai_gpt_api/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1eck671/build_a_chatbot_by_using_rag_or_openai_gpt_api/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1eck671</id><link href="https://www.reddit.com/r/LangChain/comments/1eck671/build_a_chatbot_by_using_rag_or_openai_gpt_api/" /><updated>2024-07-26T09:51:53+00:00</updated><published>2024-07-26T09:51:53+00:00</published><title>Build a chatbot by using Rag or openai gpt API?</title></entry><entry><author><name>/u/OnY86</name><uri>https://www.reddit.com/user/OnY86</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi, i am new to langchain and i hope somebody here can help me understand some basics.&lt;/p&gt; &lt;p&gt;I have a server serving the model via vllm openai endpoint. The model uses a ChatML template:&lt;/p&gt; &lt;p&gt;&amp;lt;|im_start|&amp;gt;system {system_message}&amp;lt;|im_end|&amp;gt; &amp;lt;|im_start|&amp;gt;user {prompt}&amp;lt;|im_end|&amp;gt; &amp;lt;|im_start|&amp;gt;assistant&lt;/p&gt; &lt;p&gt;Can langchain handle this and if so, how? Or did i combine the wrong stuff together?&lt;/p&gt; &lt;p&gt;I am asking, because my LLM is responding sometimes in a strange way. For example, i asked „how far is the moon“ and i get an endless response back. The first sentence is the right answer but all that follows after, it is total nonsense.&lt;/p&gt; &lt;p&gt;Thanks&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/OnY86&quot;&gt; /u/OnY86 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ectcut/vllm_dolphin26mistral_langchain/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ectcut/vllm_dolphin26mistral_langchain/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1ectcut</id><link href="https://www.reddit.com/r/LangChain/comments/1ectcut/vllm_dolphin26mistral_langchain/" /><updated>2024-07-26T17:08:06+00:00</updated><published>2024-07-26T17:08:06+00:00</published><title>Vllm + dolphin-2.6-mistral + langchain</title></entry><entry><author><name>/u/Important_Ostrich_60</name><uri>https://www.reddit.com/user/Important_Ostrich_60</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I have developed a tool calling llm using OpenAI&amp;#39;s GPT-3.5-turbo-1106, integrated with LangSmith and LangGraph. I followed the official documentation to track token usage but encountered issues:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Final Response Metadata:&lt;/strong&gt; The final response doesn&amp;#39;t include metadata about token usage.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;openai_callback Function:&lt;/strong&gt; This method returns zero tokens used every time.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Here&amp;#39;s the documentation link: &lt;a href=&quot;https://python.langchain.com/v0.1/docs/modules/model_io/llms/token_usage_tracking/&quot;&gt;Token Usage Tracking&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Could you assist me in obtaining the token cost per API request? Although LangSmith provides token usage in their UI, I need to access this information programmatically in my application.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Important_Ostrich_60&quot;&gt; /u/Important_Ostrich_60 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ecqmk7/how_to_get_token_costs_per_request/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ecqmk7/how_to_get_token_costs_per_request/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1ecqmk7</id><link href="https://www.reddit.com/r/LangChain/comments/1ecqmk7/how_to_get_token_costs_per_request/" /><updated>2024-07-26T15:15:37+00:00</updated><published>2024-07-26T15:15:37+00:00</published><title>How to get token costs per request</title></entry><entry><author><name>/u/AImEdo</name><uri>https://www.reddit.com/user/AImEdo</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;I&amp;#39;m currently exploring the Structured Output feature in LangChain for a personal project. I’ve been following the guide here: &lt;a href=&quot;https://python.langchain.com/v0.1/docs/modules/model_io/chat/structured_output/&quot;&gt;LangChain Structured Output&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Here’s a basic example I’m working with:&lt;/p&gt; &lt;p&gt;&lt;code&gt;from langchain_core.pydantic_v1 import BaseModel, Field&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;class Joke(BaseModel):&lt;/code&gt;&lt;br/&gt; &lt;code&gt;setup: str = Field(description=&amp;quot;The setup of the joke&amp;quot;)&lt;/code&gt;&lt;br/&gt; &lt;code&gt;punchline: str = Field(description=&amp;quot;The punchline to the joke&amp;quot;)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;model = ChatOpenAI(model=&amp;quot;gpt-3.5-turbo-0125&amp;quot;, temperature=0)&lt;/code&gt;&lt;br/&gt; &lt;code&gt;structured_llm = model.with_structured_output(Joke)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;structured_llm.invoke(&amp;quot;Tell me a joke about cats&amp;quot;)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;I’ve experimented with more complex structures that include multiple fields and even nested fields. It’s important to note that not all fields necessarily come from a single chunk of text, but they could.&lt;/p&gt; &lt;p&gt;However, I’ve encountered a problem: when the model can&amp;#39;t find a value for a field, it returns the field’s description instead of leaving it blank or providing a default value. This results in unnecessary token generation.&lt;/p&gt; &lt;p&gt;I’m looking for a solution to set default values such as an empty string for string fields and &lt;code&gt;-1&lt;/code&gt; for numerical fields when the model doesn’t provide a value.&lt;/p&gt; &lt;p&gt;Has anyone else dealt with this issue? Is there a method to ensure that missing field values are replaced with desired defaults rather than the field descriptions? Any insights or suggestions would be greatly appreciated!&lt;/p&gt; &lt;p&gt;Thanks in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/AImEdo&quot;&gt; /u/AImEdo &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ecqk1q/why_does_langchains_basemodel_sometimes_output_a/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ecqk1q/why_does_langchains_basemodel_sometimes_output_a/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1ecqk1q</id><link href="https://www.reddit.com/r/LangChain/comments/1ecqk1q/why_does_langchains_basemodel_sometimes_output_a/" /><updated>2024-07-26T15:12:48+00:00</updated><published>2024-07-26T15:12:48+00:00</published><title>Why does LangChain's BaseModel sometimes output a copy of the Pydantic field description instead of the expected value?</title></entry><entry><author><name>/u/alkibijad</name><uri>https://www.reddit.com/user/alkibijad</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ecimvn/memory_leak_in_langserve_app/&quot;&gt; &lt;img src=&quot;https://b.thumbs.redditmedia.com/mZII8QzqaDGJ2KPcrpL5oTa_A2wxtAvxs5gfoG1nQKQ.jpg&quot; alt=&quot;Memory leak in Langserve app&quot; title=&quot;Memory leak in Langserve app&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;m hosting a langserve app. The app is quite simple, but there seems to be a memory leak. Any ideas on why this is happening?&lt;/p&gt; &lt;p&gt;With every new requests, the RAM increases and doesn&amp;#39;t go down: &lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/9k9j0z7ikted1.png?width=2256&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=dfe91726776cfabb5060e0da96ce4a41b858e724&quot;&gt;https://preview.redd.it/9k9j0z7ikted1.png?width=2256&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=dfe91726776cfabb5060e0da96ce4a41b858e724&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/2d0hncamkted1.png?width=1128&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=51b95e833150a6bf810ff4f3ba736ef9b91da587&quot;&gt;https://preview.redd.it/2d0hncamkted1.png?width=1128&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=51b95e833150a6bf810ff4f3ba736ef9b91da587&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The code is pretty straightforward.&lt;br/&gt; Chain definition: &lt;/p&gt; &lt;h1&gt;file: public_review.py&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;from langchain_core.output_parsers import JsonOutputParser from langchain_core.prompts import ChatPromptTemplate from langchain_openai import ChatOpenAI from app.prompts.public_review_analysis_prompt import ( PUBLIC_REVIEW_ISSUE_GENERATOR_SYSTEM_PROMPT, ) public_review_text_chain = ChatPromptTemplate.from_messages( [ ( &amp;quot;system&amp;quot;, PUBLIC_REVIEW_ISSUE_GENERATOR_SYSTEM_PROMPT, ), (&amp;quot;user&amp;quot;, &amp;quot;{text}&amp;quot;), ] ) | ChatOpenAI(model=&amp;quot;gpt-4o&amp;quot;, temperature=0.03, model_kwargs={&amp;quot;seed&amp;quot;: 13}) public_review_chain = ( | public_review_text_chain | JsonOutputParser(pydantic_object=IssueList) ) # Chain added to router and router is then added to the app from fastapi import APIRouter from langserve import add_routes from app.enrichment.aggregator import aggregator_review_chain, aggregator_text_chain from app.enrichment.public_review import public_review_chain, public_review_text_chain from app.enrichment.types import ( InputFragment, InputFragmentList ) router = APIRouter() add_routes( router, public_review_chain.with_types(input_type=InputFragmentList, output_type=IssueList), path=&amp;quot;/api/v1/public_review&amp;quot;, ) add_routes(router, public_review_text_chain, path=&amp;quot;/api/v1/public_review/text&amp;quot;) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Any ideas what could be causing the leak?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/alkibijad&quot;&gt; /u/alkibijad &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ecimvn/memory_leak_in_langserve_app/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ecimvn/memory_leak_in_langserve_app/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1ecimvn</id><media:thumbnail url="https://b.thumbs.redditmedia.com/mZII8QzqaDGJ2KPcrpL5oTa_A2wxtAvxs5gfoG1nQKQ.jpg" /><link href="https://www.reddit.com/r/LangChain/comments/1ecimvn/memory_leak_in_langserve_app/" /><updated>2024-07-26T08:03:57+00:00</updated><published>2024-07-26T08:03:57+00:00</published><title>Memory leak in Langserve app</title></entry><entry><author><name>/u/Otherwise-Patient-34</name><uri>https://www.reddit.com/user/Otherwise-Patient-34</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;ve metadata( like table structure, column details, data types) of RDBMS and other sources currently stored in MYSQL database. I want to build a simple bot that answers queries from data engineers and analytics like &amp;quot;&lt;/p&gt; &lt;p&gt;1.get me ddl of .. this table&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;what is the meaning of this column&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;show me complete column list with description of this table in postgres environment: etc.&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;How can I use Vector db along with LLM to achieve this goal.&lt;/p&gt; &lt;p&gt;I&amp;#39;m not sure how to design a scheme, whether to vectorize the name of the table alone etc.&lt;/p&gt; &lt;p&gt;N.B : Some of the tables can have 500 columns also.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Otherwise-Patient-34&quot;&gt; /u/Otherwise-Patient-34 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ec02w8/using_milvusrag_as_metadata_store/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ec02w8/using_milvusrag_as_metadata_store/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1ec02w8</id><link href="https://www.reddit.com/r/LangChain/comments/1ec02w8/using_milvusrag_as_metadata_store/" /><updated>2024-07-25T17:04:50+00:00</updated><published>2024-07-25T17:04:50+00:00</published><title>Using Milvus/RAG as metadata store</title></entry><entry><author><name>/u/qa_anaaq</name><uri>https://www.reddit.com/user/qa_anaaq</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;m labeling and taking screenshot of webpages that I then send to 4V to analyze. Basically, the labeling creates borders around html elements then I ask GPT to determine if there is a popup or if there are elements at certain places on the page. (I&amp;#39;m being a little vague due to the specific use case.) &lt;/p&gt; &lt;p&gt;What would a good approach be for prompting? I am providing reference images with explanations for each reference. Even though things are being labeled ok, I don&amp;#39;t seem to be able to prompt it well. So I&amp;#39;m wondering if one prompting strategy over another might be good. &lt;/p&gt; &lt;p&gt;Note that this flow is partially based on the WebVoyager paper that uses LangGraph, though it&amp;#39;s not web browsing. Just a single labeled page. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/qa_anaaq&quot;&gt; /u/qa_anaaq &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ec8c4e/vision_analysis/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ec8c4e/vision_analysis/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1ec8c4e</id><link href="https://www.reddit.com/r/LangChain/comments/1ec8c4e/vision_analysis/" /><updated>2024-07-25T22:45:35+00:00</updated><published>2024-07-25T22:45:35+00:00</published><title>Vision Analysis</title></entry><entry><author><name>/u/PretendVermicelli657</name><uri>https://www.reddit.com/user/PretendVermicelli657</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;m new to langchain and currently learning the official [tutorial](&lt;a href=&quot;https://python.langchain.com/v0.2/docs/tutorials/agents/&quot;&gt;https://python.langchain.com/v0.2/docs/tutorials/agents/&lt;/a&gt;). I have tried Ollama and llama.cpp, but none of them can finish the tutorial.&lt;/p&gt; &lt;p&gt;As known, Ollama doesn&amp;#39;t support bind_tools originally. With the help of OllamaFunctions in langchain_experiment package, it worked and outputed similar intermediate information but failed when generating text according to response from tools.&lt;/p&gt; &lt;p&gt;When it comes to llama.cpp, it does have bind_tools function. The problem is that it didn&amp;#39;t generate text according to response from tools.&lt;/p&gt; &lt;p&gt;So, is there a way to go through the tutorials with local llms or an example about finishing those tutorials with Ollama and llama.cpp? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/PretendVermicelli657&quot;&gt; /u/PretendVermicelli657 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ebundi/how_to_build_agent_with_local_llm/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ebundi/how_to_build_agent_with_local_llm/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1ebundi</id><link href="https://www.reddit.com/r/LangChain/comments/1ebundi/how_to_build_agent_with_local_llm/" /><updated>2024-07-25T13:17:22+00:00</updated><published>2024-07-25T13:17:22+00:00</published><title>How to build agent with local llm</title></entry><entry><author><name>/u/phan_ngt</name><uri>https://www.reddit.com/user/phan_ngt</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I use Semantic Chunker from this tutorial: &lt;a href=&quot;https://python.langchain.com/v0.2/docs/how_to/semantic-chunker/&quot;&gt;https://python.langchain.com/v0.2/docs/how_to/semantic-chunker/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;However, I met the error below. I think because my pdf has 64 pages. Too long for OpenAI to handle. What should I do? If I split page by page, I am afraid that I will lost the content between pages. Recursive Chunker seems better in this case. &lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;openai.InternalServerError: Error code: 503 - {&amp;#39;error&amp;#39;: {&amp;#39;code&amp;#39;: &amp;#39;InternalServerError&amp;#39;, &amp;#39;message&amp;#39;: &amp;#39;The service is temporarily unable to process your request. Please try again later.&amp;#39;}}&lt;/p&gt; &lt;p&gt;python-BaseException&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/phan_ngt&quot;&gt; /u/phan_ngt &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ebs25f/semanticchunker_for_very_large_text/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ebs25f/semanticchunker_for_very_large_text/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1ebs25f</id><link href="https://www.reddit.com/r/LangChain/comments/1ebs25f/semanticchunker_for_very_large_text/" /><updated>2024-07-25T11:02:03+00:00</updated><published>2024-07-25T11:02:03+00:00</published><title>SemanticChunker for very large text</title></entry><entry><author><name>/u/AdAway2620</name><uri>https://www.reddit.com/user/AdAway2620</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hello all, Have anyone among here done sales forecasting using LLMs ?&lt;br/&gt; For eg: I have monthly sales data of last 2 years and i want to predict the monthly sales of upcoming year.&lt;br/&gt; What would be the best way to do it ?&lt;/p&gt; &lt;p&gt;If anyone has code snippet, I would be happy to look at it.&lt;br/&gt; I welcome ML/DL approach as well but since my dataset is very low what would be the best idea ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/AdAway2620&quot;&gt; /u/AdAway2620 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ebm8ss/salesforecasting_using_ai_models_llm/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ebm8ss/salesforecasting_using_ai_models_llm/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1ebm8ss</id><link href="https://www.reddit.com/r/LangChain/comments/1ebm8ss/salesforecasting_using_ai_models_llm/" /><updated>2024-07-25T04:42:39+00:00</updated><published>2024-07-25T04:42:39+00:00</published><title>Salesforecasting using AI models / LLM</title></entry><entry><author><name>/u/divinity27</name><uri>https://www.reddit.com/user/divinity27</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi I am trying to extract information from purchase orders PDFs with different formats , when conventional py libraries didn&amp;#39;t extract the data the way I wanted I resorted to Azure Gpt 4 vision model and converted the pages of my pdf as images and used the api to get back the response. The problem is in some documents it is deliberately missing clearly written information in the images , I tried tweaking the prompt as well. But not helping much. I am using pdf2image to convert to JPEGs and using 500 dpi as parameter in the convert_from_path function imported from library. Any recommendations or help would be much appreciated.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/divinity27&quot;&gt; /u/divinity27 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ebt2gl/improving_output_of_azure_gpt_4_vision_model/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ebt2gl/improving_output_of_azure_gpt_4_vision_model/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1ebt2gl</id><link href="https://www.reddit.com/r/LangChain/comments/1ebt2gl/improving_output_of_azure_gpt_4_vision_model/" /><updated>2024-07-25T11:59:16+00:00</updated><published>2024-07-25T11:59:16+00:00</published><title>Improving output of Azure Gpt 4 vision model , ignoring part of text present in image</title></entry><entry><author><name>/u/rayquaza_111</name><uri>https://www.reddit.com/user/rayquaza_111</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Here&amp;#39;s my chain, but looks something wrong. Earlier without the code of agents part, it was working well with chat history.&lt;/p&gt; &lt;p&gt;Any help is appreciated.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;def _prepare_chain(self): contextualize_q_system_prompt = ( &amp;quot;Given a chat history and the latest user question &amp;quot; &amp;quot;which might reference context in the chat history, &amp;quot; &amp;quot;formulate a standalone question which can be understood &amp;quot; &amp;quot;without the chat history. Do NOT answer the question, &amp;quot; &amp;quot;just reformulate it if needed and otherwise return it as is.&amp;quot; ) _llm = self.llm if self.tools: _llm = self.llm.bind_tools(self.tools) contextualize_q_prompt = ChatPromptTemplate.from_messages( [ (&amp;quot;system&amp;quot;, contextualize_q_system_prompt), MessagesPlaceholder(&amp;quot;chat_history&amp;quot;), (&amp;quot;human&amp;quot;, &amp;quot;{input}&amp;quot;), ] ) history_aware_retriever = create_history_aware_retriever( _llm, self.retriever, contextualize_q_prompt ) ### Answer question ### system_prompt = ( &amp;quot;{base_prompt}&amp;quot; &amp;quot;Act like a support person who loves helping customers. &amp;quot; &amp;quot;Use the following pieces of retrieved context to answer &amp;quot; &amp;quot;the question. If you don&amp;#39;t know the answer, say that you &amp;quot; &amp;quot;don&amp;#39;t know. Use three sentences maximum and keep the &amp;quot; &amp;quot;answer concise.&amp;quot; &amp;quot;\n\n&amp;quot; &amp;quot;{context}&amp;quot; ) ANSWER_PROMPT = PromptTemplate.from_template(system_prompt) qa_prompt = ChatPromptTemplate.from_messages( [ (&amp;quot;system&amp;quot;, system_prompt), MessagesPlaceholder(&amp;quot;chat_history&amp;quot;), (&amp;quot;human&amp;quot;, &amp;quot;{question}&amp;quot;), ] ) question_answer_chain = create_stuff_documents_chain(_llm, qa_prompt) retrieval_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain) _runnable = ( RunnablePassthrough.assign( agent_scratchpad=lambda x: format_to_openai_tool_messages(x[&amp;quot;intermediate_steps&amp;quot;]) ) | retrieval_chain | _llm | OpenAIToolsAgentOutputParser() ) _agent = RunnableAgent(runnable=_runnable) _output = RunnableParallel( answer=AgentExecutor(agent=_agent, tools=self.tools), sources=history_aware_retriever | self._extract_sources ) rag_chain = RunnablePassthrough.assign( input=lambda x: x[&amp;quot;question&amp;quot;]) | _output | RunnableLambda(self.log_chain) conversational_rag_chain = RunnableWithMessageHistory( rag_chain, self.get_session_history, input_messages_key=&amp;quot;question&amp;quot;, history_messages_key=&amp;quot;chat_history&amp;quot;, output_messages_key=&amp;quot;answer&amp;quot;, ) return conversational_rag_chain &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/rayquaza_111&quot;&gt; /u/rayquaza_111 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1eblbm7/not_able_to_figure_out_agents_with_chat_history/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1eblbm7/not_able_to_figure_out_agents_with_chat_history/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1eblbm7</id><link href="https://www.reddit.com/r/LangChain/comments/1eblbm7/not_able_to_figure_out_agents_with_chat_history/" /><updated>2024-07-25T03:50:18+00:00</updated><published>2024-07-25T03:50:18+00:00</published><title>Not able to figure out Agents with Chat History</title></entry><entry><author><name>/u/mallerius</name><uri>https://www.reddit.com/user/mallerius</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey I don&amp;#39;t know if this is the right sub. I rented a server that uses a Rtx 4000 with 20gb. I tried to get models like mistral or llamma to run on it but it fails to generate answers because it runs out of memory. Are there anyways to reduce the amount of memory needed? Or other ways to solve this problem? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/mallerius&quot;&gt; /u/mallerius &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ebsdbv/video_ram_problems_on_a_server_with_a_rtx_4000/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ebsdbv/video_ram_problems_on_a_server_with_a_rtx_4000/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1ebsdbv</id><link href="https://www.reddit.com/r/LangChain/comments/1ebsdbv/video_ram_problems_on_a_server_with_a_rtx_4000/" /><updated>2024-07-25T11:20:08+00:00</updated><published>2024-07-25T11:20:08+00:00</published><title>Video RAM Problems on a server with a Rtx 4000 20gb</title></entry><entry><author><name>/u/BigYesterday2785</name><uri>https://www.reddit.com/user/BigYesterday2785</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Why does EmbeddingStoreContentRetriever Not output directly Score in Java Langchain ? &lt;/p&gt; &lt;p&gt;It tells us to output, based on minScore, but no possibility to get score directly? Why is it ?&lt;/p&gt; &lt;p&gt;how would I go about implementing it in java or am I missing something&lt;/p&gt; &lt;p&gt;How can I get this score ?&lt;/p&gt; &lt;p&gt;this is how it looks like&lt;/p&gt; &lt;p&gt;&lt;code&gt;public class EmbeddingStoreContentRetriever implements ContentRetriever&lt;/code&gt; &lt;/p&gt; &lt;p&gt;&lt;code&gt;{&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;private final EmbeddingStore&amp;lt;TextSegment&amp;gt; embeddingStore;&lt;/code&gt; &lt;/p&gt; &lt;p&gt;&lt;code&gt;private final EmbeddingModel embeddingModel;&lt;/code&gt; &lt;/p&gt; &lt;p&gt;&lt;code&gt;private final Function&amp;lt;Query, Integer&amp;gt; maxResultsProvider;&lt;/code&gt; &lt;/p&gt; &lt;p&gt;&lt;code&gt;private final Function&amp;lt;Query, Double&amp;gt; minScoreProvider;&lt;/code&gt; &lt;/p&gt; &lt;p&gt;&lt;code&gt;private final Function&amp;lt;Query, Filter&amp;gt; filterProvider;&lt;/code&gt; &lt;/p&gt; &lt;p&gt;&lt;code&gt;}&lt;/code&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/BigYesterday2785&quot;&gt; /u/BigYesterday2785 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ebo69l/why_does_embeddingstorecontentretriever_not/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ebo69l/why_does_embeddingstorecontentretriever_not/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1ebo69l</id><link href="https://www.reddit.com/r/LangChain/comments/1ebo69l/why_does_embeddingstorecontentretriever_not/" /><updated>2024-07-25T06:41:50+00:00</updated><published>2024-07-25T06:41:50+00:00</published><title>Why does EmbeddingStoreContentRetriever Not output directly Score in Java Langchain ?</title></entry><entry><author><name>/u/Key_Science159</name><uri>https://www.reddit.com/user/Key_Science159</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I want to explore how these website with ai interviewer works like do they only works on the audio or they process the video also realtime. It is very fascinating to me. If anyone have any idea in this field, would happy to know your thoughts. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Key_Science159&quot;&gt; /u/Key_Science159 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ebnkqo/ai_interviewer_technique/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ebnkqo/ai_interviewer_technique/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1ebnkqo</id><link href="https://www.reddit.com/r/LangChain/comments/1ebnkqo/ai_interviewer_technique/" /><updated>2024-07-25T06:03:16+00:00</updated><published>2024-07-25T06:03:16+00:00</published><title>Ai interviewer technique</title></entry><entry><author><name>/u/mehul_gupta1997</name><uri>https://www.reddit.com/user/mehul_gupta1997</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;This demo talks about how to use Llama 3.1 with LangChain to build Generative AI applications: &lt;a href=&quot;https://youtu.be/LW64o3YgbE8?si=1nCi7Htoc-gH2zJ6&quot;&gt;https://youtu.be/LW64o3YgbE8?si=1nCi7Htoc-gH2zJ6&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/mehul_gupta1997&quot;&gt; /u/mehul_gupta1997 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1eay7kz/llama_31_using_langchain/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1eay7kz/llama_31_using_langchain/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1eay7kz</id><link href="https://www.reddit.com/r/LangChain/comments/1eay7kz/llama_31_using_langchain/" /><updated>2024-07-24T10:37:35+00:00</updated><published>2024-07-24T10:37:35+00:00</published><title>Llama 3.1 using LangChain</title></entry><entry><author><name>/u/Time-Artist-6900</name><uri>https://www.reddit.com/user/Time-Artist-6900</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi I want to do metadata filtering first and then retrieve the document&lt;br/&gt; Code:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;langchain_chroma = Chroma( client=self.persistent_client, collection_name=self.COLLECTION_NAME, embedding_function=self.embedding_function # Use the variable containing the collection name ) retriever = langchain_chroma.as_retriever(search_type=&amp;quot;similarity&amp;quot;,search_kwargs={&amp;#39;k&amp;#39;: 1, &amp;#39;filter&amp;#39;: cond}) query = &amp;quot;What is patient family Medical history in reverse cronological order?&amp;quot; res = retriever.get_relevant_documents(query) res &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This is not returning scores, Whereas If use , &lt;/p&gt; &lt;pre&gt;&lt;code&gt;res = langchain_chroma.similarity_search_with_score(query) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;then i am getting score as well but how to do metadata filtering here?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Time-Artist-6900&quot;&gt; /u/Time-Artist-6900 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1eb5r0v/how_to_return_similarity_scores_using/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1eb5r0v/how_to_return_similarity_scores_using/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1eb5r0v</id><link href="https://www.reddit.com/r/LangChain/comments/1eb5r0v/how_to_return_similarity_scores_using/" /><updated>2024-07-24T16:27:36+00:00</updated><published>2024-07-24T16:27:36+00:00</published><title>How to return similarity scores using retriever.get_relevant_documents(query)</title></entry><entry><author><name>/u/BigYesterday2785</name><uri>https://www.reddit.com/user/BigYesterday2785</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;i am using in memory vector database where I get scoring of responses. &lt;/p&gt; &lt;p&gt;Now i want to implement reranking to get the most accurate responses. &lt;/p&gt; &lt;p&gt;What would be the easiest way to implmement this in Java Langchain. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/BigYesterday2785&quot;&gt; /u/BigYesterday2785 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1eawsb6/easiest_way_to_implement_reranking_in_langchain/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1eawsb6/easiest_way_to_implement_reranking_in_langchain/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1eawsb6</id><link href="https://www.reddit.com/r/LangChain/comments/1eawsb6/easiest_way_to_implement_reranking_in_langchain/" /><updated>2024-07-24T09:06:28+00:00</updated><published>2024-07-24T09:06:28+00:00</published><title>Easiest way to implement reranking in Langchain and Java</title></entry><entry><author><name>/u/New-Contribution6302</name><uri>https://www.reddit.com/user/New-Contribution6302</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi all. I am an LLM enthusiast trying to use GGUF version of Llama 3.1 for summarisation task. &lt;/p&gt; &lt;p&gt;I am using Q4_K_M model from this repo: MaziyarPanahi/Meta-Llama-3.1-8B-Instruct-GGUF Link: &lt;a href=&quot;https://huggingface.co/MaziyarPanahi/Meta-Llama-3.1-8B-Instruct-GGUF&quot;&gt;https://huggingface.co/MaziyarPanahi/Meta-Llama-3.1-8B-Instruct-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I used the following code to load the model: ``` from langchain_community.llms import LlamaCpp from langchain_core.callbacks import CallbackManager, StreamingStdOutCallbackHandler&lt;/p&gt; &lt;p&gt;callback_manager = CallbackManager([StreamingStdOutCallbackHandler()]) n_gpu_layers = -1&lt;br/&gt; n_batch = 2048 &lt;/p&gt; &lt;h1&gt;Make sure the model path is correct for your system!&lt;/h1&gt; &lt;p&gt;llm = LlamaCpp( model_path=&amp;quot;./Meta-Llama-3.1-8B-Instruct.Q4_K_M.gguf&amp;quot;, n_gpu_layers=n_gpu_layers, n_ctx = 32768, rope_freq_scale=0.25, temperature = 0, n_batch=n_batch, callback_manager=callback_manager, verbose=True, # Verbose is required to pass to the callback manager ) ```&lt;/p&gt; &lt;p&gt;When I pass long inputs to this model and instruct it to summarise it, it just blabbers with random and repitive texts/numbers.&lt;/p&gt; &lt;p&gt;How do I resolve this. Requesting for guidance.&lt;/p&gt; &lt;p&gt;(PS: Tried Rope_freq_scale with values 0.125, 0.25, 1, 4, 8. But they were not so good, even comparing to the above results)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/New-Contribution6302&quot;&gt; /u/New-Contribution6302 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1eayrtc/request_for_guidance/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1eayrtc/request_for_guidance/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1eayrtc</id><link href="https://www.reddit.com/r/LangChain/comments/1eayrtc/request_for_guidance/" /><updated>2024-07-24T11:09:56+00:00</updated><published>2024-07-24T11:09:56+00:00</published><title>Request for Guidance</title></entry><entry><author><name>/u/SpaceKey6285</name><uri>https://www.reddit.com/user/SpaceKey6285</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Does anyone have best practices to share on implementing long term memory for agents? E.g., personalization based on chat history. Based on the memgpt paper it seems best practices would be to have a secondary agent that can read/write long term context into a database, like a Redis cache. Curious if anyone has tuned a model for this? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/SpaceKey6285&quot;&gt; /u/SpaceKey6285 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1eat8c4/long_term_memory_for_agents/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1eat8c4/long_term_memory_for_agents/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1eat8c4</id><link href="https://www.reddit.com/r/LangChain/comments/1eat8c4/long_term_memory_for_agents/" /><updated>2024-07-24T05:10:31+00:00</updated><published>2024-07-24T05:10:31+00:00</published><title>Long term memory for agents?</title></entry><entry><author><name>/u/thevaliantfox04</name><uri>https://www.reddit.com/user/thevaliantfox04</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hello, I am starting to use LangChain and have a question, for which I did not find a response in the documentation.&lt;/p&gt; &lt;p&gt;From my understanding, each LLM is trained with a different &lt;em&gt;chat format&lt;/em&gt; to separate AI and user messages. For instance, I am currently developing with Phi3 which uses the following format for AI messages: &lt;code&gt;&amp;lt;|assistant|&amp;gt;Assistant Message&amp;lt;|end|&amp;gt;&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;How can I pass some parameters to tell LangChain to use this format? Above all, is this handled by the &lt;code&gt;LLM&lt;/code&gt; class or by the &lt;code&gt;Message&lt;/code&gt; class?&lt;/p&gt; &lt;p&gt;I make an example to make my point clearer. When I use the following code&lt;/p&gt; &lt;pre&gt;&lt;code&gt;ChatPromptTemplate.from_messages( [ (&amp;quot;system&amp;quot;, &amp;quot;Behave like this...&amp;quot;), (&amp;quot;human&amp;quot;, &amp;quot;{input}&amp;quot;), ] ) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;How can I tell LangChain to insert &lt;code&gt;&amp;lt;|user|&amp;gt;&lt;/code&gt; at the beginning of the user message? I do not see any parameter to pass to the &lt;code&gt;HumanMessage&lt;/code&gt; object. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/thevaliantfox04&quot;&gt; /u/thevaliantfox04 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1eav643/how_to_customize_the_chat_format_langchain_uses/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1eav643/how_to_customize_the_chat_format_langchain_uses/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1eav643</id><link href="https://www.reddit.com/r/LangChain/comments/1eav643/how_to_customize_the_chat_format_langchain_uses/" /><updated>2024-07-24T07:15:45+00:00</updated><published>2024-07-24T07:15:45+00:00</published><title>How to customize the Chat Format LangChain uses for my specific LLM?</title></entry><entry><author><name>/u/MagentaSpark</name><uri>https://www.reddit.com/user/MagentaSpark</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;At the time of posting,&lt;/p&gt; &lt;p&gt;LangChain repository&amp;#39;s &lt;code&gt;master&lt;/code&gt; branch is&lt;/p&gt; &lt;p&gt;&lt;code&gt; Cloning into &amp;#39;langchain&amp;#39;... remote: Enumerating objects: 137116, done. remote: Counting objects: 100% (5275/5275), done. remote: Compressing objects: 100% (481/481), done. remote: Total 137116 (delta 5003), reused 4829 (delta 4794), pack-reused 131841 Receiving objects: 100% (137116/137116), 224.32 MiB | 4.70 MiB/s, done. Resolving deltas: 100% (101282/101282), done. Updating files: 100% (7595/7595), done. &lt;/code&gt;&lt;/p&gt; &lt;p&gt;and LangGraph repository&amp;#39;s &lt;code&gt;main&lt;/code&gt; branch is&lt;/p&gt; &lt;p&gt;&lt;code&gt; Cloning into &amp;#39;langgraph&amp;#39;... remote: Enumerating objects: 10436, done. remote: Counting objects: 100% (1815/1815), done. remote: Compressing objects: 100% (1015/1015), done. remote: Total 10436 (delta 1090), reused 1371 (delta 774), pack-reused 8621 Receiving objects: 100% (10436/10436), 327.76 MiB | 3.13 MiB/s, done. Resolving deltas: 100% (6828/6828), done. &lt;/code&gt;&lt;/p&gt; &lt;p&gt;For comparision, this is React&amp;#39;s &lt;code&gt;main&lt;/code&gt; brach is&lt;/p&gt; &lt;p&gt;&lt;code&gt; Cloning into &amp;#39;react&amp;#39;... remote: Enumerating objects: 326918, done. remote: Counting objects: 100% (813/813), done. remote: Compressing objects: 100% (324/324), done. remote: Total 326918 (delta 470), reused 718 (delta 422), pack-reused 326105 Receiving objects: 100% (326918/326918), 532.16 MiB | 5.97 MiB/s, done. Resolving deltas: 100% (232896/232896), done. &lt;/code&gt; and it doesn&amp;#39;t even have rich text files like .ipynb.&lt;/p&gt; &lt;p&gt;There are couple of observations. 1. Maintaining an open-source repository with Jupyter Notebooks is not for easy, I think. Any updates to libraries used need notebooks to rerun and reflect latest outputs. Even if there is no change in output, the git diff changes drastically. I have heard about nbdime but have no idea about it. 2. LangGraph repo is bigger in size than LangChain after decompressing. ``` du -sh langgraph 475M langgraph&lt;/p&gt; &lt;p&gt;du -sh langchain 459M langchain``` This size by du depends on multiple factors, block size being on of them.&lt;/p&gt; &lt;p&gt;What did you find interesting? Do share more insights and fun facts about the projects!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/MagentaSpark&quot;&gt; /u/MagentaSpark &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1eb19ri/langchain_vs_langgraph_git/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1eb19ri/langchain_vs_langgraph_git/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1eb19ri</id><link href="https://www.reddit.com/r/LangChain/comments/1eb19ri/langchain_vs_langgraph_git/" /><updated>2024-07-24T13:18:20+00:00</updated><published>2024-07-24T13:18:20+00:00</published><title>LangChain VS LangGraph: Git</title></entry></feed>