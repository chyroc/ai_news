<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/"><category term="LangChain" label="r/LangChain"/><updated>2024-06-27T15:11:13+00:00</updated><icon>https://www.redditstatic.com/icon.png/</icon><id>/r/LangChain.rss</id><link rel="self" href="https://www.reddit.com/r/LangChain.rss" type="application/atom+xml" /><link rel="alternate" href="https://www.reddit.com/r/LangChain" type="text/html" /><subtitle>LangChain is an open-source framework and developer toolkit that helps developers get LLM applications from prototype to production. It is available for Python and Javascript at https://www.langchain.com/.</subtitle><title>LangChain</title><entry><author><name>/u/zchaarm</name><uri>https://www.reddit.com/user/zchaarm</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;A place for members of &lt;a href=&quot;/r/LangChain&quot;&gt;r/LangChain&lt;/a&gt; to chat with each other&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/zchaarm&quot;&gt; /u/zchaarm &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_10ljho9</id><link href="https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/" /><updated>2023-01-26T04:33:05+00:00</updated><published>2023-01-26T04:33:05+00:00</published><title>r/LangChain Lounge</title></entry><entry><author><name>/u/Sevyten</name><uri>https://www.reddit.com/user/Sevyten</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey guys!&lt;/p&gt; &lt;p&gt;Just wanted to give you all a heads up about a live workshop we&amp;#39;re hosting tonight. We&amp;#39;ll be showing how to build an AI-powered tool similar to GitHub Copilot using &lt;a href=&quot;http://superduperdb.com&quot;&gt;SuperDuperDB&amp;#39;s&lt;/a&gt; latest release (v0.2). 🚀&lt;/p&gt; &lt;p&gt;🎥 Today (27/06/2024) at 9 PM CET&lt;br/&gt; 🔗 &lt;a href=&quot;https://www.youtube.com/watch?v=JgavM6QDmxQ&quot;&gt;https://www.youtube.com/watch?v=JgavM6QDmxQ&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;What to Expect:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;AI and Databases:&lt;/strong&gt; How to integrate AI models directly with your database.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Vector Search &amp;amp; Model Chaining:&lt;/strong&gt; Learn about vector search and setting up workflows by chaining models and APIs.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Real-time AI Outputs:&lt;/strong&gt; Implementing real-time AI outputs as new data arrives.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;If you&amp;#39;re into AI, databases, or just curious about how it all works, this session is for you. &lt;/p&gt; &lt;p&gt;Feel free to drop any questions or comments below. Excited to see what you all think!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Sevyten&quot;&gt; /u/Sevyten &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dpnjdx/build_your_own_github_copilot_with_superduperdb/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dpnjdx/build_your_own_github_copilot_with_superduperdb/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dpnjdx</id><link href="https://www.reddit.com/r/LangChain/comments/1dpnjdx/build_your_own_github_copilot_with_superduperdb/" /><updated>2024-06-27T09:57:36+00:00</updated><published>2024-06-27T09:57:36+00:00</published><title>Build Your Own GitHub Copilot with SuperDuperDB: Live Workshop</title></entry><entry><author><name>/u/coolcloud</name><uri>https://www.reddit.com/user/coolcloud</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey all,&lt;/p&gt; &lt;p&gt;We&amp;#39;ve spent a lot of time building new techniques for parsing and searching PDFs. They&amp;#39;ve lead to a significant improvement in our RAG search and I wanted to share what we&amp;#39;ve learned.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Some examples:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Table - SEC Docs are notoriously hard for PDF -&amp;gt; tables. We tried the top results on google &amp;amp; some opensource thins not a single one succeeded on this table. &lt;/p&gt; &lt;p&gt;Couple examples of who we looked at:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;ilovepdf&lt;/li&gt; &lt;li&gt;Adobe&lt;/li&gt; &lt;li&gt;Gonitro&lt;/li&gt; &lt;li&gt;PDFtables&lt;/li&gt; &lt;li&gt;OCR 2 Edit&lt;/li&gt; &lt;li&gt;microsoft/table-transformer-structure-recognition&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Results - our result (can be accurately converted into CSV,MD,JSON)&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/5wju5gedmy8d1.png?width=1035&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2a336bd0e1af14760fbb5ca4291284c99edaa27e&quot;&gt;https://preview.redd.it/5wju5gedmy8d1.png?width=1035&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2a336bd0e1af14760fbb5ca4291284c99edaa27e&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Example: identifying headers, paragraphs, lists/list items (purple), and ignoring the &amp;quot;junk&amp;quot; at the top aka the table of contents in the header.&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/ix7747bjmy8d1.png?width=1018&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ea0b65ae6a35581d955da282353ff63509602a38&quot;&gt;https://preview.redd.it/ix7747bjmy8d1.png?width=1018&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ea0b65ae6a35581d955da282353ff63509602a38&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why did we do this?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;W ran into a bunch of issues with existing approaches that boils down to one thing: hallucinations often happen because the chunk doesn&amp;#39;t provide enough information.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;chunking by word count doesn&amp;#39;t work. It often chunks mid-paragraph or sentence.&lt;/li&gt; &lt;li&gt;Chunking by sentence or paragraph doesn&amp;#39;t work. If the answer spans 2-3 paragraphs, you still are SOL.&lt;/li&gt; &lt;li&gt;Semantic chunking is better but still fail quite often on lists or &amp;quot;somewhat&amp;quot; different pieces of info.&lt;/li&gt; &lt;li&gt;LLM&amp;#39;s deal better with structured/semi-structured data, i.e. knowing what you&amp;#39;re sending it is a header, paragraph list etc., makes the model perform better.&lt;/li&gt; &lt;li&gt;Headers often aren&amp;#39;t included because they&amp;#39;re too far away from the relevant vector, although often times headers contain important information.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;What are we doing different?&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;We are dynamically generating chunks when a search happens, sending headers &amp;amp; sub-headers to the LLM along with the chunk/chunks that were relevant to the search.&lt;/p&gt; &lt;p&gt;Example of how this is helpful: you have 7 documents that talk about how to reset a device, and the header says the device name, but it isn&amp;#39;t talked about the paragraphs. The 7 chunks that talked about how to reset a device would come back, but the LLM wouldn&amp;#39;t know which one was relevant to which product. That is, unless the chunk happened to include both the paragraphs and the headers, which often times in our experience, it doesn&amp;#39;t.&lt;/p&gt; &lt;p&gt;This is a simplified version of what our structure looks like:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;{ &amp;quot;type&amp;quot;: &amp;quot;Root&amp;quot;, &amp;quot;children&amp;quot;: [ { &amp;quot;type&amp;quot;: &amp;quot;Header&amp;quot;, &amp;quot;text&amp;quot;: &amp;quot;How to reset an iphone&amp;quot;, &amp;quot;children&amp;quot;: [ { &amp;quot;type&amp;quot;: &amp;quot;Header&amp;quot;, &amp;quot;text&amp;quot;: &amp;quot;iphone 10 reset&amp;quot;, &amp;quot;children&amp;quot;: [ { &amp;quot;type&amp;quot;: &amp;quot;Paragraph&amp;quot;, &amp;quot;text&amp;quot;: &amp;quot;Example Paragraph.&amp;quot; }, { &amp;quot;type&amp;quot;: &amp;quot;List&amp;quot;, &amp;quot;children&amp;quot;: [ &amp;quot;Item 1&amp;quot;, &amp;quot;Item 2&amp;quot;, &amp;quot;Item 3&amp;quot; ] } ] }, { &amp;quot;type&amp;quot;: &amp;quot;Header&amp;quot;, &amp;quot;text&amp;quot;: &amp;quot;iphone 11 reset&amp;quot;, &amp;quot;children&amp;quot;: [ { &amp;quot;type&amp;quot;: &amp;quot;Paragraph&amp;quot;, &amp;quot;text&amp;quot;: &amp;quot;Example Paragraph 2&amp;quot; }, { &amp;quot;type&amp;quot;: &amp;quot;Table&amp;quot;, &amp;quot;children&amp;quot;: [ { &amp;quot;type&amp;quot;: &amp;quot;TableCell&amp;quot;, &amp;quot;row&amp;quot;: 0, &amp;quot;col&amp;quot;: 0, &amp;quot;text&amp;quot;: &amp;quot;Column 1&amp;quot;}, { &amp;quot;type&amp;quot;: &amp;quot;TableCell&amp;quot;, &amp;quot;row&amp;quot;: 0, &amp;quot;col&amp;quot;: 1, &amp;quot;text&amp;quot;: &amp;quot;Column 2&amp;quot;}, { &amp;quot;type&amp;quot;: &amp;quot;TableCell&amp;quot;, &amp;quot;row&amp;quot;: 0, &amp;quot;col&amp;quot;: 2, &amp;quot;text&amp;quot;: &amp;quot;Column 3&amp;quot;}, { &amp;quot;type&amp;quot;: &amp;quot;TableCell&amp;quot;, &amp;quot;row&amp;quot;: 1, &amp;quot;col&amp;quot;: 0, &amp;quot;text&amp;quot;: &amp;quot;Row 1, Cell 1&amp;quot;}, { &amp;quot;type&amp;quot;: &amp;quot;TableCell&amp;quot;, &amp;quot;row&amp;quot;: 1, &amp;quot;col&amp;quot;: 1, &amp;quot;text&amp;quot;: &amp;quot;Row 1, Cell 2&amp;quot;}, { &amp;quot;type&amp;quot;: &amp;quot;TableCell&amp;quot;, &amp;quot;row&amp;quot;: 1, &amp;quot;col&amp;quot;: 2, &amp;quot;text&amp;quot;: &amp;quot;Row 1, Cell 3&amp;quot;} ] } ] } ] } ] } &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;How do we get PDF&amp;#39;s into this format?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;At a high level, we are identifying different portions of PDF&amp;#39;s based on PDF metadata and heuristics. This helps solve three problems:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;OCR can often mis-identify letters/numbers, or entirely crop out words. &lt;/li&gt; &lt;li&gt;Most other companies are trying to use OCR/ML models to identify layout elements, which seems to work decent on data it&amp;#39;s seen before but fails pretty hard unexpectedly. When it fails, it&amp;#39;s a black box. For example, Microsoft released a paper a few days ago saying they trained a model on over 500M documents and still fails on a bunch of use cases that we have working&lt;/li&gt; &lt;li&gt;We can look at layout, font analysis etc. throughout the entire doc allowing us to understand the &amp;quot;structure&amp;quot; of the document more. We&amp;#39;ll talk about this more when looking at font classes&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;How?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;First, we extract tables. We use a small OCR model to identify bounding boxes, then we do use white space analysis to find cells. This is the only portion of OCR we use (we&amp;#39;re looking at doing line analysis but have punted on that thus far.) We have found OCR to poorly identify cells on more complex tables, and often turn a 4 into a 5 or a 8 into a 2 etc.&lt;/p&gt; &lt;p&gt;When we find a table, we find characters that we believe to be a cell based on distance between each other, trying to read the table as a human would. An example would be 1345 would be a &amp;quot;cell&amp;quot; or text block, where 1 345 would be two text blocks due to the distance between them. A re-occurring theme is white space can get you pretty far.&lt;/p&gt; &lt;p&gt;Second, we extract character data from the PDF:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Fonts&lt;/strong&gt;: Information about the fonts used in the document, including the font name, type (e.g., TrueType, Type 1), and embedded font files.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Character Positions:&lt;/strong&gt; The exact bounding box of each character on the page.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Character Color:&lt;/strong&gt; PDFs usually give this correctly, and when it&amp;#39;s wrong it&amp;#39;s still good enough&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;PDFs provide a other metadata, but we found them to either be inaccurate or not necessary:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Content Streams:&lt;/strong&gt; Sequences of instructions that describe the content of the page, including text, images, and vector graphics. We found these to be surprisingly inaccurate. Newline characters inserted in the middle of words, characters and words placed out of order, and whitespace is handled really inconsistently (more below)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Annotations:&lt;/strong&gt; Information about interactive elements such as links, form fields, and comments. There are useful details here that we may use in the future, but, again, a lot of PDF tools generate these incorrectly.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Third, we strip out all space, newline, and other invisible characters. We do whitespace analysis to build words from individual characters. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;After extracting PDF metadata:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;We extract out character locations, font sizes, and fonts. We then do multiple passes of whitespace analysis and clustering algorithms to find groups, then try to identify what category they fall into based on heuristics. We used to rely more heavily on clustering (DBScan specifically), but found that simpler whitespace analysis often outperformed it. &lt;/p&gt; &lt;ul&gt; &lt;li&gt;If you look at a PDF and see only a handful of characters, let&amp;#39;s say 1% that are font 32, color blue, and each time they&amp;#39;re identified together it&amp;#39;s only 2-3 words it&amp;#39;s likely a header. &lt;/li&gt; &lt;li&gt;Now you see 2% are font 28, red, it&amp;#39;s probably a sub-header. (That is if the font spans multiple pages.) If it instead is only in a single location, it&amp;#39;s most likely something important in the text that the author wants us to &amp;#39;flag&amp;#39;. &lt;/li&gt; &lt;li&gt;This makes font analysis across the document important, and another reason we stay away from OCR&lt;/li&gt; &lt;li&gt;If, the document is 80% font 12, black. It&amp;#39;s probably &amp;#39;normal text.&amp;#39; Normal text needs to be categorized into two different formats, one is paragraphs, the other is bullet points/lists. &lt;/li&gt; &lt;li&gt;For bullet points we look primarily at the white space, identifying that there&amp;#39;s a significant amount of white space, often follow by a bullet point, number, or dash. &lt;/li&gt; &lt;li&gt;For paragraphs, we text together in a &amp;#39;normal&amp;#39; format without bullet points, traditionally spanning a majority of the document.&lt;/li&gt; &lt;li&gt;Junk detection. A lot of PDF&amp;#39;s have junk in them. An example would be a header that&amp;#39;s at the top of every single document, or a footer on every document saying who wrote it, the page number etc. This junk otherwise is sent to the chunking algorithm meaning you can often have random information mid-paragraph. We generate character ngram vectors and cluster then based on L1 distance (rather than cosine). That lets us find variations like &amp;quot;Page 1&amp;quot;, &amp;quot;Page 2&amp;quot;, etc. If those appear in roughly the same location on more than 20-35% of pages, it&amp;#39;s likely just repeat junk.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The product is still in beta so if you&amp;#39;re actively trying to solve this, or a similar problem, we&amp;#39;re letting people use it for free, in exchange for feedback.&lt;/p&gt; &lt;p&gt;Have additional questions? Shoot!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/coolcloud&quot;&gt; /u/coolcloud &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dpbc4g/how_we_chunk_turning_pdfs_into_hierarchical/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dpbc4g/how_we_chunk_turning_pdfs_into_hierarchical/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dpbc4g</id><link href="https://www.reddit.com/r/LangChain/comments/1dpbc4g/how_we_chunk_turning_pdfs_into_hierarchical/" /><updated>2024-06-26T22:21:08+00:00</updated><published>2024-06-26T22:21:08+00:00</published><title>How we Chunk - turning PDF's into hierarchical structure for RAG</title></entry><entry><author><name>/u/dccpt</name><uri>https://www.reddit.com/user/dccpt</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dpt9iz/extract_data_from_chat_history_quickly_and/&quot;&gt; &lt;img src=&quot;https://external-preview.redd.it/9f3qHXFEDc5moxlRaP4wYclBrxl1FfQFS0lbxr1ol8s.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7cc629947259c03fb2bbebe47efc15b73e5319d5&quot; alt=&quot;Extract Data From Chat History: Quickly and Accurately&quot; title=&quot;Extract Data From Chat History: Quickly and Accurately&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi all - several recent posts here have discussed the challenges of extracting structured data from chat histories. This is a common challenge: fulfilling sales orders, collecting support info, booking meetings/appointments, and more.&lt;/p&gt; &lt;p&gt;Zep’s new &lt;a href=&quot;https://blog.getzep.com/structured-data-extraction/&quot;&gt;Structured Data Extraction&lt;/a&gt; is a high-accuracy tool for extracting data from chat histories. It&amp;#39;s also 10x faster than gpt-4o.&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://i.redd.it/nwrcdkgwo49d1.gif&quot;&gt;https://i.redd.it/nwrcdkgwo49d1.gif&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Versus OpenAI JSON Mode&lt;/h1&gt; &lt;p&gt;OpenAI (or other LLM provider) JSON Mode (with something like a LangChain&amp;#39;s &lt;code&gt;with_structured_output&lt;/code&gt;), only guarantees that the result will be well-formed JSON, but the LLM may still return hallucinated values, incorrectly structured fields (think a phone number or date in an incorrect format), or even fields that don&amp;#39;t exist in your &lt;code&gt;pydantic&lt;/code&gt; model!&lt;/p&gt; &lt;p&gt;It can also be super slow, and the more fields you add to your &lt;code&gt;pydantic&lt;/code&gt; model, the longer it takes.&lt;/p&gt; &lt;p&gt;To ensure fast, accurate results, Zep uses a combination of:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;dialog preprocessing, which, amongst other things, improves accuracy for machine-transcribed dialogs and allows partial dates to be extracted;&lt;/li&gt; &lt;li&gt;guided output inference techniques on fine-tuned LLMs running on our own infrastructure;&lt;/li&gt; &lt;li&gt;and post-inference validation.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Using Zep with LangChain&lt;/h1&gt; &lt;p&gt;It&amp;#39;s simple to &lt;a href=&quot;https://help.getzep.com/langchain/overview&quot;&gt;drop Zep into a LangChain application&lt;/a&gt;. Once you&amp;#39;re persisting memory to Zep, you can extract data from this dialogue.&lt;/p&gt; &lt;h1&gt;Low or zero marginal latency cost to adding additional fields&lt;/h1&gt; &lt;p&gt;Zep&amp;#39;s extraction latency scales sub-linearly with the number of fields in your model. That is, you may add additional fields with a low or no marginal increase in latency.&lt;/p&gt; &lt;h1&gt;Support for Partial and Relative Dates&lt;/h1&gt; &lt;p&gt;Zep understands various date and time formats, including relative times such as “yesterday” or “last week.” It can also parse partial dates and times, such as “at 3pm” or “on the 15th.”&lt;/p&gt; &lt;h1&gt;Extracting from Speech Transcripts&lt;/h1&gt; &lt;p&gt;Zep can understand and extract data from machine-transcribed transcripts. Spelled out numbers and dates will be parsed as if written language. Utterances such as “uh” or “um” are ignored.&lt;/p&gt; &lt;p&gt;You can read more &lt;a href=&quot;https://blog.getzep.com/structured-data-extraction/&quot;&gt;in our announcement&lt;/a&gt; and the &lt;a href=&quot;https://help.getzep.com/langchain/overview&quot;&gt;Structured Data Extraction guide&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;This was a ton of work to build and lots of fun. Would love your feedback if you give it a spin!&lt;/p&gt; &lt;p&gt;-Daniel&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/dccpt&quot;&gt; /u/dccpt &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dpt9iz/extract_data_from_chat_history_quickly_and/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dpt9iz/extract_data_from_chat_history_quickly_and/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1dpt9iz</id><media:thumbnail url="https://external-preview.redd.it/9f3qHXFEDc5moxlRaP4wYclBrxl1FfQFS0lbxr1ol8s.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=7cc629947259c03fb2bbebe47efc15b73e5319d5" /><link href="https://www.reddit.com/r/LangChain/comments/1dpt9iz/extract_data_from_chat_history_quickly_and/" /><updated>2024-06-27T14:55:57+00:00</updated><published>2024-06-27T14:55:57+00:00</published><title>Extract Data From Chat History: Quickly and Accurately</title></entry><entry><author><name>/u/harshit_nariya</name><uri>https://www.reddit.com/user/harshit_nariya</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/harshit_nariya&quot;&gt; /u/harshit_nariya &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;/r/AnyBodyCanAI/comments/1dpkjvq/we_built_an_opensource_lowcode_multiagent/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dpo9li/we_built_an_opensource_lowcode_multiagent/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dpo9li</id><link href="https://www.reddit.com/r/LangChain/comments/1dpo9li/we_built_an_opensource_lowcode_multiagent/" /><updated>2024-06-27T10:45:04+00:00</updated><published>2024-06-27T10:45:04+00:00</published><title>We built an open-source low-code multi-agent automation framework</title></entry><entry><author><name>/u/d2clon</name><uri>https://www.reddit.com/user/d2clon</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hello, people. I am a veteran programmer who is new to AI and its business use cases.&lt;/p&gt; &lt;p&gt;I am fascinated by it, and I am now working on a small prototype for a client. It is an out-of-the-book RAG case:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;~1.5K 1-page PDFs with product specs.&lt;/li&gt; &lt;li&gt;Build a chatbot to ask questions about the products.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;In our team, we are making great progress in the basic setup. The PDFs are indexed in a VectorDB and we are able to use GPT4 to interact with the VectorDB data and generate human friendly answers.&lt;/p&gt; &lt;p&gt;But there is a lot to improve about the generated recomendations, conclusions, filtering, best results, ...&lt;/p&gt; &lt;p&gt;All the tutorials and documentation we are seeing end up here, in the basic setup. And don&amp;#39;t go further in the details and improvements needed to go to &amp;quot;production&amp;quot; level. Further more, I have seen that many people on this community and others are mentioning their dissapointment with the actual state of the technology and their abandom of building a RAG architecture.&lt;/p&gt; &lt;p&gt;I just want a confirmation that it is possible. That some of you have managed to build a RAG architecture that is used satisfactorily in production. Is this the case? :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/d2clon&quot;&gt; /u/d2clon &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dp7p9j/are_there_any_rag_successful_real_production_use/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dp7p9j/are_there_any_rag_successful_real_production_use/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dp7p9j</id><link href="https://www.reddit.com/r/LangChain/comments/1dp7p9j/are_there_any_rag_successful_real_production_use/" /><updated>2024-06-26T19:47:52+00:00</updated><published>2024-06-26T19:47:52+00:00</published><title>Are there any RAG successful real production use cases out there?</title></entry><entry><author><name>/u/simaodiego</name><uri>https://www.reddit.com/user/simaodiego</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;pre&gt;&lt;code&gt;Is it possible to create a model using RAG with specific content and if my model has no response it can access ChatGPT for example? Just when RAG is not enough, how to find and develop this type of solution? &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/simaodiego&quot;&gt; /u/simaodiego &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dprerw/mix_of_rag_and_public_apis/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dprerw/mix_of_rag_and_public_apis/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dprerw</id><link href="https://www.reddit.com/r/LangChain/comments/1dprerw/mix_of_rag_and_public_apis/" /><updated>2024-06-27T13:33:47+00:00</updated><published>2024-06-27T13:33:47+00:00</published><title>Mix of RAG and public APIs</title></entry><entry><author><name>/u/Fresh_Skin130</name><uri>https://www.reddit.com/user/Fresh_Skin130</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I have built a first proof of concept of agents that generate a flow chart given some text as input. The flowchart is generated in graphML format and is compatible with yED chart editor (free).&lt;/p&gt; &lt;p&gt;The project is available here: &lt;a href=&quot;https://github.com/marco-marchesi/FlowChartGenerator&quot;&gt;https://github.com/marco-marchesi/FlowChartGenerator&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Note: it&amp;#39;s my first github project, any suggestion and contribution are very welcome.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Fresh_Skin130&quot;&gt; /u/Fresh_Skin130 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dpr1yz/text_2_flowchart_agent/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dpr1yz/text_2_flowchart_agent/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dpr1yz</id><link href="https://www.reddit.com/r/LangChain/comments/1dpr1yz/text_2_flowchart_agent/" /><updated>2024-06-27T13:17:05+00:00</updated><published>2024-06-27T13:17:05+00:00</published><title>Text 2 FlowChart agent</title></entry><entry><author><name>/u/ms-atomicbomb</name><uri>https://www.reddit.com/user/ms-atomicbomb</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;m hiring a fully-remote Agentic Software Developers to build, test and refine our agents, as well as the infrastructure around them. We&amp;#39;re a stealth-mode start up backed top VCs. Please feel free to reach out to me here or via Discord (@thebirthdaygirl) and I&amp;#39;d love to chat! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/ms-atomicbomb&quot;&gt; /u/ms-atomicbomb &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dpl6ks/hiring_fullyremote_agentic_software_developers/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dpl6ks/hiring_fullyremote_agentic_software_developers/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dpl6ks</id><link href="https://www.reddit.com/r/LangChain/comments/1dpl6ks/hiring_fullyremote_agentic_software_developers/" /><updated>2024-06-27T07:08:04+00:00</updated><published>2024-06-27T07:08:04+00:00</published><title>Hiring fully-remote Agentic Software Developers!</title></entry><entry><author><name>/u/inez-dolly</name><uri>https://www.reddit.com/user/inez-dolly</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hello. I&amp;#39;m new to LangChain and I&amp;#39;ve been wondering how to achieve shared memory/session between independent agents, without using a graph with a supervisor. I have an agent which is responsible for breaking down complex question to steps that can be executed by other agents. It is aware other agents exist.&lt;/p&gt; &lt;p&gt;For example:&lt;/p&gt; &lt;p&gt;Main question: What will be the weather tomorrow in Oslo? Will it be warmer than in Bergen?&lt;/p&gt; &lt;p&gt;Which is broken down to steps by an agent:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;{&amp;quot;agent&amp;quot;: &amp;quot;DateTimeAgent, &amp;quot;task&amp;quot;: &amp;quot;Get tomorrow&amp;#39;s date&amp;quot;}&lt;/li&gt; &lt;li&gt;{&amp;quot;agent&amp;quot;: &amp;quot;ForecastAgent, &amp;quot;task&amp;quot;: &amp;quot;Get the weather in Oslo, Norway for 2024/06/28&amp;quot;}&lt;/li&gt; &lt;li&gt;{&amp;quot;agent&amp;quot;: &amp;quot;ForecastAgent, &amp;quot;task&amp;quot;: &amp;quot;Get the weather in Bergen, Norway for 2024/06/28&amp;quot;}&lt;/li&gt; &lt;li&gt;{&amp;quot;agent&amp;quot;: &amp;quot;CalcAgent, &amp;quot;task&amp;quot;: &amp;quot;Calculate the difference between the temperatures&amp;quot;}&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;As you can see, step 4 is related to the outcome of the previous ones. What&amp;#39;s the best way to make the agents aware of the results of their peers? Is the only way to use langgraph? It seems a bit inefficient to me to have a wrapper agent using the RunnableWithMessageHistory class and have an LLM manage the routing and conversation.&lt;/p&gt; &lt;p&gt;Thank you for your assistance beforehand!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/inez-dolly&quot;&gt; /u/inez-dolly &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dpqtfw/sharing_history_between_independent_agents/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dpqtfw/sharing_history_between_independent_agents/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dpqtfw</id><link href="https://www.reddit.com/r/LangChain/comments/1dpqtfw/sharing_history_between_independent_agents/" /><updated>2024-06-27T13:05:38+00:00</updated><published>2024-06-27T13:05:38+00:00</published><title>Sharing history between independent agents</title></entry><entry><author><name>/u/MagentaSpark</name><uri>https://www.reddit.com/user/MagentaSpark</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;There are 2 ways of doing same things now. Chains and Graphs. They both offer almost identical control in most of the small workflows. Advantages, disadvantages and use cases for chains as nodes vs compiled graphs as nodes.&lt;/p&gt; &lt;p&gt;I do realise that both are inherit from runnable primitive, but application wise, practically, there are 2 distinct way of doing thing, right?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/MagentaSpark&quot;&gt; /u/MagentaSpark &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dpqltj/any_experiences_with_graph_within_a_graph_in/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dpqltj/any_experiences_with_graph_within_a_graph_in/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dpqltj</id><link href="https://www.reddit.com/r/LangChain/comments/1dpqltj/any_experiences_with_graph_within_a_graph_in/" /><updated>2024-06-27T12:55:21+00:00</updated><published>2024-06-27T12:55:21+00:00</published><title>Any experiences with Graph within a Graph in LangGraph?</title></entry><entry><author><name>/u/MajorTuttle</name><uri>https://www.reddit.com/user/MajorTuttle</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Caveat: I am very new to using langchain.&lt;/p&gt; &lt;p&gt;Langsmith seems like an excellent product for enterprise and large-scale production, but beyond my needs and pricing, and seems not an easy way to export data.&lt;/p&gt; &lt;p&gt;I just want to be able to capture what is going on under the hood -- including all LLM API call inputs/outputs -- mostly to better understand how the implemented patterns (structured outputs, tool calling, etc) are done in practice. Ideally either as a data-structure for me to persist or directly to a file (JSON or JSONL or similar I can peruse and process).&lt;/p&gt; &lt;p&gt;Is there an easy way to do this?&lt;/p&gt; &lt;p&gt;Its not clear to me if the chain design just makes it challenging to implement observability (and why langsmith is needed), or if its somewhat intentionally not made clear or well-documented as langsmith and langserve are the profit centers.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/MajorTuttle&quot;&gt; /u/MajorTuttle &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dpjf0d/trace_to_datastructure_or_file_instead_of/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dpjf0d/trace_to_datastructure_or_file_instead_of/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dpjf0d</id><link href="https://www.reddit.com/r/LangChain/comments/1dpjf0d/trace_to_datastructure_or_file_instead_of/" /><updated>2024-06-27T05:12:12+00:00</updated><published>2024-06-27T05:12:12+00:00</published><title>Trace to data-structure or file instead of langsmith?</title></entry><entry><author><name>/u/link2ani</name><uri>https://www.reddit.com/user/link2ani</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/link2ani&quot;&gt; /u/link2ani &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dpprkp/anyone_building_rag_app_in_javascript_what_stack/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dpprkp/anyone_building_rag_app_in_javascript_what_stack/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dpprkp</id><link href="https://www.reddit.com/r/LangChain/comments/1dpprkp/anyone_building_rag_app_in_javascript_what_stack/" /><updated>2024-06-27T12:10:55+00:00</updated><published>2024-06-27T12:10:55+00:00</published><title>anyone building RAG app in javascript? what stack are you using?</title></entry><entry><author><name>/u/Specialist-Cloud-448</name><uri>https://www.reddit.com/user/Specialist-Cloud-448</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I have data stored in my &lt;strong&gt;DynamoDB&lt;/strong&gt; which is frequently updated through back-end services. Now I want to create a PG vector based &lt;strong&gt;AuroraDB vector database&lt;/strong&gt; for storing &lt;strong&gt;embeddings&lt;/strong&gt;, which I want to be automatically updated whenever there is the change in the DynamoDB.&lt;/p&gt; &lt;p&gt;I thought about using the &lt;strong&gt;EventBridge&lt;/strong&gt; but need more suggestion on that.&lt;/p&gt; &lt;p&gt;My aim is to create the new embeddings everytime there is the change (Upsert) in the DynamoDB and store them in the PG Vector Database. So that I can perform the RAG on &lt;strong&gt;latest embeddings&lt;/strong&gt; to so the answer from LLM must be context aware.&lt;/p&gt; &lt;p&gt;In the phase of architectural designing and ideation of this feature.&lt;/p&gt; &lt;p&gt;Any suggestions are welcomed .&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Specialist-Cloud-448&quot;&gt; /u/Specialist-Cloud-448 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dpphgh/data_ingestion_for_the_rag_from_dynamo_db_to/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dpphgh/data_ingestion_for_the_rag_from_dynamo_db_to/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dpphgh</id><link href="https://www.reddit.com/r/LangChain/comments/1dpphgh/data_ingestion_for_the_rag_from_dynamo_db_to/" /><updated>2024-06-27T11:56:30+00:00</updated><published>2024-06-27T11:56:30+00:00</published><title>Data Ingestion for the RAG from Dynamo DB to AuroraDB with pgVector to store embeddings</title></entry><entry><author><name>/u/byrocuy</name><uri>https://www.reddit.com/user/byrocuy</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hello, I am currently developing a chatbot using LangGraph, and I&amp;#39;m facing some challenges with managing state for multiple users. Specifically, I&amp;#39;m dealing with the following constraints and setup:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The state is limited only to last 10-15 messages due to the structure of API I am interacting with.&lt;/li&gt; &lt;li&gt;All the chat history will be stored in a MySQL database. I do it by storing each input and response manually to the db, as the checkpointer implementation in MySQL is not supported yet.&lt;/li&gt; &lt;li&gt;The messages will be stored in the database with the corresponding user ID. For now the chat history in the database has no function in the chatbot flow. It only serve for the frontend to load previous interaction when user open the chatbot. &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;If I understand correctly, the state is stored in the runtime and shared across multiple users, right? I think this might lead to a memory problem if I don&amp;#39;t implement some way of handler or even if I limit the previous messages for each user it will lead to a problem.&lt;/p&gt; &lt;p&gt;My idea to handle this is as follows: - Store the chat history (user ID and message) in the database. - When a new query comes in, load the last 10 last messages from the database for the appropriate user ID. - Append this history with the new query and pass it to the chatbot.&lt;/p&gt; &lt;p&gt;How does my idea sound? Are there any potential pitfalls or improvements you would suggest? I&amp;#39;m open to any suggestions and feedback.&lt;/p&gt; &lt;p&gt;Thanks in advance for your help!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/byrocuy&quot;&gt; /u/byrocuy &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dpgr6p/how_to_manage_state_in_langgraph_for_multiple/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dpgr6p/how_to_manage_state_in_langgraph_for_multiple/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dpgr6p</id><link href="https://www.reddit.com/r/LangChain/comments/1dpgr6p/how_to_manage_state_in_langgraph_for_multiple/" /><updated>2024-06-27T02:42:53+00:00</updated><published>2024-06-27T02:42:53+00:00</published><title>How to Manage State in LangGraph for Multiple Users?</title></entry><entry><author><name>/u/sujihai</name><uri>https://www.reddit.com/user/sujihai</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I am currently working on integrating several components into a comprehensive chat application using LangServe and LangChain. Below, I detail the components involved and the specific issues I&amp;#39;m encountering. Any guidance or suggestions would be greatly appreciated.&lt;/p&gt; &lt;h1&gt;Components and Setup:&lt;/h1&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;History Aware Retriever and Question Answer Chain&lt;/strong&gt;: &lt;ul&gt; &lt;li&gt;I&amp;#39;ve created a chain that consists of a history-aware retriever and a question-answer chain.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&amp;#8203;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;contextualize_q_system_prompt = &amp;quot;&amp;quot;&amp;quot;Given a chat history and the latest user question \ which might reference context in the chat history, formulate a standalone question \ which can be understood without the chat history. Do NOT answer the question, \ just reformulate it if needed and otherwise return it as is.&amp;quot;&amp;quot;&amp;quot; contextualize_q_prompt = ChatPromptTemplate.from_messages( [ (&amp;quot;system&amp;quot;, contextualize_q_system_prompt), MessagesPlaceholder(&amp;quot;chat_history&amp;quot;), (&amp;quot;human&amp;quot;, &amp;quot;{input}&amp;quot;), ] ) history_aware_retriever = create_history_aware_retriever( llm, retriever, contextualize_q_prompt ) ### Answer question ### qa_system_prompt = &amp;quot;&amp;quot;&amp;quot;You are an assistant for question-answering tasks. \ Use the following pieces of retrieved context to answer the question. \ If you don&amp;#39;t know the answer, just say that you don&amp;#39;t know. \ Use three sentences maximum and keep the answer concise.\ {context}&amp;quot;&amp;quot;&amp;quot; qa_prompt = ChatPromptTemplate.from_messages( [ (&amp;quot;system&amp;quot;, qa_system_prompt), MessagesPlaceholder(&amp;quot;chat_history&amp;quot;), (&amp;quot;human&amp;quot;, &amp;quot;{input}&amp;quot;), ] ) question_answer_chain = create_stuff_documents_chain(llm, qa_prompt) rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain) &lt;/code&gt;&lt;/pre&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Message History Implementation&lt;/strong&gt;: &lt;ul&gt; &lt;li&gt;The application incorporates &lt;code&gt;RedisChatMessageHistory&lt;/code&gt; along with &lt;code&gt;RunnableWithMessageHistory&lt;/code&gt;. The intention is to leverage Redis for managing chat message history, tracking conversations by User ID and Conversation ID.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;LangGraph Integration&lt;/strong&gt;: &lt;ul&gt; &lt;li&gt;I&amp;#39;m attempting to integrate this setup into LangGraph. However, I&amp;#39;m facing challenges because LangGraph documentation suggests using Checkpoints with SQLite, and it&amp;#39;s unclear how to integrate RedisChatMessageHistory which is essential for my application.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ol&gt; &lt;h1&gt;Issues:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Integration with LangGraph&lt;/strong&gt;: How can I integrate &lt;code&gt;RedisChatMessageHistory&lt;/code&gt; within LangGraph, given that LangGraph primarily supports SQLite for Checkpoints?&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Consistent Message History&lt;/strong&gt;: I need to ensure that message history capabilities are maintained across the entire application, allowing tracking of conversations by User ID and Conversation ID.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Resources:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;For the retrieval chain setup, please refer to the LangChain documentation on question answering with chat history: &lt;a href=&quot;https://python.langchain.com/v0.1/docs/use_cases/question_answering/chat_history/#tying-it-together&quot;&gt;LangChain QA with Chat History&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;For details on managing persistent chat with user IDs and conversation IDs, see this example from LangServe: &lt;a href=&quot;https://github.com/langchain-ai/langserve/blob/main/examples/chat_with_persistence_and_user/server.py&quot;&gt;LangServe Chat with Persistence&lt;/a&gt;.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Request:&lt;/h1&gt; &lt;p&gt;I am seeking advice or examples on how to properly integrate RedisChatMessageHistory with LangGraph in a manner that maintains full functionality of the message history features. Any insights or pointers towards documentation or similar implementations would be incredibly helpful.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/sujihai&quot;&gt; /u/sujihai &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dposfd/integration_issues_with_langgraph/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dposfd/integration_issues_with_langgraph/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dposfd</id><link href="https://www.reddit.com/r/LangChain/comments/1dposfd/integration_issues_with_langgraph/" /><updated>2024-06-27T11:16:17+00:00</updated><published>2024-06-27T11:16:17+00:00</published><title>Integration Issues with LangGraph, RedisChatMessageHistory, and RunnableWithMessageHistory</title></entry><entry><author><name>/u/Early_Low8914</name><uri>https://www.reddit.com/user/Early_Low8914</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;When calling the retriever directly, I get a response which includes the content + metadata.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;retriever = documents.as_retriever(search_kwargs={&amp;quot;k&amp;quot;: 1}) retriever.get_relevant_documents(&amp;quot;foo&amp;quot;) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The response:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;[Document(page_content=&amp;#39;foo&amp;#39;, metadata={&amp;#39;tenant_id&amp;#39;: &amp;#39;0d122190-b761-43f7-9ea3-f1842bbe1c4d&amp;#39;, &amp;#39;page&amp;#39;: 7})] &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;When i wrap the retriever with the utiliy function provided by langchain: &amp;quot;create_retriever_tool&amp;quot;:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;tool: Tool = create_retriever_tool(documents.as_retriever(search_kwargs={ &amp;quot;k&amp;quot;: 6}), name=&amp;quot;search_documents&amp;quot;, description=&amp;quot;Search documents&amp;quot;) tool.invoke(&amp;quot;foo&amp;quot;) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The response:&lt;/p&gt; &lt;p&gt;&amp;#39;foo&amp;#39;&lt;/p&gt; &lt;p&gt;So in this case, the metadata part is completely missing. I understand that I could use a prompt_template which includes the metadata:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;document_prompt = PromptTemplate.from_template( &amp;quot;Document chunk metadata: tenant_id: {tenant_id}...\n&amp;quot; &amp;quot;Document chunk content: {page_content}. &amp;quot; ) tool: Tool = create_retriever_tool(documents.as_retriever(search_kwargs={ &amp;quot;k&amp;quot;: 6}), name=&amp;quot;search_documents&amp;quot;, description=&amp;quot;Search documents&amp;quot;, document_prompt=document_prompt) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;and this works but the output from directly calling retriever.get_relevant_documents(&amp;quot;foo&amp;quot;) is a document array which makes it easier to work with.&lt;/p&gt; &lt;p&gt;I would like to have the exact same output from the response of calling the tool. How can this be achieved? Is the only solution to create a custom tool function instead of using the utility function?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Early_Low8914&quot;&gt; /u/Early_Low8914 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dpntgo/how_to_get_a_structured_response_from_create/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dpntgo/how_to_get_a_structured_response_from_create/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dpntgo</id><link href="https://www.reddit.com/r/LangChain/comments/1dpntgo/how_to_get_a_structured_response_from_create/" /><updated>2024-06-27T10:16:00+00:00</updated><published>2024-06-27T10:16:00+00:00</published><title>How to get a structured response from &quot;create_retriever_tool&quot;?</title></entry><entry><author><name>/u/WesEd178</name><uri>https://www.reddit.com/user/WesEd178</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hello I am using create_sql_agent to query a SQL server database. The problem is for some reason the agent does not know that it has to use SQL server dialect from the beginning and also does not know the column names.&lt;/p&gt; &lt;p&gt;Is there a way to provide this initial context to the prompt?&lt;/p&gt; &lt;p&gt;This is my code: llm = ChatOpenAI(model=&amp;quot;gpt-3.5-turbo-0125&amp;quot;, temperature=0) toolkit = SQLDatabaseToolkit(db=db, llm=llm) agent_executor = create_sql_agent( llm=llm, toolkit=toolkit, verbose=True, agent_type=AgentType.ZERO_SHOT_REACT_DESCRIPTION, )&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/WesEd178&quot;&gt; /u/WesEd178 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dpbl5e/add_context_to_create_sql_agent/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dpbl5e/add_context_to_create_sql_agent/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dpbl5e</id><link href="https://www.reddit.com/r/LangChain/comments/1dpbl5e/add_context_to_create_sql_agent/" /><updated>2024-06-26T22:32:10+00:00</updated><published>2024-06-26T22:32:10+00:00</published><title>Add context to create_sql_agent</title></entry><entry><author><name>/u/fizzbyte</name><uri>https://www.reddit.com/user/fizzbyte</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;How are people versioning their RAG pipelines? &lt;/p&gt; &lt;p&gt;I&amp;#39;ve found that with context which changes/needs frequent updates, we need some type of versioning strategy. &lt;/p&gt; &lt;p&gt;Has anyone else run into this?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/fizzbyte&quot;&gt; /u/fizzbyte &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dp9m83/versioning_rag/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dp9m83/versioning_rag/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dp9m83</id><link href="https://www.reddit.com/r/LangChain/comments/1dp9m83/versioning_rag/" /><updated>2024-06-26T21:08:28+00:00</updated><published>2024-06-26T21:08:28+00:00</published><title>Versioning RAG</title></entry><entry><author><name>/u/Pokedrive123</name><uri>https://www.reddit.com/user/Pokedrive123</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;So I have a doubt, I figured out how to get responses back in a certain format using the Json output parsers. I want to know how I can map a response to an intent. Like I want to create an AI Agent. So if the query is &amp;quot;hey I wanna change my password I think I forgot it&amp;quot; it should map it to a task &amp;quot;reset password&amp;quot;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Pokedrive123&quot;&gt; /u/Pokedrive123 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dpcfh2/how_do_i_map_a_user_query_and_response_with_a/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dpcfh2/how_do_i_map_a_user_query_and_response_with_a/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dpcfh2</id><link href="https://www.reddit.com/r/LangChain/comments/1dpcfh2/how_do_i_map_a_user_query_and_response_with_a/" /><updated>2024-06-26T23:09:48+00:00</updated><published>2024-06-26T23:09:48+00:00</published><title>How do I map a user query and response with a certain set of predefined tasks using output parsers?</title></entry><entry><author><name>/u/phicreative1997</name><uri>https://www.reddit.com/user/phicreative1997</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dozoeg/use_vannaai_for_texttosql_much_more_reliable_than/&quot;&gt; &lt;img src=&quot;https://external-preview.redd.it/pwyRXfL2Ycu4z8g64OAgl3-QdCbQ5hpoAQFAUL8tfcY.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1176c8ca4c10ea82cf4ba1098828274d2c6158fd&quot; alt=&quot;Use Vanna.ai for text-to-SQL much more reliable than othe r orchestration solutions, here is how to use it for Claude Sonnet 3.5 &quot; title=&quot;Use Vanna.ai for text-to-SQL much more reliable than othe r orchestration solutions, here is how to use it for Claude Sonnet 3.5 &quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/phicreative1997&quot;&gt; /u/phicreative1997 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://arslanshahid-1997.medium.com/build-a-text-to-sql-chatbot-with-claude-sonnet-3-5-621a5bf9f922&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dozoeg/use_vannaai_for_texttosql_much_more_reliable_than/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1dozoeg</id><media:thumbnail url="https://external-preview.redd.it/pwyRXfL2Ycu4z8g64OAgl3-QdCbQ5hpoAQFAUL8tfcY.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=1176c8ca4c10ea82cf4ba1098828274d2c6158fd" /><link href="https://www.reddit.com/r/LangChain/comments/1dozoeg/use_vannaai_for_texttosql_much_more_reliable_than/" /><updated>2024-06-26T14:15:53+00:00</updated><published>2024-06-26T14:15:53+00:00</published><title>Use Vanna.ai for text-to-SQL much more reliable than othe r orchestration solutions, here is how to use it for Claude Sonnet 3.5</title></entry><entry><author><name>/u/MercuriusExMachina</name><uri>https://www.reddit.com/user/MercuriusExMachina</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1doyeb0/how_to_have_secondary_agent_ask_for/&quot;&gt; &lt;img src=&quot;https://preview.redd.it/ga4yvvma2x8d1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=521034eb3eff55cc204c99790c4263bb19f756eb&quot; alt=&quot;How to have secondary agent ask for clarifications from the primary agent?&quot; title=&quot;How to have secondary agent ask for clarifications from the primary agent?&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/MercuriusExMachina&quot;&gt; /u/MercuriusExMachina &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://i.redd.it/ga4yvvma2x8d1.jpeg&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1doyeb0/how_to_have_secondary_agent_ask_for/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1doyeb0</id><media:thumbnail url="https://preview.redd.it/ga4yvvma2x8d1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=521034eb3eff55cc204c99790c4263bb19f756eb" /><link href="https://www.reddit.com/r/LangChain/comments/1doyeb0/how_to_have_secondary_agent_ask_for/" /><updated>2024-06-26T13:17:27+00:00</updated><published>2024-06-26T13:17:27+00:00</published><title>How to have secondary agent ask for clarifications from the primary agent?</title></entry><entry><author><name>/u/alex6011</name><uri>https://www.reddit.com/user/alex6011</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hello, I&amp;#39;m building a RAG app with Langchain.js. It&amp;#39;s my first time using LLM framework, thus first time using Langchain too. I&amp;#39;m currently using the Recursive URL Loader integration to recursively fetch data from websites. And, behave as I wanted, but I have some issue with websites using modern frontend frameworks. So, I tried the Playwright Langchain integration (I&amp;#39;m a bit familiar with Playwright), and it&amp;#39;s work on the desired websites, but as you guess, it only works on one page. So my question is there is an integration that do both? Handling JS and recursively browse the website, or how can I combine the two to achieve my goal? I&amp;#39;m open to alternative solution using Puppeteer or even Python example.&lt;/p&gt; &lt;p&gt;This is how I use the Recursive URL Loader:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;import { RecursiveUrlLoader } from &amp;#39;@langchain/community/document_loaders/web/recursive_url&amp;#39;; import { compile } from &amp;#39;html-to-text&amp;#39;; export async function loadWebsite(url: string, excludeDirs?: string[]) { const compiledConvert = compile({ wordwrap: 130 }); // returns (text: string) =&amp;gt; string; const loader = new RecursiveUrlLoader(url, { extractor: compiledConvert, maxDepth: 1, excludeDirs, preventOutside: true, }); return loader.load(); } &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;And, this is my Playwright attempt:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;import { PlaywrightWebBaseLoader } from &amp;#39;@langchain/community/document_loaders/web/playwright&amp;#39;; import { MozillaReadabilityTransformer } from &amp;#39;@langchain/community/document_transformers/mozilla_readability&amp;#39;; import { RunnableSequence } from &amp;#39;@langchain/core/runnables&amp;#39;; import { RecursiveCharacterTextSplitter } from &amp;#39;@langchain/textsplitters&amp;#39;; export async function loadWebsite(url: string) { const loader = new PlaywrightWebBaseLoader(url, { launchOptions: { chromiumSandbox: false, }, }); const documents = await loader.load(); const transformChain = RunnableSequence.from([ RecursiveCharacterTextSplitter.fromLanguage(&amp;#39;html&amp;#39;), new MozillaReadabilityTransformer(), ]); return transformChain.invoke(documents); } &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Thanks for reading! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/alex6011&quot;&gt; /u/alex6011 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1doudja/how_to_use_recursive_url_loader_with/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1doudja/how_to_use_recursive_url_loader_with/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1doudja</id><link href="https://www.reddit.com/r/LangChain/comments/1doudja/how_to_use_recursive_url_loader_with/" /><updated>2024-06-26T09:24:56+00:00</updated><published>2024-06-26T09:24:56+00:00</published><title>How to use Recursive URL Loader with Playwright/Puppeteer?</title></entry><entry><author><name>/u/Electrical_Art_1518</name><uri>https://www.reddit.com/user/Electrical_Art_1518</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;m creating a chatbot using the OpenAI Assistant API and considering LangChain. Should I use it?&lt;/p&gt; &lt;p&gt;What are the pros and cons?&lt;/p&gt; &lt;p&gt;Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Electrical_Art_1518&quot;&gt; /u/Electrical_Art_1518 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dosb9q/should_i_use_langchain_for_building_a_chatbot/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dosb9q/should_i_use_langchain_for_building_a_chatbot/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dosb9q</id><link href="https://www.reddit.com/r/LangChain/comments/1dosb9q/should_i_use_langchain_for_building_a_chatbot/" /><updated>2024-06-26T06:57:51+00:00</updated><published>2024-06-26T06:57:51+00:00</published><title>Should I Use LangChain for Building a Chatbot with OpenAI Assistant API?</title></entry><entry><author><name>/u/harshit_nariya</name><uri>https://www.reddit.com/user/harshit_nariya</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/harshit_nariya&quot;&gt; /u/harshit_nariya &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;/r/AnyBodyCanAI/comments/1dovwah/which_llm_better_for_code_generation/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dovx5a/which_llm_better_for_code_generation/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dovx5a</id><link href="https://www.reddit.com/r/LangChain/comments/1dovx5a/which_llm_better_for_code_generation/" /><updated>2024-06-26T11:06:36+00:00</updated><published>2024-06-26T11:06:36+00:00</published><title>which llm better for code generation?</title></entry><entry><author><name>/u/ztide_ad</name><uri>https://www.reddit.com/user/ztide_ad</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;So, I want to automate the form-filling section. For example, here I am taking Redmine. that is basically a chatbot that will interact with the user and change values in afield according to the input given by the user. for now, I have planned to create a text-to-JSON chatbot using some free open-source LLM that will help the user change the fields by changing the natural language entered by the user to JSON format, which will be sent to execute, and the user can see the fields be changed accordingly. &lt;/p&gt; &lt;p&gt;so, how can I implement something like this?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/ztide_ad&quot;&gt; /u/ztide_ad &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dovvd2/how_to_automate_form_filling_using_llm/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dovvd2/how_to_automate_form_filling_using_llm/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dovvd2</id><link href="https://www.reddit.com/r/LangChain/comments/1dovvd2/how_to_automate_form_filling_using_llm/" /><updated>2024-06-26T11:03:43+00:00</updated><published>2024-06-26T11:03:43+00:00</published><title>How to automate form filling using LLM ?</title></entry></feed>