<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/"><category term="LangChain" label="r/LangChain"/><updated>2024-06-19T18:19:39+00:00</updated><icon>https://www.redditstatic.com/icon.png/</icon><id>/r/LangChain.rss</id><link rel="self" href="https://www.reddit.com/r/LangChain.rss" type="application/atom+xml" /><link rel="alternate" href="https://www.reddit.com/r/LangChain" type="text/html" /><subtitle>LangChain is an open-source framework and developer toolkit that helps developers get LLM applications from prototype to production. It is available for Python and Javascript at https://www.langchain.com/.</subtitle><title>LangChain</title><entry><author><name>/u/zchaarm</name><uri>https://www.reddit.com/user/zchaarm</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;A place for members of &lt;a href=&quot;/r/LangChain&quot;&gt;r/LangChain&lt;/a&gt; to chat with each other&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/zchaarm&quot;&gt; /u/zchaarm &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_10ljho9</id><link href="https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/" /><updated>2023-01-26T04:33:05+00:00</updated><published>2023-01-26T04:33:05+00:00</published><title>r/LangChain Lounge</title></entry><entry><author><name>/u/sarthakai</name><uri>https://www.reddit.com/user/sarthakai</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Google just announced Context Caching in the Gemini API — it allows you to store and reuse input tokens for repetitive requests.&lt;/p&gt; &lt;p&gt;Many LLM tasks have extensive system prompts laying down instructions and initial context.&lt;/p&gt; &lt;p&gt;If these are cached, they wouldn’t have to be encoded all over again every time, saving on costs and latency.&lt;/p&gt; &lt;p&gt;Tokens are cached for a specified duration (TTL), after which they are automatically deleted.&lt;/p&gt; &lt;p&gt;Costs depend on the number of tokens cached and their storage duration, and efficiency would be higher for prompts with context used across many LLM calls.&lt;/p&gt; &lt;p&gt;Docs: &lt;a href=&quot;https://ai.google.dev/gemini-api/docs/caching?lang=python&quot;&gt;https://ai.google.dev/gemini-api/docs/caching?lang=python&lt;/a&gt;&lt;/p&gt; &lt;p&gt;You can learn more about AI here: &lt;a href=&quot;https://linktr.ee/sarthakrastogi&quot;&gt;https://linktr.ee/sarthakrastogi&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/sarthakai&quot;&gt; /u/sarthakai &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1djjgia/apparently_geminis_context_caching_can_cut_your/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1djjgia/apparently_geminis_context_caching_can_cut_your/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1djjgia</id><link href="https://www.reddit.com/r/LangChain/comments/1djjgia/apparently_geminis_context_caching_can_cut_your/" /><updated>2024-06-19T13:41:00+00:00</updated><published>2024-06-19T13:41:00+00:00</published><title>Apparently Gemini's context caching can cut your LLM cost and latency to half</title></entry><entry><author><name>/u/bucketheadfan13</name><uri>https://www.reddit.com/user/bucketheadfan13</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;ve looked into completely using Google Big Query to store, embed, and vector search the results since they now offer Vector Searches&lt;/p&gt; &lt;p&gt;Does anyone have any experience doing this with Google Big Query alone?&lt;/p&gt; &lt;p&gt;Would it be better to just import the data into something line Pinecone and use LangChain to chunk/query?&lt;/p&gt; &lt;p&gt;Or could I also just use LangChain with Google Big Query?&lt;/p&gt; &lt;p&gt;Also not sure if I should be chunking the data, or how chunking would work if I needed it to be on an item by item basis&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/bucketheadfan13&quot;&gt; /u/bucketheadfan13 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1djm57k/whats_the_best_way_to_chunk_store_and_query/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1djm57k/whats_the_best_way_to_chunk_store_and_query/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1djm57k</id><link href="https://www.reddit.com/r/LangChain/comments/1djm57k/whats_the_best_way_to_chunk_store_and_query/" /><updated>2024-06-19T15:35:46+00:00</updated><published>2024-06-19T15:35:46+00:00</published><title>What's the best way to chunk, store and, query extremely large datasets where the data is in a CSV/SQL type format (item by item basis with name, description, etc., not a large text file)</title></entry><entry><author><name>/u/dccpt</name><uri>https://www.reddit.com/user/dccpt</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1djkolz/zep_longterm_memory_free_plan_upgraded_to_10k/&quot;&gt; &lt;img src=&quot;https://external-preview.redd.it/LW_D9uWwns4LU9ujOrFRUZxvcoo09iNqWTmeA5II1BQ.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2054db40fb5ea832bf5fcd3e98ecee071501124a&quot; alt=&quot;Zep Long-term Memory: Free Plan Upgraded to 10K Messages&quot; title=&quot;Zep Long-term Memory: Free Plan Upgraded to 10K Messages&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi all - we received some friendly criticism on this subreddit a while back about Zep&amp;#39;s Free Plan limit of 1K messages per month. We&amp;#39;ve heard y&amp;#39;all and have increased the monthly limit 10x to 10K messages. You can sign up here: &lt;a href=&quot;https://www.getzep.com/&quot;&gt;https://www.getzep.com/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We also recently &lt;a href=&quot;https://app.getzep.com/playground&quot;&gt;released a Playground&lt;/a&gt;, allowing you experiment with Zep&amp;#39;s long-term memory features without writing any code.&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://i.redd.it/g2kv7ue7hj7d1.gif&quot;&gt;https://i.redd.it/g2kv7ue7hj7d1.gif&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Getting Started&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Learn about &lt;a href=&quot;https://help.getzep.com/concepts&quot;&gt;key Zep concepts&lt;/a&gt; such as Sessions, Facts, and more.&lt;/li&gt; &lt;li&gt;Experiment with &lt;a href=&quot;https://app.getzep.com/playground&quot;&gt;Zep in the Playground&lt;/a&gt; and &lt;a href=&quot;https://help.getzep.com/building-prompt&quot;&gt;learn how to build LLM prompts&lt;/a&gt; with Zep.&lt;/li&gt; &lt;li&gt;Install Zep&amp;#39;s &lt;a href=&quot;https://help.getzep.com/sdks&quot;&gt;Python, TypeScript, or Go SDKs&lt;/a&gt; and add long-term memory to your application.&lt;/li&gt; &lt;li&gt;Learn how to use &lt;a href=&quot;https://help.getzep.com/langchain/overview&quot;&gt;Zep with LangChain LCEL&lt;/a&gt;.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Let me know if you have any questions!&lt;/p&gt; &lt;p&gt;-Daniel&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/dccpt&quot;&gt; /u/dccpt &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1djkolz/zep_longterm_memory_free_plan_upgraded_to_10k/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1djkolz/zep_longterm_memory_free_plan_upgraded_to_10k/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1djkolz</id><media:thumbnail url="https://external-preview.redd.it/LW_D9uWwns4LU9ujOrFRUZxvcoo09iNqWTmeA5II1BQ.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=2054db40fb5ea832bf5fcd3e98ecee071501124a" /><link href="https://www.reddit.com/r/LangChain/comments/1djkolz/zep_longterm_memory_free_plan_upgraded_to_10k/" /><updated>2024-06-19T14:34:27+00:00</updated><published>2024-06-19T14:34:27+00:00</published><title>Zep Long-term Memory: Free Plan Upgraded to 10K Messages</title></entry><entry><author><name>/u/Strange-Ant-4194</name><uri>https://www.reddit.com/user/Strange-Ant-4194</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;m trying to make llama 3 identify whether or not the user is in a meeting by feeding it this prompt:&lt;/p&gt; &lt;p&gt;&amp;lt;|begin_of_text|&amp;gt;&amp;lt;|start_header_id|&amp;gt;system&amp;lt;|end_header_id|&amp;gt; You are a helpful agent who will answer the user&amp;#39;s question to the best of your abilities. You are NOT allowed to return blank results. &lt;/p&gt; &lt;p&gt;Return ONLY Strings &lt;/p&gt; &lt;p&gt;&amp;lt;|eot_id|&amp;gt;&amp;lt;|start_header_id|&amp;gt;user&amp;lt;|end_header_id|&amp;gt; &lt;/p&gt; &lt;p&gt;Here is the context:&lt;/p&gt; &lt;p&gt;Here is the &amp;#39;contact list&amp;#39; containing the names of contacts and their respective numbers, and the &amp;#39;relationship list&amp;#39; containing their relationships to the user:&lt;/p&gt; &lt;p&gt;Contact list:&lt;/p&gt; &lt;p&gt;1.Priya, +911234567890&lt;/p&gt; &lt;p&gt;2.Kau, +910987654321&lt;/p&gt; &lt;p&gt;3.Laksh, +912234567890&lt;/p&gt; &lt;p&gt;4.Agilan, +919987654321&lt;/p&gt; &lt;p&gt;5.Srikar, +913234567890&lt;/p&gt; &lt;p&gt;6.Prahlad, +918987654321&lt;/p&gt; &lt;p&gt;Relationship list:&lt;/p&gt; &lt;p&gt;1.Priya is &amp;quot;Wife&amp;quot;&lt;/p&gt; &lt;p&gt;2.Kau is &amp;quot;Boss&amp;quot;&lt;/p&gt; &lt;p&gt;3.Laksh is &amp;quot;Brother&amp;quot;&lt;/p&gt; &lt;p&gt;4.Agilan is &amp;quot;Son&amp;quot;&lt;/p&gt; &lt;p&gt;5.Srikar is &amp;quot;Sister&amp;quot;&lt;/p&gt; &lt;p&gt;6.Prahlad is &amp;quot;Daughter&amp;quot;&lt;/p&gt; &lt;p&gt;Here is the calendar:&lt;/p&gt; &lt;p&gt;Meeting1 from 11:06-12:54&lt;/p&gt; &lt;p&gt;Meeting2 from 13:00-15:00&lt;/p&gt; &lt;p&gt;Meeting3 from 15:25-18:00 &lt;/p&gt; &lt;p&gt;Current time: {curt}&lt;/p&gt; &lt;p&gt;Determine: &lt;/p&gt; &lt;p&gt;1.Whether or not the user is in a meeting(use the calendar). If the current time comes after the start time of any meeting and before the end time of that same meeting. the user is in a meeting, else if the current time is before the start time of all meetings or after the end time of all meetings, the user is not in a meeting). TAKE INTO ACCOUNT the EXACT HOURS AND MINUTES of the meeting timings and current time. Even a difference of one minute must be taken into account. &lt;/p&gt; &lt;p&gt;Summarise your findings. &lt;/p&gt; &lt;p&gt;&amp;lt;|eot_id|&amp;gt;&amp;lt;|start_header_id|&amp;gt;assistant&amp;lt;|end_header_id|&amp;gt;&lt;/p&gt; &lt;p&gt;This is just one of many prompts that i&amp;#39;ve given to llama3. In each of them, it always gets it wrong for certain timings.&lt;/p&gt; &lt;p&gt;Note: The contact list and relationship list is not being used for this particular task. It&amp;#39;s just that they all belong to the same file are displayed together.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Strange-Ant-4194&quot;&gt; /u/Strange-Ant-4194 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1djfibu/how_do_i_prompt_llama38b_to_work_accurately_for/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1djfibu/how_do_i_prompt_llama38b_to_work_accurately_for/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1djfibu</id><link href="https://www.reddit.com/r/LangChain/comments/1djfibu/how_do_i_prompt_llama38b_to_work_accurately_for/" /><updated>2024-06-19T10:05:04+00:00</updated><published>2024-06-19T10:05:04+00:00</published><title>How do i prompt llama3:8b to work accurately for this specific task?</title></entry><entry><author><name>/u/Ukpersfidev</name><uri>https://www.reddit.com/user/Ukpersfidev</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I don&amp;#39;t have access to the chunks when using the `StructuredOutputParser(zodSchema)`, streaming works fine when using the text parser, but not this one.&lt;/p&gt; &lt;p&gt;My parsing needs are quite simple, I just need a an array with each variation of generated content, does anyone know which parser I can use for this?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Ukpersfidev&quot;&gt; /u/Ukpersfidev &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1djimi6/streaming_structured_output/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1djimi6/streaming_structured_output/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1djimi6</id><link href="https://www.reddit.com/r/LangChain/comments/1djimi6/streaming_structured_output/" /><updated>2024-06-19T13:01:38+00:00</updated><published>2024-06-19T13:01:38+00:00</published><title>Streaming structured output?</title></entry><entry><author><name>/u/Informal-Victory8655</name><uri>https://www.reddit.com/user/Informal-Victory8655</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi, I&amp;#39;m working on a project for learning purposes, to build a langchain app where user can upload files (for now .pdf files) and do QA with uploaded files using pinecone vector database.&lt;/p&gt; &lt;p&gt;Now, I want to know what should I do to add a functionality to display all the files uploaded by a user at any time. and at any time user can limit his QA to specific files he select from all the files he uploaded. Now to do this using pinecone I first have to get all the vectors ids and for each vector id, I need to get the source in metadata and find unique of the sources, what if the no of vectors is very large and user has uploaded many files, so this method is not feasible at all.&lt;/p&gt; &lt;p&gt;what can I do to save the list of all files uploaded by a user somewhere and instantly get the list of files for any user.&lt;/p&gt; &lt;p&gt;I&amp;#39;m considering production perspective while learning these things. I don&amp;#39;t wanna store anything locally. Consider only frontend would be deployed somewhere. &lt;/p&gt; &lt;p&gt;Is there any thing simple to use like pinecone but for only user specific information retrieval purposes.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Informal-Victory8655&quot;&gt; /u/Informal-Victory8655 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1djhxc4/an_app_to_allow_user_to_upload_files_for_now_pdf/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1djhxc4/an_app_to_allow_user_to_upload_files_for_now_pdf/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1djhxc4</id><link href="https://www.reddit.com/r/LangChain/comments/1djhxc4/an_app_to_allow_user_to_upload_files_for_now_pdf/" /><updated>2024-06-19T12:27:30+00:00</updated><published>2024-06-19T12:27:30+00:00</published><title>An app to allow user to upload files (for now .pdf) and do QA from the uploaded files. Expert Advice Needed.</title></entry><entry><author><name>/u/Lethal_Protector_404</name><uri>https://www.reddit.com/user/Lethal_Protector_404</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;m building a chatbot that can perform multiple actions, with each action managed by a separate agent tailored to a specific use case. Initially, I created a query router using an LLM chain to determine the appropriate agent for a given query. However, as the number of agents has grown, the static query router with if-else conditions is becoming inefficient and unmanageable. I&amp;#39;m seeking guidance on how to improve the query routing mechanism to handle a large number of agents more efficiently. Any suggestions or best practices would be greatly appreciated.&lt;/p&gt; &lt;p&gt;Thanks in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Lethal_Protector_404&quot;&gt; /u/Lethal_Protector_404 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1djhpg4/looking_for_a_dynamic_approach_for_query_router/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1djhpg4/looking_for_a_dynamic_approach_for_query_router/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1djhpg4</id><link href="https://www.reddit.com/r/LangChain/comments/1djhpg4/looking_for_a_dynamic_approach_for_query_router/" /><updated>2024-06-19T12:15:58+00:00</updated><published>2024-06-19T12:15:58+00:00</published><title>Looking for a Dynamic approach for Query Router</title></entry><entry><author><name>/u/93simoon</name><uri>https://www.reddit.com/user/93simoon</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I&amp;#39;ve been trying to get the &lt;a href=&quot;https://github.com/langchain-ai/langgraph-example&quot;&gt;langgraph api &lt;/a&gt;to work on my Windows machine, but I&amp;#39;ve hit a frustrating roadblock. Here&amp;#39;s what&amp;#39;s been happening:&lt;/p&gt; &lt;p&gt;I&amp;#39;ve got Docker Desktop up and running. Next step was to fire up &lt;code&gt;langgraph&lt;/code&gt;. After installing &lt;code&gt;langgraph-cli&lt;/code&gt; and setting up my Python environment, I ran &lt;code&gt;langgraph up&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;However I was greeted with this error that seem to be related to the process not being ran on a unix system. The error stack looks like this:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Traceback (most recent call last): File &amp;quot;c:\workspace\python\delfi\.venv\Lib\site-packages\langgraph_cli\exec.py&amp;quot;, line 64, in subp_exec loop.add_signal_handler(signal.SIGINT, signal_handler) File &amp;quot;C:\Users\user\AppData\Local\Programs\Python\Python311\Lib\asyncio\events.py&amp;quot;, line 574, in add_signal_handler raise NotImplementedError NotImplementedError During handling of the above exception, another exception occurred: Traceback (most recent call last): File &amp;quot;&amp;lt;frozen runpy&amp;gt;&amp;quot;, line 198, in _run_module_as_main File &amp;quot;&amp;lt;frozen runpy&amp;gt;&amp;quot;, line 88, in _run_code File &amp;quot;c:\workspace\python\delfi\.venv\Scripts\langgraph.exe\__main__.py&amp;quot;, line 7, in &amp;lt;module&amp;gt; File &amp;quot;c:\workspace\python\delfi\.venv\Lib\site-packages\click\core.py&amp;quot;, line 1157, in __call__ return self.main(*args, **kwargs) ^^^^^^^^^^^^^^^^^^^^^^^^^^ File &amp;quot;c:\workspace\python\delfi\.venv\Lib\site-packages\click\core.py&amp;quot;, line 1078, in main rv = self.invoke(ctx) ^^^^^^^^^^^^^^^^ File &amp;quot;c:\workspace\python\delfi\.venv\Lib\site-packages\click\core.py&amp;quot;, line 1688, in invoke return _process_result(sub_ctx.command.invoke(sub_ctx)) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File &amp;quot;c:\workspace\python\delfi\.venv\Lib\site-packages\click\core.py&amp;quot;, line 1434, in invoke return ctx.invoke(self.callback, **ctx.params) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File &amp;quot;c:\workspace\python\delfi\.venv\Lib\site-packages\click\core.py&amp;quot;, line 783, in invoke return __callback(*args, **kwargs) ^^^^^^^^^^^^^^^^^^^^^^^^^^^ File &amp;quot;c:\workspace\python\delfi\.venv\Lib\site-packages\langgraph_cli\cli.py&amp;quot;, line 183, in up capabilities = langgraph_cli.docker.check_capabilities(runner) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File &amp;quot;c:\workspace\python\delfi\.venv\Lib\site-packages\langgraph_cli\docker.py&amp;quot;, line 83, in check_capabilities stdout, _ = runner.run(subp_exec(&amp;quot;docker&amp;quot;, &amp;quot;info&amp;quot;, &amp;quot;-f&amp;quot;, &amp;quot;json&amp;quot;, collect=True)) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File &amp;quot;C:\Users\user\AppData\Local\Programs\Python\Python311\Lib\asyncio\runners.py&amp;quot;, line 118, in run return self._loop.run_until_complete(task) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File &amp;quot;C:\Users\user\AppData\Local\Programs\Python\Python311\Lib\asyncio\base_events.py&amp;quot;, line 650, in run_until_complete return future.result() ^^^^^^^^^^^^^^^ File &amp;quot;c:\workspace\python\delfi\.venv\Lib\site-packages\langgraph_cli\exec.py&amp;quot;, line 103, in subp_exec os.killpg(os.getpgid(proc.pid), signal.SIGINT) ^^^^^^^^^ AttributeError: module &amp;#39;os&amp;#39; has no attribute &amp;#39;killpg&amp;#39;. Did you mean: &amp;#39;kill&amp;#39;? Exception ignored in: &amp;lt;function BaseSubprocessTransport.__del__ at 0x000001EA5ECC2E80&amp;gt; Traceback (most recent call last): File &amp;quot;C:\Users\user\AppData\Local\Programs\Python\Python311\Lib\asyncio\base_subprocess.py&amp;quot;, line 126, in __del__ File &amp;quot;C:\Users\user\AppData\Local\Programs\Python\Python311\Lib\asyncio\base_subprocess.py&amp;quot;, line 104, in close File &amp;quot;C:\Users\user\AppData\Local\Programs\Python\Python311\Lib\asyncio\proactor_events.py&amp;quot;, line 108, in close File &amp;quot;C:\Users\user\AppData\Local\Programs\Python\Python311\Lib\asyncio\base_events.py&amp;quot;, line 758, in call_soon File &amp;quot;C:\Users\user\AppData\Local\Programs\Python\Python311\Lib\asyncio\base_events.py&amp;quot;, line 519, in _check_closed RuntimeError: Event loop is closed &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;I&amp;#39;m running Python 3.11 on Windows 11. The github page doesn&amp;#39;t mention anything about this being linux exclusive.&lt;/p&gt; &lt;p&gt;Has anyone else encountered similar issues or found a workaround? Your insights would be immensely helpful!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/93simoon&quot;&gt; /u/93simoon &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1djha74/issue_running_langgraph_api_on_windows/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1djha74/issue_running_langgraph_api_on_windows/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1djha74</id><link href="https://www.reddit.com/r/LangChain/comments/1djha74/issue_running_langgraph_api_on_windows/" /><updated>2024-06-19T11:53:20+00:00</updated><published>2024-06-19T11:53:20+00:00</published><title>Issue running langgraph api on Windows</title></entry><entry><author><name>/u/fsa317</name><uri>https://www.reddit.com/user/fsa317</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Through a series of cutting and pasting I&amp;#39;ve gotten a RAG solution that appears to be working but I have some questions about my own code.&lt;/p&gt; &lt;pre&gt;&lt;code&gt; const vectorStore = new MongoDBAtlasVectorSearch(new OpenAIEmbeddings(), dbConfig); // Implement RAG to answer questions on your data //const retriever = vectorStore.asRetriever(); const retriever = ScoreThresholdRetriever.fromVectorStore(vectorStore, { minSimilarityScore: 0.90, // Finds results with at least this similarity score maxK: 20, // The maximum K value to use. Use it based to your chunk size to make sure you don&amp;#39;t run out of tokens kIncrement: 2, // How much to increase K by each time. It&amp;#39;ll fetch N results, then N + kIncrement, then N + kIncrement * 2, etc. }); const prompt = PromptTemplate.fromTemplate(`You are friendly food guide helping people find restaurants .... : {context} Question: {question}`); const model = new ChatOpenAI({ temperature: 0.1, apiKey:RAG_OPENAI_KEY, model:&amp;#39;gpt-3.5-turbo&amp;#39; }); const chain = RunnableSequence.from([ { context: retriever.pipe(formatDocumentsAsString), question: new RunnablePassthrough(), }, prompt, model, new StringOutputParser(), ]); // Prompt the LLM const question = q; const answer = await chain.invoke(question); console.log(&amp;quot;Question: &amp;quot; + question); console.log(&amp;quot;Answer: &amp;quot; + answer); const retrievedResults = await retriever.invoke(question); console.log(&amp;quot;Result count &amp;quot;+retrievedResults.length); // Return source documents //const retrievedResults = await retriever.getRelevantDocuments(question) const documents = retrievedResults.map((documents =&amp;gt; ({ pageContent: documents.pageContent, url: documents.metadata.url, name: , }))) //console.log(&amp;quot;\nSource documents:\n&amp;quot; + JSON.stringify(documents))``documents.metadata.name &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;My primary questions are:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;What is being passed to the retriever, my question(set earlier as q), or the full prompt? In either case how is that working, I dont see anything passed to the original retriever used in the chain.&lt;/li&gt; &lt;li&gt;Is the retrieving called twice, once as part of the sequence and then again to get the documents? Is this normal? Is there a better way to do this?&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/fsa317&quot;&gt; /u/fsa317 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dj8fkv/trying_to_really_understand_my_own_code/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dj8fkv/trying_to_really_understand_my_own_code/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dj8fkv</id><link href="https://www.reddit.com/r/LangChain/comments/1dj8fkv/trying_to_really_understand_my_own_code/" /><updated>2024-06-19T02:31:00+00:00</updated><published>2024-06-19T02:31:00+00:00</published><title>Trying to really understand my own code</title></entry><entry><author><name>/u/Mediocre-Card8046</name><uri>https://www.reddit.com/user/Mediocre-Card8046</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;I developed a RAG bot with an in memory store (e.g. last three messages are getting saved). Now I was wondering how I can apply my RAG pipeline to follow-up questions.&lt;/p&gt; &lt;p&gt;See this example:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;User:&lt;/strong&gt; Where is the football EM 2024?&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Bot:&lt;/strong&gt; The EM 2024 is in Germany. (works fine until here)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;User&lt;/strong&gt;: Thanks, and which nations will participate? (follow up question)&lt;/p&gt; &lt;p&gt;With this follow up question I see the difficulty to find relevant Information, as without context, it is not clear that the follow up question is about the Euros 2024. The &amp;#39;correct&amp;#39; question where my Bot will find something would be: Thanks, and which nations will participate &lt;strong&gt;in the EM 2024?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Do you see my problem and how did you deal with it?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Mediocre-Card8046&quot;&gt; /u/Mediocre-Card8046 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1djcvh0/chat_history_for_rag_how_to_search_for_follow_up/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1djcvh0/chat_history_for_rag_how_to_search_for_follow_up/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1djcvh0</id><link href="https://www.reddit.com/r/LangChain/comments/1djcvh0/chat_history_for_rag_how_to_search_for_follow_up/" /><updated>2024-06-19T06:58:56+00:00</updated><published>2024-06-19T06:58:56+00:00</published><title>Chat History for RAG: How to search for follow up questions</title></entry><entry><author><name>/u/AnomalyNexus</name><uri>https://www.reddit.com/user/AnomalyNexus</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Playing with Tavily search plus langgraph. Asked it what todays top news is, which it happily retrieved and summarized so mechanically worked fine. Only problem is something is off with the news:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;A volcano in Japan spewing ash and rock 200 meters into the sky&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;...that&amp;#39;s November 2023. Same for the rest of the articles, so clearly an older index. Which is fair, can&amp;#39;t expect a search provider to be entirely live, but still a problem.&lt;/p&gt; &lt;p&gt;So couple of questions on this:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Has anyone had luck using searxng to get more current info?&lt;/li&gt; &lt;li&gt;How would you split this out in tooling? Give it one search engine for general and then a 2nd tool for news and say a third for weather etc? Stock market? Currencies? Seems like an approach that would get out of hand pretty fast and just confuse the LLM.&lt;/li&gt; &lt;li&gt;More generally - what sources for have you had luck with to make your agents more...worldly &amp;amp; current? Provders, techniques, whatever&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Thanks&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/AnomalyNexus&quot;&gt; /u/AnomalyNexus &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dj5mhi/live_data_for_agents/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dj5mhi/live_data_for_agents/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dj5mhi</id><link href="https://www.reddit.com/r/LangChain/comments/1dj5mhi/live_data_for_agents/" /><updated>2024-06-19T00:09:20+00:00</updated><published>2024-06-19T00:09:20+00:00</published><title>Live data for agents</title></entry><entry><author><name>/u/jeffrey-0711</name><uri>https://www.reddit.com/user/jeffrey-0711</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dimeme/made_dashboard_that_compares_14_retrieval/&quot;&gt; &lt;img src=&quot;https://a.thumbs.redditmedia.com/E-Tt9dJSWO5LJR1e8B77O-f1YESGpubwD6fikDQilT4.jpg&quot; alt=&quot;Made dashboard that compares 14+ retrieval combinations.&quot; title=&quot;Made dashboard that compares 14+ retrieval combinations.&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/jeffrey-0711&quot;&gt; /u/jeffrey-0711 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/gallery/1dimeme&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dimeme/made_dashboard_that_compares_14_retrieval/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1dimeme</id><media:thumbnail url="https://a.thumbs.redditmedia.com/E-Tt9dJSWO5LJR1e8B77O-f1YESGpubwD6fikDQilT4.jpg" /><link href="https://www.reddit.com/r/LangChain/comments/1dimeme/made_dashboard_that_compares_14_retrieval/" /><updated>2024-06-18T09:23:00+00:00</updated><published>2024-06-18T09:23:00+00:00</published><title>Made dashboard that compares 14+ retrieval combinations.</title></entry><entry><author><name>/u/Marcusbjol</name><uri>https://www.reddit.com/user/Marcusbjol</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I am attempting to code a persistent multi user chat bot that stores messages with a list of observers of a message to generate the context. It would be easy if I could use a list, but lists arnt hashable therefore cannot be used as keys in a dictionary. I am hoping to not have to store each message in each observers entries.... Any ideas on how to accomplish this?&lt;/p&gt; &lt;p&gt;The idea is to have the bot only use message history the user has observed and keep the rest of the history private.&lt;/p&gt; &lt;p&gt;example flow:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;User1 joins chat. User1 &amp;quot;Hey LLM, xyzpassword is xyz&amp;quot; LLM &amp;quot;Thank you for the xyxpassword&amp;quot; User2 joins chat User2 &amp;quot;Hey whats the xyzpassword&amp;quot; LLM &amp;quot;What xyzpassword?&amp;quot; User1 &amp;quot;Go ahead and repeat the xyzpassword&amp;quot; LLM &amp;quot;xyz&amp;quot; User2 &amp;quot;Thanks&amp;quot; Users 1 and 2 leave chat User3 joins chat User3 &amp;quot;whats the xyzpassword?&amp;quot; LLM &amp;quot;What xyzpassword&amp;quot; User3 leaves chat User2 joins chat User2 &amp;quot;dammit, forgot the xyzpassword, what is it?&amp;quot; LLM &amp;quot;xyz&amp;quot; &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Marcusbjol&quot;&gt; /u/Marcusbjol &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dj8lj5/chatbot_question_multi_user_observed_memory_for/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dj8lj5/chatbot_question_multi_user_observed_memory_for/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dj8lj5</id><link href="https://www.reddit.com/r/LangChain/comments/1dj8lj5/chatbot_question_multi_user_observed_memory_for/" /><updated>2024-06-19T02:39:42+00:00</updated><published>2024-06-19T02:39:42+00:00</published><title>chatbot question - multi user observed memory for context?</title></entry><entry><author><name>/u/Repulsive_Ratio8248</name><uri>https://www.reddit.com/user/Repulsive_Ratio8248</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey all! I&amp;#39;ve used LC before for a very basic RAG app on AWS but am now looking to build a more complex app. I&amp;#39;ve gone through the docs and think I have some of what I need, but wanted to solicit thoughts/feedback here before diving in (only to later realize I made a poor or problematic choice that needs to be walked back):&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The App:&lt;/strong&gt; I&amp;#39;m building a multi-step wizard application that guides users through a complex process, say, a 15-step journey. Each step is completed in order, and the subsequent step is initialized with a summary of the discussion, notes, and action items from the previous step(s).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Feature 1: Persisted Chat History across Multiple Steps&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;I have a use case-specific LLM with prompt templates for each step.&lt;/li&gt; &lt;li&gt;Users should be able to log out, come back later, and see the full chat history for each step they&amp;#39;ve completed.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Question:&lt;/strong&gt; Should I store the chat history as a running log of entries in a database (e.g., PostgreSQL)?&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Question:&lt;/strong&gt; While I plan to use Claude 3 for its large token window, I&amp;#39;m concerned about feeding the raw chat history to the LLM for every new question due to potential cost implications. What&amp;#39;s the recommended approach for retaining context without incurring excessive token costs?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Feature 2: Detailed Conversation Summaries&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;At the end of each step, the full chat history will be passed to an LLM to generate a detailed summary.&lt;/li&gt; &lt;li&gt;This summary will be fed into the initialization prompt for the LLM in the subsequent step, providing context on the user&amp;#39;s decisions from the previous step(s).&lt;/li&gt; &lt;li&gt;As the user progresses, these summaries will be chained together, so by step 6, the initialization prompt includes summaries from steps 1-5.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Question:&lt;/strong&gt; Are there any recommendations for making this process more efficient without losing or watering down important context as the user iterates through the steps?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Additional Consideration:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Potential memory limitations or performance bottlenecks as the chat history and summaries grow in size???&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I&amp;#39;m open to any suggestions or best practices from the LangChain community on architecting this application effectively and efficiently!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Repulsive_Ratio8248&quot;&gt; /u/Repulsive_Ratio8248 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dj1che/newb_questions_maintaining_full_chat_history/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dj1che/newb_questions_maintaining_full_chat_history/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dj1che</id><link href="https://www.reddit.com/r/LangChain/comments/1dj1che/newb_questions_maintaining_full_chat_history/" /><updated>2024-06-18T20:58:40+00:00</updated><published>2024-06-18T20:58:40+00:00</published><title>Newb Questions: Maintaining Full Chat History + Chaining Unique Chat Summarizations</title></entry><entry><author><name>/u/SpecialistProperty82</name><uri>https://www.reddit.com/user/SpecialistProperty82</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Suppose I want to use RAG to store knowledge about the financial reports of different companies. &lt;/p&gt; &lt;p&gt;I have used various retrievers for this, but I have not yet solved the more complex problem of how the retrievers should know which company a given chunk is for or from which source. &lt;/p&gt; &lt;p&gt;I guess the only option I have is to put a lot of information about the PDF in the metadata and try to filter chunks based on that. Have you worked on this before?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/SpecialistProperty82&quot;&gt; /u/SpecialistProperty82 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1diz9pw/how_to_not_mix_data_from_different_pdf_files_or/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1diz9pw/how_to_not_mix_data_from_different_pdf_files_or/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1diz9pw</id><link href="https://www.reddit.com/r/LangChain/comments/1diz9pw/how_to_not_mix_data_from_different_pdf_files_or/" /><updated>2024-06-18T19:30:39+00:00</updated><published>2024-06-18T19:30:39+00:00</published><title>How to not mix data from different pdf files or sources in general?</title></entry><entry><author><name>/u/Minute_Yam_1053</name><uri>https://www.reddit.com/user/Minute_Yam_1053</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I love LangChain. Previously, I used LangChain v0.1.x for a simple invoice extraction app. Recently, I tried building a more complex app, an alternative to Perplexity AI using open-source LLMs, which proved challenging. Here’s my experience:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Dependencies&lt;/strong&gt;: Getting started with LangChain required the installation of multiple dependencies, such as core, hub, community, etc. Some of these dependencies rely on third-party libraries that needed manual installation. While not a major issue, it was somewhat inconvenient. Additionally, some dependencies were quite large, which goes against my preference for avoiding hefty installations.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Documentation&lt;/strong&gt;: The documentation isn&amp;#39;t always current, and occasionally, Google searches led me to outdated versions. It&amp;#39;s crucial to be mindful of the information I use. Using outdated docs without immediate issues might cause hidden problems later.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Debugging&lt;/strong&gt;: Debugging was tough due to LangChain&amp;#39;s complexity with many abstractions. I used LangSmith to trace requests and responses. &lt;/li&gt; &lt;li&gt;&lt;strong&gt;Agent and Tools&lt;/strong&gt;: LangChain’s unified interface for adding tools and building agents is great. It likely performs better with advanced commercial LLMs like GPT4o. However, the open-source LLMs I used and agents I built with LangChain wrapper didn’t produce consistent, production-ready results.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;LangGraph&lt;/strong&gt;: LangGraph looks interesting. I plan to explore it more in the future.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;In the end, I built an agent without LangChain, using the OpenAI client, Python coroutines for async flow, and FastAPI for the web server. The code is a few hundred lines and can be find here &lt;a href=&quot;https://github.com/jjleng/sensei/blob/main/backend/sensei_search/agents/samurai_agent.py&quot;&gt;Open Source Perplexity&lt;/a&gt;.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Minute_Yam_1053&quot;&gt; /u/Minute_Yam_1053 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1diers2/my_experience_of_building_a_rag_based_agent_with/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1diers2/my_experience_of_building_a_rag_based_agent_with/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1diers2</id><link href="https://www.reddit.com/r/LangChain/comments/1diers2/my_experience_of_building_a_rag_based_agent_with/" /><updated>2024-06-18T01:30:51+00:00</updated><published>2024-06-18T01:30:51+00:00</published><title>My experience of building a RAG based agent with LangChain</title></entry><entry><author><name>/u/CodingButStillAlive</name><uri>https://www.reddit.com/user/CodingButStillAlive</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;To me, langgraph appears to be the better backbone structure. And it can completely substitute langchain‘s concept of „a chain“. Thus, langchain seems to provide only all the integrations.&lt;/p&gt; &lt;p&gt;Will these integrations finally become a part of langgraph, instead of the other way around?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/CodingButStillAlive&quot;&gt; /u/CodingButStillAlive &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dikyp6/will_langgraph_absorb_langchain/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dikyp6/will_langgraph_absorb_langchain/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dikyp6</id><link href="https://www.reddit.com/r/LangChain/comments/1dikyp6/will_langgraph_absorb_langchain/" /><updated>2024-06-18T07:37:43+00:00</updated><published>2024-06-18T07:37:43+00:00</published><title>Will langgraph absorb langchain?</title></entry><entry><author><name>/u/Unique-Drink-9916</name><uri>https://www.reddit.com/user/Unique-Drink-9916</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey ppl,&lt;/p&gt; &lt;p&gt;I am facing an issue while retrieving docs through silimarity search.&lt;/p&gt; &lt;p&gt;String to be matched is a small piece of text that contains details about an entity may be a 10 to 20 words. However the docs in my vector db are pdf files (each doc a single pdf file) which can have 1000 words each. &lt;/p&gt; &lt;p&gt;When I do a similarity search with the string, my top document is not the relevant one that I want to fetch. How can I improve this?&lt;/p&gt; &lt;p&gt;Thanks&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Unique-Drink-9916&quot;&gt; /u/Unique-Drink-9916 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dizkoq/best_retrieval_strategy_while_comparing_small/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dizkoq/best_retrieval_strategy_while_comparing_small/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dizkoq</id><link href="https://www.reddit.com/r/LangChain/comments/1dizkoq/best_retrieval_strategy_while_comparing_small/" /><updated>2024-06-18T19:43:50+00:00</updated><published>2024-06-18T19:43:50+00:00</published><title>Best retrieval strategy while comparing small piece of text with documents in vector db</title></entry><entry><author><name>/u/ravediamond000</name><uri>https://www.reddit.com/user/ravediamond000</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hello everyone,&lt;/p&gt; &lt;p&gt;Here&amp;#39;s a nice article showing how to add LLM monitoring to your Langchain application that uses LCEL. I found out first hand how painful it can be to not have anything to monitor your token consumption so I hope it helps you avoid this.&lt;br/&gt; Here&amp;#39;s the link: &lt;a href=&quot;https://www.metadocs.co/2024/06/18/add-monitoring-easily-to-your-langchain-chains-with-langfuse/&quot;&gt;article&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Have a nice read :D&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/ravediamond000&quot;&gt; /u/ravediamond000 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1din8vv/llm_monitoring_with_langfuse_in_lcel_langchain/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1din8vv/llm_monitoring_with_langfuse_in_lcel_langchain/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1din8vv</id><link href="https://www.reddit.com/r/LangChain/comments/1din8vv/llm_monitoring_with_langfuse_in_lcel_langchain/" /><updated>2024-06-18T10:19:53+00:00</updated><published>2024-06-18T10:19:53+00:00</published><title>LLM monitoring with Langfuse in LCEL Langchain application</title></entry><entry><author><name>/u/ChallengeOk6437</name><uri>https://www.reddit.com/user/ChallengeOk6437</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Right now I’m using LlamaParse and it works really well. I want to know what is the best open source tool out there for parsing my PDFs before sending it to the other parts of my RAG. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/ChallengeOk6437&quot;&gt; /u/ChallengeOk6437 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dicr6p/best_open_source_document_parser/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dicr6p/best_open_source_document_parser/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dicr6p</id><link href="https://www.reddit.com/r/LangChain/comments/1dicr6p/best_open_source_document_parser/" /><updated>2024-06-17T23:51:23+00:00</updated><published>2024-06-17T23:51:23+00:00</published><title>Best open source document PARSER??!!</title></entry><entry><author><name>/u/Important-Motor-3383</name><uri>https://www.reddit.com/user/Important-Motor-3383</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I have a json file that contains data in below format. { &amp;quot;key1&amp;quot;: &amp;quot;{json1}&amp;quot;, key2&amp;quot;: &amp;quot;{json2}&amp;quot;,&lt;/p&gt; &lt;p&gt;key3&amp;quot;: &amp;quot;{json3}&amp;quot;, }&lt;/p&gt; &lt;p&gt;json1,json2, json3... has same Structure. I am trying to summarize above data. I have tried: 1. Summarizing using stuffchain, without splitting text. &amp;amp; with splitting. 2. Summarizing using refine,.with and without splitting. &lt;/p&gt; &lt;p&gt;The summarization is working but it is not taking the whole data in consideration. Sometimes it summarizes only key1,key2 data only. Sometimes on key1 json. &lt;/p&gt; &lt;p&gt;I am using mistral: text model for summarization via langchain. &lt;/p&gt; &lt;ol&gt; &lt;li&gt;How to tackle this problem? &lt;/li&gt; &lt;li&gt;How do we summarize json properly? &lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Important-Motor-3383&quot;&gt; /u/Important-Motor-3383 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1diug6j/how_to_summarize_really_large_json/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1diug6j/how_to_summarize_really_large_json/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1diug6j</id><link href="https://www.reddit.com/r/LangChain/comments/1diug6j/how_to_summarize_really_large_json/" /><updated>2024-06-18T16:10:51+00:00</updated><published>2024-06-18T16:10:51+00:00</published><title>How to summarize really large json</title></entry><entry><author><name>/u/Hour-Benefit-7389</name><uri>https://www.reddit.com/user/Hour-Benefit-7389</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey, there!&lt;/p&gt; &lt;p&gt;Running into an error trying to stuff a large amount of text into GPT 4o -- it&amp;#39;s small enough for the context window, but I&amp;#39;m exceeding the TPM limit (30k, according to OpenAI). Here&amp;#39;s the exact error:&lt;/p&gt; &lt;p&gt;RateLimitError: Error code: 429 - {&amp;#39;error&amp;#39;: {&amp;#39;message&amp;#39;: &amp;#39;Request too large for gpt-4o in organization org-Qvve8O3Iihgaa2kduvOVIemS on tokens per min (TPM): Limit 30000, Requested 45544. The input or output tokens must be reduced in order to run successfully. Visit &lt;a href=&quot;https://platform.openai.com/account/rate-limits&quot;&gt;https://platform.openai.com/account/rate-limits&lt;/a&gt; to learn more.&amp;#39;, &amp;#39;type&amp;#39;: &amp;#39;tokens&amp;#39;, &amp;#39;param&amp;#39;: None, &amp;#39;code&amp;#39;: &amp;#39;rate_limit_exceeded&amp;#39;}}&lt;/p&gt; &lt;p&gt;I&amp;#39;ve looked at a few solutions, but those all rely on feeding the prompt in chunks with delay in between, whereas I need GPT to spit out a single response to my prompt based on the text.&lt;/p&gt; &lt;p&gt;Any way you guys know of to slow down the speed at which I&amp;#39;m passing tokens to GPT? &lt;/p&gt; &lt;p&gt;Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Hour-Benefit-7389&quot;&gt; /u/Hour-Benefit-7389 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ditzwa/help_tpm_rate_limit_error/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ditzwa/help_tpm_rate_limit_error/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1ditzwa</id><link href="https://www.reddit.com/r/LangChain/comments/1ditzwa/help_tpm_rate_limit_error/" /><updated>2024-06-18T15:52:00+00:00</updated><published>2024-06-18T15:52:00+00:00</published><title>HELP: TPM Rate Limit Error</title></entry><entry><author><name>/u/Ukpersfidev</name><uri>https://www.reddit.com/user/Ukpersfidev</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;&lt;strong&gt;Context&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I&amp;#39;m making a sales pitch creator where users can upload a bunch of PDF files relating to their company, and it will output a sales pitch.&lt;/p&gt; &lt;p&gt;I&amp;#39;m handling file uploads separately and would to extract key information from the PDF file and save it in my database, the key here is that I don&amp;#39;t want to summarise the info but rather extract the info without the filler and redundancy.&lt;/p&gt; &lt;p&gt;I can see there are a bunch of pre-made chains, but I&amp;#39;m not sure which can be used in the case of extracting and not summarising, I can also see there is a general map reduce chain, however I can&amp;#39;t tell what this is doing exactly or how to use it with my prompts where I want to state specifically what information should be extracted.&lt;/p&gt; &lt;p&gt;I&amp;#39;ve taken a look through this:&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://js.langchain.com/v0.1/docs/modules/chains/document/map_reduce/#with-lcel&quot;&gt;https://js.langchain.com/v0.1/docs/modules/chains/document/map_reduce/#with-lcel&lt;/a&gt;&lt;/p&gt; &lt;p&gt;And find it extremely confusing, I can&amp;#39;t see what all the complexity is about as in my mind doing what I want is just a case of:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Breaking down the file into text chunks&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Extracting info from each using an await Promise.all(chunks.map(c =&amp;gt; chain.invoke(...)))&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Combining the extracted info &lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;Question&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Why the complexity in the above, what am I missing? Should I just do my simple version, or will it break in certain contexts?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Ukpersfidev&quot;&gt; /u/Ukpersfidev &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ditui4/should_you_use_the_premade_chains_for_map_reducing/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ditui4/should_you_use_the_premade_chains_for_map_reducing/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1ditui4</id><link href="https://www.reddit.com/r/LangChain/comments/1ditui4/should_you_use_the_premade_chains_for_map_reducing/" /><updated>2024-06-18T15:45:26+00:00</updated><published>2024-06-18T15:45:26+00:00</published><title>Should you use the pre-made chains for map reducing?</title></entry><entry><author><name>/u/_b_2_v_</name><uri>https://www.reddit.com/user/_b_2_v_</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Am curious how you articulate an actionable response for end-user&amp;#39;s intent, e.g. delete account option that actually deletes account and not a link to the how-to guide. How would you even go about abstracting it without some sort GUI on the frontend? Do you just implement your own or are there solutions out there? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/_b_2_v_&quot;&gt; /u/_b_2_v_ &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1disrgq/how_do_you_return_actionable_responses/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1disrgq/how_do_you_return_actionable_responses/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1disrgq</id><link href="https://www.reddit.com/r/LangChain/comments/1disrgq/how_do_you_return_actionable_responses/" /><updated>2024-06-18T14:59:47+00:00</updated><published>2024-06-18T14:59:47+00:00</published><title>How do you return actionable responses?</title></entry><entry><author><name>/u/i_am_innovative</name><uri>https://www.reddit.com/user/i_am_innovative</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/i_am_innovative&quot;&gt; /u/i_am_innovative &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dimixi/can_someone_explain_me_what_is_the_difference/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dimixi/can_someone_explain_me_what_is_the_difference/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dimixi</id><link href="https://www.reddit.com/r/LangChain/comments/1dimixi/can_someone_explain_me_what_is_the_difference/" /><updated>2024-06-18T09:31:32+00:00</updated><published>2024-06-18T09:31:32+00:00</published><title>Can someone explain me what is the difference between nvidia nemo and nvidia nim framework?</title></entry></feed>