<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/"><category term="LangChain" label="r/LangChain"/><updated>2024-04-11T03:54:59+00:00</updated><icon>https://www.redditstatic.com/icon.png/</icon><id>/r/LangChain.rss</id><link rel="self" href="https://www.reddit.com/r/LangChain.rss" type="application/atom+xml" /><link rel="alternate" href="https://www.reddit.com/r/LangChain" type="text/html" /><subtitle>LangChain is an open-source framework and developer toolkit that helps developers get LLM applications from prototype to production. It is available for Python and Javascript at https://www.langchain.com/.</subtitle><title>LangChain</title><entry><author><name>/u/zchaarm</name><uri>https://www.reddit.com/user/zchaarm</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;A place for members of &lt;a href=&quot;/r/LangChain&quot;&gt;r/LangChain&lt;/a&gt; to chat with each other&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/zchaarm&quot;&gt; /u/zchaarm &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_10ljho9</id><link href="https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/" /><updated>2023-01-26T04:33:05+00:00</updated><published>2023-01-26T04:33:05+00:00</published><title>r/LangChain Lounge</title></entry><entry><author><name>/u/Desik_1998</name><uri>https://www.reddit.com/user/Desik_1998</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;As the title describes, I&amp;#39;ve used Claude 3 Sonnet to create a 30K word story which heavily grounds in details. Here is the &lt;a href=&quot;https://github.com/desik1998/NovelWithLLMs/blob/main/Novel.md&quot;&gt;story link&lt;/a&gt; (For now put this on Github itself). The story is about American founding fathers returning back to 21st Century. Currently it consists of 3 chapters and there are 4 more chapters to write. I&amp;#39;ve already reviewed it with few of my friends who&amp;#39;re avid novel readers and most of them have responded with &amp;#39;it doesn&amp;#39;t feel AI written&amp;#39;, it&amp;#39;s interesting (subjective but most have said this), grounds heavily on details. Requesting to read the novel and provide the feedback &lt;/p&gt; &lt;p&gt;Github Link: &lt;a href=&quot;https://github.com/desik1998/NovelWithLLMs/tree/main&quot;&gt;https://github.com/desik1998/NovelWithLLMs/tree/main&lt;/a&gt; &lt;/p&gt; &lt;h1&gt;Approach to create long story:&lt;/h1&gt; &lt;p&gt;LLMs such as Claude 3 / Gpt 4 currently allows input context length of 150K words and can output 3K words at once. A typical novel in general has a total of 60K-100K words. Considering the 3K output limit, it isn&amp;#39;t possible to generate a novel in one single take. So the intuition here is that let the LLM &lt;strong&gt;generate 1 event at a time and once the event is generated, add it to the existing story and continously repeat this process&lt;/strong&gt;. Although theoretically this approach might seem to work, just doing this leads to LLM moving quickly from one event to another, not being very grounded in details, llm not generating event which is a continuation of the current story, LLM generating mistakes based on the current story etc. &lt;/p&gt; &lt;p&gt;To address this, the following steps are taken: &lt;/p&gt; &lt;h1&gt;1. Initially fix on the high level story:&lt;/h1&gt; &lt;p&gt;Ask LLM to generate high level plot of the story like at a 30K depth. Generate multiple plots as such. In our case, the high level line in mind was &lt;strong&gt;Founding Fathers returning back&lt;/strong&gt;. Using this line, LLM was asked to generated many plots enhancing this line. It suggested many plots such as Founding fathers called back for being judged based on their actions, founding fathers called back to solve AI crisis, founding fathers come back for fighting against China, Come back and fight 2nd revolutionary war etc. Out of all these, the 2nd revolutionary war seemed the best. Post the plot, LLM was prompted to generate many stories from this plot. Out of these, multiple ideas in the stories were combined (manually) to get to fix on high level story. Once this is done, get the chapters for the high level story (again generated multiple outputs instead of 1). And generating chapters should be easy if the high level story is already present &lt;/p&gt; &lt;h1&gt;2. Do the event based generation for events in chapter:&lt;/h1&gt; &lt;p&gt;Once chapters are fixed, now start with the generation of events in a chapter but &lt;strong&gt;1 event at a time like described above&lt;/strong&gt;. To make sure that the event is grounded in details, a little prompting is reqd telling the LLM to avoid moving too fast into the event and ground to details, avoid generating same events as past etc. &lt;a href=&quot;https://github.com/desik1998/NovelWithLLMs/blob/main/PROMPT.md&quot;&gt;Prompt used till now&lt;/a&gt; (There are some repetitions in the prompt but this works well). Even after this, the output generated by LLM might not be very compelling so to get a good output, generate the output multiple times. And in general generating &lt;strong&gt;5-10 outputs&lt;/strong&gt;, results in a good possible result. And it&amp;#39;s better to do this by varying temperatures. In case of current story, the temperature b/w 0.4-0.8 worked well. Additionally, the rationale behind generating multiple outputs is, given LLMs generate different output everytime, the chances of getting good output when prompted multiple times increases. Even after generating multiple outputs with different temperatures, if it doesn&amp;#39;t yield good results, understand what it&amp;#39;s doing wrong for example like avoid repeating events and tell it to avoid doing that. For example in the 3rd chapter when the LLM was asked to explain the founders about the history since their time, it was rushing off, so &lt;a href=&quot;https://github.com/desik1998/NovelWithLLMs/blob/main/HistoryChapterPrompt.md&quot;&gt;an instruction to explain the historic events year-by-year&lt;/a&gt; was added in the prompt. Sometimes the LLM also generates part of the event which is too good but the overall event is not good, in this scenario adding the part of the event to the story and continuing to generate the story worked well. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Overall Gist:&lt;/strong&gt; Generate the event multiple times with different temperatures and take the best amongst them. If it still doesn&amp;#39;t work, prompt it to avoid doing the wrong things it&amp;#39;s doing &lt;/p&gt; &lt;p&gt;Overall Event Generation: Instead of generating the next event in a chat conversation mode, giving the whole story till now as a combination of events in a single prompt and asking it to generate next event worked better. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Conversation Type 1:&lt;/strong&gt; &lt;/p&gt; &lt;pre&gt;&lt;code&gt;human: generate 1st event Claude: Event1 human: generate next, Claude: Event2, human: generate next ... &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Conversation Type 2:&lt;/strong&gt; (Better) &lt;/p&gt; &lt;pre&gt;&lt;code&gt;Human: Story till now: Event1 + Event2 + ... + EventN. Generate next event Claude: Event(N+1) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Also as the events are generated, one keeps getting new ideas to proceed on the story chapters. And if any event generated is so good, but aligns little different from current story, one can also change the future story/chapters. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;The current approach, doesn&amp;#39;t require any code&lt;/strong&gt; and long stories can be generated directly using the &lt;strong&gt;Claude Playground or Amazon Bedrock Playground&lt;/strong&gt; (Claude is hosted). Claude Playground has the best Claude Model Opus which Bedrock currently lacks but given this Model is 10X costly, avoided it and went with the 2nd Best Sonnet Model. As per my experience, the results on Bedrock are better than the ones in Claude Playground &lt;/p&gt; &lt;h1&gt;Questions:&lt;/h1&gt; &lt;ol&gt; &lt;li&gt;Why wasn&amp;#39;t Gpt4 used to create this story?&lt;br/&gt;&lt;/li&gt; &lt;/ol&gt; &lt;ul&gt; &lt;li&gt;When asked Gpt4 to generate the next event in the story, there was no coherence in the next event generated with the existing story. Maybe with more prompt engineering, this might be solved but Claude 3 was giving better output without much effort so went with it. Infact, Claude 3 Sonnet (the 2nd best model from Claude) is doing much better when compared to Gpt4.&lt;br/&gt;&lt;/li&gt; &lt;/ul&gt; &lt;ol&gt; &lt;li&gt;How much cost did it take to do this?&lt;br/&gt;&lt;/li&gt; &lt;/ol&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;$50-100&lt;/strong&gt;&lt;br/&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Further Improvements:&lt;/h1&gt; &lt;ol&gt; &lt;li&gt;Explore ways to avoid long input contexts. This can further reduce the cost considering most of the cost is going into this step. Possible Solutions:&lt;br/&gt;&lt;/li&gt; &lt;/ol&gt; &lt;ul&gt; &lt;li&gt;Give gists of the events happened in the story till now instead of whole story as an input to the LLM. References: &lt;a href=&quot;https://deepmind.google/research/publications/74917/&quot;&gt;1&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/html/2310.00785v3&quot;&gt;2&lt;/a&gt;&lt;br/&gt;&lt;/li&gt; &lt;/ul&gt; &lt;ol&gt; &lt;li&gt;Avoid the human loop as part of the choosing the best event generated. Currently it takes a lot of human time when choosing the best event generated. Due to this, the time to generate a story can take from few weeks to few months (1-1.5 months). If this step is automated atleast to some degree, the time to write the long story will further decrease. Possible Solutions:&lt;br/&gt;&lt;/li&gt; &lt;/ol&gt; &lt;ul&gt; &lt;li&gt;Use an LLM to determine what are the best events or top 2-3 events generated. This can be done based on multiple factors such as whether the event is a continuation, the event is not repeating itself. And based on these factors, LLM can rate the top responses. References: &lt;a href=&quot;https://huggingface.co/papers/2308.06259&quot;&gt;Last page in this paper&lt;/a&gt; &lt;/li&gt; &lt;li&gt;Train a reward model (With or without LLM) for determining which generated event is better. &lt;a href=&quot;https://arxiv.org/html/2401.10020v1&quot;&gt;LLM as Reward model&lt;/a&gt;&lt;br/&gt;&lt;/li&gt; &lt;/ul&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;The current approach generates only 1 story. Instead generate a Tree of possible stories for a given plot. For example, multiple generations for an event can be good, in this case, select all of them and create different stories. &lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Use the same approach for other things such as movie story generation, Text Books, Product document generation etc &lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Benchmark LLMs Long Context not only on RAG but also on Generation &lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Desik_1998&quot;&gt; /u/Desik_1998 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c0w79c/used_claudes_200k_context_length_to_write_a_30k/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c0w79c/used_claudes_200k_context_length_to_write_a_30k/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1c0w79c</id><link href="https://www.reddit.com/r/LangChain/comments/1c0w79c/used_claudes_200k_context_length_to_write_a_30k/" /><updated>2024-04-10T20:50:00+00:00</updated><published>2024-04-10T20:50:00+00:00</published><title>Used Claude's 200K context length to write a 30K word novel which heavily grounds in details unlike the existing novels</title></entry><entry><author><name>/u/usnavy13</name><uri>https://www.reddit.com/user/usnavy13</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I started building internal LLM tools for my company and originally thought LangChain would be a good tool. At the time I was wrong, there were many issues with the project and I found out I was better off removing and replacing LangChain with my own implementations. &lt;/p&gt; &lt;p&gt;I&amp;#39;m glad to say I&amp;#39;ve started to bring LangChain back into my projects. I have to commend the LangChain team for all their work to improve the project. The project still has its issues (mainly documentation and over-abstraction) but overall much better. The community tools provide the best suite of integrations of any LLM package.&lt;/p&gt; &lt;p&gt;The thing that impresses me the most is LangSmith. It gives you unparalleled visibility into what your app is doing and provides tools that supercharge the development process. Fantastic product!&lt;/p&gt; &lt;p&gt;TLDR: its better now&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/usnavy13&quot;&gt; /u/usnavy13 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c0oucr/i_am_coming_back_to_langchain/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c0oucr/i_am_coming_back_to_langchain/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1c0oucr</id><link href="https://www.reddit.com/r/LangChain/comments/1c0oucr/i_am_coming_back_to_langchain/" /><updated>2024-04-10T15:49:44+00:00</updated><published>2024-04-10T15:49:44+00:00</published><title>I am coming back to LangChain!</title></entry><entry><author><name>/u/br0kelyn</name><uri>https://www.reddit.com/user/br0kelyn</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;m looking for guidance on creating a multi-agent conversational AI that can dynamically switch between specialized agents based on the user&amp;#39;s needs, while retaining the full conversation history to provide a personalized experience.&lt;/p&gt; &lt;p&gt;The high-level idea is:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;When the user asks a question or makes a request, the conversational AI analyzes the input to determine which specialized agent is best suited to assist (e.g. a math agent for solving math problems, a history agent for discussing historical topics, etc.)&lt;/li&gt; &lt;li&gt;The relevant specialized agent then engages with the user to address their specific query&lt;/li&gt; &lt;li&gt;Throughout the conversation, even as different specialized agents kick in, the full chat history is retained and passed along, so each agent has the full context of the conversation&lt;/li&gt; &lt;li&gt;This allows the conversational AI to provide a seamless experience that is personalized to the user&amp;#39;s ongoing needs&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I&amp;#39;m fairly new to working with tools like Langchain and could use any advice or best practices for architecting and orchestrating something like this. Some specific questions:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;What&amp;#39;s the best way to structure the specialized agents? Should they be separate fine-tuned models, separate knowledge bases that plug into foundational models (GPT-4), or something else?&lt;/li&gt; &lt;li&gt;How can I efficiently store and pass along the full conversation history to each agent, without hitting token limits of the underlying models?&lt;/li&gt; &lt;li&gt;Are there any existing open-source projects or frameworks that could serve as a good starting point or reference for orchestrating a conversational AI like this?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Any guidance or resources are much appreciated! I&amp;#39;m excited to dive into this but could use a push in the right direction. Let me know if any additional details would be helpful.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/br0kelyn&quot;&gt; /u/br0kelyn &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c0xi7a/advice_needed_on_orchestrating_a_multiagent/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c0xi7a/advice_needed_on_orchestrating_a_multiagent/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1c0xi7a</id><link href="https://www.reddit.com/r/LangChain/comments/1c0xi7a/advice_needed_on_orchestrating_a_multiagent/" /><updated>2024-04-10T21:43:13+00:00</updated><published>2024-04-10T21:43:13+00:00</published><title>Advice needed on orchestrating a multi-agent conversational AI with chat history retention</title></entry><entry><author><name>/u/Key_Radiant</name><uri>https://www.reddit.com/user/Key_Radiant</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Key_Radiant&quot;&gt; /u/Key_Radiant &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c0srpo/what_vector_database_do_you_use/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c0srpo/what_vector_database_do_you_use/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1c0srpo</id><link href="https://www.reddit.com/r/LangChain/comments/1c0srpo/what_vector_database_do_you_use/" /><updated>2024-04-10T18:31:35+00:00</updated><published>2024-04-10T18:31:35+00:00</published><title>What vector database do you use?</title></entry><entry><author><name>/u/StalkerMuffin</name><uri>https://www.reddit.com/user/StalkerMuffin</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I am currently deciding on a backend technology to use for my AI backend app. I will be looking into implementing/utilizing RAG, Background jobs for data loading, OAuth etc.&lt;/p&gt; &lt;p&gt;NestJS is a pretty good backend choice these days for the same but I am just worried if Langchain JS is as good as Python Langchain as the documentation of Python version is awesome compared to JS. &lt;/p&gt; &lt;p&gt;So first question, can Langchain JS handle everything Python version can and only the documentation is lacking? &lt;/p&gt; &lt;p&gt;I love FastAPI&amp;#39;s developer experience and how lightweight it is and I would like to stick with it if in Python ecosystem due to Langchain but I am worried I will have to do too much work from scratch that Django already handles. (Note I will be using AWS wherever I can so does it really matter?) &lt;/p&gt; &lt;p&gt;So yeah, second question, if using Langchain JS, which is more recommended, Django or FastAPI for an AI heavy app? I hate Django&amp;#39;s learning curve but is it worth it compared to FastAPI?&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/StalkerMuffin&quot;&gt; /u/StalkerMuffin &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c127nu/can_langchain_js_handle_everything_python_version/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c127nu/can_langchain_js_handle_everything_python_version/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1c127nu</id><link href="https://www.reddit.com/r/LangChain/comments/1c127nu/can_langchain_js_handle_everything_python_version/" /><updated>2024-04-11T01:10:17+00:00</updated><published>2024-04-11T01:10:17+00:00</published><title>Can Langchain JS handle everything Python version can?</title></entry><entry><author><name>/u/LeDebardeur</name><uri>https://www.reddit.com/user/LeDebardeur</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hello everyone,&lt;br/&gt; I&amp;#39;m working on a chatbot app (frontend React +backend Python with fastAPI), and I&amp;#39;m using a python agent to generate some graphs and charts for the users. However, the frontend and the backend are hosted in different containers for security purposes, so I can&amp;#39;t use the native save image on python and return the location of the image (as it&amp;#39;s gonna be on the backend container that is isolated from the frontend one so I won&amp;#39;t be able to access it).&lt;br/&gt; How can I solve this problem?&lt;br/&gt; Thanks in advance&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/LeDebardeur&quot;&gt; /u/LeDebardeur &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c10vvb/display_charts_and_images_generated_from_a_python/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c10vvb/display_charts_and_images_generated_from_a_python/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1c10vvb</id><link href="https://www.reddit.com/r/LangChain/comments/1c10vvb/display_charts_and_images_generated_from_a_python/" /><updated>2024-04-11T00:08:13+00:00</updated><published>2024-04-11T00:08:13+00:00</published><title>Display charts and images generated from a python agent</title></entry><entry><author><name>/u/onsies</name><uri>https://www.reddit.com/user/onsies</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I used Gradio to deploy, which is quick and easy. Whatâ€™s the easiest way to add stripe payment collection for subscription?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/onsies&quot;&gt; /u/onsies &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c0rvek/best_method_to_quickly_and_easily_deploy/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c0rvek/best_method_to_quickly_and_easily_deploy/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1c0rvek</id><link href="https://www.reddit.com/r/LangChain/comments/1c0rvek/best_method_to_quickly_and_easily_deploy/" /><updated>2024-04-10T17:55:05+00:00</updated><published>2024-04-10T17:55:05+00:00</published><title>Best Method to Quickly and Easily Deploy?</title></entry><entry><author><name>/u/XariZaru</name><uri>https://www.reddit.com/user/XariZaru</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;So, I noticed on ChatGPT and also on my own chatbot (for a brief period of time), the chatbot would apologize and correct itself when provided feedback by the user. For example:&lt;/p&gt; &lt;p&gt;Q: What is 2+2?&lt;br/&gt; A: 5&lt;/p&gt; &lt;p&gt;Q: No, it is 4.&lt;/p&gt; &lt;p&gt;Expected A: Sorry, you&amp;#39;re right. It is 4.&lt;/p&gt; &lt;p&gt;Actual A: 5.&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;My chatbot now is just sticking strongly to its answer instead of remedying itself. What is the best way to acknowledge the corrected answer?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/XariZaru&quot;&gt; /u/XariZaru &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c0t2tt/allow_chatbot_to_correct_itself_to_user_feedback/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c0t2tt/allow_chatbot_to_correct_itself_to_user_feedback/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1c0t2tt</id><link href="https://www.reddit.com/r/LangChain/comments/1c0t2tt/allow_chatbot_to_correct_itself_to_user_feedback/" /><updated>2024-04-10T18:43:49+00:00</updated><published>2024-04-10T18:43:49+00:00</published><title>Allow Chatbot to Correct Itself to User Feedback</title></entry><entry><author><name>/u/iclickedca</name><uri>https://www.reddit.com/user/iclickedca</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;e.g. like this:&lt;br/&gt; &lt;a href=&quot;https://docs.litellm.ai/docs/caching/redis_cache#custom-cache-keys&quot;&gt;https://docs.litellm.ai/docs/caching/redis_cache#custom-cache-keys&lt;/a&gt; &lt;/p&gt; &lt;p&gt;would be nice to have non-hash keys e.g, for searching in Redis GUI&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/iclickedca&quot;&gt; /u/iclickedca &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c0y53d/langchain_cache_any_way_to_define_a_custom_cache/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c0y53d/langchain_cache_any_way_to_define_a_custom_cache/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1c0y53d</id><link href="https://www.reddit.com/r/LangChain/comments/1c0y53d/langchain_cache_any_way_to_define_a_custom_cache/" /><updated>2024-04-10T22:09:20+00:00</updated><published>2024-04-10T22:09:20+00:00</published><title>Langchain Cache - Any way to define a custom cache key?</title></entry><entry><author><name>/u/profepcot</name><uri>https://www.reddit.com/user/profepcot</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I wrote a piece on &lt;a href=&quot;https://www.mirascope.io/post/langchain-prompt-template&quot;&gt;prompt templates in LangChain&lt;/a&gt;, how they work and the different approach &lt;a href=&quot;https://github.com/Mirascope/mirascope&quot;&gt;Mirascope&lt;/a&gt; takes with colocation. I hope you find it useful.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/profepcot&quot;&gt; /u/profepcot &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c0qjq2/prompt_templates_in_langchain/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c0qjq2/prompt_templates_in_langchain/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1c0qjq2</id><link href="https://www.reddit.com/r/LangChain/comments/1c0qjq2/prompt_templates_in_langchain/" /><updated>2024-04-10T17:00:13+00:00</updated><published>2024-04-10T17:00:13+00:00</published><title>Prompt templates in LangChain</title></entry><entry><author><name>/u/Albert_AG</name><uri>https://www.reddit.com/user/Albert_AG</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hello! I&amp;#39;m a LangChain beginner and I need some help.&lt;/p&gt; &lt;p&gt;I&amp;#39;m working a PDF Chatbot that takes in a stock annual report as a PDF and does technical question answering on it [Mathematical] - Please help me! How do I approach this problem and where do I begin? I&amp;#39;ll take any help I can.&lt;/p&gt; &lt;p&gt;I&amp;#39;ve been trying to do this with RAG. Any Suggestions? Thank you.&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Albert_AG&quot;&gt; /u/Albert_AG &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c0q8qm/beginner_to_langchain_need_help_stock_annual/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c0q8qm/beginner_to_langchain_need_help_stock_annual/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1c0q8qm</id><link href="https://www.reddit.com/r/LangChain/comments/1c0q8qm/beginner_to_langchain_need_help_stock_annual/" /><updated>2024-04-10T16:47:43+00:00</updated><published>2024-04-10T16:47:43+00:00</published><title>Beginner to LangChain. Need help! - [Stock Annual Report PDF Chatbot using RAG].</title></entry><entry><author><name>/u/Proof-Character-9828</name><uri>https://www.reddit.com/user/Proof-Character-9828</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I am quite new to LangChain and Python as im mainly doing C# but i am interested in using AI on my own data.&lt;br/&gt; So i wrote some python code using langchain that:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Gets my Emails via IMAP&lt;/li&gt; &lt;li&gt;Creates JSON from my E-Mails (JSONLoader)&lt;/li&gt; &lt;li&gt;Creates a Vectordatabase where each mail is a vector (FAISS, OpenAIEmbeddings)&lt;/li&gt; &lt;li&gt;Does a similarity search according to the query returning the 3 mails that match the query the most&lt;/li&gt; &lt;li&gt;feeds the result of the similarity search to the LLM (GPT 3.5 Turbo) using the query AGAIN&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;The LLM Prompt then looks something like:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;The question is &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;{query}&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Here are some information that can help you to answer the question: &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;{similarity_search_result}&lt;/p&gt; &lt;p&gt;Ok so far so good... when my question is:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;When was my last mail sent to xyz@gmail.com? &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;i get a correct answer... -&amp;gt; e.g last mail received 10.04.2024 14:11 &lt;/p&gt; &lt;p&gt;But what if i want to have an answer to the following question&lt;/p&gt; &lt;pre&gt;&lt;code&gt;How many mails have been sent by xyz@gmail.com? &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Because the similarity search only gets the vectors that are most similar, how can i just get an answer about the amount?&lt;br/&gt; Even if the similarity search would deliver 150 mails instead of 3 sent by [&lt;a href=&quot;mailto:xyz@gmail.com&quot;&gt;xyz@gmail.com&lt;/a&gt;](mailto:&lt;a href=&quot;mailto:xyz@gmail.com&quot;&gt;xyz@gmail.com&lt;/a&gt;) i cant just feed them all into the LLM prompt right? &lt;/p&gt; &lt;p&gt;So what is my mistake here?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Proof-Character-9828&quot;&gt; /u/Proof-Character-9828 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c0k2qo/langchain_emails_with_llm/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c0k2qo/langchain_emails_with_llm/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1c0k2qo</id><link href="https://www.reddit.com/r/LangChain/comments/1c0k2qo/langchain_emails_with_llm/" /><updated>2024-04-10T12:13:13+00:00</updated><published>2024-04-10T12:13:13+00:00</published><title>LangChain E-Mails with LLM</title></entry><entry><author><name>/u/NasserAAA</name><uri>https://www.reddit.com/user/NasserAAA</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;&lt;strong&gt;The following code:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;tools = load_tools([&amp;quot;llm-math&amp;quot;],llm=llm)&lt;br/&gt; chain = ConversationalRetrievalChain.from_llm(&lt;br/&gt; llm=llm,&lt;br/&gt; tools=tools,&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Causes the following error message:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;\venv\lib\site-packages\langchain_core\load\serializable.py&amp;quot;, line 120, in &lt;strong&gt;init&lt;/strong&gt;&lt;br/&gt; super().&lt;strong&gt;init&lt;/strong&gt;(**kwargs)&lt;br/&gt; File &amp;quot;pydantic\main.py&amp;quot;, line 341, in pydantic.main.BaseModel.&lt;strong&gt;init&lt;/strong&gt;&lt;br/&gt; pydantic.error_wrappers.ValidationError: 1 validation error for ConversationalRetrievalChain&lt;br/&gt; tools&lt;br/&gt; extra fields not permitted (type=value_error.extra)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;I want a way to call math tool because my chain fails to calculate summations for example like this:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;29577.30 + 24683.36 + 23262.12 + 26421.73 + 52409.77 + 25314.39 = 137,605.52, which is wrong and most likely it gets a new answer each time.&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/NasserAAA&quot;&gt; /u/NasserAAA &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c0mkl4/adding_tools_to_conversationalretrievalchainfrom/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c0mkl4/adding_tools_to_conversationalretrievalchainfrom/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1c0mkl4</id><link href="https://www.reddit.com/r/LangChain/comments/1c0mkl4/adding_tools_to_conversationalretrievalchainfrom/" /><updated>2024-04-10T14:11:59+00:00</updated><published>2024-04-10T14:11:59+00:00</published><title>Adding tools to ConversationalRetrievalChain.from_llm causes Pydantic error</title></entry><entry><author><name>/u/PresentationSevere89</name><uri>https://www.reddit.com/user/PresentationSevere89</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;m a bit frustrated. We&amp;#39;ve been working for more than 6 months on an MVP for a Q&amp;amp;A chat about product documentation. After all that, the LLM still hallucinates a lot and gives very basic responses. I would love to have, at this time, a system capable of using several documents to formulate a sound response to a user&amp;#39;s question. I&amp;#39;m using GPT-3.5. I know how capable the model is, and I hate how basic our chat answers are. Maybe it&amp;#39;s the chain, the steps to formulate a response and validate it, or the bad retriever that can&amp;#39;t bring useful documents from the user&amp;#39;s reduced query... I feel like we&amp;#39;ve tried a lot: few shots, fine-tuning embeddings, fine-tuning the GPT, etc... But somehow, we don&amp;#39;t get it to work. I just feel we can, but in the end, we don&amp;#39;t. Any advice to make a killer LLM-powered chat about product documentation?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/PresentationSevere89&quot;&gt; /u/PresentationSevere89 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c0cnyj/easiest_way_to_improve_rag_chat_help/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c0cnyj/easiest_way_to_improve_rag_chat_help/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1c0cnyj</id><link href="https://www.reddit.com/r/LangChain/comments/1c0cnyj/easiest_way_to_improve_rag_chat_help/" /><updated>2024-04-10T04:13:47+00:00</updated><published>2024-04-10T04:13:47+00:00</published><title>Easiest way to improve RAG chat - help</title></entry><entry><author><name>/u/cryptokaykay</name><uri>https://www.reddit.com/user/cryptokaykay</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c0do04/results_of_evaluating_the_ai_oracle_approach_a/&quot;&gt; &lt;img src=&quot;https://b.thumbs.redditmedia.com/GCdPwBj4teGkowjNA4iTF6pGHJldADy1b4cFZ_8gtFI.jpg&quot; alt=&quot;Results of evaluating the AI Oracle approach - a novel way to improve your LLM application's accuracy&quot; title=&quot;Results of evaluating the AI Oracle approach - a novel way to improve your LLM application's accuracy&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Recently, I saw this tweet about the AI Oracle approach for improving the accuracy and quality of responses for your LLM application. The technique is super simple:&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://twitter.com/mattshumer_/status/1777382373283299365&quot;&gt;https://twitter.com/mattshumer_/status/1777382373283299365&lt;/a&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Send the request to 3 LLMs - Claude, GPT4, and Perplexity.&lt;/li&gt; &lt;li&gt;Give the responses to Claude again and prompt engineer to pick the best and accurate response.&lt;br/&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;I got curious about this and decided to do some evaluations on this approach. Sharing some metrics/measurements in this post.&lt;/p&gt; &lt;p&gt;This one is pretty obvious, the latency on having all 3 LLMs generate a response and picking the best out of the 3 is high. But, I do recognize that this can be improved by parallelizing the operations. &lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/lndc3gwp3ltc1.png?width=1200&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=02ec4109e9a3211724686a40ecbb9110dc70033c&quot;&gt;https://preview.redd.it/lndc3gwp3ltc1.png?width=1200&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=02ec4109e9a3211724686a40ecbb9110dc70033c&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Ran the following tests for both the combined AI Oracle approach and using a single LLM:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;Factual Accuracy&lt;/strong&gt; - Evaluated for correctness of responses.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;Realtime data&lt;/strong&gt; - Evaluated based on asking information related to realtime data.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;Adversarial Testing&lt;/strong&gt; - Evaluated on whether the LLM is able to pickup the signal correctly by placing the question in between a bunch of garbage data. The LLM was given a positive score if it correctly responded to the question without mentioning the garbage data. &lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;Consistency checks&lt;/strong&gt; - Evaluated on whether the LLM gave a response consistently when the same question was asking many times. Mainly looked for structural consistency of the response.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;Quality&lt;/strong&gt; - Evaluated on the quality - sentence structure, adherence to the prompt etc. &lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;AI Oracle Approach&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Results for the AI Oracle approach: For some reason, it could not pick up the realtime information even once. I am sure with some prompt engineering, this metric can be improved. It did poorly on Adversarial testing - mostly because Claude and Pplx&amp;#39;s responses. &lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/9ggjkthz3ltc1.png?width=1200&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=033d105e3793e68fb5b2bd9a134cbbefec423cc7&quot;&gt;https://preview.redd.it/9ggjkthz3ltc1.png?width=1200&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=033d105e3793e68fb5b2bd9a134cbbefec423cc7&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Claude (claude-3-opus-20240229)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;As expected, Claude did not do well on Realtime testing. But, interestingly, it did not do great with adversarial and consistency tests either.&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/b33q8qz24ltc1.png?width=1200&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ef154b780655bb520c406d1aa53c8b91fc2c8038&quot;&gt;https://preview.redd.it/b33q8qz24ltc1.png?width=1200&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ef154b780655bb520c406d1aa53c8b91fc2c8038&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;GPT4&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Again, GPT4 does not have realtime capabilities. But it did extremely well on everything else except consistency checks where the responses were structured quite differently each time.&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/2jz4y6864ltc1.png?width=1200&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a35bfe557f4164251c4900d4e2c62cf8a5c7b04d&quot;&gt;https://preview.redd.it/2jz4y6864ltc1.png?width=1200&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a35bfe557f4164251c4900d4e2c62cf8a5c7b04d&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Perplexity (pplx-70b-online)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;As expected Perplexity&amp;#39;s realtime capabilities are unmatched. But, it did not do that well with adversarial and consistency tests which in turn skewed the metrics for AI Oracle approach as well.Notably, the quality of responses from Perplexity were far better than the rest.&lt;/p&gt; &lt;p&gt;&amp;#x200B;&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/y0mqsr4b4ltc1.png?width=1200&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3ba1acfd7159b8f0bd5164a0e99af7f8ab4f5071&quot;&gt;https://preview.redd.it/y0mqsr4b4ltc1.png?width=1200&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3ba1acfd7159b8f0bd5164a0e99af7f8ab4f5071&lt;/a&gt;&lt;/p&gt; &lt;p&gt;In conclusion, you can get a near perfect score for the AI Oracle approach with a bit of prompt engineering. But you definitely lose performance in the process. Even when parallelized, it is only as slow as the slowest LLM. Token usage/cost is also going to be higher.&lt;/p&gt; &lt;p&gt;Finally, if you are curious, all these evaluations were done using Langtrace - an open source LLM monitoring and evaluations tool that I am currently developing.&lt;/p&gt; &lt;p&gt;Github: &lt;a href=&quot;https://github.com/Scale3-Labs/langtrace&quot;&gt;https://github.com/Scale3-Labs/langtrace&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/cryptokaykay&quot;&gt; /u/cryptokaykay &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c0do04/results_of_evaluating_the_ai_oracle_approach_a/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c0do04/results_of_evaluating_the_ai_oracle_approach_a/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1c0do04</id><media:thumbnail url="https://b.thumbs.redditmedia.com/GCdPwBj4teGkowjNA4iTF6pGHJldADy1b4cFZ_8gtFI.jpg" /><link href="https://www.reddit.com/r/LangChain/comments/1c0do04/results_of_evaluating_the_ai_oracle_approach_a/" /><updated>2024-04-10T05:11:47+00:00</updated><published>2024-04-10T05:11:47+00:00</published><title>Results of evaluating the AI Oracle approach - a novel way to improve your LLM application's accuracy</title></entry><entry><author><name>/u/Dear_Insect_5295</name><uri>https://www.reddit.com/user/Dear_Insect_5295</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I am using a Mistral model 4b and huggingface&lt;/p&gt; &lt;pre&gt;&lt;code&gt;pipeline text_generation_pipeline = pipeline( model=model, tokenizer=tokenizer, task=&amp;quot;text-generation&amp;quot;, batch_size=2 ) llm = HuggingFacePipeline(pipeline=text_generation_pipeline,batch_size=2) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;and then using RAG through langchain&lt;/p&gt; &lt;pre&gt;&lt;code&gt;rag_chain_from_docs = ( RunnablePassthrough.assign(context=(lambda x: format_docs(x[&amp;quot;context&amp;quot;]))) | prompt | llm | StrOutputParser() ) rag_chain_with_source = RunnableParallel( {&amp;quot;context&amp;quot;: retriever, &amp;quot;question&amp;quot;: RunnablePassthrough()} ).assign(answer=rag_chain_from_docs) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;My GPU (T4) is underutilized, its only 8GB/16GB. so I want to use all of My GPU, Is there any way to do this, I tried chain.batch() but it did not work(It still ran sequentially). Any suggestions would be helpful to run the chain concurrently.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Dear_Insect_5295&quot;&gt; /u/Dear_Insect_5295 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c0k4oh/how_to_make_use_of_complete_gpu_memory/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c0k4oh/how_to_make_use_of_complete_gpu_memory/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1c0k4oh</id><link href="https://www.reddit.com/r/LangChain/comments/1c0k4oh/how_to_make_use_of_complete_gpu_memory/" /><updated>2024-04-10T12:16:02+00:00</updated><published>2024-04-10T12:16:02+00:00</published><title>How to make use of Complete GPU memory?</title></entry><entry><author><name>/u/arjavparikh</name><uri>https://www.reddit.com/user/arjavparikh</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I am a non-tech person looking for a tool to ask questions to my 50GB worth of PDF files. Is there a tool which can help me build this project or something which is already there which can help? &lt;/p&gt; &lt;p&gt;Please share relevant blogs or approaches to follow. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/arjavparikh&quot;&gt; /u/arjavparikh &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c0czng/is_there_a_toolplatform_to_put_an_llm_to_a_large/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c0czng/is_there_a_toolplatform_to_put_an_llm_to_a_large/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1c0czng</id><link href="https://www.reddit.com/r/LangChain/comments/1c0czng/is_there_a_toolplatform_to_put_an_llm_to_a_large/" /><updated>2024-04-10T04:32:23+00:00</updated><published>2024-04-10T04:32:23+00:00</published><title>Is there a tool/platform to put an LLM to a large data set of PDF files (like 50GB)</title></entry><entry><author><name>/u/ANil1729</name><uri>https://www.reddit.com/user/ANil1729</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c0lzq7/chatbase_alternative_with_langchain_and_openai/&quot;&gt; &lt;img src=&quot;https://external-preview.redd.it/DNAJ1-L9hvh9FLOVL4A0b8RG1f_jG20rVFCJ33scYpQ.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0a8fbe3c2b21acfadc3e1117c2260e2f322ec969&quot; alt=&quot;Chatbase alternative with Langchain and OpenAI&quot; title=&quot;Chatbase alternative with Langchain and OpenAI&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/ANil1729&quot;&gt; /u/ANil1729 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=ZSfdZVvZ99Q&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c0lzq7/chatbase_alternative_with_langchain_and_openai/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1c0lzq7</id><media:thumbnail url="https://external-preview.redd.it/DNAJ1-L9hvh9FLOVL4A0b8RG1f_jG20rVFCJ33scYpQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=0a8fbe3c2b21acfadc3e1117c2260e2f322ec969" /><link href="https://www.reddit.com/r/LangChain/comments/1c0lzq7/chatbase_alternative_with_langchain_and_openai/" /><updated>2024-04-10T13:46:53+00:00</updated><published>2024-04-10T13:46:53+00:00</published><title>Chatbase alternative with Langchain and OpenAI</title></entry><entry><author><name>/u/Big-Big354</name><uri>https://www.reddit.com/user/Big-Big354</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi all,&lt;/p&gt; &lt;p&gt;I am using RetrievalQA chain with custom prompt. When invoked with a question it is returning prompt with answer embedded in it even when the return_only_outputs is set to True.&lt;/p&gt; &lt;p&gt;I was wondering how can I get only the generated answer without the prompt (System message + Context + Question)?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Big-Big354&quot;&gt; /u/Big-Big354 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c06txb/retrievalqa_chain_returning_generated_answer/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c06txb/retrievalqa_chain_returning_generated_answer/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1c06txb</id><link href="https://www.reddit.com/r/LangChain/comments/1c06txb/retrievalqa_chain_returning_generated_answer/" /><updated>2024-04-09T23:32:50+00:00</updated><published>2024-04-09T23:32:50+00:00</published><title>RetrievalQA chain returning generated answer embedded in prompt even when return_only_outputs=True</title></entry><entry><author><name>/u/VegetableAddendum888</name><uri>https://www.reddit.com/user/VegetableAddendum888</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;So guys thereâ€™s vectaraâ€™s upcoming hackathon,anybody interested to participate and needs a team.DM meâ€¦&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/VegetableAddendum888&quot;&gt; /u/VegetableAddendum888 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c00e4g/need_teammates_for_a_rag_hackathon/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c00e4g/need_teammates_for_a_rag_hackathon/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1c00e4g</id><link href="https://www.reddit.com/r/LangChain/comments/1c00e4g/need_teammates_for_a_rag_hackathon/" /><updated>2024-04-09T19:09:43+00:00</updated><published>2024-04-09T19:09:43+00:00</published><title>Need teammates for a RAG hackathon</title></entry><entry><author><name>/u/isthatashark</name><uri>https://www.reddit.com/user/isthatashark</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/isthatashark&quot;&gt; /u/isthatashark &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://vectorize.io/what-is-a-vector-database/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1c000jh/the_ultimate_guide_to_vector_database_success_in/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1c000jh</id><link href="https://www.reddit.com/r/LangChain/comments/1c000jh/the_ultimate_guide_to_vector_database_success_in/" /><updated>2024-04-09T18:54:40+00:00</updated><published>2024-04-09T18:54:40+00:00</published><title>The Ultimate Guide To Vector Database Success In AI</title></entry><entry><author><name>/u/anderl1980</name><uri>https://www.reddit.com/user/anderl1980</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Iâ€™d be interested in whether anyone here is using LangChainâ€™s SQL Agent (or similar self-built agents with LangChain or autogen). Iâ€™d love to conenct to learn from your experiences as I have not seen it be used in productive systems yet!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/anderl1980&quot;&gt; /u/anderl1980 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bzn1yw/sql_agent_in_production/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bzn1yw/sql_agent_in_production/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bzn1yw</id><link href="https://www.reddit.com/r/LangChain/comments/1bzn1yw/sql_agent_in_production/" /><updated>2024-04-09T08:29:57+00:00</updated><published>2024-04-09T08:29:57+00:00</published><title>SQL Agent in production?</title></entry><entry><author><name>/u/mehul_gupta1997</name><uri>https://www.reddit.com/user/mehul_gupta1997</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/mehul_gupta1997&quot;&gt; /u/mehul_gupta1997 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;/r/ArtificialInteligence/comments/1bzuuov/tested_code_gemma_by_google/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bzuxvz/tested_code_gemma_by_google/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bzuxvz</id><link href="https://www.reddit.com/r/LangChain/comments/1bzuxvz/tested_code_gemma_by_google/" /><updated>2024-04-09T15:26:22+00:00</updated><published>2024-04-09T15:26:22+00:00</published><title>Tested Code Gemma by Google</title></entry><entry><author><name>/u/bwenneker</name><uri>https://www.reddit.com/user/bwenneker</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;m building several chat based apps with LangChain for clients. I&amp;#39;m asking for feedback with each answer, users can leave a ðŸ‘ or ðŸ‘Ž.&lt;/p&gt; &lt;p&gt;Often I get the question: &amp;quot;does this &amp;#39;self-improve&amp;#39;?&amp;quot;&lt;/p&gt; &lt;p&gt;This got me thinking, why not use the positive feedback to improve future answers? Has anyone tried something like this:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Store (positive) user feedback in a VectorDB with questions-answer pairs.&lt;/li&gt; &lt;li&gt;When a new question is asked, run the usual pipeline (RAG for example).&lt;/li&gt; &lt;li&gt;Then also query the feedback VectorDB and add the top-k feedback question-answer pairs with high relevance to the question and add it as extra context.&lt;/li&gt; &lt;li&gt;Let the LLM answer the question using the context and top-k feedback items.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Looking forward to your experience, otherwise I might build this, it doesn&amp;#39;t seem to hard to make.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/bwenneker&quot;&gt; /u/bwenneker &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bzntdm/using_user_feedback_to_optimize_rag/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bzntdm/using_user_feedback_to_optimize_rag/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bzntdm</id><link href="https://www.reddit.com/r/LangChain/comments/1bzntdm/using_user_feedback_to_optimize_rag/" /><updated>2024-04-09T09:25:46+00:00</updated><published>2024-04-09T09:25:46+00:00</published><title>Using user feedback to optimize RAG</title></entry><entry><author><name>/u/mehul_gupta1997</name><uri>https://www.reddit.com/user/mehul_gupta1997</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Checkout how you can leverage Multi-Agent Orchestration for developing an auto Interview system where the Interviewer asks questions to interviewee, evaluates it and eventually shares whether the candidate should be selected or not. Right now, both interviewer and interviewee are played by AI agents. &lt;a href=&quot;https://youtu.be/VrjqR4dIawo?si=1sMYs7lI-c8WZrwP&quot;&gt;https://youtu.be/VrjqR4dIawo?si=1sMYs7lI-c8WZrwP&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/mehul_gupta1997&quot;&gt; /u/mehul_gupta1997 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bzkzkt/multiagent_interview_using_langgraph/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1bzkzkt/multiagent_interview_using_langgraph/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1bzkzkt</id><link href="https://www.reddit.com/r/LangChain/comments/1bzkzkt/multiagent_interview_using_langgraph/" /><updated>2024-04-09T06:07:23+00:00</updated><published>2024-04-09T06:07:23+00:00</published><title>Multi-Agent Interview using LangGraph</title></entry></feed>