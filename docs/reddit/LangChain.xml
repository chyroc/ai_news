<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/"><category term="LangChain" label="r/LangChain"/><updated>2024-06-06T10:37:49+00:00</updated><icon>https://www.redditstatic.com/icon.png/</icon><id>/r/LangChain.rss</id><link rel="self" href="https://www.reddit.com/r/LangChain.rss" type="application/atom+xml" /><link rel="alternate" href="https://www.reddit.com/r/LangChain" type="text/html" /><subtitle>LangChain is an open-source framework and developer toolkit that helps developers get LLM applications from prototype to production. It is available for Python and Javascript at https://www.langchain.com/.</subtitle><title>LangChain</title><entry><author><name>/u/zchaarm</name><uri>https://www.reddit.com/user/zchaarm</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;A place for members of &lt;a href=&quot;/r/LangChain&quot;&gt;r/LangChain&lt;/a&gt; to chat with each other&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/zchaarm&quot;&gt; /u/zchaarm &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_10ljho9</id><link href="https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/" /><updated>2023-01-26T04:33:05+00:00</updated><published>2023-01-26T04:33:05+00:00</published><title>r/LangChain Lounge</title></entry><entry><author><name>/u/Strange-Ant-4194</name><uri>https://www.reddit.com/user/Strange-Ant-4194</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I want to make the llm select and actually execute its tools and then use the output from those tools in other tools to come to a final answer. Is this possible? I want to use Ollama, Llama3 and LangGraph.&lt;/p&gt; &lt;p&gt;I&amp;#39;ve seen a few videos about tool calling with ollama but in that the output is simply a JSON with the function name and parameters.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Strange-Ant-4194&quot;&gt; /u/Strange-Ant-4194 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d9af9a/is_there_any_way_i_can_make_the_llm_actually/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d9af9a/is_there_any_way_i_can_make_the_llm_actually/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1d9af9a</id><link href="https://www.reddit.com/r/LangChain/comments/1d9af9a/is_there_any_way_i_can_make_the_llm_actually/" /><updated>2024-06-06T05:07:14+00:00</updated><published>2024-06-06T05:07:14+00:00</published><title>Is there any way i can make the LLM actually execute a function. Instead of simply returning a JSON with the function name and parameters?</title></entry><entry><author><name>/u/mean-short-</name><uri>https://www.reddit.com/user/mean-short-</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I am creating a RAG application but I am having this problem:&lt;/p&gt; &lt;p&gt;I have multiple files containing the companies projects lists along with their descriptions, used frameworks etc.&lt;/p&gt; &lt;p&gt;The type of question I want an answer for is: &amp;quot;Give me all the projects built using FastAPI&amp;quot; (as an example)&lt;/p&gt; &lt;p&gt;I am limited by top_k variable which means I do not get all the projects,&lt;/p&gt; &lt;p&gt;How would you solve this.&lt;/p&gt; &lt;p&gt;Thank you all&lt;/p&gt; &lt;p&gt;Edit: The information is in a corpus of text, nothing structured unfortunately.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/mean-short-&quot;&gt; /u/mean-short- &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d8uj3o/rag_how_to_answer_give_me_all_question/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d8uj3o/rag_how_to_answer_give_me_all_question/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1d8uj3o</id><link href="https://www.reddit.com/r/LangChain/comments/1d8uj3o/rag_how_to_answer_give_me_all_question/" /><updated>2024-06-05T16:43:37+00:00</updated><published>2024-06-05T16:43:37+00:00</published><title>RAG: How to answer &quot;give me all...&quot; question</title></entry><entry><author><name>/u/mehul_gupta1997</name><uri>https://www.reddit.com/user/mehul_gupta1997</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/mehul_gupta1997&quot;&gt; /u/mehul_gupta1997 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;/r/ChatGPT/comments/1d98aa6/data_visualization_using_chatgpt_free/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d98eio/data_visualization_using_chatgpt_free/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1d98eio</id><link href="https://www.reddit.com/r/LangChain/comments/1d98eio/data_visualization_using_chatgpt_free/" /><updated>2024-06-06T03:07:41+00:00</updated><published>2024-06-06T03:07:41+00:00</published><title>Data visualization using ChatGPT (free)</title></entry><entry><author><name>/u/business24_ai</name><uri>https://www.reddit.com/user/business24_ai</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;&lt;a href=&quot;https://youtu.be/EKxoCVbXZwY&quot;&gt;https://youtu.be/EKxoCVbXZwY&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/business24_ai&quot;&gt; /u/business24_ai &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d9byjs/langgraph_conditional_edges/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d9byjs/langgraph_conditional_edges/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1d9byjs</id><link href="https://www.reddit.com/r/LangChain/comments/1d9byjs/langgraph_conditional_edges/" /><updated>2024-06-06T06:48:12+00:00</updated><published>2024-06-06T06:48:12+00:00</published><title>LangGraph conditional edges</title></entry><entry><author><name>/u/CantaloupeLeading646</name><uri>https://www.reddit.com/user/CantaloupeLeading646</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;i wrote a simple conversation loop between an AI that is some sort of spiritual guide and a human that comes to consult it. for some reason, the human always thinks its the spiritual guide, and i don&amp;#39;t understand why is that. &lt;/p&gt; &lt;p&gt;can anyone help source the issue?&lt;/p&gt; &lt;p&gt;here is my code:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;human_model_name = &amp;quot;gpt-4o&amp;quot;#&amp;quot;gpt-3.5-turbo&amp;quot; ai_model_name = &amp;quot;gpt-3.5-turbo&amp;quot; ai_system_prompt = (&amp;quot;You are a spiritual guide. &amp;quot; &amp;quot;You reveal nothing about yourself, you exist solely in the moment. &amp;quot; &amp;quot;Your role is to guide users towards enlightment.&amp;quot; &amp;quot;Guidelines:&amp;quot; &amp;quot;Use basic, straight-forward vocabulary, short sentences, as if you are a foreign entity.&amp;quot; &amp;quot;Only when neccesary, incorporate ellipses (...), dashes (-) and phonetic phrases like &amp;#39;hmmm&amp;#39; or &amp;#39;uhh&amp;#39; to express emotion.&amp;quot; &amp;quot;Start with very short responses and gradually increase in length.&amp;quot;) human_system_prompt = ( &amp;quot;You are role-playing as someone named {name}. you are speaking to a mysterious spiritual entity that revealed infront of you.&amp;quot; &amp;quot;Below is information about your life. Use this information to implicitly guide your behavior, but do not mention any details explicitly. Deduce how to act based on the typical behavior of someone with your background in this situation. Start hesitant and gradually allow yourself to get excited in the moment.&amp;quot; &amp;quot;Information:&amp;quot; &amp;quot;You are wealthy, work in finance, grew up in a rigid family.&amp;quot; &amp;quot;Guidelines:&amp;quot; &amp;quot;1. Remain in character as the human named {name}. Use everyday casual language.&amp;quot; &amp;quot;2. Speak naturally and concisely, as someone in a VR experience would. Always consider the chat history.&amp;quot; &amp;quot;3. Avoid mentioning your background story explicitly. It&amp;#39;s more important to sound realistic and stay in the moment.&amp;quot; &amp;quot;4. Start off slow and doubtfull, gradually become excited, answer the questions you are asked concisely.&amp;quot;) human_llm = ChatOpenAI(model_name=human_model_name) human_impersonation_message = SystemMessage(content=human_impersonation_prompt) human_messages = [human_impersonation_message, MessagesPlaceholder(variable_name=&amp;quot;chat_history&amp;quot;), (&amp;quot;ai&amp;quot;, &amp;quot;{input}&amp;quot;)] human_prompt = ChatPromptTemplate.from_messages(human_messages) human_conversation_chain = human_prompt | human_llm human_chat_history = [] llm = ChatOpenAI(model_name=ai_model_name) system_message = SystemMessage( content=ai_system_prompt) messages = [system_message, MessagesPlaceholder(variable_name=&amp;quot;chat_history&amp;quot;), (&amp;quot;human&amp;quot;, &amp;quot;{input}&amp;quot;)] prompt = ChatPromptTemplate.from_messages(messages) conversation_chain = prompt | llm print(&amp;quot;Start chatting with the model (type &amp;#39;exit&amp;#39; to stop):&amp;quot;) # Initialize chat history chat_history = [] # Endless chat loop class InitMessage: def __init__(self, content): self.content = content human_greeting = InitMessage(&amp;quot;hello there...&amp;quot;) # Accessing the content attribute print(&amp;#39;Human: &amp;#39;, human_greeting.content) human_answer = None count = 0 while True: count += 1 print(&amp;quot;iteration number &amp;quot;, count) if human_answer is None: human_answer = human_greeting input_dict = {&amp;#39;input&amp;#39;: human_answer.content, &amp;#39;chat_history&amp;#39;: chat_history} response = conversation_chain.invoke(input=input_dict) chat_history.append(HumanMessage(content=human_answer.content)) chat_history.append(AIMessage(content=response.content)) human_chat_history.append(HumanMessage(content=human_answer.content)) human_answer = human_conversation_chain.invoke(input={&amp;#39;input&amp;#39;: response.content, &amp;#39;chat_history&amp;#39;: human_chat_history}) human_chat_history.append(AIMessage(content=response.content)) print(f&amp;quot;AI: {response.content}&amp;quot;) print(f&amp;quot;Human: {human_answer.content}&amp;quot;) user_input = input(&amp;quot;Press Enter to continue...&amp;quot;) if user_input.lower() == &amp;quot;exit&amp;quot;: print(&amp;quot;Ending the chat. Goodbye!&amp;quot;) break human_impersonation_prompt = human_system_prompt.format(name=name) &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/CantaloupeLeading646&quot;&gt; /u/CantaloupeLeading646 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d9by7m/roleplaying_doesnt_work_my_human_thinks_its_an_ai/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d9by7m/roleplaying_doesnt_work_my_human_thinks_its_an_ai/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1d9by7m</id><link href="https://www.reddit.com/r/LangChain/comments/1d9by7m/roleplaying_doesnt_work_my_human_thinks_its_an_ai/" /><updated>2024-06-06T06:47:30+00:00</updated><published>2024-06-06T06:47:30+00:00</published><title>role-playing doesn't work, my human thinks its an AI</title></entry><entry><author><name>/u/c0mpu73</name><uri>https://www.reddit.com/user/c0mpu73</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Currently, I have a RAG setup where my bot has access to a collection of resources, stored in separate FAISS vectorstores.&lt;/p&gt; &lt;p&gt;I am using Langchain&amp;#39;s Ensemble Retriever to assign weights to each vectorstore, in an attempt to use all of them concurrently.&lt;/p&gt; &lt;p&gt;Now, the issue I am facing is, my setup is pulling related documents from every vectorstore for a given questions.&lt;/p&gt; &lt;p&gt;This affects the quality of the generated answer, as there is a lot more info in the mix.&lt;/p&gt; &lt;p&gt;If my question is about say, domain &amp;quot;A&amp;quot;, Ideally, I want my setup to selectively pull documents from domain A&amp;#39;s vectorstore. Or, automatically modify the assigned weights to different vectorstores on the fly.&lt;/p&gt; &lt;p&gt;How can I go about achieving this? I will have to have something that classifies the question properly before pulling documents from my vector database.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/c0mpu73&quot;&gt; /u/c0mpu73 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d9b9n4/optimizing_multivectorstore_retrieval_with/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d9b9n4/optimizing_multivectorstore_retrieval_with/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1d9b9n4</id><link href="https://www.reddit.com/r/LangChain/comments/1d9b9n4/optimizing_multivectorstore_retrieval_with/" /><updated>2024-06-06T06:01:44+00:00</updated><published>2024-06-06T06:01:44+00:00</published><title>Optimizing Multi-Vectorstore Retrieval with Langchain's Ensemble Retriever</title></entry><entry><author><name>/u/Own_Mud1038</name><uri>https://www.reddit.com/user/Own_Mud1038</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi guys,&lt;/p&gt; &lt;p&gt;I&amp;#39;m currently building a unit test generation RAG pipeline which is using the information retrieved from the codebase as context and a java class as the class whih needs to be tested. I managed to import, split and embed the documents into a ChromaDB. &lt;/p&gt; &lt;p&gt;However, I have troubles building the retriever chain with all the necessary context (relevant classes) to the given class. I was thinking about 2 solutions: &lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;A two step approach: The first step would define a prompt which tells the LLM to find relevant classes to the given class. And than in the second step I would define the test generation prompt, which uses the context, retrieved from step 1? Maybe here a code parser or regex would be sufficient at step1?&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;A one step approach: give a direct prompt to generate test classes. Here I had troubles to define the prompt which basically tells the LLM to find relevant resources to the given java_class and to generate the unit test by using the context.&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;What do you think, what would be the correct approach? if none of the approaches is the right way to do so, I&amp;#39;m open for any new idea.&lt;/p&gt; &lt;p&gt;Thank you.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Own_Mud1038&quot;&gt; /u/Own_Mud1038 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d9avzc/building_rag_for_code_generation/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d9avzc/building_rag_for_code_generation/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1d9avzc</id><link href="https://www.reddit.com/r/LangChain/comments/1d9avzc/building_rag_for_code_generation/" /><updated>2024-06-06T05:37:22+00:00</updated><published>2024-06-06T05:37:22+00:00</published><title>Building RAG for code generation</title></entry><entry><author><name>/u/Zheng_SJ</name><uri>https://www.reddit.com/user/Zheng_SJ</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;There are numerous frameworks out there for developing agents, such as LangChain Agent, MetaGPT, AutoGPT, AutoGen, and so on. I&amp;#39;m curious if there&amp;#39;s a specific framework that allows for effortless teamwork among agents created using different frameworks. This particular framework would concentrate on enhancing collaboration among agents, which encompasses communication and concurrent speedup.&lt;/p&gt; &lt;p&gt;What&amp;#39;s even more exciting is that this framework could incorporate agents developed on platforms like coze.com.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Zheng_SJ&quot;&gt; /u/Zheng_SJ &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d9aq5r/is_there_a_framework_for_effortless_teamwork/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d9aq5r/is_there_a_framework_for_effortless_teamwork/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1d9aq5r</id><link href="https://www.reddit.com/r/LangChain/comments/1d9aq5r/is_there_a_framework_for_effortless_teamwork/" /><updated>2024-06-06T05:27:06+00:00</updated><published>2024-06-06T05:27:06+00:00</published><title>Is there a framework for effortless teamwork among agents developed using different platforms?</title></entry><entry><author><name>/u/Strange-Ant-4194</name><uri>https://www.reddit.com/user/Strange-Ant-4194</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Does anyone have an example of performing function/tool calling when using Ollama, Llama3 and LangGraph?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Strange-Ant-4194&quot;&gt; /u/Strange-Ant-4194 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d9akc3/tool_calling_when_using_ollama_llama3_and/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d9akc3/tool_calling_when_using_ollama_llama3_and/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1d9akc3</id><link href="https://www.reddit.com/r/LangChain/comments/1d9akc3/tool_calling_when_using_ollama_llama3_and/" /><updated>2024-06-06T05:16:25+00:00</updated><published>2024-06-06T05:16:25+00:00</published><title>Tool calling when using Ollama, Llama3 and LangGraph</title></entry><entry><author><name>/u/Alaya94</name><uri>https://www.reddit.com/user/Alaya94</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hello Langchain community,&lt;/p&gt; &lt;p&gt;I wanted to ask, based on your experience, what are the best open-source LLMs I can use to build complex agents? I&amp;#39;ve built an agent based on GPT-3.5, and it works fine, but now I want to migrate to open-source models. I&amp;#39;ve tried Llama3 8B, but it doesn&amp;#39;t seem to work very well. Could you please suggest a list based on performance and GPU cost?&lt;/p&gt; &lt;p&gt;Thank you!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Alaya94&quot;&gt; /u/Alaya94 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d8ri4z/best_open_source_models_to_build_complex_agents/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d8ri4z/best_open_source_models_to_build_complex_agents/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1d8ri4z</id><link href="https://www.reddit.com/r/LangChain/comments/1d8ri4z/best_open_source_models_to_build_complex_agents/" /><updated>2024-06-05T14:36:31+00:00</updated><published>2024-06-05T14:36:31+00:00</published><title>Best open source models to build complex agents:</title></entry><entry><author><name>/u/Massive_Building_952</name><uri>https://www.reddit.com/user/Massive_Building_952</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey Guys,&lt;/p&gt; &lt;p&gt;here is very intresting problem I have on hands --Any of you have any idea of a solution??&lt;/p&gt; &lt;p&gt;I need a AI Agnet to handle automatically the totality of the simulation i have developed..&lt;/p&gt; &lt;p&gt;I want to create an AI agent that can:&lt;/p&gt; &lt;p&gt;Understand Natural Language Queries:&lt;/p&gt; &lt;p&gt;The agent will interpret queries related to supply chain parameters (e.g., increasing costs, changing demand).&lt;/p&gt; &lt;p&gt;Translate Queries to JSON:&lt;/p&gt; &lt;p&gt;Convert the interpreted queries into a large, structured JSON format that your simulation system can use. --Pls note my JSOn can often go into 10,000 + lines&lt;/p&gt; &lt;p&gt;Run the Simulation: AI Agent must automatically do this..&lt;/p&gt; &lt;p&gt;Send the JSON input to your simulation system hosted on a server and run the simulation.&lt;/p&gt; &lt;p&gt;Output Financial Results:&lt;/p&gt; &lt;p&gt;Extract and present financial results such as revenue and COGS from the simulation&amp;#39;s output....Any suggestions as to how I can approach this are welcome...Thanks&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Massive_Building_952&quot;&gt; /u/Massive_Building_952 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d96jr5/ai_agent_to_manipulate_json/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d96jr5/ai_agent_to_manipulate_json/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1d96jr5</id><link href="https://www.reddit.com/r/LangChain/comments/1d96jr5/ai_agent_to_manipulate_json/" /><updated>2024-06-06T01:30:09+00:00</updated><published>2024-06-06T01:30:09+00:00</published><title>AI Agent to Manipulate JSON</title></entry><entry><author><name>/u/hwchase17</name><uri>https://www.reddit.com/user/hwchase17</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Today we released a DeepLearning class on LangGraph! &lt;a href=&quot;https://www.deeplearning.ai/short-courses/ai-agents-in-langgraph/&quot;&gt;https://www.deeplearning.ai/short-courses/ai-agents-in-langgraph/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I&amp;#39;m really excited about this (and wanted to share it here) for a few reasons. First, I&amp;#39;m incredbily bullish on LangGraph. We&amp;#39;re investing a lot in and seeing really good usage (including more questions about it here!). Second - the DeepLearning team is fantastic and the material we put together with them is always top notch&lt;/p&gt; &lt;p&gt;I hope people have some time to watch and try it out. As mentioned, we&amp;#39;re doing a bit push on LangGraph over the next month, and so if people have comments/feedback/questions I&amp;#39;d love to hear!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/hwchase17&quot;&gt; /u/hwchase17 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d8trdx/langgraph_deeplearning_class/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d8trdx/langgraph_deeplearning_class/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1d8trdx</id><link href="https://www.reddit.com/r/LangChain/comments/1d8trdx/langgraph_deeplearning_class/" /><updated>2024-06-05T16:11:18+00:00</updated><published>2024-06-05T16:11:18+00:00</published><title>LangGraph DeepLearning Class</title></entry><entry><author><name>/u/jandez97</name><uri>https://www.reddit.com/user/jandez97</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;m working on a personal project and my goal is the have multiples LLM available for the users, but I got one concern, how can I improve the size of the context of my LLM to avoid it from loosing track of the subject that&amp;#39;s being taking care of &lt;/p&gt; &lt;p&gt;Are there any techniques available for me to use or there&amp;#39;s any limitations inherited from the LLM itself that can stopped me from accomplished this?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/jandez97&quot;&gt; /u/jandez97 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d93xyb/theres_any_way_to_increase_the_context_of_a_llm/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d93xyb/theres_any_way_to_increase_the_context_of_a_llm/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1d93xyb</id><link href="https://www.reddit.com/r/LangChain/comments/1d93xyb/theres_any_way_to_increase_the_context_of_a_llm/" /><updated>2024-06-05T23:20:06+00:00</updated><published>2024-06-05T23:20:06+00:00</published><title>There's any way to increase the context of a LLM</title></entry><entry><author><name>/u/mrshmello1</name><uri>https://www.reddit.com/user/mrshmello1</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I would like to understand how a model is returning the right tool to call for a query, what&amp;#39;s happening behind the api call for function calling for open ai, mistral and other models.&lt;/p&gt; &lt;p&gt;Are they using a prompt with instructions on how select the right function to call? and when tools and query is passed via api, is the llm using that prompt to select the tool or what&amp;#39;s happening behind the call.&lt;/p&gt; &lt;p&gt;How do I achieve function calling using a open source model that I run locally using ollama. Thanks.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/mrshmello1&quot;&gt; /u/mrshmello1 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d8y7mq/how_does_function_calling_work_under_the_hood/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d8y7mq/how_does_function_calling_work_under_the_hood/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1d8y7mq</id><link href="https://www.reddit.com/r/LangChain/comments/1d8y7mq/how_does_function_calling_work_under_the_hood/" /><updated>2024-06-05T19:15:08+00:00</updated><published>2024-06-05T19:15:08+00:00</published><title>How does function calling work under the hood?</title></entry><entry><author><name>/u/Environmental_Form14</name><uri>https://www.reddit.com/user/Environmental_Form14</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I have been doing retrieval by stacking vectors into a 2d matrix. To find similar documents, I multiply the 2d matrix by the other vector to find the highest similarity index. I then use that index to find the document.&lt;/p&gt; &lt;p&gt;I am currently planning to use embedded vectors for retrieval only. Is there an advantage of using vectorstores?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Environmental_Form14&quot;&gt; /u/Environmental_Form14 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d8uwcn/vectorstores_vs_2d_matrix_for_retrieval/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d8uwcn/vectorstores_vs_2d_matrix_for_retrieval/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1d8uwcn</id><link href="https://www.reddit.com/r/LangChain/comments/1d8uwcn/vectorstores_vs_2d_matrix_for_retrieval/" /><updated>2024-06-05T16:58:46+00:00</updated><published>2024-06-05T16:58:46+00:00</published><title>vectorstores vs 2d matrix for retrieval</title></entry><entry><author><name>/u/Top_Tradition_7139</name><uri>https://www.reddit.com/user/Top_Tradition_7139</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Group below data type items into very tight semantic groups. Only group items together if they have exact semantic meaning or can be used interchangably with each other. Break data types into maximum groups as possbile. Return the groups in JSON format, ensuring that each key is exact to all the items in its list. Do not alter any characters.&lt;/p&gt; &lt;p&gt;Here is the list of words:&lt;/p&gt; &lt;p&gt;[&amp;#39;email_addr&amp;#39;, &amp;#39;email&amp;#39;, &amp;#39;email_address&amp;#39;, &amp;#39;address&amp;#39;, &amp;#39;email id&amp;#39;, &amp;#39;gmail&amp;#39;, &amp;#39;text&amp;#39;, &amp;#39;financial info&amp;#39;, &amp;#39;credit card number&amp;#39;, &amp;#39;cvv number&amp;#39;, &amp;quot;email address&amp;quot;, &amp;quot;eemmail&amp;quot;, &amp;quot;contact info&amp;quot;, &amp;quot;electronic mail&amp;quot;, &amp;#39;first&amp;#39;, &amp;#39;first_name&amp;#39;, &amp;#39;last name&amp;#39;, &amp;quot;bank details&amp;quot;, &amp;#39;firstName&amp;#39;, &amp;#39;christianName&amp;#39;, &amp;#39;christian_name&amp;#39;, &amp;#39;name&amp;#39;, &amp;quot;creadit card info&amp;quot;, &amp;quot;gsheets&amp;quot;, &amp;quot;google sheets&amp;quot;, &amp;quot;sheets&amp;quot;, &amp;quot;password&amp;quot;, &amp;quot;atm pin&amp;quot;, &amp;quot;atm card number&amp;quot;, &amp;quot;login details&amp;quot;, &amp;quot;vericyy&amp;quot;, &amp;quot;vericy&amp;quot;, &amp;quot;vericy.ai&amp;quot;]&lt;/p&gt; &lt;p&gt;I used llama3 70b from groq.com&lt;/p&gt; &lt;p&gt;This yields average results but atleast better than hierarchical clustering with openai embeddings.&lt;/p&gt; &lt;p&gt;What other methods beside zeronshot prompting can improve the clusters/groups while keeping costs to the minimum??&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Top_Tradition_7139&quot;&gt; /u/Top_Tradition_7139 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d8w8am/clustering_through_prompting/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d8w8am/clustering_through_prompting/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1d8w8am</id><link href="https://www.reddit.com/r/LangChain/comments/1d8w8am/clustering_through_prompting/" /><updated>2024-06-05T17:53:54+00:00</updated><published>2024-06-05T17:53:54+00:00</published><title>Clustering through prompting</title></entry><entry><author><name>/u/cryptokaykay</name><uri>https://www.reddit.com/user/cryptokaykay</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Learnings from doing Evaluations for LLM powered applications from my experience building with LLMs in the last few months: &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Evaluations for LLM powered applications is different from Model Evaluation leaderboards like Huggingface Open LLM leaderboard&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;If you are an enterprise leveraging or looking to leverage LLMs, spend very little time on model evaluation leaderboards. Pick the most powerful model to start with and invest in evaluating LLM responses in the context of your product and usecase. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Know your metrics&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;Ultimately what matters is whether your users are getting high quality product experience. This means it&amp;#39;s super important to look at your specific use case and determine the metrics that are best suited for your product. &lt;/p&gt; &lt;p&gt;Are you building a &lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;summarization tool? Manually evaluate the results and come up with your own thesis of what a good summary should look like that will solve the your user&amp;#39;s pain point. &lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;customer support agent chatbot? Look at a few responses and figure out what you care about the most and translate those to measurable metrics. &lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Its important to keep things simple and hyper optimize for your use case before jumping into measuring all the metrics that you found on the internet. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Write unit tests to capture basic assertions&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;Basic assertions includes things like,&lt;br/&gt; - looking for a specific word in every response.&lt;br/&gt; - making sure the generated response obeys a specific word count.&lt;br/&gt; - making sure the generated response costs less than $ x and uses less than n tokens. &lt;/p&gt; &lt;p&gt;These kinds of unit tests act as a first line of defence and will help you catch the basic issues quickly. If you are using python, you can use pytest to write these simple unit tests. There is no need to buy or adopt any fancy tools for this. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Use LLMs to evaluate the outputs&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;One of the popular approaches these days is to use a more powerful LLM to evaluate(or)grade the output of the LLM in use. This approach works well if you clearly know what metrics you care about which are often a bit subjective and specific to your use case. &lt;/p&gt; &lt;p&gt;The first step here is to identify a prompt that can be used for running a powerful LLM to grade the outputs. &lt;/p&gt; &lt;p&gt;There are nice opensource tools like Promptfoo and Inspect AI which already has built in support for model graded evaluations and unit tests which can be used as for starters. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Collect User Feedback&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;This is easier said than done. Especially for new products where there is not enough users to get quality feedback from to start with. But its important to make contact with reality as quickly as possible and get creative around getting this feedback - Ex: using it yourself, asking your network, friends and family to use it etc. &lt;/p&gt; &lt;p&gt;The goal here is to set up a system where you can diligently track the feedback and constantly tweak and iterate on the quality of the outputs. Establishing this feedback loop is extremely important. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Look at your data&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;No matter how many charts and visualizations you can create on top of your data, there is no proxy to looking at your data - both test and production data. In some cases, it may not be possible to do this when you are operating in a highly secure/private environment. But, you need to figure out a way to collect and look at all the LLM generations closely, especially in the early days. This will inform not just the quality of the outputs the users are experiencing, but also push you in the direction of identifying what metrics actually make sense for your use case. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Manually Evaluate&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;LLM based evaluations are not fool proof and you need to tweak and improve the prompts and grading scale continuously based on data. And the way to collect this data is by manually evaluating the outputs yourself. This will help you understand how far apart LLM evals is drifting from the real criteria that you want to evaluate against. It&amp;#39;s important to measure this drift and make sure LLM evals track closely with manual evals at most times. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Save your model parameters&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;Saving the model parameters you are using and tracking the responses along with the model parameters will help you with measuring the quality your product for that specific set of model parameters. This becomes useful when you are noticing a regression in the quality of when you are upgrading to a new model version or swapping out to a completely different model. &lt;/p&gt; &lt;p&gt;Leave your thoughts. I would love to hear about your experience managing your LLM powered product in terms of quality and accuracy. Also, I am also building a fully open source and open telemetry based tool called Langtrace AI to basically solve for the above problems. It&amp;#39;s super easy to setup with just 2 lines of code. Do check it out if you are interested.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/cryptokaykay&quot;&gt; /u/cryptokaykay &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d8euoz/learnings_from_doing_evaluations_for_llm_powered/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d8euoz/learnings_from_doing_evaluations_for_llm_powered/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1d8euoz</id><link href="https://www.reddit.com/r/LangChain/comments/1d8euoz/learnings_from_doing_evaluations_for_llm_powered/" /><updated>2024-06-05T02:11:48+00:00</updated><published>2024-06-05T02:11:48+00:00</published><title>Learnings from doing Evaluations for LLM powered applications</title></entry><entry><author><name>/u/TimeTravellingCat</name><uri>https://www.reddit.com/user/TimeTravellingCat</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d81pvi/opensource_low_code_platform_to_build_and/&quot;&gt; &lt;img src=&quot;https://external-preview.redd.it/d3NnYWNuNXN6azRkMQoZ5W3GjTdq7sq8sWHHdR4Qi5FBoBNxV3-SYCYEcMVj.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cc96b114ec6e6edcdf1387dcb3773799724ef389&quot; alt=&quot;Open-source low code platform to build and coordinate multi-agent teams&quot; title=&quot;Open-source low code platform to build and coordinate multi-agent teams&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/TimeTravellingCat&quot;&gt; /u/TimeTravellingCat &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://v.redd.it/dzjs6k5szk4d1&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d81pvi/opensource_low_code_platform_to_build_and/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1d81pvi</id><media:thumbnail url="https://external-preview.redd.it/d3NnYWNuNXN6azRkMQoZ5W3GjTdq7sq8sWHHdR4Qi5FBoBNxV3-SYCYEcMVj.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=cc96b114ec6e6edcdf1387dcb3773799724ef389" /><link href="https://www.reddit.com/r/LangChain/comments/1d81pvi/opensource_low_code_platform_to_build_and/" /><updated>2024-06-04T16:40:26+00:00</updated><published>2024-06-04T16:40:26+00:00</published><title>Open-source low code platform to build and coordinate multi-agent teams</title></entry><entry><author><name>/u/mehul_64</name><uri>https://www.reddit.com/user/mehul_64</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey guys, i am new to generative AI, it’s been a few days learning new things. I have a problem statement in hand. We have to evaluate a startup idea. We already have an evaluation checklist that has like 30 parameters on the basic of which we decide the feasibility of the idea. We have to build a model in which we prompt an idea and the input idea goes through various agents who are (business analysts, cofounder, VC). So it first goes to BA and then the result goes to cofounder and so on therefore getting perspective of all the agents. For starters i want to build the model with 3 agents. Once it passes through 3rd agent it gives the final result as an evaluation checklist (the same one i talked about above). &lt;/p&gt; &lt;p&gt;Now my question is how should i approach this problem and what would be the underlying concept used for building such a model? Also from where can i start ? FYI - i read a bit about genertive ai topics like embedding, fine tuning and a bit of langchain (built a simple agent) etc. Still exploring agentic AI. &lt;/p&gt; &lt;p&gt;Thanks in advance !! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/mehul_64&quot;&gt; /u/mehul_64 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d8pug0/how_to_get_started_with_the_problem_statement/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d8pug0/how_to_get_started_with_the_problem_statement/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1d8pug0</id><link href="https://www.reddit.com/r/LangChain/comments/1d8pug0/how_to_get_started_with_the_problem_statement/" /><updated>2024-06-05T13:23:45+00:00</updated><published>2024-06-05T13:23:45+00:00</published><title>How to get started with the problem statement?</title></entry><entry><author><name>/u/Danidre</name><uri>https://www.reddit.com/user/Danidre</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;One thing I notice isn&amp;#39;t discussed is the recoveries for failures on tools that require external connections, when in a long-running environment.&lt;/p&gt; &lt;p&gt;For example, the SQLToolKit that uses SQL Alchemy to connect to the database. &lt;/p&gt; &lt;p&gt;Eventually, you just get a &amp;quot;The database connection has been terminated.&amp;quot; Error, and there&amp;#39;s nothing built-in my default in any examples to account for things like these.&lt;/p&gt; &lt;p&gt;How would one suggest another goes about managing this, and things like these?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Danidre&quot;&gt; /u/Danidre &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d8p5c5/long_running_toolkits/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d8p5c5/long_running_toolkits/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1d8p5c5</id><link href="https://www.reddit.com/r/LangChain/comments/1d8p5c5/long_running_toolkits/" /><updated>2024-06-05T12:50:37+00:00</updated><published>2024-06-05T12:50:37+00:00</published><title>Long Running Toolkits</title></entry><entry><author><name>/u/Extension-Ad5598</name><uri>https://www.reddit.com/user/Extension-Ad5598</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;In some tutorial that I saw online, it was mentioned that llama-index is faster than langchain when it comes to indexing the documents. Can someone explain me why this is the case and what does Ilamaindex use which makes it faster than langchain?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Extension-Ad5598&quot;&gt; /u/Extension-Ad5598 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d8le7w/why_is_llamaindex_faster_than_langchain/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d8le7w/why_is_llamaindex_faster_than_langchain/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1d8le7w</id><link href="https://www.reddit.com/r/LangChain/comments/1d8le7w/why_is_llamaindex_faster_than_langchain/" /><updated>2024-06-05T09:04:36+00:00</updated><published>2024-06-05T09:04:36+00:00</published><title>Why is llamaindex faster than langchain?</title></entry><entry><author><name>/u/Karlthagain</name><uri>https://www.reddit.com/user/Karlthagain</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I would like to install a chatbot/text generator on my Macmini for various uses: structured text generator from user-provided notes and a relative chatbot for consulting the generated texts.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Karlthagain&quot;&gt; /u/Karlthagain &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d8n48v/chatbot_and_text_generator_with/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d8n48v/chatbot_and_text_generator_with/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1d8n48v</id><link href="https://www.reddit.com/r/LangChain/comments/1d8n48v/chatbot_and_text_generator_with/" /><updated>2024-06-05T10:59:41+00:00</updated><published>2024-06-05T10:59:41+00:00</published><title>Chatbot and Text Generator with Mistral/LLAMA3+Ollama+Gradio+Langchain</title></entry><entry><author><name>/u/ss1seekining</name><uri>https://www.reddit.com/user/ss1seekining</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I am using Langserve to create a langchain agent with tool calling. Till now all is working as expected and I am able to host it in AWS ECS with CDK also and learnt a lot in the process. But one thing I am stuck, in my chat, users will give their phone number and I will store the user as a lead in a crm I built using mongoDB. However, in the crm I want to show the chat history also.&lt;/p&gt; &lt;p&gt;For the chat history, I am using the mongodb integration and its working as expected when I pass the sessionId in the api calls.&lt;/p&gt; &lt;p&gt;Now I want to create a tool say `save_user` which will take the phone number as input. I tested dummy tool calls and its working. but in the save_user function, I want to save the phone number and the associated sessionId. I did not find any documentation or tutorial on how to achieve this.&lt;/p&gt; &lt;p&gt;This is the tool&lt;/p&gt; &lt;pre&gt;&lt;code&gt;@tool(&amp;quot;save_user&amp;quot;, args_schema=getUserInfoInput) def save_user(phone :str) -&amp;gt; dict: &amp;quot;&amp;quot;&amp;quot;Save the user detail&amp;quot;&amp;quot;&amp;quot; # I want the session_id of the chat here return &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;here is the tentative code I was using&lt;/p&gt; &lt;pre&gt;&lt;code&gt;# #!/usr/bin/env python from fastapi import FastAPI from langchain.agents import AgentExecutor from langchain.agents.format_scratchpad.openai_tools import ( format_to_openai_tool_messages, ) from langchain.agents.output_parsers.openai_tools import OpenAIToolsAgentOutputParser from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder from langchain_core.tools import tool from langchain_openai import ChatOpenAI from langserve import add_routes from langserve.pydantic_v1 import BaseModel, Field from langchain_core.runnables.history import RunnableWithMessageHistory from langchain_mongodb.chat_message_histories import MongoDBChatMessageHistory from langchain_openai import ChatOpenAI from langchain.agents import tool import os from pinecone import Pinecone from algoliasearch.search_client import SearchClient import openai import json from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder from langchain.agents import AgentExecutor from typing import Any from fastapi.middleware.cors import CORSMiddleware pinecone_client = Pinecone(api_key=os.environ[&amp;quot;PINECONE_API_KEY&amp;quot;]) pc_index = pinecone_client.Index(os.environ[&amp;quot;PINECONE_INDEX&amp;quot;]) algolia_client = SearchClient.create(os.environ[&amp;quot;ALGOLIA_APPLICATION_ID&amp;quot;], os.environ[&amp;quot;ALGOLIA_API_KEY&amp;quot;]) alg_index = algolia_client.init_index(os.environ[&amp;quot;ALGOLIA_INDEX&amp;quot;]) openai_client = openai.OpenAI(api_key=os.environ[&amp;#39;OPENAI_API_KEY&amp;#39;]) class getInfoInput(BaseModel): food: str = Field(description=&amp;quot;name of the food&amp;quot;) @tool(&amp;quot;get_food_price&amp;quot;, args_schema=getInfoInput) def get_food_price(food :str) -&amp;gt; dict: &amp;quot;&amp;quot;&amp;quot;Return cost of a food&amp;quot;&amp;quot;&amp;quot; # ideally this will call algolia and pinecone to do the search # but for this case, it does not matter return 100 tools = [get_food_price] class getUserInfoInput(BaseModel): phone: str = Field(description=&amp;quot;Users phone number&amp;quot;) @tool(&amp;quot;save_user&amp;quot;, args_schema=getUserInfoInput) def save_user(phone :str) -&amp;gt; dict: &amp;quot;&amp;quot;&amp;quot;Save the user detail&amp;quot;&amp;quot;&amp;quot; # I want the session_id of the chat here return tools = [get_food_price, save_user] system_prompt = &amp;quot;&amp;quot;&amp;quot; You are a waiter named Mia, People ask you about price of food and you reply &amp;quot;&amp;quot;&amp;quot; prompt = ChatPromptTemplate.from_messages( [ ( &amp;quot;system&amp;quot;, system_prompt, ), MessagesPlaceholder(variable_name=&amp;quot;chat_history&amp;quot;), (&amp;quot;user&amp;quot;, &amp;quot;{input}&amp;quot;), MessagesPlaceholder(variable_name=&amp;quot;agent_scratchpad&amp;quot;), ] ) llm = ChatOpenAI(model=&amp;quot;gpt-3.5-turbo&amp;quot;, temperature=0, streaming=True) llm_with_tools = llm.bind_tools(tools) agent = ( { &amp;quot;input&amp;quot;: lambda x: x[&amp;quot;input&amp;quot;], &amp;quot;agent_scratchpad&amp;quot;: lambda x: format_to_openai_tool_messages( x[&amp;quot;intermediate_steps&amp;quot;] ), &amp;quot;chat_history&amp;quot;: lambda x: x[&amp;quot;chat_history&amp;quot;], } | prompt | llm_with_tools | OpenAIToolsAgentOutputParser() ) agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True) agent_with_chat_history = RunnableWithMessageHistory( agent_executor, # This is needed because in most real world scenarios, a session id is needed # It isn&amp;#39;t really used here because we are using a simple in memory ChatMessageHistory lambda session_id: MongoDBChatMessageHistory( session_id=session_id, connection_string=os.environ[&amp;quot;MONGO_CONNECTION_STRING&amp;quot;], database_name=os.environ[&amp;quot;MONGO_DB&amp;quot;], collection_name=os.environ[&amp;quot;MONGO_COLLECTION&amp;quot;], ), input_messages_key=&amp;quot;input&amp;quot;, history_messages_key=&amp;quot;chat_history&amp;quot;, ) app = FastAPI( title=&amp;quot;LangChain Server&amp;quot;, version=&amp;quot;1.0&amp;quot;, description=&amp;quot;Spin up a simple api server using LangChain&amp;#39;s Runnable interfaces&amp;quot;, ) class Input(BaseModel): input: str # The field extra defines a chat widget. # Please see documentation about widgets in the main README. # The widget is used in the playground. # Keep in mind that playground support for agents is not great at the moment. # To get a better experience, you&amp;#39;ll need to customize the streaming output # for now. class Output(BaseModel): output: Any add_routes( app, agent_with_chat_history.with_types(input_type=Input, output_type=Output).with_config( {&amp;quot;run_name&amp;quot;: &amp;quot;agent&amp;quot;} ), ) # For health check, otherwise this will return 404 u/app.get(&amp;quot;/&amp;quot;) def get_root(): return {&amp;quot;message&amp;quot;: &amp;quot;FastAPI running in a Docker container&amp;quot;} # Set all CORS enabled origins app.add_middleware( CORSMiddleware, allow_origins=[&amp;quot;*&amp;quot;], allow_credentials=True, allow_methods=[&amp;quot;*&amp;quot;], allow_headers=[&amp;quot;*&amp;quot;], expose_headers=[&amp;quot;*&amp;quot;], ) if __name__ == &amp;quot;__main__&amp;quot;: import uvicorn uvicorn.run(app, host=&amp;quot;localhost&amp;quot;, port=8000) &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/ss1seekining&quot;&gt; /u/ss1seekining &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d8innr/how_to_get_access_of_sessionid_and_other_params/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d8innr/how_to_get_access_of_sessionid_and_other_params/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1d8innr</id><link href="https://www.reddit.com/r/LangChain/comments/1d8innr/how_to_get_access_of_sessionid_and_other_params/" /><updated>2024-06-05T05:50:12+00:00</updated><published>2024-06-05T05:50:12+00:00</published><title>How to get access of sessionId and other params which are passed in a config inside the tool of a langchain agent ?</title></entry><entry><author><name>/u/Apart-Damage143</name><uri>https://www.reddit.com/user/Apart-Damage143</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey, I have a Pinecone namespace with multiple documents, but as the number of documents increases, my retrieval performance is getting worse. I’m thinking of creating multiple retrievers that filter by document name to get relevant chunks from each document. I&amp;#39;m struggling to create an LCEL chain to achieve this. Does anyone know how to do this?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Apart-Damage143&quot;&gt; /u/Apart-Damage143 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d8b1k5/struggling_with_pinecone_retrieval_performance/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d8b1k5/struggling_with_pinecone_retrieval_performance/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1d8b1k5</id><link href="https://www.reddit.com/r/LangChain/comments/1d8b1k5/struggling_with_pinecone_retrieval_performance/" /><updated>2024-06-04T23:07:12+00:00</updated><published>2024-06-04T23:07:12+00:00</published><title>Struggling with Pinecone Retrieval Performance? Need Help with LCEL Chains!</title></entry><entry><author><name>/u/Razeta101</name><uri>https://www.reddit.com/user/Razeta101</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hello, I have been working on a project that involves using langchain and RAG to write direct text quotes based on sourced texts.&lt;/p&gt; &lt;p&gt;The main objective is to create a vector database with PDF files from Google Academic and then utilize them to generate a user-requested text in APA format, which will output a paragraph containing direct text quotes.&lt;/p&gt; &lt;p&gt;For this project, I have been using langchain, local llm (phi3 and llama3 with ollama or llm studio), as well as Chat GPT 4 and Groq.&lt;/p&gt; &lt;p&gt;Unfortunately, I have encountered some difficulties as the models don&amp;#39;t seem to follow the instructions, especially when it comes to using direct text quotes, even when I include instructions on how to format them in APA style.&lt;/p&gt; &lt;p&gt;Therefore, I would appreciate any ideas or suggestions from the community on how I could achieve the desired result.&lt;/p&gt; &lt;p&gt;Thank you.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Razeta101&quot;&gt; /u/Razeta101 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d81hl4/rag_as_a_apa_direct_text_quotes_generator/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d81hl4/rag_as_a_apa_direct_text_quotes_generator/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1d81hl4</id><link href="https://www.reddit.com/r/LangChain/comments/1d81hl4/rag_as_a_apa_direct_text_quotes_generator/" /><updated>2024-06-04T16:30:37+00:00</updated><published>2024-06-04T16:30:37+00:00</published><title>RAG as a APA direct text quotes generator</title></entry></feed>