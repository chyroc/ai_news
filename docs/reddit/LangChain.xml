<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/"><category term="LangChain" label="r/LangChain"/><updated>2024-07-06T12:06:20+00:00</updated><icon>https://www.redditstatic.com/icon.png/</icon><id>/r/LangChain.rss</id><link rel="self" href="https://www.reddit.com/r/LangChain.rss" type="application/atom+xml" /><link rel="alternate" href="https://www.reddit.com/r/LangChain" type="text/html" /><subtitle>LangChain is an open-source framework and developer toolkit that helps developers get LLM applications from prototype to production. It is available for Python and Javascript at https://www.langchain.com/.</subtitle><title>LangChain</title><entry><author><name>/u/zchaarm</name><uri>https://www.reddit.com/user/zchaarm</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;A place for members of &lt;a href=&quot;/r/LangChain&quot;&gt;r/LangChain&lt;/a&gt; to chat with each other&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/zchaarm&quot;&gt; /u/zchaarm &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_10ljho9</id><link href="https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/" /><updated>2023-01-26T04:33:05+00:00</updated><published>2023-01-26T04:33:05+00:00</published><title>r/LangChain Lounge</title></entry><entry><author><name>/u/Albert_AG</name><uri>https://www.reddit.com/user/Albert_AG</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;m trying to develop an application that can perform statistical analysis of CSV files and generate plots. I&amp;#39;ve been trying to do this with rag, but I&amp;#39;ve no IDEA how to split/load/embed the CSV files, I&amp;#39;ve done this before with PDFs. PLEASE HELP!!! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Albert_AG&quot;&gt; /u/Albert_AG &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dwm3xh/help_with_csv_rag/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dwm3xh/help_with_csv_rag/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dwm3xh</id><link href="https://www.reddit.com/r/LangChain/comments/1dwm3xh/help_with_csv_rag/" /><updated>2024-07-06T09:54:04+00:00</updated><published>2024-07-06T09:54:04+00:00</published><title>Help with CSV RAG.</title></entry><entry><author><name>/u/pjbacelar</name><uri>https://www.reddit.com/user/pjbacelar</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey folks, we‚Äôve just launched an open-source library called Django AI Assistant, and we‚Äôd love your feedback!&lt;/p&gt; &lt;p&gt;What It Does:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Function/Tool Calling&lt;/strong&gt;: Simplifies complex AI implementations with easy-to-use Python classes&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Retrieval-Augmented Generation&lt;/strong&gt;: Enhance AI functionalities efficiently.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Full Django Integration&lt;/strong&gt;: AI can access databases, check permissions, send emails, manage media files, and call external APIs effortlessly.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;How You Can Help:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Try It: &lt;a href=&quot;https://github.com/vintasoftware/django-ai-assistant/&quot;&gt;https://github.com/vintasoftware/django-ai-assistant/&lt;/a&gt;&lt;/li&gt; &lt;li&gt;‚ñ∂Ô∏è &lt;a href=&quot;https://www.youtube.com/watch?v=bSJv4OIKLog&amp;amp;ab_channel=VintaSoftware&quot;&gt;Watch the Demo&lt;/a&gt;&lt;/li&gt; &lt;li&gt;üìñ &lt;a href=&quot;https://vintasoftware.github.io/django-ai-assistant/latest/get-started/&quot;&gt;Read the Docs&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Test It &amp;amp; Break Things: Integrate it, experiment, and see what works (and what doesn‚Äôt).&lt;/li&gt; &lt;li&gt;Give Feedback: Drop your thoughts here or on our GitHub issues page.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Your input will help us make this lib better for everyone. Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/pjbacelar&quot;&gt; /u/pjbacelar &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dw6dws/django_ai_assistant_opensource_lib_launch/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dw6dws/django_ai_assistant_opensource_lib_launch/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dw6dws</id><link href="https://www.reddit.com/r/LangChain/comments/1dw6dws/django_ai_assistant_opensource_lib_launch/" /><updated>2024-07-05T19:34:19+00:00</updated><published>2024-07-05T19:34:19+00:00</published><title>Django AI Assistant - Open-source Lib Launch</title></entry><entry><author><name>/u/jscraft</name><uri>https://www.reddit.com/user/jscraft</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Made this short LangChain.js example on how to improve AI math accuracy by asking the LLM to create and execute JavaScript code. &lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://www.js-craft.io/blog/langchain-javascript-execute-generated-code/&quot;&gt;https://www.js-craft.io/blog/langchain-javascript-execute-generated-code/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/jscraft&quot;&gt; /u/jscraft &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dwk40w/langchain_javascript_execute_generated_code/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dwk40w/langchain_javascript_execute_generated_code/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dwk40w</id><link href="https://www.reddit.com/r/LangChain/comments/1dwk40w/langchain_javascript_execute_generated_code/" /><updated>2024-07-06T07:27:17+00:00</updated><published>2024-07-06T07:27:17+00:00</published><title>LangChain JavaScript ‚Äì execute generated code</title></entry><entry><author><name>/u/AudibleDruid</name><uri>https://www.reddit.com/user/AudibleDruid</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dwlfju/what_is_suppose_to_go_into_here_langflow/&quot;&gt; &lt;img src=&quot;https://b.thumbs.redditmedia.com/KWGg6CZOUlYqCi8xD4i6Ay2AYX8NAL1h6WHCBUYVk5w.jpg&quot; alt=&quot;What is suppose to go into here? Langflow&quot; title=&quot;What is suppose to go into here? Langflow&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;&lt;a href=&quot;https://preview.redd.it/ty9hu9dz5vad1.png?width=900&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2627d1ad0ee0bb3f888b9c583e96060447d55b77&quot;&gt;https://preview.redd.it/ty9hu9dz5vad1.png?width=900&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2627d1ad0ee0bb3f888b9c583e96060447d55b77&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/AudibleDruid&quot;&gt; /u/AudibleDruid &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dwlfju/what_is_suppose_to_go_into_here_langflow/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dwlfju/what_is_suppose_to_go_into_here_langflow/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1dwlfju</id><media:thumbnail url="https://b.thumbs.redditmedia.com/KWGg6CZOUlYqCi8xD4i6Ay2AYX8NAL1h6WHCBUYVk5w.jpg" /><link href="https://www.reddit.com/r/LangChain/comments/1dwlfju/what_is_suppose_to_go_into_here_langflow/" /><updated>2024-07-06T09:03:05+00:00</updated><published>2024-07-06T09:03:05+00:00</published><title>What is suppose to go into here? Langflow</title></entry><entry><author><name>/u/shanumas</name><uri>https://www.reddit.com/user/shanumas</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I think currenlty the langchain implementations like chat-langchain supports conversational memory. But the conversation can sometimes be too long.&lt;/p&gt; &lt;p&gt;I am lookin for memory-summarization like this. &lt;a href=&quot;https://www.youtube.com/watch?v=oPCKB9MUP6c&amp;amp;t=81s&quot;&gt;https://www.youtube.com/watch?v=oPCKB9MUP6c&amp;amp;t=81s&lt;/a&gt;&lt;/p&gt; &lt;p&gt;to reduce tokens. Is there any chatbot implementation like this on github ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/shanumas&quot;&gt; /u/shanumas &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dwkr14/langchain_with_personalized_memory_or_summarized/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dwkr14/langchain_with_personalized_memory_or_summarized/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dwkr14</id><link href="https://www.reddit.com/r/LangChain/comments/1dwkr14/langchain_with_personalized_memory_or_summarized/" /><updated>2024-07-06T08:12:46+00:00</updated><published>2024-07-06T08:12:46+00:00</published><title>Langchain with personalized memory (or summarized conversational memory)</title></entry><entry><author><name>/u/business24_ai</name><uri>https://www.reddit.com/user/business24_ai</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;&lt;a href=&quot;https://youtu.be/DBXdE_5Jces&quot;&gt;https://youtu.be/DBXdE_5Jces&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/business24_ai&quot;&gt; /u/business24_ai &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dwkzj8/langgraph_state_create_a_cyclic_graph_and/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dwkzj8/langgraph_state_create_a_cyclic_graph_and/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dwkzj8</id><link href="https://www.reddit.com/r/LangChain/comments/1dwkzj8/langgraph_state_create_a_cyclic_graph_and/" /><updated>2024-07-06T08:30:17+00:00</updated><published>2024-07-06T08:30:17+00:00</published><title>LangGraph state - Create a cyclic graph and watchdog a directory</title></entry><entry><author><name>/u/Front-Show7358</name><uri>https://www.reddit.com/user/Front-Show7358</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Potentially dumb question lol. Basically when I run my RAG, it takes a long time to process all the documents that it will then retrieve. Is there a way to just save off the model after it is done reading the documents so that when you run it again, it can skip that step? Similar to how a fine-tuned model would work? It doesn&amp;#39;t really make sense in my head, but I haven&amp;#39;t been able to find a concrete answer to this so I want to be sure.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Front-Show7358&quot;&gt; /u/Front-Show7358 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dw1mk2/is_there_a_way_to_save_a_rag_after_it_has_read/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dw1mk2/is_there_a_way_to_save_a_rag_after_it_has_read/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dw1mk2</id><link href="https://www.reddit.com/r/LangChain/comments/1dw1mk2/is_there_a_way_to_save_a_rag_after_it_has_read/" /><updated>2024-07-05T16:10:29+00:00</updated><published>2024-07-05T16:10:29+00:00</published><title>Is there a way to save a RAG after it has read its documents?</title></entry><entry><author><name>/u/macxgaming</name><uri>https://www.reddit.com/user/macxgaming</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi!&lt;/p&gt; &lt;p&gt;I&amp;#39;m working on a RAG system for my company where we can use it to search through our internal wiki page.&lt;br/&gt; My system is nearly in a releasable state and finds the correct information 90% of the times, and I&amp;#39;m happy about it, but I&amp;#39;m constantly thinking, can I make it better?&lt;/p&gt; &lt;p&gt;I&amp;#39;ve made a custom scraper for our wiki, we&amp;#39;re using an older version of MediaWiki.&lt;br/&gt; The scraper I&amp;#39;ve made is basically extracting all sections out into its own &amp;quot;document&amp;quot; and then sending it into qdrant vector database.&lt;br/&gt; That means that in the vector database, it doesn&amp;#39;t have a full wiki page but rather a cut up version to make it easier for the search query to hit something right. But I feel like this is kinda wrong?&lt;/p&gt; &lt;p&gt;Whenever you send in your query to the backend, it&amp;#39;ll then search for the 10 documents matching and then reranking with BAAI/bge-reranker-large. Then the context is being sent to Llama3:8b with your question in mind.&lt;br/&gt; This means that Llama3 will never get a fully contextual article, since the vectors are only smaller sections from the full page.&lt;/p&gt; &lt;p&gt;What could be done do make this better in the end? The one thing I see as an issue here, is that it will never know anything about the rest of the full page, but if it has the full page, it feels like Llama3 get overwhelmed by the data and then craps out.&lt;/p&gt; &lt;p&gt;We have ~258 articles and that&amp;#39;s resulting in about 1488 points in qdrant.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/macxgaming&quot;&gt; /u/macxgaming &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dvr774/what_is_the_best_approach_to_achieve_a_better/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dvr774/what_is_the_best_approach_to_achieve_a_better/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dvr774</id><link href="https://www.reddit.com/r/LangChain/comments/1dvr774/what_is_the_best_approach_to_achieve_a_better/" /><updated>2024-07-05T06:17:40+00:00</updated><published>2024-07-05T06:17:40+00:00</published><title>What is the best approach to achieve a better performant RAG?</title></entry><entry><author><name>/u/PuzzleheadedDay5615</name><uri>https://www.reddit.com/user/PuzzleheadedDay5615</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;&lt;a href=&quot;https://ai.google.dev/competition&quot;&gt;Join the Gemini API Developer Competition | Google for Developers&lt;/a&gt; Here is the link&lt;/p&gt; &lt;p&gt;I have been freelancing for 4+ years and have decent experience of python. need someone who is competitive, creative, and willing to sacrifice at least 4 hours a day&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/PuzzleheadedDay5615&quot;&gt; /u/PuzzleheadedDay5615 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dwd1f9/trying_to_find_teammate_for_google_gemini/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dwd1f9/trying_to_find_teammate_for_google_gemini/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dwd1f9</id><link href="https://www.reddit.com/r/LangChain/comments/1dwd1f9/trying_to_find_teammate_for_google_gemini/" /><updated>2024-07-06T00:34:32+00:00</updated><published>2024-07-06T00:34:32+00:00</published><title>trying to find teammate for google gemini developer competition</title></entry><entry><author><name>/u/Party_Jellyfish5380</name><uri>https://www.reddit.com/user/Party_Jellyfish5380</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;YouTube has a new feature where it organizes comments by. It it possible to organize a list of chat by topic with langchain?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Party_Jellyfish5380&quot;&gt; /u/Party_Jellyfish5380 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dw5fr6/youtube_comments_feature/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dw5fr6/youtube_comments_feature/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dw5fr6</id><link href="https://www.reddit.com/r/LangChain/comments/1dw5fr6/youtube_comments_feature/" /><updated>2024-07-05T18:53:47+00:00</updated><published>2024-07-05T18:53:47+00:00</published><title>YouTube comments feature</title></entry><entry><author><name>/u/frellothings</name><uri>https://www.reddit.com/user/frellothings</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I want to deploy a Huggingface model in Sagemaker with a context size of around 25-32k. I am having trouble finding a suitable model that performs well with this context size. The model&amp;#39;s task will be to map raw data to a target framework. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/frellothings&quot;&gt; /u/frellothings &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dvs05a/deploy_hugging_face_model_in_sagemaker/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dvs05a/deploy_hugging_face_model_in_sagemaker/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dvs05a</id><link href="https://www.reddit.com/r/LangChain/comments/1dvs05a/deploy_hugging_face_model_in_sagemaker/" /><updated>2024-07-05T07:11:15+00:00</updated><published>2024-07-05T07:11:15+00:00</published><title>Deploy Hugging Face model in Sagemaker</title></entry><entry><author><name>/u/gibriyagi</name><uri>https://www.reddit.com/user/gibriyagi</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I would like to use Postgres with pgvector but could not figure out a way to do hybrid search using bm25.&lt;/p&gt; &lt;p&gt;Anyone using Postgres only for RAG? Do you do hybrid search? If not do you combine it with something else?&lt;/p&gt; &lt;p&gt;Would love to hear your experiences.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/gibriyagi&quot;&gt; /u/gibriyagi &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dvdnzc/hybrid_search_with_postgres/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dvdnzc/hybrid_search_with_postgres/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dvdnzc</id><link href="https://www.reddit.com/r/LangChain/comments/1dvdnzc/hybrid_search_with_postgres/" /><updated>2024-07-04T18:23:03+00:00</updated><published>2024-07-04T18:23:03+00:00</published><title>Hybrid search with Postgres</title></entry><entry><author><name>/u/ExplorerTechnical808</name><uri>https://www.reddit.com/user/ExplorerTechnical808</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;like title. Thanks in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/ExplorerTechnical808&quot;&gt; /u/ExplorerTechnical808 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dvrv8g/any_good_resourceguide_about_how_to_do_rag_on_a/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dvrv8g/any_good_resourceguide_about_how_to_do_rag_on_a/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dvrv8g</id><link href="https://www.reddit.com/r/LangChain/comments/1dvrv8g/any_good_resourceguide_about_how_to_do_rag_on_a/" /><updated>2024-07-05T07:01:51+00:00</updated><published>2024-07-05T07:01:51+00:00</published><title>Any good resource/guide about how to do RAG on a codebase? (e.g. Github repo)</title></entry><entry><author><name>/u/Volodymyr_steax</name><uri>https://www.reddit.com/user/Volodymyr_steax</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;My question might be a bit basic, but I‚Äôm new to all of this and eager to learn.&lt;/p&gt; &lt;p&gt;I have a basic setup where I initialize an LLM using vLLM with Langchain RAG and the Llama model (specifically, llama2-13b-chat-hf). Here‚Äôs what I do:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;I define a system prompt and an instruction f&lt;/li&gt; &lt;li&gt;I create an &lt;code&gt;llm_chain&lt;/code&gt;&lt;/li&gt; &lt;li&gt;I then run the chain with &lt;code&gt;llm_chain.run(text)&lt;/code&gt; , which works for a single input.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I have build an app with FastAPI. Previously I used asyncio method to handle multiple request to llm, but with each new request it become slower in response. So I decide to use vLLM method, but I got a problem now how to provide parallel or concurrent requests to vLLM when I have dealing with dozen or more users. Is there a way to call &lt;code&gt;run&lt;/code&gt; in parallel for several inputs and receive valid results for each input?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Volodymyr_steax&quot;&gt; /u/Volodymyr_steax &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dvrj1k/concurrentparallel_requests_with_vllm/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dvrj1k/concurrentparallel_requests_with_vllm/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dvrj1k</id><link href="https://www.reddit.com/r/LangChain/comments/1dvrj1k/concurrentparallel_requests_with_vllm/" /><updated>2024-07-05T06:39:27+00:00</updated><published>2024-07-05T06:39:27+00:00</published><title>Concurrent/parallel requests with vLLM</title></entry><entry><author><name>/u/muditjps</name><uri>https://www.reddit.com/user/muditjps</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dvhvam/hey_rlangchain_weve_created_an_app_template_for/&quot;&gt; &lt;img src=&quot;https://external-preview.redd.it/6vdYm5zqVtm_CFLlTGc1uWkGnd-hhEdr4D2wHuVz3iQ.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b991cdf7470fc68f086bd51039bd6575a8817deb&quot; alt=&quot;Hey r/langchain, we've created an app template for multimodal RAG (MM-RAG) using GPT4o and Pathway. The incremental indexing pipeline parses tables as images, explains them in detail, and saves the table content with the document chunk. This outperforms traditional RAG methods. More in the link.&quot; title=&quot;Hey r/langchain, we've created an app template for multimodal RAG (MM-RAG) using GPT4o and Pathway. The incremental indexing pipeline parses tables as images, explains them in detail, and saves the table content with the document chunk. This outperforms traditional RAG methods. More in the link.&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/muditjps&quot;&gt; /u/muditjps &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://pathway.com/developers/templates/multimodal-rag&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dvhvam/hey_rlangchain_weve_created_an_app_template_for/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1dvhvam</id><media:thumbnail url="https://external-preview.redd.it/6vdYm5zqVtm_CFLlTGc1uWkGnd-hhEdr4D2wHuVz3iQ.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=b991cdf7470fc68f086bd51039bd6575a8817deb" /><link href="https://www.reddit.com/r/LangChain/comments/1dvhvam/hey_rlangchain_weve_created_an_app_template_for/" /><updated>2024-07-04T21:30:21+00:00</updated><published>2024-07-04T21:30:21+00:00</published><title>Hey r/langchain, we've created an app template for multimodal RAG (MM-RAG) using GPT4o and Pathway. The incremental indexing pipeline parses tables as images, explains them in detail, and saves the table content with the document chunk. This outperforms traditional RAG methods. More in the link.</title></entry><entry><author><name>/u/meamysace</name><uri>https://www.reddit.com/user/meamysace</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;m searching for a tool that allows users to compare outputs generated by several LLMs using just one prompt. While I understand that LangChain could potentially enable building such a solution locally, I&amp;#39;m curious if any existing products offer this functionality.&lt;/p&gt; &lt;p&gt;I&amp;#39;m weary of manually inputting the same prompt across different models like GPT, Claude, Bard, and Perplexity to cross-reference answers and verify accuracy. Any recommendations or insights would be greatly appreciated! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/meamysace&quot;&gt; /u/meamysace &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dvaenf/tool_for_comparing_outputs_of_multiple_llms_from/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dvaenf/tool_for_comparing_outputs_of_multiple_llms_from/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dvaenf</id><link href="https://www.reddit.com/r/LangChain/comments/1dvaenf/tool_for_comparing_outputs_of_multiple_llms_from/" /><updated>2024-07-04T16:02:08+00:00</updated><published>2024-07-04T16:02:08+00:00</published><title>Tool for Comparing Outputs of Multiple LLMs from Single Prompts</title></entry><entry><author><name>/u/QuasiEvil</name><uri>https://www.reddit.com/user/QuasiEvil</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;ve been playing around with GPT4All and langchain, for which there is a minimal demo here:&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://python.langchain.com/v0.2/docs/integrations/llms/gpt4all/&quot;&gt;https://python.langchain.com/v0.2/docs/integrations/llms/gpt4all/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;In this demo, they invoke the following:&lt;/p&gt; &lt;p&gt;&lt;code&gt;from langchain_core.callbacks import StreamingStdOutCallbackHandler&lt;/code&gt;&lt;/p&gt; &lt;p&gt;From the API, it states that this only works with LLMs that support streaming. According to the integrations page:&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://python.langchain.com/v0.2/docs/integrations/llms/&quot;&gt;https://python.langchain.com/v0.2/docs/integrations/llms/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;gpt4all does NOT support streaming. So I&amp;#39;m confused - what gives with this demo?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/QuasiEvil&quot;&gt; /u/QuasiEvil &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dvfs2b/beginner_here_found_something_confusing/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dvfs2b/beginner_here_found_something_confusing/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dvfs2b</id><link href="https://www.reddit.com/r/LangChain/comments/1dvfs2b/beginner_here_found_something_confusing/" /><updated>2024-07-04T19:56:38+00:00</updated><published>2024-07-04T19:56:38+00:00</published><title>Beginner here: found something confusing</title></entry><entry><author><name>/u/liljuden</name><uri>https://www.reddit.com/user/liljuden</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi there,&lt;/p&gt; &lt;p&gt;I am currently working on building a chatbot for internal use in my company. I have developed a relatively classic RAG framework and now want to include a knowledge graph. I have read several papers on this topic, but I am confused about how to actually &amp;quot;activate&amp;quot; a graph in the RAG flow.&lt;/p&gt; &lt;p&gt;So far, I have found different approaches based on extracting entities and relationships from chunks to generate community summaries of the related entities. This process occurs in the offline stage. At least, that is what I have tried to do. I am wondering how to activate this correctly. Currently, I match entities from the input with summaries as the flow runs, but I have the impression that others use the graph aspect differently, possibly using a function to inject relevant context into the LLM.&lt;/p&gt; &lt;p&gt;Can you help me understand this better?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/liljuden&quot;&gt; /u/liljuden &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1duz8qc/how_to_incorporate_a_knowledge_graph_in_rag/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1duz8qc/how_to_incorporate_a_knowledge_graph_in_rag/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1duz8qc</id><link href="https://www.reddit.com/r/LangChain/comments/1duz8qc/how_to_incorporate_a_knowledge_graph_in_rag/" /><updated>2024-07-04T05:33:58+00:00</updated><published>2024-07-04T05:33:58+00:00</published><title>How to incorporate a knowledge graph in RAG</title></entry><entry><author><name>/u/imharesh20</name><uri>https://www.reddit.com/user/imharesh20</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;m working with Langchain to build a tool that calls an agent. Currently, I&amp;#39;m passing the chat history as an input variable to the agent. However, I&amp;#39;ve encountered an issue where the agent doesn&amp;#39;t always seem to utilize the history data to answer questions consistently. This is especially problematic when users have queries spaced out over 10‚Äì15 days.&lt;/p&gt; &lt;p&gt;Is there a more efficient way to ensure the agent consistently remembers all chat history and context over multiple sessions? What approach or best practices should I follow to address this issue?&lt;/p&gt; &lt;p&gt;Thanks in advance for your guidance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/imharesh20&quot;&gt; /u/imharesh20 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dv7e89/passing_chat_history_to_langchain_tool_calling/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dv7e89/passing_chat_history_to_langchain_tool_calling/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dv7e89</id><link href="https://www.reddit.com/r/LangChain/comments/1dv7e89/passing_chat_history_to_langchain_tool_calling/" /><updated>2024-07-04T13:49:43+00:00</updated><published>2024-07-04T13:49:43+00:00</published><title>Passing Chat History to Langchain Tool Calling Agent</title></entry><entry><author><name>/u/Mediocre-Card8046</name><uri>https://www.reddit.com/user/Mediocre-Card8046</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;I set up Mixtral 8x22B on Azure AI/Machine Learning and now want to use it with Langchain. I have difficulties with the format I am getting, e.g. a ChatOpenAI response looks like this:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;from langchain_openai import ChatOpenAI llmm = ChatOpenAI() llmm.invoke(&amp;quot;Hallo&amp;quot;) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;code&gt;AIMessage(content=&amp;#39;Hallo! Wie kann ich Ihnen helfen?&amp;#39;, response_metadata={&amp;#39;token_usage&amp;#39;: {&amp;#39;completion_tokens&amp;#39;: 8, &amp;#39;prompt_tokens&amp;#39;: 8, &amp;#39;total_tokens&amp;#39;: 16}, &amp;#39;model_name&amp;#39;: &amp;#39;gpt-3.5-turbo&amp;#39;, &amp;#39;system_fingerprint&amp;#39;: None, &amp;#39;finish_reason&amp;#39;: &amp;#39;stop&amp;#39;, &amp;#39;logprobs&amp;#39;: None}, id=&amp;#39;r&amp;#39;)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;This is how it looks when I am loading Mixtral 8x22B with AzureMLChatOnlineEndpoint:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;from langchain_community.chat_models.azureml_endpoint import AzureMLChatOnlineEndpoint from langchain_community.chat_models.azureml_endpoint import ( AzureMLEndpointApiType, CustomOpenAIChatContentFormatter, ) from langchain_core.messages import HumanMessage chat = AzureMLChatOnlineEndpoint( endpoint_url=&amp;quot;...&amp;quot;, endpoint_api_type=AzureMLEndpointApiType.dedicated, endpoint_api_key=&amp;quot;...&amp;quot;, content_formatter=CustomOpenAIChatContentFormatter(), ) chat.invoke(&amp;quot;Hallo&amp;quot;) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;code&gt;BaseMessage(content=&amp;#39;Hallo, ich bin ein deutscher Sprachassistent. Was kann ich f√ºr&amp;#39;, type=&amp;#39;assistant&amp;#39;, id=&amp;#39;run-23&amp;#39;)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;So with the Mixtral model the output seems to be truncated and also the format is different (BaseMessage vs. AIMessage). How can I change this to make it work just like an ChatOpenAI model?&lt;/p&gt; &lt;p&gt;In my application I want to easily switch between these two models.&lt;/p&gt; &lt;p&gt;Thanks in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Mediocre-Card8046&quot;&gt; /u/Mediocre-Card8046 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dv64ip/load_llm_mixtral_8x22b_from_azure_ai_endpoint_as/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dv64ip/load_llm_mixtral_8x22b_from_azure_ai_endpoint_as/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dv64ip</id><link href="https://www.reddit.com/r/LangChain/comments/1dv64ip/load_llm_mixtral_8x22b_from_azure_ai_endpoint_as/" /><updated>2024-07-04T12:47:41+00:00</updated><published>2024-07-04T12:47:41+00:00</published><title>Load LLM (Mixtral 8x22B) from Azure AI endpoint as Langchain Model</title></entry><entry><author><name>/u/ab-carti</name><uri>https://www.reddit.com/user/ab-carti</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I‚Äôm developing an app that creates a knowledge base based on transcripts of YouTube videos. And I need a way to have the LLM recognize where the transcript came from, I have the data I just don‚Äôt know how to implement it effectively &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/ab-carti&quot;&gt; /u/ab-carti &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dv5p0g/how_do_i_add_meta_data_to_pinecone_documents_i/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dv5p0g/how_do_i_add_meta_data_to_pinecone_documents_i/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dv5p0g</id><link href="https://www.reddit.com/r/LangChain/comments/1dv5p0g/how_do_i_add_meta_data_to_pinecone_documents_i/" /><updated>2024-07-04T12:24:56+00:00</updated><published>2024-07-04T12:24:56+00:00</published><title>How do I add meta data to Pinecone documents, I want to maintain overlap between chunks so I don‚Äôt wanna replace first words</title></entry><entry><author><name>/u/taskade</name><uri>https://www.reddit.com/user/taskade</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1duy3hj/new_document_loader_for_taskade_langchain/&quot;&gt; &lt;img src=&quot;https://external-preview.redd.it/UallS6eTqMQ6Q_VovzOpEJke6ZgDb6GzqRJtFsGug9s.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=82f0fba791dd0f75899e3dc5480f915fe8c0cafd&quot; alt=&quot;New Document Loader for Taskade | ü¶úÔ∏èüîó Langchain&quot; title=&quot;New Document Loader for Taskade | ü¶úÔ∏èüîó Langchain&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/taskade&quot;&gt; /u/taskade &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://js.langchain.com/v0.2/docs/integrations/document_loaders/web_loaders/taskade/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1duy3hj/new_document_loader_for_taskade_langchain/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1duy3hj</id><media:thumbnail url="https://external-preview.redd.it/UallS6eTqMQ6Q_VovzOpEJke6ZgDb6GzqRJtFsGug9s.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=82f0fba791dd0f75899e3dc5480f915fe8c0cafd" /><link href="https://www.reddit.com/r/LangChain/comments/1duy3hj/new_document_loader_for_taskade_langchain/" /><updated>2024-07-04T04:24:50+00:00</updated><published>2024-07-04T04:24:50+00:00</published><title>New Document Loader for Taskade | ü¶úÔ∏èüîó Langchain</title></entry><entry><author><name>/u/Mediocre-Card8046</name><uri>https://www.reddit.com/user/Mediocre-Card8046</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;I built a CRAG application and now want to further improve it. As a fist step I would like to check if the given question is a followup question of a previous question or not. If it is, then I want to use my messages history to create a new question based on the context in the chat history and the actual question. This is similar to the &amp;quot;history_aware_retriever&amp;quot; from Langchain.&lt;/p&gt; &lt;p&gt;However I am not satisfied with the classification of my model, as it often returns &amp;quot;False&amp;quot; even if it is a followup question. So is there maybe a more elegant way of doing this or would you improve my prompt?&lt;/p&gt; &lt;p&gt;Heres the function within my Langgraph app:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;class CheckFollowupQuestion(BaseModel): &amp;quot;&amp;quot;&amp;quot;Bool Werte um zu bestimmen, ob die Frage auf eine zuvor gestellte Frage aufbaut.&amp;quot;&amp;quot;&amp;quot; score: str = Field( description=&amp;quot;Die Frage bezieht sich auf eine vorherige Antwort oder zuvor gestellte Frage, &amp;#39;True&amp;#39; oder &amp;#39;False&amp;#39;&amp;quot; ) def followup_question_classifier(state: AgentState): messages = state[&amp;quot;messages&amp;quot;][-5:] print(f&amp;quot;MESSAGES (in follow up): {messages}&amp;quot;) question = state[&amp;quot;question&amp;quot;] system = &amp;quot;&amp;quot;&amp;quot;&amp;lt;s&amp;gt;[INST] You assess whether the user&amp;#39;s question is a follow-up question or not. For this, you get questions from the chat history and assess whether the question builds on a question or answer from the chat history or not.\n Evaluate with &amp;#39;True&amp;#39; if it is a typical follow-up question. Also evaluate with &amp;#39;True&amp;#39; if it seems that the question refers to a previous answer. \n Evaluate with &amp;#39;False&amp;#39; if it is a normal question, or if the question has nothing to do with the chat history. Here is an example:\n\n Example of a &amp;#39;False&amp;#39; evaluation:\n Question: &amp;#39;How much does a kebab currently cost?&amp;#39;\n Chat history: [HumanMessage(content=&amp;#39;Hello, I am Max, who are you?&amp;#39;, id=&amp;#39;8&amp;#39;), HumanMessage(content=&amp;#39;What was my name?&amp;#39;, id=&amp;#39;7&amp;#39;), HumanMessage(content=&amp;#39;Name exactly one advantage of Multicloud.&amp;#39;, id=&amp;#39;f&amp;#39;)]\n Your evaluation: &amp;#39;False&amp;#39;. Reason: The questions from the chat history have nothing to do with the question asked.\n\n Example of a &amp;#39;True&amp;#39; evaluation: Question: &amp;#39;And how warm will it be there tomorrow?&amp;#39; Chat history: [HumanMessage(content=&amp;#39;Where is Munich located?&amp;#39;, id=&amp;#39;8&amp;#39;), HumanMessage(content=&amp;#39;Which dialect is spoken in Munich?&amp;#39;, id=&amp;#39;7&amp;#39;)] Your evaluation: &amp;#39;True&amp;#39;. Reason: The chat history is about the city of Munich. In the follow-up question, the user wants to know what the weather will be &amp;#39;there&amp;#39; tomorrow. Since the previous discussion was about Munich, &amp;#39;there&amp;#39; refers to the city of Munich. [/INST]&amp;quot;&amp;quot;&amp;quot; grade_prompt = ChatPromptTemplate.from_messages( [ (&amp;quot;system&amp;quot;, system), ( &amp;quot;human&amp;quot;, &amp;quot;User&amp;#39;s question: {question} \n\n Chat history: {chat_history}&amp;quot;, ), ] ) llm = ChatOpenAI() structured_llm = llm.with_structured_output(GradeQuestion) grader_llm = grade_prompt | structured_llm result = grader_llm.invoke({&amp;quot;question&amp;quot;: question, &amp;quot;chat_history&amp;quot;: messages}) state[&amp;quot;is_followup_question&amp;quot;] = result.score return state &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Mediocre-Card8046&quot;&gt; /u/Mediocre-Card8046 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dv203r/history_aware_agent_in_langgraph/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dv203r/history_aware_agent_in_langgraph/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dv203r</id><link href="https://www.reddit.com/r/LangChain/comments/1dv203r/history_aware_agent_in_langgraph/" /><updated>2024-07-04T08:32:45+00:00</updated><published>2024-07-04T08:32:45+00:00</published><title>History Aware Agent in Langgraph</title></entry><entry><author><name>/u/bastormator</name><uri>https://www.reddit.com/user/bastormator</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I was not able to find anything on the web, the cases for which the pedantic parsers fail to parse the data coming from LLMs. I tried looking under the hood working and say that they are using json parsing, if anyone has info about this please enlighten me.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/bastormator&quot;&gt; /u/bastormator &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dv1nfr/pedantic_data_parsing/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dv1nfr/pedantic_data_parsing/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dv1nfr</id><link href="https://www.reddit.com/r/LangChain/comments/1dv1nfr/pedantic_data_parsing/" /><updated>2024-07-04T08:08:06+00:00</updated><published>2024-07-04T08:08:06+00:00</published><title>Pedantic data parsing</title></entry><entry><author><name>/u/New-Contribution6302</name><uri>https://www.reddit.com/user/New-Contribution6302</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I have a problem. It takes literally 30 mins for my map-reduce summarisation chain to produce it&amp;#39;s final output. Current vRAM is 16GB. What should I do to increase the speed?&lt;/p&gt; &lt;p&gt;Model: Llama2&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/New-Contribution6302&quot;&gt; /u/New-Contribution6302 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dv0wbv/how_to_increase_the_inference_speed_of_map_reduce/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dv0wbv/how_to_increase_the_inference_speed_of_map_reduce/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dv0wbv</id><link href="https://www.reddit.com/r/LangChain/comments/1dv0wbv/how_to_increase_the_inference_speed_of_map_reduce/" /><updated>2024-07-04T07:18:04+00:00</updated><published>2024-07-04T07:18:04+00:00</published><title>How to increase the inference speed of Map reduce chain in langchain</title></entry></feed>