<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/"><category term="LangChain" label="r/LangChain"/><updated>2024-07-27T19:25:50+00:00</updated><icon>https://www.redditstatic.com/icon.png/</icon><id>/r/LangChain.rss</id><link rel="self" href="https://www.reddit.com/r/LangChain.rss" type="application/atom+xml" /><link rel="alternate" href="https://www.reddit.com/r/LangChain" type="text/html" /><subtitle>LangChain is an open-source framework and developer toolkit that helps developers get LLM applications from prototype to production. It is available for Python and Javascript at https://www.langchain.com/.</subtitle><title>LangChain</title><entry><author><name>/u/zchaarm</name><uri>https://www.reddit.com/user/zchaarm</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;A place for members of &lt;a href=&quot;/r/LangChain&quot;&gt;r/LangChain&lt;/a&gt; to chat with each other&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/zchaarm&quot;&gt; /u/zchaarm &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_10ljho9</id><link href="https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/" /><updated>2023-01-26T04:33:05+00:00</updated><published>2023-01-26T04:33:05+00:00</published><title>r/LangChain Lounge</title></entry><entry><author><name>/u/SadPianist871</name><uri>https://www.reddit.com/user/SadPianist871</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey guys,&lt;/p&gt; &lt;p&gt;I have a general question for those of you developing agentic AI systems. Have you had the problem of a service not having an API and how did you solve it (i.e., how did you define the &amp;quot;tool&amp;quot; to be used by the LLM)? A simple example: I want my personal AI assistant to purchase groceries for me, but there&amp;#39;s no API provided by the supermarket. How can I achieve that?&lt;/p&gt; &lt;p&gt;Do you think this is another reason why AI agents are still not in use for tasks that are not critical (thus, it&amp;#39;s fine if they&amp;#39;re not 100% reliable), but could be very useful in our daily lives?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/SadPianist871&quot;&gt; /u/SadPianist871 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1edkn7p/lack_of_apis/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1edkn7p/lack_of_apis/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1edkn7p</id><link href="https://www.reddit.com/r/LangChain/comments/1edkn7p/lack_of_apis/" /><updated>2024-07-27T16:42:08+00:00</updated><published>2024-07-27T16:42:08+00:00</published><title>Lack of APIs</title></entry><entry><author><name>/u/random_name_03</name><uri>https://www.reddit.com/user/random_name_03</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey guys,&lt;/p&gt; &lt;p&gt;Recently I have familiarized myself with Langchain quite a bit. I was hoping to make a basic web-app, for a class of less than 100 people, as a personal project. &lt;/p&gt; &lt;p&gt;Unfortunately, while I do know JavaScript, Python, CSS, and HTML. I am unfamiliar with the structure behind such an application. Until now I have built some local CLI applications while using APIs (DeepSeek is crazy cheap and nice but might need an alternative because they also use API accessed data).&lt;/p&gt; &lt;p&gt;Where can I host a website, or a local easier llm like llama with it, or any options I can use with Langchain chain APIs? Also should I host an open-source LLM somewhere or depend on APIs?&lt;/p&gt; &lt;p&gt;There is just so much information that I cannot find anything concrete over it, please help :&amp;quot;)&lt;/p&gt; &lt;p&gt;Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/random_name_03&quot;&gt; /u/random_name_03 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ed8seg/using_langchain_for_a_smallscale_webapp/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ed8seg/using_langchain_for_a_smallscale_webapp/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1ed8seg</id><link href="https://www.reddit.com/r/LangChain/comments/1ed8seg/using_langchain_for_a_smallscale_webapp/" /><updated>2024-07-27T05:22:45+00:00</updated><published>2024-07-27T05:22:45+00:00</published><title>Using Langchain for a small-scale web-app</title></entry><entry><author><name>/u/UnderstandLingAI</name><uri>https://www.reddit.com/user/UnderstandLingAI</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ecw305/rag_provenance_computation/&quot;&gt; &lt;img src=&quot;https://b.thumbs.redditmedia.com/pX7nNYffb4qu9TyoIOylYhkzp_M3654eqi6vN3p4GQE.jpg&quot; alt=&quot;RAG provenance computation&quot; title=&quot;RAG provenance computation&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;&lt;a href=&quot;https://preview.redd.it/4ld6chb9vwed1.png?width=1052&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fe05dfd00ac3b7e109b70ba612a003e096a9fa63&quot;&gt;https://preview.redd.it/4ld6chb9vwed1.png?width=1052&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fe05dfd00ac3b7e109b70ba612a003e096a9fa63&lt;/a&gt;&lt;/p&gt; &lt;p&gt;When performing RAG with any LLM, it&amp;#39;s pretty tricky to decide how much of the answer given by the LLM was actually due to a specific document that was fed into it. Did the LLM pay attention to 1 document? 3? or perhaps none and just the query?&lt;/p&gt; &lt;p&gt;This provenance attribution is a difficult challenge in RAG and has not yet been properly solved. That being said, we have added quite a few ways to compute provenance to our framework RAG Me Up - &lt;a href=&quot;https://github.com/AI-Commandos/RAGMeUp&quot;&gt;https://github.com/AI-Commandos/RAGMeUp&lt;/a&gt;&lt;/p&gt; &lt;p&gt;One of the coolest and most accurate is when you are using local/open source LLMs, we can actually retrieve the attention scores from the model and use those to compute provenance:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;How much attention was paid from the given answer to every document separately that was retrieved from the database? Vice versa from every document to the answer?&lt;/li&gt; &lt;li&gt;How much attention was paid from the query to every document and vice versa?&lt;/li&gt; &lt;li&gt;How much attention was paid from the answer to the query, disregarding the documents... and vice versa.&lt;/li&gt; &lt;li&gt;And finally from the query to itself and from the answer to itself (remember that LLMs are autoregressive!)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;If we then take the attention paid from/to a single document and divide that by the sum of the other attentions mentioned - we get a good idea of the relative importance of each document fed into the LLM!&lt;/p&gt; &lt;p&gt;In RAG Me Up this is now implemented as a provenance method &amp;quot;attention&amp;quot; and you can use it with any OS LLM on any given dataset.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/UnderstandLingAI&quot;&gt; /u/UnderstandLingAI &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ecw305/rag_provenance_computation/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ecw305/rag_provenance_computation/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1ecw305</id><media:thumbnail url="https://b.thumbs.redditmedia.com/pX7nNYffb4qu9TyoIOylYhkzp_M3654eqi6vN3p4GQE.jpg" /><link href="https://www.reddit.com/r/LangChain/comments/1ecw305/rag_provenance_computation/" /><updated>2024-07-26T19:02:16+00:00</updated><published>2024-07-26T19:02:16+00:00</published><title>RAG provenance computation</title></entry><entry><author><name>/u/underrated-squirrel</name><uri>https://www.reddit.com/user/underrated-squirrel</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ecuq2g/askitright_pdfs_query_fullstack_application_rag/&quot;&gt; &lt;img src=&quot;https://external-preview.redd.it/ZJCf56Nh_Pt7yRbmmvCFkShWeJMBCwhbny8msEM-w_4.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6e109fb1c6c0d6d93450dc68e1c2d6c9933b8b26&quot; alt=&quot;AskItRight: PDFs Query Fullstack Application (RAG Ollama, LangChain, ChromaDB )&quot; title=&quot;AskItRight: PDFs Query Fullstack Application (RAG Ollama, LangChain, ChromaDB )&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hello Reddit!&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/2l247zaekwed1.png?width=250&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4365916258171dffcfe820e9ad138cf1e847078d&quot;&gt;https://preview.redd.it/2l247zaekwed1.png?width=250&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4365916258171dffcfe820e9ad138cf1e847078d&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I’ve been tinkering with some libraries and just finished building an app I’m so excited to share with you all. I’d love for you to check it out and let me know what you think!&lt;/p&gt; &lt;p&gt;🔗 GitHub Repository: &lt;a href=&quot;https://github.com/AbdArdati/PDFQueryAI&quot;&gt;https://github.com/AbdArdati/PDFQueryAI&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Key Features 🔑&lt;/h1&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;PDF Management&lt;/strong&gt;: &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Upload PDFs&lt;/strong&gt;: 📤 Users can upload PDF files through the upload interface. These files are processed and stored in the system.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;List PDFs&lt;/strong&gt;: 📋 Users can view a list of all uploaded PDF files through the available PDFs interface.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Delete PDFs&lt;/strong&gt;: 🗑️ Users can remove specific PDF files using the delete functionality available in the PDF management interface.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;View PDFs&lt;/strong&gt;: 👁️ Users can open and view the content of PDF files in a new browser tab directly from the list of PDFs.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Query Handling&lt;/strong&gt;: &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Ask Questions to PDF&lt;/strong&gt;: 🤔 Users can submit questions about the content of uploaded PDFs using the query interface. The application uses the AI model to provide answers based on the PDF contents.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;AI Integration&lt;/strong&gt;: 🤖 The l**lama3.1 model **is used to generate answers to queries from the content of the PDFs. This functionality is accessible through the AI query interface.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Prompt Templates&lt;/strong&gt;: 📝 Users can view and select from various prompt templates to guide the AI&amp;#39;s responses, ensuring they are tailored to specific needs. (Currently in progress, with frontend Create, Update, and Delete to be implemented.)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Statistics and Administration&lt;/strong&gt;: &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Clear Chat History&lt;/strong&gt;: 🧹 Users can clear previous chat interactions using the clear chat history button in the query section.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Clear Database&lt;/strong&gt;: 🚮 Deletes all stored PDFs and related data, effectively resetting the application’s state. This action is available in the database management section.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;PDF Usage Statistics&lt;/strong&gt;: 📈 Provides information on how frequently each PDF has been queried, viewable through the statistics dashboard.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;This example demonstrated below is based on the &amp;#39;Essays Expert&amp;#39; prompt template. The screenshot highlights how the system utilises PDF content to generate comprehensive responses at the top, while the lower section shows the output generated without PDFs, illustrating the impact of including detailed content.&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/2hta2yrgkwed1.png?width=2880&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=91ecb0a7056034b6fd1a57b19311e3f9a8703add&quot;&gt;https://preview.redd.it/2hta2yrgkwed1.png?width=2880&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=91ecb0a7056034b6fd1a57b19311e3f9a8703add&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I’m not an expert in this domain—just a big fan of its potential who’s been reading up on it. All feedback is welcome:)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/underrated-squirrel&quot;&gt; /u/underrated-squirrel &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ecuq2g/askitright_pdfs_query_fullstack_application_rag/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ecuq2g/askitright_pdfs_query_fullstack_application_rag/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1ecuq2g</id><media:thumbnail url="https://external-preview.redd.it/ZJCf56Nh_Pt7yRbmmvCFkShWeJMBCwhbny8msEM-w_4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=6e109fb1c6c0d6d93450dc68e1c2d6c9933b8b26" /><link href="https://www.reddit.com/r/LangChain/comments/1ecuq2g/askitright_pdfs_query_fullstack_application_rag/" /><updated>2024-07-26T18:04:37+00:00</updated><published>2024-07-26T18:04:37+00:00</published><title>AskItRight: PDFs Query Fullstack Application (RAG Ollama, LangChain, ChromaDB )</title></entry><entry><author><name>/u/Shingma</name><uri>https://www.reddit.com/user/Shingma</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;m playing around with Crew, Autogen and LangChain wanted to know what tools are best suited for Multi Agent applications, where to find them or if I need to build them myself. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Shingma&quot;&gt; /u/Shingma &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ecn0fq/what_tools_are_you_using_with_your_ai_agents/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ecn0fq/what_tools_are_you_using_with_your_ai_agents/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1ecn0fq</id><link href="https://www.reddit.com/r/LangChain/comments/1ecn0fq/what_tools_are_you_using_with_your_ai_agents/" /><updated>2024-07-26T12:36:31+00:00</updated><published>2024-07-26T12:36:31+00:00</published><title>What tools are you using with your AI agents?</title></entry><entry><author><name>/u/Ok_Opinion_5729</name><uri>https://www.reddit.com/user/Ok_Opinion_5729</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Please point to some reference of using RAGAS with local models&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Ok_Opinion_5729&quot;&gt; /u/Ok_Opinion_5729 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ed9rhu/how_can_i_use_ragas_without_openai_key/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ed9rhu/how_can_i_use_ragas_without_openai_key/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1ed9rhu</id><link href="https://www.reddit.com/r/LangChain/comments/1ed9rhu/how_can_i_use_ragas_without_openai_key/" /><updated>2024-07-27T06:26:26+00:00</updated><published>2024-07-27T06:26:26+00:00</published><title>How can I use RAGAS without OpenAI key?</title></entry><entry><author><name>/u/captam_morgan</name><uri>https://www.reddit.com/user/captam_morgan</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi all, &lt;/p&gt; &lt;p&gt;I’m trying to design a the graph workflow with LangGraph/LangChain. In building CRAG, what’s a good way to integrate sequential/parallel decomposition and self-query in the nodes and conditional edges. &lt;/p&gt; &lt;p&gt;For examples, let’s say there’s a question that can be broken into sequential sub-queries: &amp;quot;What is the weather of the Big Apple?&amp;quot; would first look up &amp;quot;What is the Big Apple&amp;quot; then do &amp;quot;Weather in NYC&amp;quot;.&lt;/p&gt; &lt;p&gt;I guess I can view this more like ReAct function calling: &lt;/p&gt; &lt;p&gt;Start &amp;gt; Planner &amp;gt; self-query &amp;gt; retriever &amp;gt; docs &amp;gt; grade &amp;gt; Planner &amp;gt; generate-end/self-query&lt;/p&gt; &lt;p&gt;Planner node will do both tool calling of RAG and also query decomposition/optimization/CoT.&lt;/p&gt; &lt;p&gt;Appreciate any help!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/captam_morgan&quot;&gt; /u/captam_morgan &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ecwyuk/langgraph_rag_w_cot_sequential_decomposition/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ecwyuk/langgraph_rag_w_cot_sequential_decomposition/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1ecwyuk</id><link href="https://www.reddit.com/r/LangChain/comments/1ecwyuk/langgraph_rag_w_cot_sequential_decomposition/" /><updated>2024-07-26T19:40:40+00:00</updated><published>2024-07-26T19:40:40+00:00</published><title>LangGraph RAG w/ CoT Sequential Decomposition</title></entry><entry><author><name>/u/Either-Ambassador738</name><uri>https://www.reddit.com/user/Either-Ambassador738</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;ve seen in lots of tutorials from the langchain/langraph documentation using bind_tools and bind_functions.&lt;br/&gt; But I haven&amp;#39;t yet understood the difference between them and where to use each one.&lt;br/&gt; Also, do they both work with all chat models?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Either-Ambassador738&quot;&gt; /u/Either-Ambassador738 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ed6cmm/whats_the_difference_between_bind_tools_and_bind/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ed6cmm/whats_the_difference_between_bind_tools_and_bind/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1ed6cmm</id><link href="https://www.reddit.com/r/LangChain/comments/1ed6cmm/whats_the_difference_between_bind_tools_and_bind/" /><updated>2024-07-27T03:02:13+00:00</updated><published>2024-07-27T03:02:13+00:00</published><title>What's the difference between bind_tools and bind_functions?</title></entry><entry><author><name>/u/phicreative1997</name><uri>https://www.reddit.com/user/phicreative1997</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ecp82w/building_a_human_resource_graphrag_application/&quot;&gt; &lt;img src=&quot;https://external-preview.redd.it/MM3_iXjJ5f9zLbFgI_OqW5HbRDQZypY4ocXngeaUmpI.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bf25726624e218fbb8aed206f16571b9216ef7c4&quot; alt=&quot;Building a Human Resource GraphRAG application&quot; title=&quot;Building a Human Resource GraphRAG application&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/phicreative1997&quot;&gt; /u/phicreative1997 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://medium.com/firebird-technologies/building-a-human-resource-graphrag-application-279f07cf71d6&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ecp82w/building_a_human_resource_graphrag_application/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1ecp82w</id><media:thumbnail url="https://external-preview.redd.it/MM3_iXjJ5f9zLbFgI_OqW5HbRDQZypY4ocXngeaUmpI.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=bf25726624e218fbb8aed206f16571b9216ef7c4" /><link href="https://www.reddit.com/r/LangChain/comments/1ecp82w/building_a_human_resource_graphrag_application/" /><updated>2024-07-26T14:17:47+00:00</updated><published>2024-07-26T14:17:47+00:00</published><title>Building a Human Resource GraphRAG application</title></entry><entry><author><name>/u/SaltShakerOW</name><uri>https://www.reddit.com/user/SaltShakerOW</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey guys,&lt;/p&gt; &lt;p&gt;I&amp;#39;m doing a little project to familiarize myself with langchain and gen AI as a whole, and the tldr is that I&amp;#39;m trying to create an agent that has the capability to call tools and put the result into a complete sentence at the end for the user to interpret. This seems pretty simple, but I&amp;#39;ve never come across a solution that can do both after hours of google searching (maybe I&amp;#39;m just looking in the wrong places lol). Regardless, I&amp;#39;m currently using the create_tool_calling_agent method from the langchain.agents python library, and it works great for doing the complete sentence part, but it&amp;#39;s not actually calling the tool that I have set to call and is coming up with a nonsensical answer to boot. Here is an example of what I&amp;#39;m talking about and the code I have setup:&lt;/p&gt; &lt;p&gt;The tool:&lt;/p&gt; &lt;pre&gt;&lt;code&gt; u/tool def get_weather(self, location=None) -&amp;gt; int: &amp;quot;Gets the temperature at a chosen location AND ONLY THE TEMPERATURE. The location argument is optional, and if left blank it will retrieve the user&amp;#39;s current location.&amp;quot; print(&amp;quot;get_weather called&amp;quot;) #DEBUGGING if location == None: temp = self.get_location_data() location = temp[&amp;quot;city&amp;quot;] complete_url = &amp;quot;http://api.openweathermap.org/data/2.5/weather?&amp;quot; + &amp;quot;appid=&amp;quot; + os.getenv(&amp;quot;OPENWEATHERMAP_API_KEY&amp;quot;) + &amp;quot;&amp;amp;q=&amp;quot; + location response = requests.get(complete_url) x = response.json() print(x) try: current_temperature = x[&amp;quot;main&amp;quot;][&amp;quot;temp&amp;quot;] except: return -1 return int((current_temperature - 273.15)*1.8 + 32) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The agent/LLM setup:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;class Agent: def __init__(self) -&amp;gt; None: t = Custom_Tool() self.model = ChatOllama(model=&amp;quot;llama3.1:8b&amp;quot;, temperature=0) self.prompt = ChatPromptTemplate.from_messages( [ (&amp;quot;system&amp;quot;, system_prompt), (&amp;quot;placeholder&amp;quot;, &amp;quot;{chat_history}&amp;quot;), (&amp;quot;human&amp;quot;, &amp;quot;{input}&amp;quot;), (&amp;quot;placeholder&amp;quot;, &amp;quot;{agent_scratchpad}&amp;quot;), ] ) self.tools = [t.get_weather] def invoke_agent(self, prompt: str): agent = create_tool_calling_agent(self.model, self.tools, self.prompt) agent_executor = AgentExecutor(agent=agent, tools=self.tools, verbose=True) agent_executor.invoke({&amp;quot;input&amp;quot;: prompt}) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The prompt:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;def run(prompt): a = Agent() a.invoke_agent(prompt=prompt) if __name__ == &amp;quot;__main__&amp;quot;: run(&amp;quot;What&amp;#39;s the weather in Minneapolis?&amp;quot;) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The result (it is not 42 degrees in Minneapolis in the middle of July):&lt;/p&gt; &lt;pre&gt;&lt;code&gt;&amp;gt; Entering new AgentExecutor chain... I&amp;#39;d be happy to check the current temperature in Minneapolis for you. Let me just make an API call using our `get_weather` tool... The current temperature in Minneapolis is 42°F. Would you like to know more about the weather conditions or forecast? &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;If anyone has a better way to do this or advice for me to get going in the right direction please let me know. Thanks.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/SaltShakerOW&quot;&gt; /u/SaltShakerOW &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ecvnfk/tool_calling_agent_not_actually_calling_tools/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ecvnfk/tool_calling_agent_not_actually_calling_tools/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1ecvnfk</id><link href="https://www.reddit.com/r/LangChain/comments/1ecvnfk/tool_calling_agent_not_actually_calling_tools/" /><updated>2024-07-26T18:43:50+00:00</updated><published>2024-07-26T18:43:50+00:00</published><title>Tool Calling Agent not actually calling tools</title></entry><entry><author><name>/u/Longjumping-Buddy501</name><uri>https://www.reddit.com/user/Longjumping-Buddy501</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ecoex7/gpt4omini_is_terribly_slow_today_anyone_else/&quot;&gt; &lt;img src=&quot;https://b.thumbs.redditmedia.com/9Cgucf2SuD7c0iqUpWblJ0VTLYnsAKdl8qtg9yblXio.jpg&quot; alt=&quot;GPT-4o-mini is terribly slow today. Anyone else facing this issue?&quot; title=&quot;GPT-4o-mini is terribly slow today. Anyone else facing this issue?&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;&lt;a href=&quot;https://preview.redd.it/hmxu408raved1.png?width=170&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5bbc2d537d3c183ab7e684f0457754f38a967083&quot;&gt;https://preview.redd.it/hmxu408raved1.png?width=170&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5bbc2d537d3c183ab7e684f0457754f38a967083&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/k0xf4x7raved1.png?width=170&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=725eb06885156a134e53e45e928089ffa8e5a97f&quot;&gt;https://preview.redd.it/k0xf4x7raved1.png?width=170&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=725eb06885156a134e53e45e928089ffa8e5a97f&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The latency on GPT-4o-mini is terrible today. It is taking 96 seconds and above for simple answers &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Longjumping-Buddy501&quot;&gt; /u/Longjumping-Buddy501 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ecoex7/gpt4omini_is_terribly_slow_today_anyone_else/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ecoex7/gpt4omini_is_terribly_slow_today_anyone_else/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1ecoex7</id><media:thumbnail url="https://b.thumbs.redditmedia.com/9Cgucf2SuD7c0iqUpWblJ0VTLYnsAKdl8qtg9yblXio.jpg" /><link href="https://www.reddit.com/r/LangChain/comments/1ecoex7/gpt4omini_is_terribly_slow_today_anyone_else/" /><updated>2024-07-26T13:42:47+00:00</updated><published>2024-07-26T13:42:47+00:00</published><title>GPT-4o-mini is terribly slow today. Anyone else facing this issue?</title></entry><entry><author><name>/u/remmmm_</name><uri>https://www.reddit.com/user/remmmm_</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;In my personal project, I originally plan to use rag fusion to match resumes and job descriptions (preprocessed by summarizing with llm). Right now I&amp;#39;m stuck on how to overcome limitation of context awareness in embeddings and semantic/similarity searches. For example, in query generation, one of query is focused on experience. ideally I&amp;#39;d like to produce an assessment like below&lt;/p&gt; &lt;p&gt;&amp;quot;Assessment&amp;quot;: &amp;quot;Limited experience but shows potential. Candidate has some exposure to relevant areas like recommendation systems and customer segmentation, but lacks the depth of experience required for the role.&amp;quot;&lt;/p&gt; &lt;p&gt;&amp;quot;responsibilities in Job Description&amp;quot;: &amp;quot;Experience creating and implementing machine learning techniques for recommender systems and time series analysis. Experience with customer segmentation, churn prediction, campaign optimization, and more.&amp;quot;&lt;/p&gt; &lt;p&gt;&amp;quot;experience section in Resume&amp;quot;: &amp;quot;Developed basic machine learning models for customer segmentation and product recommendation at XYZ Retail. Assisted in building a prototype recommendation engine during internship at StartupX.&amp;quot;&lt;br/&gt; My implementation of fusion rag will identify similar keywords, but fail to recognize the difference in depth and breadth of experience, and unable to infer and interpret. Any ideas on how to address this limitation of context awareness? Thanks in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/remmmm_&quot;&gt; /u/remmmm_ &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ecuavd/seeking_ideas_on_how_to_overcome_limitation_of/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ecuavd/seeking_ideas_on_how_to_overcome_limitation_of/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1ecuavd</id><link href="https://www.reddit.com/r/LangChain/comments/1ecuavd/seeking_ideas_on_how_to_overcome_limitation_of/" /><updated>2024-07-26T17:47:06+00:00</updated><published>2024-07-26T17:47:06+00:00</published><title>seeking ideas on how to overcome limitation of context awareness in similarity search and fusion rag</title></entry><entry><author><name>/u/Jorge_at_Startino</name><uri>https://www.reddit.com/user/Jorge_at_Startino</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;have you guys seen any trends or evidence that could potentially show a turn for TypeScript?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Jorge_at_Startino&quot;&gt; /u/Jorge_at_Startino &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ed9kiw/will_ts_ever_surpass_python_for_generative_ai/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ed9kiw/will_ts_ever_surpass_python_for_generative_ai/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1ed9kiw</id><link href="https://www.reddit.com/r/LangChain/comments/1ed9kiw/will_ts_ever_surpass_python_for_generative_ai/" /><updated>2024-07-27T06:13:44+00:00</updated><published>2024-07-27T06:13:44+00:00</published><title>will TS ever surpass Python for generative AI development?</title></entry><entry><author><name>/u/OnY86</name><uri>https://www.reddit.com/user/OnY86</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi, i am new to langchain and i hope somebody here can help me understand some basics.&lt;/p&gt; &lt;p&gt;I have a server serving the model via vllm openai endpoint. The model uses a ChatML template:&lt;/p&gt; &lt;p&gt;&amp;lt;|im_start|&amp;gt;system {system_message}&amp;lt;|im_end|&amp;gt; &amp;lt;|im_start|&amp;gt;user {prompt}&amp;lt;|im_end|&amp;gt; &amp;lt;|im_start|&amp;gt;assistant&lt;/p&gt; &lt;p&gt;Can langchain handle this and if so, how? Or did i combine the wrong stuff together?&lt;/p&gt; &lt;p&gt;I am asking, because my LLM is responding sometimes in a strange way. For example, i asked „how far is the moon“ and i get an endless response back. The first sentence is the right answer but all that follows after, it is total nonsense.&lt;/p&gt; &lt;p&gt;Thanks&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/OnY86&quot;&gt; /u/OnY86 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ectcut/vllm_dolphin26mistral_langchain/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ectcut/vllm_dolphin26mistral_langchain/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1ectcut</id><link href="https://www.reddit.com/r/LangChain/comments/1ectcut/vllm_dolphin26mistral_langchain/" /><updated>2024-07-26T17:08:06+00:00</updated><published>2024-07-26T17:08:06+00:00</published><title>Vllm + dolphin-2.6-mistral + langchain</title></entry><entry><author><name>/u/Substantial_Gift_861</name><uri>https://www.reddit.com/user/Substantial_Gift_861</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I plan to build a chatbot to answer those product information. But I don&amp;#39;t know which one to use, RAG or openai gpt?&lt;/p&gt; &lt;p&gt;I heard that RAG might not accurate and cant generate reply very well&lt;/p&gt; &lt;p&gt;Which one will you choose? &lt;/p&gt; &lt;p&gt;If you want to build a chatbot like that, what will you use?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Substantial_Gift_861&quot;&gt; /u/Substantial_Gift_861 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1eck671/build_a_chatbot_by_using_rag_or_openai_gpt_api/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1eck671/build_a_chatbot_by_using_rag_or_openai_gpt_api/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1eck671</id><link href="https://www.reddit.com/r/LangChain/comments/1eck671/build_a_chatbot_by_using_rag_or_openai_gpt_api/" /><updated>2024-07-26T09:51:53+00:00</updated><published>2024-07-26T09:51:53+00:00</published><title>Build a chatbot by using Rag or openai gpt API?</title></entry><entry><author><name>/u/Important_Ostrich_60</name><uri>https://www.reddit.com/user/Important_Ostrich_60</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I have developed a tool calling llm using OpenAI&amp;#39;s GPT-3.5-turbo-1106, integrated with LangSmith and LangGraph. I followed the official documentation to track token usage but encountered issues:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Final Response Metadata:&lt;/strong&gt; The final response doesn&amp;#39;t include metadata about token usage.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;openai_callback Function:&lt;/strong&gt; This method returns zero tokens used every time.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Here&amp;#39;s the documentation link: &lt;a href=&quot;https://python.langchain.com/v0.1/docs/modules/model_io/llms/token_usage_tracking/&quot;&gt;Token Usage Tracking&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Could you assist me in obtaining the token cost per API request? Although LangSmith provides token usage in their UI, I need to access this information programmatically in my application.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Important_Ostrich_60&quot;&gt; /u/Important_Ostrich_60 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ecqmk7/how_to_get_token_costs_per_request/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ecqmk7/how_to_get_token_costs_per_request/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1ecqmk7</id><link href="https://www.reddit.com/r/LangChain/comments/1ecqmk7/how_to_get_token_costs_per_request/" /><updated>2024-07-26T15:15:37+00:00</updated><published>2024-07-26T15:15:37+00:00</published><title>How to get token costs per request</title></entry><entry><author><name>/u/AImEdo</name><uri>https://www.reddit.com/user/AImEdo</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;I&amp;#39;m currently exploring the Structured Output feature in LangChain for a personal project. I’ve been following the guide here: &lt;a href=&quot;https://python.langchain.com/v0.1/docs/modules/model_io/chat/structured_output/&quot;&gt;LangChain Structured Output&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Here’s a basic example I’m working with:&lt;/p&gt; &lt;p&gt;&lt;code&gt;from langchain_core.pydantic_v1 import BaseModel, Field&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;class Joke(BaseModel):&lt;/code&gt;&lt;br/&gt; &lt;code&gt;setup: str = Field(description=&amp;quot;The setup of the joke&amp;quot;)&lt;/code&gt;&lt;br/&gt; &lt;code&gt;punchline: str = Field(description=&amp;quot;The punchline to the joke&amp;quot;)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;model = ChatOpenAI(model=&amp;quot;gpt-3.5-turbo-0125&amp;quot;, temperature=0)&lt;/code&gt;&lt;br/&gt; &lt;code&gt;structured_llm = model.with_structured_output(Joke)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;structured_llm.invoke(&amp;quot;Tell me a joke about cats&amp;quot;)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;I’ve experimented with more complex structures that include multiple fields and even nested fields. It’s important to note that not all fields necessarily come from a single chunk of text, but they could.&lt;/p&gt; &lt;p&gt;However, I’ve encountered a problem: when the model can&amp;#39;t find a value for a field, it returns the field’s description instead of leaving it blank or providing a default value. This results in unnecessary token generation.&lt;/p&gt; &lt;p&gt;I’m looking for a solution to set default values such as an empty string for string fields and &lt;code&gt;-1&lt;/code&gt; for numerical fields when the model doesn’t provide a value.&lt;/p&gt; &lt;p&gt;Has anyone else dealt with this issue? Is there a method to ensure that missing field values are replaced with desired defaults rather than the field descriptions? Any insights or suggestions would be greatly appreciated!&lt;/p&gt; &lt;p&gt;Thanks in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/AImEdo&quot;&gt; /u/AImEdo &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ecqk1q/why_does_langchains_basemodel_sometimes_output_a/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ecqk1q/why_does_langchains_basemodel_sometimes_output_a/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1ecqk1q</id><link href="https://www.reddit.com/r/LangChain/comments/1ecqk1q/why_does_langchains_basemodel_sometimes_output_a/" /><updated>2024-07-26T15:12:48+00:00</updated><published>2024-07-26T15:12:48+00:00</published><title>Why does LangChain's BaseModel sometimes output a copy of the Pydantic field description instead of the expected value?</title></entry><entry><author><name>/u/alkibijad</name><uri>https://www.reddit.com/user/alkibijad</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ecimvn/memory_leak_in_langserve_app/&quot;&gt; &lt;img src=&quot;https://b.thumbs.redditmedia.com/mZII8QzqaDGJ2KPcrpL5oTa_A2wxtAvxs5gfoG1nQKQ.jpg&quot; alt=&quot;Memory leak in Langserve app&quot; title=&quot;Memory leak in Langserve app&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;m hosting a langserve app. The app is quite simple, but there seems to be a memory leak. Any ideas on why this is happening?&lt;/p&gt; &lt;p&gt;With every new requests, the RAM increases and doesn&amp;#39;t go down: &lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/9k9j0z7ikted1.png?width=2256&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=dfe91726776cfabb5060e0da96ce4a41b858e724&quot;&gt;https://preview.redd.it/9k9j0z7ikted1.png?width=2256&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=dfe91726776cfabb5060e0da96ce4a41b858e724&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/2d0hncamkted1.png?width=1128&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=51b95e833150a6bf810ff4f3ba736ef9b91da587&quot;&gt;https://preview.redd.it/2d0hncamkted1.png?width=1128&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=51b95e833150a6bf810ff4f3ba736ef9b91da587&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The code is pretty straightforward.&lt;br/&gt; Chain definition: &lt;/p&gt; &lt;h1&gt;file: public_review.py&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;from langchain_core.output_parsers import JsonOutputParser from langchain_core.prompts import ChatPromptTemplate from langchain_openai import ChatOpenAI from app.prompts.public_review_analysis_prompt import ( PUBLIC_REVIEW_ISSUE_GENERATOR_SYSTEM_PROMPT, ) public_review_text_chain = ChatPromptTemplate.from_messages( [ ( &amp;quot;system&amp;quot;, PUBLIC_REVIEW_ISSUE_GENERATOR_SYSTEM_PROMPT, ), (&amp;quot;user&amp;quot;, &amp;quot;{text}&amp;quot;), ] ) | ChatOpenAI(model=&amp;quot;gpt-4o&amp;quot;, temperature=0.03, model_kwargs={&amp;quot;seed&amp;quot;: 13}) public_review_chain = ( | public_review_text_chain | JsonOutputParser(pydantic_object=IssueList) ) # Chain added to router and router is then added to the app from fastapi import APIRouter from langserve import add_routes from app.enrichment.aggregator import aggregator_review_chain, aggregator_text_chain from app.enrichment.public_review import public_review_chain, public_review_text_chain from app.enrichment.types import ( InputFragment, InputFragmentList ) router = APIRouter() add_routes( router, public_review_chain.with_types(input_type=InputFragmentList, output_type=IssueList), path=&amp;quot;/api/v1/public_review&amp;quot;, ) add_routes(router, public_review_text_chain, path=&amp;quot;/api/v1/public_review/text&amp;quot;) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Any ideas what could be causing the leak?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/alkibijad&quot;&gt; /u/alkibijad &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ecimvn/memory_leak_in_langserve_app/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ecimvn/memory_leak_in_langserve_app/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1ecimvn</id><media:thumbnail url="https://b.thumbs.redditmedia.com/mZII8QzqaDGJ2KPcrpL5oTa_A2wxtAvxs5gfoG1nQKQ.jpg" /><link href="https://www.reddit.com/r/LangChain/comments/1ecimvn/memory_leak_in_langserve_app/" /><updated>2024-07-26T08:03:57+00:00</updated><published>2024-07-26T08:03:57+00:00</published><title>Memory leak in Langserve app</title></entry><entry><author><name>/u/Otherwise-Patient-34</name><uri>https://www.reddit.com/user/Otherwise-Patient-34</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;ve metadata( like table structure, column details, data types) of RDBMS and other sources currently stored in MYSQL database. I want to build a simple bot that answers queries from data engineers and analytics like &amp;quot;&lt;/p&gt; &lt;p&gt;1.get me ddl of .. this table&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;what is the meaning of this column&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;show me complete column list with description of this table in postgres environment: etc.&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;How can I use Vector db along with LLM to achieve this goal.&lt;/p&gt; &lt;p&gt;I&amp;#39;m not sure how to design a scheme, whether to vectorize the name of the table alone etc.&lt;/p&gt; &lt;p&gt;N.B : Some of the tables can have 500 columns also.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Otherwise-Patient-34&quot;&gt; /u/Otherwise-Patient-34 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ec02w8/using_milvusrag_as_metadata_store/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ec02w8/using_milvusrag_as_metadata_store/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1ec02w8</id><link href="https://www.reddit.com/r/LangChain/comments/1ec02w8/using_milvusrag_as_metadata_store/" /><updated>2024-07-25T17:04:50+00:00</updated><published>2024-07-25T17:04:50+00:00</published><title>Using Milvus/RAG as metadata store</title></entry><entry><author><name>/u/qa_anaaq</name><uri>https://www.reddit.com/user/qa_anaaq</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;m labeling and taking screenshot of webpages that I then send to 4V to analyze. Basically, the labeling creates borders around html elements then I ask GPT to determine if there is a popup or if there are elements at certain places on the page. (I&amp;#39;m being a little vague due to the specific use case.) &lt;/p&gt; &lt;p&gt;What would a good approach be for prompting? I am providing reference images with explanations for each reference. Even though things are being labeled ok, I don&amp;#39;t seem to be able to prompt it well. So I&amp;#39;m wondering if one prompting strategy over another might be good. &lt;/p&gt; &lt;p&gt;Note that this flow is partially based on the WebVoyager paper that uses LangGraph, though it&amp;#39;s not web browsing. Just a single labeled page. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/qa_anaaq&quot;&gt; /u/qa_anaaq &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ec8c4e/vision_analysis/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ec8c4e/vision_analysis/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1ec8c4e</id><link href="https://www.reddit.com/r/LangChain/comments/1ec8c4e/vision_analysis/" /><updated>2024-07-25T22:45:35+00:00</updated><published>2024-07-25T22:45:35+00:00</published><title>Vision Analysis</title></entry><entry><author><name>/u/PretendVermicelli657</name><uri>https://www.reddit.com/user/PretendVermicelli657</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;m new to langchain and currently learning the official [tutorial](&lt;a href=&quot;https://python.langchain.com/v0.2/docs/tutorials/agents/&quot;&gt;https://python.langchain.com/v0.2/docs/tutorials/agents/&lt;/a&gt;). I have tried Ollama and llama.cpp, but none of them can finish the tutorial.&lt;/p&gt; &lt;p&gt;As known, Ollama doesn&amp;#39;t support bind_tools originally. With the help of OllamaFunctions in langchain_experiment package, it worked and outputed similar intermediate information but failed when generating text according to response from tools.&lt;/p&gt; &lt;p&gt;When it comes to llama.cpp, it does have bind_tools function. The problem is that it didn&amp;#39;t generate text according to response from tools.&lt;/p&gt; &lt;p&gt;So, is there a way to go through the tutorials with local llms or an example about finishing those tutorials with Ollama and llama.cpp? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/PretendVermicelli657&quot;&gt; /u/PretendVermicelli657 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ebundi/how_to_build_agent_with_local_llm/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ebundi/how_to_build_agent_with_local_llm/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1ebundi</id><link href="https://www.reddit.com/r/LangChain/comments/1ebundi/how_to_build_agent_with_local_llm/" /><updated>2024-07-25T13:17:22+00:00</updated><published>2024-07-25T13:17:22+00:00</published><title>How to build agent with local llm</title></entry><entry><author><name>/u/phan_ngt</name><uri>https://www.reddit.com/user/phan_ngt</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I use Semantic Chunker from this tutorial: &lt;a href=&quot;https://python.langchain.com/v0.2/docs/how_to/semantic-chunker/&quot;&gt;https://python.langchain.com/v0.2/docs/how_to/semantic-chunker/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;However, I met the error below. I think because my pdf has 64 pages. Too long for OpenAI to handle. What should I do? If I split page by page, I am afraid that I will lost the content between pages. Recursive Chunker seems better in this case. &lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;openai.InternalServerError: Error code: 503 - {&amp;#39;error&amp;#39;: {&amp;#39;code&amp;#39;: &amp;#39;InternalServerError&amp;#39;, &amp;#39;message&amp;#39;: &amp;#39;The service is temporarily unable to process your request. Please try again later.&amp;#39;}}&lt;/p&gt; &lt;p&gt;python-BaseException&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/phan_ngt&quot;&gt; /u/phan_ngt &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ebs25f/semanticchunker_for_very_large_text/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ebs25f/semanticchunker_for_very_large_text/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1ebs25f</id><link href="https://www.reddit.com/r/LangChain/comments/1ebs25f/semanticchunker_for_very_large_text/" /><updated>2024-07-25T11:02:03+00:00</updated><published>2024-07-25T11:02:03+00:00</published><title>SemanticChunker for very large text</title></entry><entry><author><name>/u/divinity27</name><uri>https://www.reddit.com/user/divinity27</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi I am trying to extract information from purchase orders PDFs with different formats , when conventional py libraries didn&amp;#39;t extract the data the way I wanted I resorted to Azure Gpt 4 vision model and converted the pages of my pdf as images and used the api to get back the response. The problem is in some documents it is deliberately missing clearly written information in the images , I tried tweaking the prompt as well. But not helping much. I am using pdf2image to convert to JPEGs and using 500 dpi as parameter in the convert_from_path function imported from library. Any recommendations or help would be much appreciated.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/divinity27&quot;&gt; /u/divinity27 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ebt2gl/improving_output_of_azure_gpt_4_vision_model/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ebt2gl/improving_output_of_azure_gpt_4_vision_model/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1ebt2gl</id><link href="https://www.reddit.com/r/LangChain/comments/1ebt2gl/improving_output_of_azure_gpt_4_vision_model/" /><updated>2024-07-25T11:59:16+00:00</updated><published>2024-07-25T11:59:16+00:00</published><title>Improving output of Azure Gpt 4 vision model , ignoring part of text present in image</title></entry><entry><author><name>/u/AdAway2620</name><uri>https://www.reddit.com/user/AdAway2620</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hello all, Have anyone among here done sales forecasting using LLMs ?&lt;br/&gt; For eg: I have monthly sales data of last 2 years and i want to predict the monthly sales of upcoming year.&lt;br/&gt; What would be the best way to do it ?&lt;/p&gt; &lt;p&gt;If anyone has code snippet, I would be happy to look at it.&lt;br/&gt; I welcome ML/DL approach as well but since my dataset is very low what would be the best idea ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/AdAway2620&quot;&gt; /u/AdAway2620 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ebm8ss/salesforecasting_using_ai_models_llm/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1ebm8ss/salesforecasting_using_ai_models_llm/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1ebm8ss</id><link href="https://www.reddit.com/r/LangChain/comments/1ebm8ss/salesforecasting_using_ai_models_llm/" /><updated>2024-07-25T04:42:39+00:00</updated><published>2024-07-25T04:42:39+00:00</published><title>Salesforecasting using AI models / LLM</title></entry><entry><author><name>/u/rayquaza_111</name><uri>https://www.reddit.com/user/rayquaza_111</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Here&amp;#39;s my chain, but looks something wrong. Earlier without the code of agents part, it was working well with chat history.&lt;/p&gt; &lt;p&gt;Any help is appreciated.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;def _prepare_chain(self): contextualize_q_system_prompt = ( &amp;quot;Given a chat history and the latest user question &amp;quot; &amp;quot;which might reference context in the chat history, &amp;quot; &amp;quot;formulate a standalone question which can be understood &amp;quot; &amp;quot;without the chat history. Do NOT answer the question, &amp;quot; &amp;quot;just reformulate it if needed and otherwise return it as is.&amp;quot; ) _llm = self.llm if self.tools: _llm = self.llm.bind_tools(self.tools) contextualize_q_prompt = ChatPromptTemplate.from_messages( [ (&amp;quot;system&amp;quot;, contextualize_q_system_prompt), MessagesPlaceholder(&amp;quot;chat_history&amp;quot;), (&amp;quot;human&amp;quot;, &amp;quot;{input}&amp;quot;), ] ) history_aware_retriever = create_history_aware_retriever( _llm, self.retriever, contextualize_q_prompt ) ### Answer question ### system_prompt = ( &amp;quot;{base_prompt}&amp;quot; &amp;quot;Act like a support person who loves helping customers. &amp;quot; &amp;quot;Use the following pieces of retrieved context to answer &amp;quot; &amp;quot;the question. If you don&amp;#39;t know the answer, say that you &amp;quot; &amp;quot;don&amp;#39;t know. Use three sentences maximum and keep the &amp;quot; &amp;quot;answer concise.&amp;quot; &amp;quot;\n\n&amp;quot; &amp;quot;{context}&amp;quot; ) ANSWER_PROMPT = PromptTemplate.from_template(system_prompt) qa_prompt = ChatPromptTemplate.from_messages( [ (&amp;quot;system&amp;quot;, system_prompt), MessagesPlaceholder(&amp;quot;chat_history&amp;quot;), (&amp;quot;human&amp;quot;, &amp;quot;{question}&amp;quot;), ] ) question_answer_chain = create_stuff_documents_chain(_llm, qa_prompt) retrieval_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain) _runnable = ( RunnablePassthrough.assign( agent_scratchpad=lambda x: format_to_openai_tool_messages(x[&amp;quot;intermediate_steps&amp;quot;]) ) | retrieval_chain | _llm | OpenAIToolsAgentOutputParser() ) _agent = RunnableAgent(runnable=_runnable) _output = RunnableParallel( answer=AgentExecutor(agent=_agent, tools=self.tools), sources=history_aware_retriever | self._extract_sources ) rag_chain = RunnablePassthrough.assign( input=lambda x: x[&amp;quot;question&amp;quot;]) | _output | RunnableLambda(self.log_chain) conversational_rag_chain = RunnableWithMessageHistory( rag_chain, self.get_session_history, input_messages_key=&amp;quot;question&amp;quot;, history_messages_key=&amp;quot;chat_history&amp;quot;, output_messages_key=&amp;quot;answer&amp;quot;, ) return conversational_rag_chain &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/rayquaza_111&quot;&gt; /u/rayquaza_111 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1eblbm7/not_able_to_figure_out_agents_with_chat_history/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1eblbm7/not_able_to_figure_out_agents_with_chat_history/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1eblbm7</id><link href="https://www.reddit.com/r/LangChain/comments/1eblbm7/not_able_to_figure_out_agents_with_chat_history/" /><updated>2024-07-25T03:50:18+00:00</updated><published>2024-07-25T03:50:18+00:00</published><title>Not able to figure out Agents with Chat History</title></entry></feed>