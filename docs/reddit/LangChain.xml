<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/"><category term="LangChain" label="r/LangChain"/><updated>2024-06-17T21:19:09+00:00</updated><icon>https://www.redditstatic.com/icon.png/</icon><id>/r/LangChain.rss</id><link rel="self" href="https://www.reddit.com/r/LangChain.rss" type="application/atom+xml" /><link rel="alternate" href="https://www.reddit.com/r/LangChain" type="text/html" /><subtitle>LangChain is an open-source framework and developer toolkit that helps developers get LLM applications from prototype to production. It is available for Python and Javascript at https://www.langchain.com/.</subtitle><title>LangChain</title><entry><author><name>/u/zchaarm</name><uri>https://www.reddit.com/user/zchaarm</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;A place for members of &lt;a href=&quot;/r/LangChain&quot;&gt;r/LangChain&lt;/a&gt; to chat with each other&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/zchaarm&quot;&gt; /u/zchaarm &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_10ljho9</id><link href="https://www.reddit.com/r/LangChain/comments/10ljho9/rlangchain_lounge/" /><updated>2023-01-26T04:33:05+00:00</updated><published>2023-01-26T04:33:05+00:00</published><title>r/LangChain Lounge</title></entry><entry><author><name>/u/sarthakai</name><uri>https://www.reddit.com/user/sarthakai</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Information on entities like people, institutions, etc. is often highly interconnected, and this might be the case for your data too.&lt;/p&gt; &lt;p&gt;If so, you can:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;Create a graph connecting documents which have common n-grams, using TF-IDF etc.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;During inference, search this graph to get neighbours containing common n-grams and use them in the LLMâ€™s context.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Search results from Graph RAG are more likely to give you a comprehensive view of the entity being searched and the info connected to it.&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Eg, , if doc A is selected as highly relevant, the docs containing data closely linked to doc A must be included in the context to give a full picture.&lt;/p&gt; &lt;p&gt;I spent the weekend creating a Python library which automatically creates this graph for the documents in your vectordb. It also makes it easy for you to retrieve relevant documents connected to the best matches.&lt;/p&gt; &lt;p&gt;Hereâ€™s the repo for the library: &lt;a href=&quot;https://github.com/sarthakrastogi/graph-rag/tree/main&quot;&gt;https://github.com/sarthakrastogi/graph-rag/tree/main&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/sarthakai&quot;&gt; /u/sarthakai &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1di09bu/heres_how_to_use_graph_rag_to_get_better_accuracy/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1di09bu/heres_how_to_use_graph_rag_to_get_better_accuracy/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1di09bu</id><link href="https://www.reddit.com/r/LangChain/comments/1di09bu/heres_how_to_use_graph_rag_to_get_better_accuracy/" /><updated>2024-06-17T15:03:40+00:00</updated><published>2024-06-17T15:03:40+00:00</published><title>Hereâ€™s how to use Graph RAG to get better accuracy than std RAG</title></entry><entry><author><name>/u/maniac_runner</name><uri>https://www.reddit.com/user/maniac_runner</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1di19ey/comparing_approaches_for_using_llms_for/&quot;&gt; &lt;img src=&quot;https://external-preview.redd.it/aF6AJsUDANteUHYF9N5cw_xPHzLYcy-pY8wM5iGPATI.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d3309587d649ed30160c31f656e0d9d8e74573b0&quot; alt=&quot;Comparing approaches for using LLMs for Structured Data Extraction from PDFs &quot; title=&quot;Comparing approaches for using LLMs for Structured Data Extraction from PDFs &quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/maniac_runner&quot;&gt; /u/maniac_runner &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://unstract.com/blog/comparing-approaches-for-using-llms-for-structured-data-extraction-from-pdfs/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1di19ey/comparing_approaches_for_using_llms_for/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1di19ey</id><media:thumbnail url="https://external-preview.redd.it/aF6AJsUDANteUHYF9N5cw_xPHzLYcy-pY8wM5iGPATI.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=d3309587d649ed30160c31f656e0d9d8e74573b0" /><link href="https://www.reddit.com/r/LangChain/comments/1di19ey/comparing_approaches_for_using_llms_for/" /><updated>2024-06-17T15:45:21+00:00</updated><published>2024-06-17T15:45:21+00:00</published><title>Comparing approaches for using LLMs for Structured Data Extraction from PDFs</title></entry><entry><author><name>/u/bubble_h13</name><uri>https://www.reddit.com/user/bubble_h13</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I using llm locally when i use &lt;code&gt;create_openai_tools_agent&lt;/code&gt; &lt;/p&gt; &lt;p&gt;, then i will get &lt;code&gt;TypeError: __call__() got an unexpected keyword argument &amp;#39;tools&amp;#39;&lt;/code&gt;&lt;/p&gt; &lt;p&gt;how do i fix it, my code is below:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;from langchain.tools.retriever import create_retriever_tool retriever_tool = create_retriever_tool( retriever = retriever, name = &amp;quot;retriever_tool&amp;quot;, description = &amp;quot;Retrieve the most relevant document from the database&amp;quot;, ) tools = [retriever_tool] from langchain import hub prompt = hub.pull(&amp;quot;hwchase17/openai-tools-agent&amp;quot;) from langchain.agents import AgentExecutor, create_openai_tools_agent agent = create_openai_tools_agent(llm, tools, prompt) agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True) agent_executor.invoke({&amp;quot;input&amp;quot;:&amp;quot;What can i do in this system?&amp;quot;}) &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/bubble_h13&quot;&gt; /u/bubble_h13 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1di3cfa/using_rag_agent_got_error_with_tools_parameter/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1di3cfa/using_rag_agent_got_error_with_tools_parameter/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1di3cfa</id><link href="https://www.reddit.com/r/LangChain/comments/1di3cfa/using_rag_agent_got_error_with_tools_parameter/" /><updated>2024-06-17T17:11:05+00:00</updated><published>2024-06-17T17:11:05+00:00</published><title>Using RAG Agent got error with tools parameter</title></entry><entry><author><name>/u/Hemidt</name><uri>https://www.reddit.com/user/Hemidt</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I know this might be a really stupid question, but i thought I could gain some insight, by asking it here.&lt;br/&gt; I am currently working on a simple RAG application, where I load in documents from a larger local library.&lt;br/&gt; This library consists of multiple different document types, although mostly .docx and .pdf files.&lt;br/&gt; When loading the PDF files I am currently using the PyMuPDFLoader. However it chunks the pdf into pages from the start.&lt;br/&gt; My questoin is, whether there is a way to circumvent loading a PDF file, and already chunking it into a document for each individual page. Some information is lost between the documents, as it might traverse at a page break. &lt;/p&gt; &lt;p&gt;I would like to implement the chunking strategy later on, as we are looking into implementing a costum chunking strategy. &lt;/p&gt; &lt;p&gt;Is there a smart strategy for this? &lt;/p&gt; &lt;p&gt;Thanks in advance&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Hemidt&quot;&gt; /u/Hemidt &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dhv3a5/how_to_circumvent_one_document_per_page_when/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dhv3a5/how_to_circumvent_one_document_per_page_when/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dhv3a5</id><link href="https://www.reddit.com/r/LangChain/comments/1dhv3a5/how_to_circumvent_one_document_per_page_when/" /><updated>2024-06-17T10:44:45+00:00</updated><published>2024-06-17T10:44:45+00:00</published><title>How to circumvent one document per page when loading PDF files?</title></entry><entry><author><name>/u/copp</name><uri>https://www.reddit.com/user/copp</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I am trying to use the helper method create_sql_agent to create an agent and add it in a simple graph to create a database team. I tried to see if &lt;a href=&quot;https://python.langchain.com/v0.2/docs/integrations/toolkits/sql_database/&quot;&gt;SQL tutorial&lt;/a&gt; can be repurposed.&lt;/p&gt; &lt;p&gt;The graph structure that I wanted to design is very simple. A supervisor that delegates to a SQL Team.&lt;/p&gt; &lt;p&gt;I tried following &lt;a href=&quot;https://python.langchain.com/v0.2/docs/tutorials/sql_qa/#agents&quot;&gt;https://python.langchain.com/v0.2/docs/tutorials/sql_qa/#agents&lt;/a&gt; as suggested by &lt;a href=&quot;/u/hwchase17&quot;&gt;u/hwchase17&lt;/a&gt; for one of the previous &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d4lwt0/am_i_the_only_one_who_feels_langgraph/&quot;&gt;question&lt;/a&gt;. However, it uses the AgentExecutor instead of using langgraph as Agent Executor.&lt;/p&gt; &lt;p&gt;There are examples of connecting to SQL database in the &lt;a href=&quot;https://langchain-ai.github.io/langgraph/tutorials/customer-support/customer-support/&quot;&gt;customer support chat bot tutorial&lt;/a&gt;. How we are not able to leverage the advantages that we get by using an SQL Agent namely:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Recovering from errors and regenerating query correctly.&lt;/li&gt; &lt;li&gt;Querying database multiple times.&lt;/li&gt; &lt;li&gt;Save Tokens by looking into schema etc.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;If there are any notebooks or code that you could point to that would be great.&lt;/p&gt; &lt;p&gt;I started learning LangGraph for the promise of composing and using agents. If this is not doable with critical agents like SQL Database, then that defeats the purpose. Also that AgentExecutor is deprecated, it would be great if some of these tutorials are updated to use LangGraph as AgentExecutor / ChatExecutor.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/copp&quot;&gt; /u/copp &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dhy51m/how_to_add_an_agent_in_a_langgraph_as_a_node/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dhy51m/how_to_add_an_agent_in_a_langgraph_as_a_node/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dhy51m</id><link href="https://www.reddit.com/r/LangChain/comments/1dhy51m/how_to_add_an_agent_in_a_langgraph_as_a_node/" /><updated>2024-06-17T13:31:16+00:00</updated><published>2024-06-17T13:31:16+00:00</published><title>How to add an Agent in a LangGraph as a node? Specifically agent created out of create_sql_agent</title></entry><entry><author><name>/u/mmedinajp</name><uri>https://www.reddit.com/user/mmedinajp</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Using Langchain v0.2, model is limited to gpt-3.5-turbo with Azure (environment provided by the client)&lt;/p&gt; &lt;p&gt;Hello!&lt;/p&gt; &lt;p&gt;Looking for some directions on where to read or how to proceed to solve the following situation:&lt;/p&gt; &lt;p&gt;I have very big texts that I want to summarize. In the past I had read about MapReduce with Langchain so I tried with it. I am creating chunks of texts and processing them. It works, but when something in a chunk triggers the OpenAI content filter, the summarization for the whole chunk fails. I was wondering if there is a way to keep processing the chunk in order to get a result for the parts of the chunk that didn&amp;#39;t have issues with the content filter. Otherwise, I&amp;#39;d have to implement the MapReduce myself.&lt;/p&gt; &lt;p&gt;I&amp;#39;d appreciate if you could point me to some documents, or if you could suggest how I may approach this problem.&lt;/p&gt; &lt;p&gt;Thank you very much in advance.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/mmedinajp&quot;&gt; /u/mmedinajp &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dhl76g/openai_content_filter_when_using_mapreduce/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dhl76g/openai_content_filter_when_using_mapreduce/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dhl76g</id><link href="https://www.reddit.com/r/LangChain/comments/1dhl76g/openai_content_filter_when_using_mapreduce/" /><updated>2024-06-17T00:14:11+00:00</updated><published>2024-06-17T00:14:11+00:00</published><title>OpenAI Content Filter when using MapReduce</title></entry><entry><author><name>/u/sarthakai</name><uri>https://www.reddit.com/user/sarthakai</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Most RAG apps use Dense Passage Retrieval to find relevant docs. But there are better methods:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;RAG-Token:&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;It generates each token by considering different docs and chooses the most probable token at each step. So that every part of the answer is influenced by the best possible context.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;RAG-Sequence:&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;It calculates the probability of each answer and selects the one with the highest combined probability, getting you the best possible answer based on multiple sources. Itâ€™s a lot like RAG-token but less granular.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Fusion-in-Decoder (FiD):&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;It encodes all pairs of questions and chunks in parallel and then combines these encodings before feeding them into the decoder, which generates the answer step-by-step.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Graph RAG:&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;In case your documents are highly interconnected, the links between them are probably important to generate a relevant response.&lt;/p&gt; &lt;p&gt;Search results from Graph RAG are more likely to give you a comprehensive view of the entity being searched and the info connected to it.&lt;/p&gt; &lt;p&gt;I spent the weekend creating a Python library which automatically creates this graph for the documents present in your vectordb. It also makes it easy for you to retrieve relevant documents connected to the best matches.&lt;/p&gt; &lt;p&gt;Currently testing the library on medical documents to gauge its performance.&lt;/p&gt; &lt;p&gt;Sharing version 0.1 tomorrow! You can follow my social media to stay tuned: &lt;a href=&quot;https://linktr.ee/sarthakrastogi&quot;&gt;https://linktr.ee/sarthakrastogi&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/sarthakai&quot;&gt; /u/sarthakai &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dh79xx/suggesting_which_rag_method_will_work_best_for/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dh79xx/suggesting_which_rag_method_will_work_best_for/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dh79xx</id><link href="https://www.reddit.com/r/LangChain/comments/1dh79xx/suggesting_which_rag_method_will_work_best_for/" /><updated>2024-06-16T13:10:38+00:00</updated><published>2024-06-16T13:10:38+00:00</published><title>Suggesting which RAG method will work best for you, based on your use case ðŸ”ŽðŸ“‘</title></entry><entry><author><name>/u/aviation_expert</name><uri>https://www.reddit.com/user/aviation_expert</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I have a use case where I generate a json output. The json is sometimes so large that it gets over the output range capability of my llm, rendering my structured output not parseable. What method you guys apply when faced with an incomplete Structured output?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/aviation_expert&quot;&gt; /u/aviation_expert &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dhe1es/dealing_with_incomplete_structured_output/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dhe1es/dealing_with_incomplete_structured_output/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dhe1es</id><link href="https://www.reddit.com/r/LangChain/comments/1dhe1es/dealing_with_incomplete_structured_output/" /><updated>2024-06-16T18:34:11+00:00</updated><published>2024-06-16T18:34:11+00:00</published><title>Dealing with Incomplete Structured Output?</title></entry><entry><author><name>/u/cotimbo</name><uri>https://www.reddit.com/user/cotimbo</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dh5qr2/determinism_control/&quot;&gt; &lt;img src=&quot;https://a.thumbs.redditmedia.com/1AxizTcDr5jabPDa8ybjYV6209DIdcL_hFiPr3S4oG0.jpg&quot; alt=&quot;Determinism control&quot; title=&quot;Determinism control&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Im trying to get my workflow to be accurate and help me give the same response every time.&lt;/p&gt; &lt;p&gt;I have temp set to zero. base prompt bossing the model around to be &amp;#39;deterministic&amp;#39; but you can see, i still have wildly different outputs each time thing thing runs.&lt;/p&gt; &lt;p&gt;any advice on getting this to be more accurate?&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/ibu2ivj68x6d1.png?width=1908&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=47bd31b849fa690dc12e11cc9604702587aae6fa&quot;&gt;https://preview.redd.it/ibu2ivj68x6d1.png?width=1908&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=47bd31b849fa690dc12e11cc9604702587aae6fa&lt;/a&gt;&lt;/p&gt; &lt;p&gt;base prompt: You are a deterministic GPT model designed to analyze a list of transactions in a SQL table. Your task is to provide the same response every time you are asked the same question. Follow these guidelines to ensure determinism:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Always follow the exact format provided in the examples below.&lt;/li&gt; &lt;li&gt;Provide responses based strictly on the information available in the given table.&lt;/li&gt; &lt;li&gt;Do not include any additional or inferred information.&lt;/li&gt; &lt;li&gt;Ensure the order of transactions is consistent based on the primary key or specified order.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Note: the actual correct answer is 106 items. the SQL has 2,000 lines on it&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/cotimbo&quot;&gt; /u/cotimbo &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dh5qr2/determinism_control/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dh5qr2/determinism_control/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1dh5qr2</id><media:thumbnail url="https://a.thumbs.redditmedia.com/1AxizTcDr5jabPDa8ybjYV6209DIdcL_hFiPr3S4oG0.jpg" /><link href="https://www.reddit.com/r/LangChain/comments/1dh5qr2/determinism_control/" /><updated>2024-06-16T11:40:29+00:00</updated><published>2024-06-16T11:40:29+00:00</published><title>Determinism control</title></entry><entry><author><name>/u/HomunMage</name><uri>https://www.reddit.com/user/HomunMage</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Because langchain/langgraph example and tutorial is sxcking, I beleieve many people agree that. such &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1d4lwt0/am_i_the_only_one_who_feels_langgraph/&quot;&gt;Am I the only one who feels LangGraph documentation and tutorials by lanfchain absolutely sxck?&lt;/a&gt;&lt;/p&gt; &lt;p&gt;for example, all examples are openai related llm interface and hard to convert to local such ollama.&lt;br/&gt; This makes me even a &lt;a href=&quot;https://github.com/HomunMage/AI_Agents/blob/main/LangChain/Hello%20World.py&quot;&gt;simple hello world&lt;/a&gt; need hours to coding it.&lt;/p&gt; &lt;p&gt;That is why I had crate a &lt;a href=&quot;https://github.com/HomunMage/CrewAI-GUI&quot;&gt;GUI for CrewAI&lt;/a&gt; , not a gui for langchain or langgraph. But I think learning langgraph still important.&lt;/p&gt; &lt;p&gt;I want to find or build a langgraph learning group. And also want to build a LangGraph-GUI&lt;/p&gt; &lt;p&gt;current discord server: &lt;a href=&quot;https://discord.gg/SUTerWEJ&quot;&gt;https://discord.gg/SUTerWEJ&lt;/a&gt;&lt;/p&gt; &lt;p&gt;give me your github, i will add you to &lt;a href=&quot;https://github.com/LangGraph-GUI&quot;&gt;https://github.com/LangGraph-GUI&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/HomunMage&quot;&gt; /u/HomunMage &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dgxdvq/any_learning_group_for_langgraph/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dgxdvq/any_learning_group_for_langgraph/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dgxdvq</id><link href="https://www.reddit.com/r/LangChain/comments/1dgxdvq/any_learning_group_for_langgraph/" /><updated>2024-06-16T02:09:17+00:00</updated><published>2024-06-16T02:09:17+00:00</published><title>Any learning Group for LangGraph?</title></entry><entry><author><name>/u/filet_mign0n</name><uri>https://www.reddit.com/user/filet_mign0n</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/filet_mign0n&quot;&gt; /u/filet_mign0n &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;/r/LLMDevs/comments/1dgokij/any_agent_marketplace_worth_looking_into_these/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dh2amw/any_agent_marketplace_worth_looking_into_these/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dh2amw</id><link href="https://www.reddit.com/r/LangChain/comments/1dh2amw/any_agent_marketplace_worth_looking_into_these/" /><updated>2024-06-16T07:26:50+00:00</updated><published>2024-06-16T07:26:50+00:00</published><title>Any agent marketplace worth looking into these days?</title></entry><entry><author><name>/u/victorevolves</name><uri>https://www.reddit.com/user/victorevolves</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi there! &lt;/p&gt; &lt;p&gt;I am trying to query an api with its auth value as its get parameter&lt;br/&gt; &lt;a href=&quot;https://test.com/?key=apikey&quot;&gt;https://test.com/?key=apikey&lt;/a&gt;&lt;br/&gt; How do I tell the Langchain agent to pass an additional url param? (in this case, key) in every requests?&lt;/p&gt; &lt;p&gt;This is how it is executed atm. Thanks in advance &amp;lt;3&lt;/p&gt; &lt;pre&gt;&lt;code&gt;model = Chat(model=&amp;quot;claude-3-haiku-20240307&amp;quot;, api_key=CLAUDE_API_KEY) weather_agent = planner.create_openapi_agent( MY_OPENAPI_SPEC, RequestsWrapper(), llm=model, allow_dangerous_requests=True ) &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/victorevolves&quot;&gt; /u/victorevolves &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dh3fq8/passing_api_key_via_url_params/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dh3fq8/passing_api_key_via_url_params/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dh3fq8</id><link href="https://www.reddit.com/r/LangChain/comments/1dh3fq8/passing_api_key_via_url_params/" /><updated>2024-06-16T08:53:47+00:00</updated><published>2024-06-16T08:53:47+00:00</published><title>Passing API Key via URL Params</title></entry><entry><author><name>/u/Nimitzxz</name><uri>https://www.reddit.com/user/Nimitzxz</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;I&amp;#39;ve been working on a project that aims to enhance job applications using AI. It&amp;#39;s called &lt;a href=&quot;https://github.com/DAVEinside/GenAI_Job_Fit&quot;&gt;GenAI_Job_Fit&lt;/a&gt;, and I would love for you all to check it out. I took inspiration from the Agent-Supervisor example notebook provided.&lt;/p&gt; &lt;p&gt;The system leverages AI to analyze job descriptions and tailor resumes to match job requirements, increasing the chances of getting noticed by recruiters and ATS (Applicant Tracking Systems). Here are some key features:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Automated resume tailoring&lt;/li&gt; &lt;li&gt;Keyword optimization&lt;/li&gt; &lt;li&gt;Compatibility scoring between job descriptions and resumes&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I&amp;#39;d really appreciate it if you could take a look and let me know what you think. I&amp;#39;m particularly interested in any suggestions for improvements or additional features that could make the tool even more useful.&lt;/p&gt; &lt;p&gt;Feel free to fork the repo, open issues, or submit pull requests. Your feedback will be invaluable in making this project better!&lt;/p&gt; &lt;p&gt;Repo Link : &lt;a href=&quot;https://github.com/DAVEinside/GenAI_Job_Fit&quot;&gt;https://github.com/DAVEinside/GenAI_Job_Fit&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Thank you!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Nimitzxz&quot;&gt; /u/Nimitzxz &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dgnmjt/aidriven_job_application_enhancement_system/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dgnmjt/aidriven_job_application_enhancement_system/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dgnmjt</id><link href="https://www.reddit.com/r/LangChain/comments/1dgnmjt/aidriven_job_application_enhancement_system/" /><updated>2024-06-15T17:57:17+00:00</updated><published>2024-06-15T17:57:17+00:00</published><title>AI-Driven Job Application Enhancement System - Seeking Feedback and Suggestions!</title></entry><entry><author><name>/u/darkziosj</name><uri>https://www.reddit.com/user/darkziosj</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hello! Im using the APIchain i use it to get data from an api endpoint, the problem is that the api returns a largue amount of data in json format (i need all the data that the api returns), i will then use a agent to ask questions about that data but since is so massive there&amp;#39;s problems like the token amount or time it takes to analyze, can anyone giveme some tips about what can i do to better the performance or what to do to solve this kind of problem? thanks alot!!!&lt;/p&gt; &lt;p&gt;this is the apiChain im using:&lt;/p&gt; &lt;p&gt;llm = OpenAI(temperature=0)&lt;br/&gt; chain = APIChain.from_llm_and_api_docs(&lt;br/&gt; llm,&lt;br/&gt; open_meteo_docs.OPEN_METEO_DOCS,&lt;br/&gt; verbose=True,&lt;br/&gt; limit_to_domains=[&amp;quot;&lt;a href=&quot;https://api.open-meteo.com/%22%5C&quot;&gt;https://api.open-meteo.com/&amp;quot;\&lt;/a&gt;],&lt;br/&gt; )&lt;br/&gt; chain.run(&lt;br/&gt; &amp;quot;What is the weather like right now in Munich, Germany in degrees Fahrenheit?&amp;quot;&lt;br/&gt; )&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/darkziosj&quot;&gt; /u/darkziosj &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dgm1ij/how_to_work_with_large_data_that_is_returned_from/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dgm1ij/how_to_work_with_large_data_that_is_returned_from/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dgm1ij</id><link href="https://www.reddit.com/r/LangChain/comments/1dgm1ij/how_to_work_with_large_data_that_is_returned_from/" /><updated>2024-06-15T16:42:29+00:00</updated><published>2024-06-15T16:42:29+00:00</published><title>How to work with large data that is returned from an api?</title></entry><entry><author><name>/u/Perfect_Manner8494</name><uri>https://www.reddit.com/user/Perfect_Manner8494</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;i used chromadb with langchain to create embeddings. i used persistent_directory to save those locally and it did but now i am not able to load them. these are the codes&lt;/p&gt; &lt;h1&gt;saving embeddings&lt;/h1&gt; &lt;p&gt;vector_storage=Chroma.from_documents(split,OllamaEmbeddings(model=&amp;quot;nomic-embed-text&amp;quot;), persist_directory=&amp;quot;vector_store&amp;quot;,collection_name=&amp;quot;qna_embeddings&amp;quot;)&lt;/p&gt; &lt;h1&gt;loading embeddings&lt;/h1&gt; &lt;p&gt;vector_store2=Chroma(persist_directory=&amp;quot;vector_store&amp;quot;,embedding_function=OllamaEmbeddings(model=&amp;quot;nomic-embed-text&amp;quot;))&lt;/p&gt; &lt;p&gt;to check i printed the following and it gives 0 as output&lt;/p&gt; &lt;p&gt;print(vector_store2._collection.count())&lt;/p&gt; &lt;p&gt;pls help me &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Perfect_Manner8494&quot;&gt; /u/Perfect_Manner8494 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dgpc6w/save_and_load_embeddings/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dgpc6w/save_and_load_embeddings/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dgpc6w</id><link href="https://www.reddit.com/r/LangChain/comments/1dgpc6w/save_and_load_embeddings/" /><updated>2024-06-15T19:17:27+00:00</updated><published>2024-06-15T19:17:27+00:00</published><title>save and load embeddings</title></entry><entry><author><name>/u/phicreative1997</name><uri>https://www.reddit.com/user/phicreative1997</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dgn8l5/improving_performance_for_data_visualization_ai/&quot;&gt; &lt;img src=&quot;https://external-preview.redd.it/Q6J3m0iBOylgxJPGbdH3ZhbDZVBtgunJRGQdTd5E1bw.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=84ec9c09ce6ecb6530e7a84a850f1b5278517368&quot; alt=&quot;Improving Performance for Data Visualization AI Agent&quot; title=&quot;Improving Performance for Data Visualization AI Agent&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/phicreative1997&quot;&gt; /u/phicreative1997 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://medium.com/firebird-technologies/improving-performance-for-data-visualization-ai-agent-d677ccb71e81&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dgn8l5/improving_performance_for_data_visualization_ai/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1dgn8l5</id><media:thumbnail url="https://external-preview.redd.it/Q6J3m0iBOylgxJPGbdH3ZhbDZVBtgunJRGQdTd5E1bw.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=84ec9c09ce6ecb6530e7a84a850f1b5278517368" /><link href="https://www.reddit.com/r/LangChain/comments/1dgn8l5/improving_performance_for_data_visualization_ai/" /><updated>2024-06-15T17:38:58+00:00</updated><published>2024-06-15T17:38:58+00:00</published><title>Improving Performance for Data Visualization AI Agent</title></entry><entry><author><name>/u/sarthakai</name><uri>https://www.reddit.com/user/sarthakai</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;First, how it works:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;Memory Tuning fine-tunes millions of LoRA adapters (memory experts) on any open-source LLM to ensure accurate fact recall.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;During inference, the model retrieves and integrates the most relevant experts, (a lot like information retrieval). This gives much high accuracy and reduced hallucinations.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;This approach maintains the model&amp;#39;s ability to generalise â€” while at the same time focusing on zero error for specified facts.&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Why is this better than RAG?&lt;/p&gt; &lt;p&gt;RAG shifts probabilities without eliminating errors â€” while Memory Tuning fully corrects inaccuracies.&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://www.linkedin.com/company/lamini-ai/&quot;&gt;Lamini&lt;/a&gt; released their Memory Tuning solution for enterprises with case studies showing amazing accuracy boosts for text-to-sql, labelling, and even recommendation tasks.&lt;/p&gt; &lt;p&gt;Paper: &lt;a href=&quot;https://github.com/lamini-ai/Lamini-Memory-Tuning/blob/main/research-paper.pdf&quot;&gt;https://github.com/lamini-ai/Lamini-Memory-Tuning/blob/main/research-paper.pdf&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I share high quality AI updates and tutorials daily on my LinkedIn: &lt;a href=&quot;https://www.linkedin.com/in/sarthakrastogi/&quot;&gt;https://www.linkedin.com/in/sarthakrastogi/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;If you like this post and want to stay updated on latest AI research, you can check out: &lt;a href=&quot;https://linktr.ee/sarthakrastogi&quot;&gt;https://linktr.ee/sarthakrastogi&lt;/a&gt;.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/sarthakai&quot;&gt; /u/sarthakai &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dgi0vj/whats_memory_tuning_and_how_does_it_give_higher/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dgi0vj/whats_memory_tuning_and_how_does_it_give_higher/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dgi0vj</id><link href="https://www.reddit.com/r/LangChain/comments/1dgi0vj/whats_memory_tuning_and_how_does_it_give_higher/" /><updated>2024-06-15T13:30:01+00:00</updated><published>2024-06-15T13:30:01+00:00</published><title>Whatâ€™s Memory Tuning and how does it give higher accuracy + speed than RAG and prompting?</title></entry><entry><author><name>/u/Sweaty-Minimum5423</name><uri>https://www.reddit.com/user/Sweaty-Minimum5423</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hello all, OpenAI assistant should support streaming. But I am not sure why the current OpenAIAssistantV2Runnable do not supports it. Is there a solution to this?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Sweaty-Minimum5423&quot;&gt; /u/Sweaty-Minimum5423 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dgfsmg/streaming_of_openaiassistant_v2/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dgfsmg/streaming_of_openaiassistant_v2/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dgfsmg</id><link href="https://www.reddit.com/r/LangChain/comments/1dgfsmg/streaming_of_openaiassistant_v2/" /><updated>2024-06-15T11:23:04+00:00</updated><published>2024-06-15T11:23:04+00:00</published><title>Streaming of OpenAIAssistant v2</title></entry><entry><author><name>/u/filet_mign0n</name><uri>https://www.reddit.com/user/filet_mign0n</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Wondering if anyone here has dealt with passing private information from end user inputs to your LLM, later to interact with an external API? I&amp;#39;m not talking about authentication data per se, just private information (e.g PII) people wouldn&amp;#39;t normally want to share on the internet.&lt;br/&gt; What solution have you come up with to ensure some privacy for your users?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/filet_mign0n&quot;&gt; /u/filet_mign0n &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dfyz50/how_to_securely_pass_private_data/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dfyz50/how_to_securely_pass_private_data/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dfyz50</id><link href="https://www.reddit.com/r/LangChain/comments/1dfyz50/how_to_securely_pass_private_data/" /><updated>2024-06-14T19:23:46+00:00</updated><published>2024-06-14T19:23:46+00:00</published><title>How to securely pass private data?</title></entry><entry><author><name>/u/RaeudigerRaffi</name><uri>https://www.reddit.com/user/RaeudigerRaffi</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;m excited to share an updated open-source resource weâ€™ve been working onâ€”an improved version of the Spider dataset originally published by Yale University for Text2SQL tasks. You can check it out here: &lt;a href=&quot;https://huggingface.co/datasets/RaffaSch121/fixed_spider&quot;&gt;https://huggingface.co/datasets/RaffaSch121/fixed_spider&lt;/a&gt;&lt;/p&gt; &lt;p&gt;During our own model training at &lt;a href=&quot;http://www.turbular.com&quot;&gt;Turbular&lt;/a&gt; we identified several issues in the original dataset. To help the community and give back, we decided to address these problems and release a corrected version. We hope this enhanced dataset will benefit everyone working on Text2SQL and similar projects.&lt;/p&gt; &lt;p&gt;Feel free to download, experiment, and contribute back if you find ways to make it even better!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/RaeudigerRaffi&quot;&gt; /u/RaeudigerRaffi &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dfsdbw/improved_text2sql_dataset_now_available_on/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dfsdbw/improved_text2sql_dataset_now_available_on/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dfsdbw</id><link href="https://www.reddit.com/r/LangChain/comments/1dfsdbw/improved_text2sql_dataset_now_available_on/" /><updated>2024-06-14T14:37:24+00:00</updated><published>2024-06-14T14:37:24+00:00</published><title>Improved Text2SQL Dataset Now Available on Huggingface!</title></entry><entry><author><name>/u/FunInformation2332</name><uri>https://www.reddit.com/user/FunInformation2332</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dfxwga/evaluating_with_ragas/&quot;&gt; &lt;img src=&quot;https://a.thumbs.redditmedia.com/XCOcROHXeB7lrz95uNOAiGKakHooUXoHIPXAQ38I2n0.jpg&quot; alt=&quot;Evaluating with Ragas&quot; title=&quot;Evaluating with Ragas&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;ve finished my rag job, and performed a evaluation on my rag. results given below&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/pt0khqy10l6d1.png?width=1280&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4979a08f0e648937407d23feeb494f02a8e793ba&quot;&gt;ragas output&lt;/a&gt;&lt;/p&gt; &lt;p&gt;context_precision is better than good but why the other metrics sucks and how to improve them?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/FunInformation2332&quot;&gt; /u/FunInformation2332 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dfxwga/evaluating_with_ragas/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dfxwga/evaluating_with_ragas/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1dfxwga</id><media:thumbnail url="https://a.thumbs.redditmedia.com/XCOcROHXeB7lrz95uNOAiGKakHooUXoHIPXAQ38I2n0.jpg" /><link href="https://www.reddit.com/r/LangChain/comments/1dfxwga/evaluating_with_ragas/" /><updated>2024-06-14T18:36:01+00:00</updated><published>2024-06-14T18:36:01+00:00</published><title>Evaluating with Ragas</title></entry><entry><author><name>/u/UnderstandLingAI</name><uri>https://www.reddit.com/user/UnderstandLingAI</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;We are in early stages of developing our project so keen feedback. RAG Me Up is a robust layer on top of Langchain designed to make RAG easy and also not prone to simple issues like document re-retrieval, performance for rephrasind and perhaps most importantly: make Langchain work well with Instruct/Chat models&amp;#39; templates.&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/AI-Commandos/RAGMeUp&quot;&gt;https://github.com/AI-Commandos/RAGMeUp &lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/UnderstandLingAI&quot;&gt; /u/UnderstandLingAI &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dfx2di/rag_me_up_rag_for_chat_w_langchain/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dfx2di/rag_me_up_rag_for_chat_w_langchain/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dfx2di</id><link href="https://www.reddit.com/r/LangChain/comments/1dfx2di/rag_me_up_rag_for_chat_w_langchain/" /><updated>2024-06-14T18:00:18+00:00</updated><published>2024-06-14T18:00:18+00:00</published><title>RAG Me Up - RAG for chat /w Langchain</title></entry><entry><author><name>/u/MoronSlayer42</name><uri>https://www.reddit.com/user/MoronSlayer42</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I have implementing streaming with a chain based runnable which gives token by token output ( word by word), making UI similar to how ChatGPT has its UI. But while implementing the same with an Agent based runnable I see that it gives 3 outputs in order, actions, steps and, output which contains answer. All three come as a whole, one after the other, not word by word.&lt;/p&gt; &lt;p&gt;I want to get word by word streaming for the agent&amp;#39;s final answer.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/MoronSlayer42&quot;&gt; /u/MoronSlayer42 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dfsv2t/streaming_with_agents/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dfsv2t/streaming_with_agents/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dfsv2t</id><link href="https://www.reddit.com/r/LangChain/comments/1dfsv2t/streaming_with_agents/" /><updated>2024-06-14T14:59:04+00:00</updated><published>2024-06-14T14:59:04+00:00</published><title>Streaming with agents</title></entry><entry><author><name>/u/ANil1729</name><uri>https://www.reddit.com/user/ANil1729</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I have written an article on how to create a Text to Video AI generator which generates video from a topic by collecting relevant stock videos and stitching them together. &lt;/p&gt; &lt;p&gt;The code is completely open-source and uses free to use tools to generate videos&lt;/p&gt; &lt;p&gt;Link to article :- &lt;a href=&quot;https://medium.com/@anilmatcha/text-to-video-ai-how-to-create-videos-for-free-a-complete-guide-a25c91de50b8&quot;&gt;https://medium.com/@anilmatcha/text-to-video-ai-how-to-create-videos-for-free-a-complete-guide-a25c91de50b8&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/ANil1729&quot;&gt; /u/ANil1729 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dfsc15/a_tutorial_on_creating_video_from_text_using_ai/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dfsc15/a_tutorial_on_creating_video_from_text_using_ai/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dfsc15</id><link href="https://www.reddit.com/r/LangChain/comments/1dfsc15/a_tutorial_on_creating_video_from_text_using_ai/" /><updated>2024-06-14T14:35:53+00:00</updated><published>2024-06-14T14:35:53+00:00</published><title>A tutorial on creating video from text using AI</title></entry><entry><author><name>/u/profsartor</name><uri>https://www.reddit.com/user/profsartor</uri></author><category term="LangChain" label="r/LangChain"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;As the title reads, I&amp;#39;m building a side project to chat with my google calendar + assignments from Canvas (learning management system). I&amp;#39;m using GCP to practice working with the cloud. &lt;/p&gt; &lt;p&gt;As of April 2024, Cloud SQL for MySQL now supports vector embeddings. Essentially, I have all of my coursework and assignments in an events table. At first I embedded at the row level but this lost the understanding of columns. Now, I have a new column that is JSON representation of all the relevant columns for my eventual retrieval (event_title, start_time, end_time, tag (Assignment, Discussion, Quiz, Study Times, Personal Events)). In a new column, I&amp;#39;ve successfully embedded all of these JSON&amp;#39;s. What I&amp;#39;ve described above is pretty much the extent of what I&amp;#39;ve done. &lt;/p&gt; &lt;p&gt;My end goal is to develop a streamlit UI to query this vector column in my SQL database. I have a few different paths I can go down, but I&amp;#39;m intentionally keeping this at a high level to hear diverse responses. &lt;/p&gt; &lt;p&gt;Any advice? All thoughts are greatly appreciated. &lt;/p&gt; &lt;p&gt;Cheers&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/profsartor&quot;&gt; /u/profsartor &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dfsjwl/newbie_seeking_advice_on_side_project_chat_with/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LangChain/comments/1dfsjwl/newbie_seeking_advice_on_side_project_chat_with/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1dfsjwl</id><link href="https://www.reddit.com/r/LangChain/comments/1dfsjwl/newbie_seeking_advice_on_side_project_chat_with/" /><updated>2024-06-14T14:45:29+00:00</updated><published>2024-06-14T14:45:29+00:00</published><title>Newbie Seeking Advice on Side Project - Chat with Calendar</title></entry></feed>