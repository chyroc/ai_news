<?xml version="1.0" encoding="UTF-8"?><rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title>Twitter @Google AI</title><link>https://x.com/GoogleAI</link><atom:link href="http://rsshub.app/twitter/user/GoogleAI" rel="self" type="application/rss+xml"></atom:link><description>Google AI is focused on bringing the benefits of AI to everyone. In conducting and applying our research, we advance the state-of-the-art in many domains. - Made with love by RSSHub(https://github.com/DIYgod/RSSHub)</description><generator>RSSHub</generator><webMaster>i@diygod.me (DIYgod)</webMaster><language>en</language><image><url>https://pbs.twimg.com/profile_images/993649592422907904/yD7LkqU2.jpg</url><title>Twitter @Google AI</title><link>https://x.com/GoogleAI</link></image><lastBuildDate>Fri, 21 Jun 2024 00:55:50 GMT</lastBuildDate><ttl>180</ttl><item><title>Congratulations to the authors of the “Rich Human Feedback for Text-to-Image Generation” paper, which received the #CVPR2024 Best Paper Award. Check...</title><description>Congratulations to the authors of the “Rich Human Feedback for Text-to-Image Generation” paper, which received the #CVPR2024 Best Paper Award. Check out the paper at: https://arxiv.org/pdf/2312.10240&lt;br&gt;&lt;img style=&quot;&quot; src=&quot;https://pbs.twimg.com/media/GQjuHV_akAEgHd-?format=jpg&amp;amp;name=orig&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;br&gt;&lt;img style=&quot;&quot; src=&quot;https://pbs.twimg.com/media/GQjuHV_akAAXh7-?format=jpg&amp;amp;name=orig&quot; referrerpolicy=&quot;no-referrer&quot;&gt;</description><link>https://x.com/GoogleAI/status/1803953513040576763</link><guid isPermaLink="false">https://twitter.com/GoogleAI/status/1803953513040576763</guid><pubDate>Fri, 21 Jun 2024 00:50:08 GMT</pubDate><author>Google AI</author><category>CVPR2024</category></item><item><title>Congratulations to @zhengqi_li, Richard Tucker, @Jimantha, and @holynski_. Their paper “Generative Image Dynamics” received the #CVPR2024 Best Paper...</title><description>Congratulations to @zhengqi_li, Richard Tucker, @Jimantha, and @holynski_. Their paper “Generative Image Dynamics” received the #CVPR2024 Best Paper Award. Read the paper: https://arxiv.org/pdf/2309.07906&lt;br&gt;&lt;img style=&quot;&quot; src=&quot;https://pbs.twimg.com/media/GQjry54aMAA58BU?format=jpg&amp;amp;name=orig&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;br&gt;&lt;img style=&quot;&quot; src=&quot;https://pbs.twimg.com/media/GQjsDo-akAA5t_q?format=jpg&amp;amp;name=orig&quot; referrerpolicy=&quot;no-referrer&quot;&gt;</description><link>https://x.com/GoogleAI/status/1803951301916790921</link><guid isPermaLink="false">https://twitter.com/GoogleAI/status/1803951301916790921</guid><pubDate>Fri, 21 Jun 2024 00:41:20 GMT</pubDate><author>Google AI</author><category>CVPR2024</category></item><item><title>At 3:45 PM today, the #CVPR2024 Google booth will host @hbXNov and @YonatanBitton for a talk on how VideoCon provides a framework to curate high-quali...</title><description>At 3:45 PM today, the #CVPR2024 Google booth will host @hbXNov and @YonatanBitton for a talk on how VideoCon provides a framework to curate high-quality video-text data – enabling video-centric learning, and comprehensive coverage of semantic variations.&lt;br&gt;&lt;img style=&quot;&quot; src=&quot;https://pbs.twimg.com/media/GQjC_yaakAIJRB1?format=jpg&amp;amp;name=orig&quot; referrerpolicy=&quot;no-referrer&quot;&gt;</description><link>https://x.com/GoogleAI/status/1803906101395755198</link><guid isPermaLink="false">https://twitter.com/GoogleAI/status/1803906101395755198</guid><pubDate>Thu, 20 Jun 2024 21:41:44 GMT</pubDate><author>Google AI</author><category>CVPR2024</category></item><item><title>Stop by the #CVPR2024 Google booth today at 2:30 PM to learn about PaliGemma, a lightweight open vision-language model inspired by PaLI-3. Don’t miss...</title><description>Stop by the #CVPR2024 Google booth today at 2:30 PM to learn about PaliGemma, a lightweight open vision-language model inspired by PaLI-3. Don’t miss an opportunity to see an interactive demo that showcases PaliGemma’s capabilities.&lt;br&gt;&lt;img style=&quot;&quot; src=&quot;https://pbs.twimg.com/media/GQiyRZBagAA7VWs?format=jpg&amp;amp;name=orig&quot; referrerpolicy=&quot;no-referrer&quot;&gt;</description><link>https://x.com/GoogleAI/status/1803887683007320311</link><guid isPermaLink="false">https://twitter.com/GoogleAI/status/1803887683007320311</guid><pubDate>Thu, 20 Jun 2024 20:28:32 GMT</pubDate><author>Google AI</author><category>CVPR2024</category></item><item><title>Today at 1:30 PM, stop by the #CVPR2024 Google booth, where @jparkerholder &amp; @YugeTen will present Genie, a foundation model trained exclusively from ...</title><description>Today at 1:30 PM, stop by the #CVPR2024 Google booth, where @jparkerholder &amp;amp; @YugeTen will present Genie, a foundation model trained exclusively from Internet videos that can generate an endless variety of action-controllable 2D worlds given image prompts.&lt;br&gt;&lt;video src=&quot;https://video.twimg.com/ext_tw_video/1803870363618807810/pu/vid/avc1/640x360/C-KBZwF4Z4G1S8i0.mp4?tag=12&quot; controls=&quot;controls&quot; poster=&quot;https://pbs.twimg.com/ext_tw_video_thumb/1803870363618807810/pu/img/fSrRnMULLmbucNmo.jpg&quot;&gt;&lt;/video&gt;</description><link>https://x.com/GoogleAI/status/1803870486692270431</link><guid isPermaLink="false">https://twitter.com/GoogleAI/status/1803870486692270431</guid><pubDate>Thu, 20 Jun 2024 19:20:12 GMT</pubDate><author>Google AI</author><category>CVPR2024</category></item><item><title>Visit the #CVPR2024 Google booth today at 12:30 PM to hear @natanielruizg describe how HyperDreamBooth can take a single input image and personalize a...</title><description>Visit the #CVPR2024 Google booth today at 12:30 PM to hear @natanielruizg describe how HyperDreamBooth can take a single input image and personalize a text-to-image diffusion model 25x faster than DreamBooth by leveraging HyperNetworks.&lt;br&gt;&lt;img style=&quot;&quot; src=&quot;https://pbs.twimg.com/media/GQiS2j9akAM8gTF?format=png&amp;amp;name=orig&quot; referrerpolicy=&quot;no-referrer&quot;&gt;</description><link>https://x.com/GoogleAI/status/1803853209653661920</link><guid isPermaLink="false">https://twitter.com/GoogleAI/status/1803853209653661920</guid><pubDate>Thu, 20 Jun 2024 18:11:33 GMT</pubDate><author>Google AI</author><category>CVPR2024</category></item><item><title>At @CVPR and want to learn about research careers at Google? Visit the #CVPR2024 Google booth today at 10 AM to meet Google Research Recruiter, Rachel...</title><description>At @CVPR and want to learn about research careers at Google? Visit the #CVPR2024 Google booth today at 10 AM to meet Google Research Recruiter, Rachel Dean, to discuss opportunities and the hiring process.</description><link>https://x.com/GoogleAI/status/1803822504827363691</link><guid isPermaLink="false">https://twitter.com/GoogleAI/status/1803822504827363691</guid><pubDate>Thu, 20 Jun 2024 16:09:33 GMT</pubDate><author>Google AI</author><category>CVPR2024</category></item><item><title>With OpenMask3D, you can search 3D scenes using free-form text queries. Drop by the #CVPR2024 Google booth today at 3:45 PM for a demo with @fedassa a...</title><description>With OpenMask3D, you can search 3D scenes using free-form text queries. Drop by the #CVPR2024 Google booth today at 3:45 PM for a demo with @fedassa and @FrancisEngelman on using visual-language models to achieve open-vocabulary 3D instance segmentation.&lt;br&gt;&lt;video src=&quot;https://video.twimg.com/ext_tw_video/1803545605131886592/pu/vid/avc1/1280x720/s8EBWRf61P35xD2O.mp4?tag=12&quot; controls=&quot;controls&quot; poster=&quot;https://pbs.twimg.com/ext_tw_video_thumb/1803545605131886592/pu/img/Mk9xtQMRulBJM0lI.jpg&quot;&gt;&lt;/video&gt;</description><link>https://x.com/GoogleAI/status/1803545750334492706</link><guid isPermaLink="false">https://twitter.com/GoogleAI/status/1803545750334492706</guid><pubDate>Wed, 19 Jun 2024 21:49:49 GMT</pubDate><author>Google AI</author><category>CVPR2024</category></item><item><title>Today at 2:30 PM, drop by the #CVPR2024 Google booth, where Yang Zhao will demonstrate an efficient on-device text-to-image generation approach that u...</title><description>Today at 2:30 PM, drop by the #CVPR2024 Google booth, where Yang Zhao will demonstrate an efficient on-device text-to-image generation approach that uses Diffusion GANs.&lt;br&gt;&lt;video src=&quot;https://video.twimg.com/tweet_video/GQdrcEGasAA8BR_.mp4&quot; autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot; webkit-playsinline=&quot;&quot; playsinline=&quot;&quot; controls=&quot;controls&quot; poster=&quot;https://pbs.twimg.com/tweet_video_thumb/GQdrcEGasAA8BR_.jpg&quot;&gt;&lt;/video&gt;</description><link>https://x.com/GoogleAI/status/1803528361035309250</link><guid isPermaLink="false">https://twitter.com/GoogleAI/status/1803528361035309250</guid><pubDate>Wed, 19 Jun 2024 20:40:43 GMT</pubDate><author>Google AI</author><category>CVPR2024</category></item><item><title>TacticAI provides soccer experts &amp; coaches with tactical insights, particularly on corner kicks, through predictive and generative AI. Visit the #CVPR...</title><description>TacticAI provides soccer experts &amp;amp; coaches with tactical insights, particularly on corner kicks, through predictive and generative AI. Visit the #CVPR2024 Google booth today at 1:30 PM to hear @PetarV_93 &lt;br&gt; describe how TacticAI achieves impressive results.&lt;br&gt;&lt;img style=&quot;&quot; src=&quot;https://pbs.twimg.com/media/GQdbMffasAAx_rg?format=jpg&amp;amp;name=orig&quot; referrerpolicy=&quot;no-referrer&quot;&gt;</description><link>https://x.com/GoogleAI/status/1803510677300236574</link><guid isPermaLink="false">https://twitter.com/GoogleAI/status/1803510677300236574</guid><pubDate>Wed, 19 Jun 2024 19:30:27 GMT</pubDate><author>Google AI</author><category>CVPR2024</category></item><item><title>Today at 12:30 PM, stop by the #CVPR2024 Google booth for a Q&amp;A session on Medical AI Research. Don’t miss the opportunity to meet @AziziShekoofeh, w...</title><description>Today at 12:30 PM, stop by the #CVPR2024 Google booth for a Q&amp;amp;A session on Medical AI Research. Don’t miss the opportunity to meet @AziziShekoofeh, who has 4+ years of experience leading various health-related efforts, such as Med-PaLM and Med-Gemini.&lt;br&gt;&lt;img style=&quot;&quot; src=&quot;https://pbs.twimg.com/media/GQdLyMXaIAIh-7B?format=jpg&amp;amp;name=orig&quot; referrerpolicy=&quot;no-referrer&quot;&gt;</description><link>https://x.com/GoogleAI/status/1803493628528173204</link><guid isPermaLink="false">https://twitter.com/GoogleAI/status/1803493628528173204</guid><pubDate>Wed, 19 Jun 2024 18:22:42 GMT</pubDate><author>Google AI</author><category>CVPR2024</category></item><item><title>At @CVPR? We hope you’ll visit the Google booth to chat with researchers who are actively pursuing the latest innovations in computer vision! Read ab...</title><description>At @CVPR? We hope you’ll visit the Google booth to chat with researchers who are actively pursuing the latest innovations in computer vision! Read about our #CVPR2024 involvement and see the Google booth activity schedule → https://goo.gle/cvpr24&lt;br&gt;&lt;img style=&quot;&quot; src=&quot;https://pbs.twimg.com/media/GQdAwYzbgAAnq5s?format=jpg&amp;amp;name=orig&quot; referrerpolicy=&quot;no-referrer&quot;&gt;</description><link>https://x.com/GoogleAI/status/1803481462479659192</link><guid isPermaLink="false">https://twitter.com/GoogleAI/status/1803481462479659192</guid><pubDate>Wed, 19 Jun 2024 17:34:22 GMT</pubDate><author>Google AI</author><category>CVPR2024</category></item><item><title>“Ultimately, we’re trying to understand one of the most fascinating and complex objects we have in the known universe.” 🧠 Learn how our Connecto...</title><description>“Ultimately, we’re trying to understand one of the most fascinating and complex objects we have in the known universe.” 🧠&lt;br&gt;&lt;br&gt;Learn how our Connectomics team and @Harvard University researchers collaborate to reveal new information on how brains work. → https://www.youtube.com/watch?v=VSG3_JvnCkU&lt;br&gt;&lt;video src=&quot;https://video.twimg.com/amplify_video/1803236105992675330/vid/avc1/1280x720/EHIdK24VDoXiUDpp.mp4?tag=14&quot; controls=&quot;controls&quot; poster=&quot;https://pbs.twimg.com/amplify_video_thumb/1803236105992675330/img/aZGaSFDdvbeiMwjI.jpg&quot;&gt;&lt;/video&gt;</description><link>https://x.com/GoogleAI/status/1803236620738658349</link><guid isPermaLink="false">https://twitter.com/GoogleAI/status/1803236620738658349</guid><pubDate>Wed, 19 Jun 2024 01:21:27 GMT</pubDate><author>Google AI</author></item><item><title>In one of the earliest cases of a quantum processor being used to resolve an outstanding debate in a different area of physics, we show the first sign...</title><description>In one of the earliest cases of a quantum processor being used to resolve an outstanding debate in a different area of physics, we show the first signatures that a 1D Heisenberg spin chain at infinite temperature deviates from the KPZ universality class. https://goo.gle/3KVm8bz&lt;br&gt;&lt;video src=&quot;https://video.twimg.com/tweet_video/GQYfvrtWAAAHQ-G.mp4&quot; autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot; webkit-playsinline=&quot;&quot; playsinline=&quot;&quot; controls=&quot;controls&quot; poster=&quot;https://pbs.twimg.com/tweet_video_thumb/GQYfvrtWAAAHQ-G.jpg&quot;&gt;&lt;/video&gt;</description><link>https://x.com/GoogleAI/status/1803163697583706449</link><guid isPermaLink="false">https://twitter.com/GoogleAI/status/1803163697583706449</guid><pubDate>Tue, 18 Jun 2024 20:31:41 GMT</pubDate><author>Google AI</author></item><item><title>The Computer Vision and Pattern Recognition conference (@CVPR) kicked off today in Seattle, Washington. Google has a strong presence at #CVPR2024 with...</title><description>The Computer Vision and Pattern Recognition conference (@CVPR) kicked off today in Seattle, Washington. Google has a strong presence at #CVPR2024 with 95+ accepted papers and participation in 70+ workshops and tutorials. Read about our involvement at: https://goo.gle/cvpr24&lt;br&gt;&lt;img style=&quot;&quot; src=&quot;https://pbs.twimg.com/media/GQUoIS-aIAAfMOz?format=jpg&amp;amp;name=orig&quot; referrerpolicy=&quot;no-referrer&quot;&gt;</description><link>https://x.com/GoogleAI/status/1802891399890923691</link><guid isPermaLink="false">https://twitter.com/GoogleAI/status/1802891399890923691</guid><pubDate>Tue, 18 Jun 2024 02:29:40 GMT</pubDate><author>Google AI</author><category>CVPR2024</category></item><item><title>Congratulations to Google Research’s Paul Duetting, Vahab Mirrokni, Renato Paes Leme, Song Zuo, and co-author for winning the ACM Web Conference 2024...</title><description>Congratulations to Google Research’s Paul Duetting, Vahab Mirrokni, Renato Paes Leme, Song Zuo, and co-author for winning the ACM Web Conference 2024 Best Paper Award, for “Mechanism Design for Large Language Models” #TheWebConf24 →https://goo.gle/4bfgRqo</description><link>https://x.com/GoogleAI/status/1802790686724694124</link><guid isPermaLink="false">https://twitter.com/GoogleAI/status/1802790686724694124</guid><pubDate>Mon, 17 Jun 2024 19:49:28 GMT</pubDate><author>Google AI</author><category>TheWebConf24</category></item><item><title>In today&#39;s post, we re-evaluate the value of pre-translation for LLMs and illustrate that direct inference can be more efficient and effective for app...</title><description>In today&#39;s post, we re-evaluate the value of pre-translation for LLMs and illustrate that direct inference can be more efficient and effective for application in multilingual settings while alleviating the limitations of pre-translation. → https://goo.gle/3VJnD2Z&lt;br&gt;&lt;img style=&quot;&quot; src=&quot;https://pbs.twimg.com/media/GQS0QgBXcAATZ7N?format=jpg&amp;amp;name=orig&quot; referrerpolicy=&quot;no-referrer&quot;&gt;</description><link>https://x.com/GoogleAI/status/1802765351178088451</link><guid isPermaLink="false">https://twitter.com/GoogleAI/status/1802765351178088451</guid><pubDate>Mon, 17 Jun 2024 18:08:48 GMT</pubDate><author>Google AI</author></item><item><title>Human I/O is a unified approach that uses egocentric vision, multimodal sensing, and LLM reasoning to detect situational impairments and assess a user...</title><description>Human I/O is a unified approach that uses egocentric vision, multimodal sensing, and LLM reasoning to detect situational impairments and assess a user’s ability to interact with their hands, vision, hearing or speech in a given situation. Learn more at: https://goo.gle/3KJEP1T&lt;br&gt;&lt;img style=&quot;&quot; src=&quot;https://pbs.twimg.com/media/GQDwsDEaQAASTUk?format=jpg&amp;amp;name=orig&quot; referrerpolicy=&quot;no-referrer&quot;&gt;</description><link>https://x.com/GoogleAI/status/1801704543736238187</link><guid isPermaLink="false">https://twitter.com/GoogleAI/status/1801704543736238187</guid><pubDate>Fri, 14 Jun 2024 19:53:31 GMT</pubDate><author>Google AI</author></item><item><title>In this video, you&#39;ll hear from two members of the Colab team about three new tools you can use to supercharge your programming, whether you&#39;re a stud...</title><description>In this video, you&#39;ll hear from two members of the Colab team about three new tools you can use to supercharge your programming, whether you&#39;re a student or a seasoned developer. These AI tools are for everyone who works with code →https://www.youtube.com/watch?v=V7RXyqFUR98&lt;br&gt;&lt;video src=&quot;https://video.twimg.com/amplify_video/1801344817030696962/vid/avc1/1280x720/Dtc_HjponGzQmt7r.mp4?tag=14&quot; controls=&quot;controls&quot; poster=&quot;https://pbs.twimg.com/media/GP-w0a-aUAA98V4?format=jpg&amp;amp;name=orig&quot;&gt;&lt;/video&gt;</description><link>https://x.com/GoogleAI/status/1801353724822175849</link><guid isPermaLink="false">https://twitter.com/GoogleAI/status/1801353724822175849</guid><pubDate>Thu, 13 Jun 2024 20:39:30 GMT</pubDate><author>Google AI</author></item><item><title>Applications will open on June 27 for the Google Academic Research Awards, a new program that supports academic research in computing and technology t...</title><description>Applications will open on June 27 for the Google Academic Research Awards, a new program that supports academic research in computing and technology that makes a positive difference in the world. Learn more at →http://goo.gle/GARA&lt;br&gt;&lt;img style=&quot;&quot; src=&quot;https://pbs.twimg.com/media/GP-B9UQbEAQduE5?format=jpg&amp;amp;name=orig&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;br&gt;&lt;img style=&quot;&quot; src=&quot;https://pbs.twimg.com/media/GP-B9USbEAEVixI?format=jpg&amp;amp;name=orig&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;br&gt;&lt;img style=&quot;&quot; src=&quot;https://pbs.twimg.com/media/GP-B9UPbEAAu8eT?format=jpg&amp;amp;name=orig&quot; referrerpolicy=&quot;no-referrer&quot;&gt;</description><link>https://x.com/GoogleAI/status/1801301280062902338</link><guid isPermaLink="false">https://twitter.com/GoogleAI/status/1801301280062902338</guid><pubDate>Thu, 13 Jun 2024 17:11:06 GMT</pubDate><author>Google AI</author></item></channel></rss>